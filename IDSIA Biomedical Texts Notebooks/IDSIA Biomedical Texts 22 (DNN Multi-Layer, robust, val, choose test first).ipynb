{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FaYdm_iQWHv2STqrNON1iuPvROFE6IqJ","timestamp":1689780757648},{"file_id":"1uiEYfBVhnw4iG25VL7JGht90QkCQ6SC1","timestamp":1689747244256}],"gpuType":"T4","authorship_tag":"ABX9TyM4MTzPeV82/yb/RUN4scG9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POzA_GGacj6B","executionInfo":{"status":"ok","timestamp":1690629646657,"user_tz":-330,"elapsed":25586,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"c6c9eb9b-9147-4285-cb6a-bf210672c945"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle"],"metadata":{"id":"2v8HPluLcudI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/IDSIA Biomedical Texts/AllSource_Intensity_ThirdJuly.csv', low_memory=False)\n","df.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"UV6xvkLSc1ZZ","executionInfo":{"status":"ok","timestamp":1690629649240,"user_tz":-330,"elapsed":2151,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1d4334c0-780f-49db-b16d-42ef568df740"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                urls  \\\n","0  https://www.quora.com/What-are-panic-attacks-l...   \n","\n","                                                text source  label   WC  \\\n","0  i have been dealing with these for quite some ...  Quora      1  607   \n","\n","   Analytic  Clout  Authentic  Tone    WPS  ...  \\\n","0     55.22  35.35      48.82   1.0  26.39  ...   \n","\n","                                      all_emo_labels  \\\n","0  ['fear', 'nervousness', 'confusion', 'curiosit...   \n","\n","                                  all_emo_label_rank  anger_intensity  \\\n","0  {'fear': 1, 'nervousness': 2, 'confusion': 3, ...         0.415048   \n","\n","   anticipation_intensity  disgust_intensity  fear_intensity  joy_intensity  \\\n","0                0.553423           0.272333        0.568205         0.4095   \n","\n","   sadness_intensity  surprise_intensity  trust_intensity  \n","0           0.467625              0.4345         0.522773  \n","\n","[1 rows x 165 columns]"],"text/html":["\n","\n","  <div id=\"df-49ca90ee-9dd0-44a0-819b-23407c4832bb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>urls</th>\n","      <th>text</th>\n","      <th>source</th>\n","      <th>label</th>\n","      <th>WC</th>\n","      <th>Analytic</th>\n","      <th>Clout</th>\n","      <th>Authentic</th>\n","      <th>Tone</th>\n","      <th>WPS</th>\n","      <th>...</th>\n","      <th>all_emo_labels</th>\n","      <th>all_emo_label_rank</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>https://www.quora.com/What-are-panic-attacks-l...</td>\n","      <td>i have been dealing with these for quite some ...</td>\n","      <td>Quora</td>\n","      <td>1</td>\n","      <td>607</td>\n","      <td>55.22</td>\n","      <td>35.35</td>\n","      <td>48.82</td>\n","      <td>1.0</td>\n","      <td>26.39</td>\n","      <td>...</td>\n","      <td>['fear', 'nervousness', 'confusion', 'curiosit...</td>\n","      <td>{'fear': 1, 'nervousness': 2, 'confusion': 3, ...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.4095</td>\n","      <td>0.467625</td>\n","      <td>0.4345</td>\n","      <td>0.522773</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows × 165 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49ca90ee-9dd0-44a0-819b-23407c4832bb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-bdc6556a-84c7-4a3c-ad2a-2aeddfb62b2d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdc6556a-84c7-4a3c-ad2a-2aeddfb62b2d')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-bdc6556a-84c7-4a3c-ad2a-2aeddfb62b2d button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-49ca90ee-9dd0-44a0-819b-23407c4832bb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-49ca90ee-9dd0-44a0-819b-23407c4832bb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[],"metadata":{"id":"cClrfrz3c2-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting 2 panic features"],"metadata":{"id":"vNM-C1avc59u"}},{"cell_type":"code","source":["import regex as re"],"metadata":{"id":"FjKZR5P6c6-F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["panic_symptoms = [\"Palpitations\", \"Pounding heart\", \"Accelerated heart rate\", \"Sweating\", \"Trembling\", \"Shaking\", \"Shortness of breath\",\n","\"Smothering\", \"Feelings of choking\", \"Chest pain\", \"Discomfort\", \"Abdominal distress\", \"Nausea\", \"Dizziness\", \"Unsteadiness\", \"Lightheadedness\",\n","\"Faintness\", \"Chills\", \"Heat flashes\", \"Paresthesia\", \"Numbness\", \"Tingling sensations\", \"Derealization\", \"Depersonalization\", \"Fear of losing control\",\n","\"Fear of going crazy\", \"Fear of dying\", \"Mental images of dying\", \"Mental images of collapsing\", \"Agoraphobia\", \"Need to escape\"]\n","\n","\n","panic_symptoms_ext = [\"Palpitations\", \"Pounding heart\", \"Accelerated heart rate\", \"Sweating\", \"Trembling\", \"Shaking\", \"Shortness of breath\",\n","\"Smothering\", \"Feelings of choking\", \"Chest pain\", \"Discomfort\", \"Abdominal distress\", \"Nausea\", \"Dizziness\", \"Unsteadiness\", \"Lightheadedness\",\n","\"Faintness\", \"Chills\", \"Heat flashes\", \"Paresthesia\", \"Numbness\", \"Tingling sensations\", \"Derealization\", \"Depersonalization\", \"Fear of losing control\",\n","\"Fear of going crazy\", \"Fear of dying\", \"Mental images of dying\", \"Mental images of collapsing\", \"Agoraphobia\", \"Need to escape\"\n","\"Sweat\", \"Tremble\", \"Shake\", \"Shortage of breath\",\"Feeling of choking\",\"Dizzy\",\n","\"Faint\",\"Fainted\",\"Chill\",\"Heat flash\", \"Numb\", \"Tingling sensation\",\"Mental image of dying\", \"Mental image of collapsing\"]"],"metadata":{"id":"coacXg3Sc8u7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_symptoms_in_text(text, panic_list):\n","    if isinstance(text, str):\n","        count = 0\n","        for symptom in panic_list:\n","            match = re.search(r'\\b{}\\b'.format(symptom), text, re.IGNORECASE)\n","            if match:\n","                count += 1\n","        return count"],"metadata":{"id":"BbpjCIMLc-G1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['symptoms_count'] = df['text'].apply(lambda x: count_symptoms_in_text(x,panic_symptoms))\n","df['symptoms_ext_count'] = df['text'].apply(lambda x: count_symptoms_in_text(x,panic_symptoms_ext))"],"metadata":{"id":"tESUupBgc_6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RDVOyi81dBhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZV-Ir0kydEBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n"],"metadata":{"id":"6itEHx-jdEDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pickle_in = open(\"/content/drive/MyDrive/IDSIA Biomedical Texts/Sentence Embeddings/AllSource_alldistilrobertav1_via_UMAP_SHORTembeddings.pickle\", 'rb')\n","sentence_embeddings = pickle.load(pickle_in)\n","sentence_embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OK9PWgG7dEG3","executionInfo":{"status":"ok","timestamp":1690629662756,"user_tz":-330,"elapsed":842,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"03f872bb-4523-4577-8df7-ffeae439cdde"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[10.64194  ,  5.0430765,  5.6824026, ...,  4.9058275,  6.8707986,\n","         4.538727 ],\n","       [11.312859 ,  5.364349 ,  4.41365  , ...,  4.92234  ,  6.8475184,\n","         4.5590596],\n","       [10.531799 ,  4.894456 ,  5.387705 , ...,  4.8968716,  6.8360796,\n","         4.530069 ],\n","       ...,\n","       [10.346373 ,  4.4247556,  3.5815325, ...,  5.0401225,  6.552696 ,\n","         4.490976 ],\n","       [10.454275 ,  4.5640407,  3.6035635, ...,  5.0320673,  6.564847 ,\n","         4.4918733],\n","       [11.222271 ,  5.1468487,  4.0054016, ...,  5.079988 ,  6.6341186,\n","         4.5597043]], dtype=float32)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"2ZHd-LcbdnI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ApIkoC0pdnWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"682EAc95dEeW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentemb_column_names = [\"sentemb\" + str(i+1) for i in range(28)]"],"metadata":{"id":"pjEhoNSAdlvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentembdf = pd.DataFrame(sentence_embeddings, columns=sentemb_column_names)\n","sentembdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"a_8NGCwldpS7","executionInfo":{"status":"ok","timestamp":1690629662758,"user_tz":-330,"elapsed":13,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"b7b4f253-4001-4a3a-9348-da46ff5a3cc6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  sentemb19  sentemb20  sentemb21  \\\n","0     1.789982  1.404625   7.134058  ...   6.754729   4.308768   2.225483   \n","1     1.780003  1.160577   6.596563  ...   7.253571   4.070558   2.360492   \n","2     1.776312  1.471099   6.926485  ...   6.786397   4.259876   2.223957   \n","3     1.780587  1.134592   6.539715  ...   7.298054   4.057921   2.369854   \n","4     1.775400  1.475707   6.684998  ...   7.061465   4.159153   2.327722   \n","...        ...       ...        ...  ...        ...        ...        ...   \n","7400  1.769552  1.720969   6.056923  ...   7.375135   3.952120   2.305240   \n","7401  1.688843  1.656772   5.678233  ...   7.371179   3.950364   2.242772   \n","7402  1.686590  1.781551   5.741636  ...   7.339849   3.958114   2.226620   \n","7403  1.702558  1.761993   5.793803  ...   7.350019   3.961870   2.231675   \n","7404  1.753800  1.507380   6.102673  ...   7.279993   4.084011   2.253345   \n","\n","      sentemb22  sentemb23  sentemb24  sentemb25  sentemb26  sentemb27  \\\n","0      3.196717   5.969540   5.937778   5.176552   4.905828   6.870799   \n","1      3.181391   6.144005   5.662973   5.073269   4.922340   6.847518   \n","2      3.178261   5.996410   5.876356   5.186982   4.896872   6.836080   \n","3      3.181388   6.149067   5.650796   5.068528   4.935538   6.840829   \n","4      3.183620   6.181925   5.809379   5.113574   4.873700   6.771983   \n","...         ...        ...        ...        ...        ...        ...   \n","7400   3.210462   6.340523   5.595186   5.102685   4.922517   6.645929   \n","7401   3.236705   6.118145   5.502766   5.187657   5.083427   6.556334   \n","7402   3.229278   6.177763   5.498463   5.200507   5.040123   6.552696   \n","7403   3.226660   6.186529   5.515781   5.197260   5.032067   6.564847   \n","7404   3.232021   6.046115   5.681019   5.217414   5.079988   6.634119   \n","\n","      sentemb28  \n","0      4.538727  \n","1      4.559060  \n","2      4.530069  \n","3      4.564339  \n","4      4.528791  \n","...         ...  \n","7400   4.465776  \n","7401   4.512558  \n","7402   4.490976  \n","7403   4.491873  \n","7404   4.559704  \n","\n","[7405 rows x 28 columns]"],"text/html":["\n","\n","  <div id=\"df-252e16f4-487a-4c86-b470-9c690ded3ae2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>sentemb19</th>\n","      <th>sentemb20</th>\n","      <th>sentemb21</th>\n","      <th>sentemb22</th>\n","      <th>sentemb23</th>\n","      <th>sentemb24</th>\n","      <th>sentemb25</th>\n","      <th>sentemb26</th>\n","      <th>sentemb27</th>\n","      <th>sentemb28</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>6.754729</td>\n","      <td>4.308768</td>\n","      <td>2.225483</td>\n","      <td>3.196717</td>\n","      <td>5.969540</td>\n","      <td>5.937778</td>\n","      <td>5.176552</td>\n","      <td>4.905828</td>\n","      <td>6.870799</td>\n","      <td>4.538727</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>7.253571</td>\n","      <td>4.070558</td>\n","      <td>2.360492</td>\n","      <td>3.181391</td>\n","      <td>6.144005</td>\n","      <td>5.662973</td>\n","      <td>5.073269</td>\n","      <td>4.922340</td>\n","      <td>6.847518</td>\n","      <td>4.559060</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>6.786397</td>\n","      <td>4.259876</td>\n","      <td>2.223957</td>\n","      <td>3.178261</td>\n","      <td>5.996410</td>\n","      <td>5.876356</td>\n","      <td>5.186982</td>\n","      <td>4.896872</td>\n","      <td>6.836080</td>\n","      <td>4.530069</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>7.298054</td>\n","      <td>4.057921</td>\n","      <td>2.369854</td>\n","      <td>3.181388</td>\n","      <td>6.149067</td>\n","      <td>5.650796</td>\n","      <td>5.068528</td>\n","      <td>4.935538</td>\n","      <td>6.840829</td>\n","      <td>4.564339</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>7.061465</td>\n","      <td>4.159153</td>\n","      <td>2.327722</td>\n","      <td>3.183620</td>\n","      <td>6.181925</td>\n","      <td>5.809379</td>\n","      <td>5.113574</td>\n","      <td>4.873700</td>\n","      <td>6.771983</td>\n","      <td>4.528791</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>7.375135</td>\n","      <td>3.952120</td>\n","      <td>2.305240</td>\n","      <td>3.210462</td>\n","      <td>6.340523</td>\n","      <td>5.595186</td>\n","      <td>5.102685</td>\n","      <td>4.922517</td>\n","      <td>6.645929</td>\n","      <td>4.465776</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>7.371179</td>\n","      <td>3.950364</td>\n","      <td>2.242772</td>\n","      <td>3.236705</td>\n","      <td>6.118145</td>\n","      <td>5.502766</td>\n","      <td>5.187657</td>\n","      <td>5.083427</td>\n","      <td>6.556334</td>\n","      <td>4.512558</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>7.339849</td>\n","      <td>3.958114</td>\n","      <td>2.226620</td>\n","      <td>3.229278</td>\n","      <td>6.177763</td>\n","      <td>5.498463</td>\n","      <td>5.200507</td>\n","      <td>5.040123</td>\n","      <td>6.552696</td>\n","      <td>4.490976</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>7.350019</td>\n","      <td>3.961870</td>\n","      <td>2.231675</td>\n","      <td>3.226660</td>\n","      <td>6.186529</td>\n","      <td>5.515781</td>\n","      <td>5.197260</td>\n","      <td>5.032067</td>\n","      <td>6.564847</td>\n","      <td>4.491873</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>7.279993</td>\n","      <td>4.084011</td>\n","      <td>2.253345</td>\n","      <td>3.232021</td>\n","      <td>6.046115</td>\n","      <td>5.681019</td>\n","      <td>5.217414</td>\n","      <td>5.079988</td>\n","      <td>6.634119</td>\n","      <td>4.559704</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-252e16f4-487a-4c86-b470-9c690ded3ae2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-4c74b917-c4ba-4dc2-8b6b-62dc8440bc1d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c74b917-c4ba-4dc2-8b6b-62dc8440bc1d')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-4c74b917-c4ba-4dc2-8b6b-62dc8440bc1d button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-252e16f4-487a-4c86-b470-9c690ded3ae2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-252e16f4-487a-4c86-b470-9c690ded3ae2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"sKUBPcSpdpsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stdliwcdf = df.loc[:, 'WC':'Emoji']\n","stdliwcdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"S39j3QfVdy2J","executionInfo":{"status":"ok","timestamp":1690629663275,"user_tz":-330,"elapsed":528,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"20b19c56-f769-4e41-ad9e-968a7b54ef6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       WC  Analytic  Clout  Authentic   Tone    WPS  BigWords    Dic  \\\n","0     607     55.22  35.35      48.82   1.00  26.39     25.86  93.74   \n","1     401     54.08   1.31      99.00   8.57  13.83     14.21  90.02   \n","2     446     25.83  93.36      75.79   1.00  12.74     15.47  95.74   \n","3     525     30.63   2.97      96.06   6.86  15.44     12.38  90.86   \n","4     323     21.57   1.00      99.00   1.12  13.46     15.48  93.50   \n","...   ...       ...    ...        ...    ...    ...       ...    ...   \n","7400  237      5.24  20.52      53.23  15.51  16.93     18.57  93.25   \n","7401   20     39.70   1.00      28.56  99.00  20.00     25.00  95.00   \n","7402   19     36.67   3.34      70.28  99.00  19.00     21.05  94.74   \n","7403   38     36.67  90.88      33.61  92.27  19.00     21.05  97.37   \n","7404  186     26.78  52.89      86.48   2.39  23.25     15.59  98.39   \n","\n","      Linguistic  function  ...  nonflu  filler  AllPunc  Period  Comma  \\\n","0          68.86     57.50  ...     0.0     0.0    19.93    4.61   9.88   \n","1          74.81     61.60  ...     0.0     0.0    13.47    6.98   4.74   \n","2          75.34     59.64  ...     0.0     0.0    16.59    8.07   2.69   \n","3          73.33     56.57  ...     1.9     0.0    21.52    5.33   4.95   \n","4          75.85     60.99  ...     0.0     0.0    18.27    7.12   4.33   \n","...          ...       ...  ...     ...     ...      ...     ...    ...   \n","7400       81.86     62.87  ...     0.0     0.0    10.13    5.49   1.69   \n","7401       65.00     40.00  ...     0.0     0.0     0.00    0.00   0.00   \n","7402       73.68     47.37  ...     0.0     0.0     0.00    0.00   0.00   \n","7403       63.16     52.63  ...     0.0     0.0     2.63    2.63   0.00   \n","7404       79.03     61.83  ...     0.0     0.0    12.90    3.76   5.38   \n","\n","      QMark  Exclam  Apostro  OtherP  Emoji  \n","0      0.16    0.16     2.80    2.31   0.33  \n","1      0.25    0.00     1.50    0.00   0.00  \n","2      0.22    0.00     4.71    0.90   0.00  \n","3      1.14    0.00     6.67    3.43   0.00  \n","4      0.93    0.00     5.26    0.62   0.00  \n","...     ...     ...      ...     ...    ...  \n","7400   0.00    0.00     2.53    0.42   0.00  \n","7401   0.00    0.00     0.00    0.00   0.00  \n","7402   0.00    0.00     0.00    0.00   0.00  \n","7403   0.00    0.00     0.00    0.00   2.63  \n","7404   0.00    0.54     1.08    2.15   0.00  \n","\n","[7405 rows x 118 columns]"],"text/html":["\n","\n","  <div id=\"df-b1a97395-884b-4286-be3b-166389f03bd2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WC</th>\n","      <th>Analytic</th>\n","      <th>Clout</th>\n","      <th>Authentic</th>\n","      <th>Tone</th>\n","      <th>WPS</th>\n","      <th>BigWords</th>\n","      <th>Dic</th>\n","      <th>Linguistic</th>\n","      <th>function</th>\n","      <th>...</th>\n","      <th>nonflu</th>\n","      <th>filler</th>\n","      <th>AllPunc</th>\n","      <th>Period</th>\n","      <th>Comma</th>\n","      <th>QMark</th>\n","      <th>Exclam</th>\n","      <th>Apostro</th>\n","      <th>OtherP</th>\n","      <th>Emoji</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>607</td>\n","      <td>55.22</td>\n","      <td>35.35</td>\n","      <td>48.82</td>\n","      <td>1.00</td>\n","      <td>26.39</td>\n","      <td>25.86</td>\n","      <td>93.74</td>\n","      <td>68.86</td>\n","      <td>57.50</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.93</td>\n","      <td>4.61</td>\n","      <td>9.88</td>\n","      <td>0.16</td>\n","      <td>0.16</td>\n","      <td>2.80</td>\n","      <td>2.31</td>\n","      <td>0.33</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>401</td>\n","      <td>54.08</td>\n","      <td>1.31</td>\n","      <td>99.00</td>\n","      <td>8.57</td>\n","      <td>13.83</td>\n","      <td>14.21</td>\n","      <td>90.02</td>\n","      <td>74.81</td>\n","      <td>61.60</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>13.47</td>\n","      <td>6.98</td>\n","      <td>4.74</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>446</td>\n","      <td>25.83</td>\n","      <td>93.36</td>\n","      <td>75.79</td>\n","      <td>1.00</td>\n","      <td>12.74</td>\n","      <td>15.47</td>\n","      <td>95.74</td>\n","      <td>75.34</td>\n","      <td>59.64</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>16.59</td>\n","      <td>8.07</td>\n","      <td>2.69</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>4.71</td>\n","      <td>0.90</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>525</td>\n","      <td>30.63</td>\n","      <td>2.97</td>\n","      <td>96.06</td>\n","      <td>6.86</td>\n","      <td>15.44</td>\n","      <td>12.38</td>\n","      <td>90.86</td>\n","      <td>73.33</td>\n","      <td>56.57</td>\n","      <td>...</td>\n","      <td>1.9</td>\n","      <td>0.0</td>\n","      <td>21.52</td>\n","      <td>5.33</td>\n","      <td>4.95</td>\n","      <td>1.14</td>\n","      <td>0.00</td>\n","      <td>6.67</td>\n","      <td>3.43</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>323</td>\n","      <td>21.57</td>\n","      <td>1.00</td>\n","      <td>99.00</td>\n","      <td>1.12</td>\n","      <td>13.46</td>\n","      <td>15.48</td>\n","      <td>93.50</td>\n","      <td>75.85</td>\n","      <td>60.99</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>18.27</td>\n","      <td>7.12</td>\n","      <td>4.33</td>\n","      <td>0.93</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.62</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>237</td>\n","      <td>5.24</td>\n","      <td>20.52</td>\n","      <td>53.23</td>\n","      <td>15.51</td>\n","      <td>16.93</td>\n","      <td>18.57</td>\n","      <td>93.25</td>\n","      <td>81.86</td>\n","      <td>62.87</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.13</td>\n","      <td>5.49</td>\n","      <td>1.69</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.53</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>20</td>\n","      <td>39.70</td>\n","      <td>1.00</td>\n","      <td>28.56</td>\n","      <td>99.00</td>\n","      <td>20.00</td>\n","      <td>25.00</td>\n","      <td>95.00</td>\n","      <td>65.00</td>\n","      <td>40.00</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>19</td>\n","      <td>36.67</td>\n","      <td>3.34</td>\n","      <td>70.28</td>\n","      <td>99.00</td>\n","      <td>19.00</td>\n","      <td>21.05</td>\n","      <td>94.74</td>\n","      <td>73.68</td>\n","      <td>47.37</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>38</td>\n","      <td>36.67</td>\n","      <td>90.88</td>\n","      <td>33.61</td>\n","      <td>92.27</td>\n","      <td>19.00</td>\n","      <td>21.05</td>\n","      <td>97.37</td>\n","      <td>63.16</td>\n","      <td>52.63</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>186</td>\n","      <td>26.78</td>\n","      <td>52.89</td>\n","      <td>86.48</td>\n","      <td>2.39</td>\n","      <td>23.25</td>\n","      <td>15.59</td>\n","      <td>98.39</td>\n","      <td>79.03</td>\n","      <td>61.83</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12.90</td>\n","      <td>3.76</td>\n","      <td>5.38</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>1.08</td>\n","      <td>2.15</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 118 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1a97395-884b-4286-be3b-166389f03bd2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-d56cce5a-36eb-4df9-b5a5-d0cc97938364\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d56cce5a-36eb-4df9-b5a5-d0cc97938364')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-d56cce5a-36eb-4df9-b5a5-d0cc97938364 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b1a97395-884b-4286-be3b-166389f03bd2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b1a97395-884b-4286-be3b-166389f03bd2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"PIONnicXdzTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emodf = df.loc[:, 'admiration':'neutral']\n","emodf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"XhRKa7preIUd","executionInfo":{"status":"ok","timestamp":1690629663277,"user_tz":-330,"elapsed":21,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1d69a164-1926-47a4-8a99-22f66f7b7fc7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      admiration  amusement     anger  annoyance  approval    caring  \\\n","0       0.000016   0.000150  0.000705   0.001447  0.000643  0.009114   \n","1       0.000054   0.167888  0.011522   0.201494  0.000443  0.001504   \n","2       0.000101   0.000596  0.000428   0.001275  0.006295  0.013178   \n","3       0.000054   0.000639  0.043696   0.003672  0.000041  0.007463   \n","4       0.000004   0.000040  0.000176   0.000860  0.000181  0.000255   \n","...          ...        ...       ...        ...       ...       ...   \n","7400    0.000907   0.000017  0.000027   0.000027  0.000360  0.001514   \n","7401    0.000451   0.000075  0.000005   0.000022  0.000736  0.000157   \n","7402    0.000228   0.000024  0.000010   0.000034  0.000405  0.000364   \n","7403    0.000064   0.000080  0.000109   0.000155  0.012040  0.932658   \n","7404    0.003813   0.000036  0.000079   0.000095  0.039129  0.920536   \n","\n","      confusion  curiosity.1    desire  disappointment  ...      love  \\\n","0      0.048850     0.010012  0.000057        0.000440  ...  0.000210   \n","1      0.002532     0.000539  0.000486        0.053486  ...  0.003212   \n","2      0.412494     0.034596  0.000124        0.002020  ...  0.001481   \n","3      0.000333     0.000094  0.000294        0.002422  ...  0.005427   \n","4      0.001674     0.000182  0.000016        0.001535  ...  0.000145   \n","...         ...          ...       ...             ...  ...       ...   \n","7400   0.000162     0.000076  0.000035        0.000038  ...  0.000018   \n","7401   0.000070     0.000227  0.000052        0.000015  ...  0.000010   \n","7402   0.000123     0.000703  0.000040        0.000015  ...  0.000008   \n","7403   0.000593     0.001206  0.000311        0.000126  ...  0.000124   \n","7404   0.000142     0.000082  0.000070        0.000138  ...  0.000231   \n","\n","      nervousness  optimism     pride  realization    relief   remorse  \\\n","0        0.068864  0.000185  0.000041     0.000323  0.001267  0.000330   \n","1        0.013415  0.000246  0.000617     0.022932  0.000231  0.003261   \n","2        0.035853  0.000615  0.000224     0.336612  0.002241  0.001373   \n","3        0.010949  0.000076  0.000740     0.000619  0.000275  0.001561   \n","4        0.079422  0.000020  0.000017     0.000338  0.000211  0.000071   \n","...           ...       ...       ...          ...       ...       ...   \n","7400     0.000038  0.000060  0.000021     0.000021  0.000448  0.000136   \n","7401     0.000004  0.000037  0.000003     0.000018  0.000051  0.000023   \n","7402     0.000007  0.000077  0.000003     0.000009  0.000134  0.000023   \n","7403     0.016793  0.015985  0.000026     0.000285  0.005689  0.000256   \n","7404     0.000211  0.007032  0.000072     0.001102  0.007063  0.000425   \n","\n","       sadness  surprise   neutral  \n","0     0.000575  0.000205  0.000843  \n","1     0.416074  0.001216  0.003240  \n","2     0.007805  0.013074  0.041802  \n","3     0.057985  0.000332  0.000073  \n","4     0.002956  0.000103  0.000174  \n","...        ...       ...       ...  \n","7400  0.000022  0.000016  0.000068  \n","7401  0.000008  0.000011  0.001105  \n","7402  0.000008  0.000013  0.000393  \n","7403  0.000404  0.000174  0.003708  \n","7404  0.000141  0.000089  0.015839  \n","\n","[7405 rows x 28 columns]"],"text/html":["\n","\n","  <div id=\"df-26ec1f1f-ab4f-4a27-b211-6d4f01409e92\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity.1</th>\n","      <th>desire</th>\n","      <th>disappointment</th>\n","      <th>...</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000016</td>\n","      <td>0.000150</td>\n","      <td>0.000705</td>\n","      <td>0.001447</td>\n","      <td>0.000643</td>\n","      <td>0.009114</td>\n","      <td>0.048850</td>\n","      <td>0.010012</td>\n","      <td>0.000057</td>\n","      <td>0.000440</td>\n","      <td>...</td>\n","      <td>0.000210</td>\n","      <td>0.068864</td>\n","      <td>0.000185</td>\n","      <td>0.000041</td>\n","      <td>0.000323</td>\n","      <td>0.001267</td>\n","      <td>0.000330</td>\n","      <td>0.000575</td>\n","      <td>0.000205</td>\n","      <td>0.000843</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000054</td>\n","      <td>0.167888</td>\n","      <td>0.011522</td>\n","      <td>0.201494</td>\n","      <td>0.000443</td>\n","      <td>0.001504</td>\n","      <td>0.002532</td>\n","      <td>0.000539</td>\n","      <td>0.000486</td>\n","      <td>0.053486</td>\n","      <td>...</td>\n","      <td>0.003212</td>\n","      <td>0.013415</td>\n","      <td>0.000246</td>\n","      <td>0.000617</td>\n","      <td>0.022932</td>\n","      <td>0.000231</td>\n","      <td>0.003261</td>\n","      <td>0.416074</td>\n","      <td>0.001216</td>\n","      <td>0.003240</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000101</td>\n","      <td>0.000596</td>\n","      <td>0.000428</td>\n","      <td>0.001275</td>\n","      <td>0.006295</td>\n","      <td>0.013178</td>\n","      <td>0.412494</td>\n","      <td>0.034596</td>\n","      <td>0.000124</td>\n","      <td>0.002020</td>\n","      <td>...</td>\n","      <td>0.001481</td>\n","      <td>0.035853</td>\n","      <td>0.000615</td>\n","      <td>0.000224</td>\n","      <td>0.336612</td>\n","      <td>0.002241</td>\n","      <td>0.001373</td>\n","      <td>0.007805</td>\n","      <td>0.013074</td>\n","      <td>0.041802</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000054</td>\n","      <td>0.000639</td>\n","      <td>0.043696</td>\n","      <td>0.003672</td>\n","      <td>0.000041</td>\n","      <td>0.007463</td>\n","      <td>0.000333</td>\n","      <td>0.000094</td>\n","      <td>0.000294</td>\n","      <td>0.002422</td>\n","      <td>...</td>\n","      <td>0.005427</td>\n","      <td>0.010949</td>\n","      <td>0.000076</td>\n","      <td>0.000740</td>\n","      <td>0.000619</td>\n","      <td>0.000275</td>\n","      <td>0.001561</td>\n","      <td>0.057985</td>\n","      <td>0.000332</td>\n","      <td>0.000073</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000004</td>\n","      <td>0.000040</td>\n","      <td>0.000176</td>\n","      <td>0.000860</td>\n","      <td>0.000181</td>\n","      <td>0.000255</td>\n","      <td>0.001674</td>\n","      <td>0.000182</td>\n","      <td>0.000016</td>\n","      <td>0.001535</td>\n","      <td>...</td>\n","      <td>0.000145</td>\n","      <td>0.079422</td>\n","      <td>0.000020</td>\n","      <td>0.000017</td>\n","      <td>0.000338</td>\n","      <td>0.000211</td>\n","      <td>0.000071</td>\n","      <td>0.002956</td>\n","      <td>0.000103</td>\n","      <td>0.000174</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>0.000907</td>\n","      <td>0.000017</td>\n","      <td>0.000027</td>\n","      <td>0.000027</td>\n","      <td>0.000360</td>\n","      <td>0.001514</td>\n","      <td>0.000162</td>\n","      <td>0.000076</td>\n","      <td>0.000035</td>\n","      <td>0.000038</td>\n","      <td>...</td>\n","      <td>0.000018</td>\n","      <td>0.000038</td>\n","      <td>0.000060</td>\n","      <td>0.000021</td>\n","      <td>0.000021</td>\n","      <td>0.000448</td>\n","      <td>0.000136</td>\n","      <td>0.000022</td>\n","      <td>0.000016</td>\n","      <td>0.000068</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>0.000451</td>\n","      <td>0.000075</td>\n","      <td>0.000005</td>\n","      <td>0.000022</td>\n","      <td>0.000736</td>\n","      <td>0.000157</td>\n","      <td>0.000070</td>\n","      <td>0.000227</td>\n","      <td>0.000052</td>\n","      <td>0.000015</td>\n","      <td>...</td>\n","      <td>0.000010</td>\n","      <td>0.000004</td>\n","      <td>0.000037</td>\n","      <td>0.000003</td>\n","      <td>0.000018</td>\n","      <td>0.000051</td>\n","      <td>0.000023</td>\n","      <td>0.000008</td>\n","      <td>0.000011</td>\n","      <td>0.001105</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>0.000228</td>\n","      <td>0.000024</td>\n","      <td>0.000010</td>\n","      <td>0.000034</td>\n","      <td>0.000405</td>\n","      <td>0.000364</td>\n","      <td>0.000123</td>\n","      <td>0.000703</td>\n","      <td>0.000040</td>\n","      <td>0.000015</td>\n","      <td>...</td>\n","      <td>0.000008</td>\n","      <td>0.000007</td>\n","      <td>0.000077</td>\n","      <td>0.000003</td>\n","      <td>0.000009</td>\n","      <td>0.000134</td>\n","      <td>0.000023</td>\n","      <td>0.000008</td>\n","      <td>0.000013</td>\n","      <td>0.000393</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>0.000064</td>\n","      <td>0.000080</td>\n","      <td>0.000109</td>\n","      <td>0.000155</td>\n","      <td>0.012040</td>\n","      <td>0.932658</td>\n","      <td>0.000593</td>\n","      <td>0.001206</td>\n","      <td>0.000311</td>\n","      <td>0.000126</td>\n","      <td>...</td>\n","      <td>0.000124</td>\n","      <td>0.016793</td>\n","      <td>0.015985</td>\n","      <td>0.000026</td>\n","      <td>0.000285</td>\n","      <td>0.005689</td>\n","      <td>0.000256</td>\n","      <td>0.000404</td>\n","      <td>0.000174</td>\n","      <td>0.003708</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>0.003813</td>\n","      <td>0.000036</td>\n","      <td>0.000079</td>\n","      <td>0.000095</td>\n","      <td>0.039129</td>\n","      <td>0.920536</td>\n","      <td>0.000142</td>\n","      <td>0.000082</td>\n","      <td>0.000070</td>\n","      <td>0.000138</td>\n","      <td>...</td>\n","      <td>0.000231</td>\n","      <td>0.000211</td>\n","      <td>0.007032</td>\n","      <td>0.000072</td>\n","      <td>0.001102</td>\n","      <td>0.007063</td>\n","      <td>0.000425</td>\n","      <td>0.000141</td>\n","      <td>0.000089</td>\n","      <td>0.015839</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26ec1f1f-ab4f-4a27-b211-6d4f01409e92')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-bdab6a60-854c-4064-9267-90678947fe08\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdab6a60-854c-4064-9267-90678947fe08')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-bdab6a60-854c-4064-9267-90678947fe08 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-26ec1f1f-ab4f-4a27-b211-6d4f01409e92 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-26ec1f1f-ab4f-4a27-b211-6d4f01409e92');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"0Gu-9yZkeIzZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["intensitydf = df.loc[:, 'anger_intensity':'trust_intensity']\n","intensitydf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Sb3tFdoweL9Q","executionInfo":{"status":"ok","timestamp":1690629663279,"user_tz":-330,"elapsed":21,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7ace14b0-9d6d-41f3-f681-851dd699e263"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      anger_intensity  anticipation_intensity  disgust_intensity  \\\n","0            0.415048                0.553423           0.272333   \n","1            0.530400                0.519750           0.541250   \n","2            0.428600                0.533500           0.228167   \n","3            0.567200                0.533462           0.114667   \n","4            0.487000                0.508000           0.482250   \n","...               ...                     ...                ...   \n","7400         0.396000                0.609000           0.484000   \n","7401         0.000000                0.000000           0.000000   \n","7402         0.000000                0.000000           0.000000   \n","7403         0.344000                0.528667           0.000000   \n","7404         0.376750                0.502500           0.422000   \n","\n","      fear_intensity  joy_intensity  sadness_intensity  surprise_intensity  \\\n","0           0.568205       0.409500           0.467625            0.434500   \n","1           0.432167       0.453429           0.315600            0.247333   \n","2           0.526192       0.413444           0.468533            0.348500   \n","3           0.501952       0.505000           0.522095            0.320500   \n","4           0.624833       0.489167           0.505333            0.000000   \n","...              ...            ...                ...                 ...   \n","7400        0.527500       0.434000           0.591000            0.793000   \n","7401        0.156000       0.000000           0.000000            0.000000   \n","7402        0.156000       0.000000           0.000000            0.000000   \n","7403        0.414000       0.515500           0.500000            0.363500   \n","7404        0.515333       0.431900           0.418800            0.316500   \n","\n","      trust_intensity  \n","0            0.522773  \n","1            0.508875  \n","2            0.504500  \n","3            0.593615  \n","4            0.527167  \n","...               ...  \n","7400         0.540800  \n","7401         0.000000  \n","7402         0.641000  \n","7403         0.613000  \n","7404         0.519286  \n","\n","[7405 rows x 8 columns]"],"text/html":["\n","\n","  <div id=\"df-e9ac4d24-1893-403a-b921-11bdc9da1114\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9ac4d24-1893-403a-b921-11bdc9da1114')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-0aa00052-00c4-4c7c-9036-917d70cf7c10\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0aa00052-00c4-4c7c-9036-917d70cf7c10')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-0aa00052-00c4-4c7c-9036-917d70cf7c10 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e9ac4d24-1893-403a-b921-11bdc9da1114 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e9ac4d24-1893-403a-b921-11bdc9da1114');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":[],"metadata":{"id":"_IHHilFiePUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df = pd.concat([sentembdf, stdliwcdf, emodf, intensitydf, df['symptoms_ext_count']], axis=1)\n","final_df['label'] = df['label']\n","final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"v0e0WIhZeSyu","executionInfo":{"status":"ok","timestamp":1690629663282,"user_tz":-330,"elapsed":22,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8c717b7c-9347-4733-c0cb-b1495f11f52e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-60fe3aa4-dbc0-4d06-b0cb-4bdfcca8cf06\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60fe3aa4-dbc0-4d06-b0cb-4bdfcca8cf06')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-b7ad71b7-8806-407a-b1d2-a71fb7a9add6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7ad71b7-8806-407a-b1d2-a71fb7a9add6')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-b7ad71b7-8806-407a-b1d2-a71fb7a9add6 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-60fe3aa4-dbc0-4d06-b0cb-4bdfcca8cf06 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-60fe3aa4-dbc0-4d06-b0cb-4bdfcca8cf06');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"id":"CXNGvOXm5Oef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kIovAAj29Ixb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Random Undersampling\n","\n"],"metadata":{"id":"MluOTq7y9I5X"}},{"cell_type":"code","source":["final_df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-W0KEws5Og9","executionInfo":{"status":"ok","timestamp":1689798727902,"user_tz":-330,"elapsed":14,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"e253d230-542d-430b-951b-31c98ebb4f61"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    4640\n","0    2765\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["from imblearn.under_sampling import RandomUnderSampler"],"metadata":{"id":"lfeARuRv5Ojd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtemp = final_df.drop('label', axis=1)\n","ytemp = final_df['label']"],"metadata":{"id":"OhtO_Dmx5vbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["undersampler = RandomUnderSampler(sampling_strategy=1, random_state=42)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html"],"metadata":{"id":"hO_ykuZS5veJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)"],"metadata":{"id":"Vkgbu8ka5vgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_undersampled"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"NuXJBnCV8pdN","executionInfo":{"status":"ok","timestamp":1689798729005,"user_tz":-330,"elapsed":56,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"a1810d67-6900-4367-cd0d-5f1599e60762"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     12.319857  6.399014  4.530356  2.865929  4.445246  5.065509  6.307887   \n","1     11.981301  5.855036  4.455084  2.877874  4.566389  4.754900  6.224051   \n","2     12.306263  6.337527  4.459904  2.941618  4.498397  5.115978  6.315321   \n","3     11.924356  5.831368  4.232168  3.345199  4.772059  5.092751  6.328898   \n","4     11.751195  5.696429  4.387308  3.400336  4.734811  4.937661  6.364964   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","5525  10.427677  4.712157  4.923399  3.956240  4.162726  5.006993  7.153138   \n","5526   8.398520  2.988861  2.252746  3.048421  5.798511  5.158401  4.810905   \n","5527  10.368590  4.730114  5.105931  3.170660  4.135172  4.259716  6.612472   \n","5528   9.916215  3.867956  3.379334  3.135911  5.451319  4.645429  5.375762   \n","5529   9.306035  3.675035  3.137268  2.864412  5.409729  4.707642  5.179926   \n","\n","      sentemb8  sentemb9  sentemb10  ...   neutral  anger_intensity  \\\n","0     1.829383  1.249323   6.747220  ...  0.000320         0.278333   \n","1     1.759501  1.451402   6.514011  ...  0.003006         0.459300   \n","2     1.815021  1.249940   6.686715  ...  0.000928         0.344000   \n","3     1.776461  1.269352   6.457453  ...  0.001544         0.294750   \n","4     1.778782  1.192163   6.539055  ...  0.010126         0.453000   \n","...        ...       ...        ...  ...       ...              ...   \n","5525  1.760024  1.536282   6.597105  ...  0.005895         0.000000   \n","5526  1.738988  1.730299   4.858199  ...  0.006126         0.147000   \n","5527  1.772626  1.521696   6.705473  ...  0.001772         0.000000   \n","5528  1.727962  1.417521   5.503230  ...  0.000054         0.620500   \n","5529  1.715438  1.629607   5.339017  ...  0.013418         0.369000   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.583333             0.1410        0.625333   \n","1                   0.560700             0.4735        0.531849   \n","2                   0.609000             0.0000        0.594000   \n","3                   0.590600             0.1410        0.509800   \n","4                   0.516000             0.3910        0.375000   \n","...                      ...                ...             ...   \n","5525                0.644500             0.1800        0.750000   \n","5526                0.536500             0.1410        0.328000   \n","5527                0.000000             0.0000        0.000000   \n","5528                0.516000             0.2890        0.906000   \n","5529                0.515750             0.1410        0.380000   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.671500           0.410250              0.5430         0.518000   \n","1          0.490187           0.535074              0.4725         0.549682   \n","2          0.000000           0.500000              0.0000         0.453000   \n","3          0.523333           0.418000              0.3515         0.492500   \n","4          0.441000           0.422000              0.3200         0.468500   \n","...             ...                ...                 ...              ...   \n","5525       0.419750           0.281000              0.3435         0.562500   \n","5526       0.599750           0.172000              0.2890         0.567500   \n","5527       0.000000           0.000000              0.0000         0.000000   \n","5528       0.000000           0.433429              0.0000         0.000000   \n","5529       0.523750           0.424800              0.3590         0.518000   \n","\n","      symptoms_ext_count  \n","0                      0  \n","1                      3  \n","2                      0  \n","3                      1  \n","4                      0  \n","...                  ...  \n","5525                   0  \n","5526                   0  \n","5527                   0  \n","5528                   1  \n","5529                   0  \n","\n","[5530 rows x 183 columns]"],"text/html":["\n","\n","  <div id=\"df-a208f8a0-054f-4ea5-9e40-5a727009808e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>neutral</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12.319857</td>\n","      <td>6.399014</td>\n","      <td>4.530356</td>\n","      <td>2.865929</td>\n","      <td>4.445246</td>\n","      <td>5.065509</td>\n","      <td>6.307887</td>\n","      <td>1.829383</td>\n","      <td>1.249323</td>\n","      <td>6.747220</td>\n","      <td>...</td>\n","      <td>0.000320</td>\n","      <td>0.278333</td>\n","      <td>0.583333</td>\n","      <td>0.1410</td>\n","      <td>0.625333</td>\n","      <td>0.671500</td>\n","      <td>0.410250</td>\n","      <td>0.5430</td>\n","      <td>0.518000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.981301</td>\n","      <td>5.855036</td>\n","      <td>4.455084</td>\n","      <td>2.877874</td>\n","      <td>4.566389</td>\n","      <td>4.754900</td>\n","      <td>6.224051</td>\n","      <td>1.759501</td>\n","      <td>1.451402</td>\n","      <td>6.514011</td>\n","      <td>...</td>\n","      <td>0.003006</td>\n","      <td>0.459300</td>\n","      <td>0.560700</td>\n","      <td>0.4735</td>\n","      <td>0.531849</td>\n","      <td>0.490187</td>\n","      <td>0.535074</td>\n","      <td>0.4725</td>\n","      <td>0.549682</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12.306263</td>\n","      <td>6.337527</td>\n","      <td>4.459904</td>\n","      <td>2.941618</td>\n","      <td>4.498397</td>\n","      <td>5.115978</td>\n","      <td>6.315321</td>\n","      <td>1.815021</td>\n","      <td>1.249940</td>\n","      <td>6.686715</td>\n","      <td>...</td>\n","      <td>0.000928</td>\n","      <td>0.344000</td>\n","      <td>0.609000</td>\n","      <td>0.0000</td>\n","      <td>0.594000</td>\n","      <td>0.000000</td>\n","      <td>0.500000</td>\n","      <td>0.0000</td>\n","      <td>0.453000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.924356</td>\n","      <td>5.831368</td>\n","      <td>4.232168</td>\n","      <td>3.345199</td>\n","      <td>4.772059</td>\n","      <td>5.092751</td>\n","      <td>6.328898</td>\n","      <td>1.776461</td>\n","      <td>1.269352</td>\n","      <td>6.457453</td>\n","      <td>...</td>\n","      <td>0.001544</td>\n","      <td>0.294750</td>\n","      <td>0.590600</td>\n","      <td>0.1410</td>\n","      <td>0.509800</td>\n","      <td>0.523333</td>\n","      <td>0.418000</td>\n","      <td>0.3515</td>\n","      <td>0.492500</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11.751195</td>\n","      <td>5.696429</td>\n","      <td>4.387308</td>\n","      <td>3.400336</td>\n","      <td>4.734811</td>\n","      <td>4.937661</td>\n","      <td>6.364964</td>\n","      <td>1.778782</td>\n","      <td>1.192163</td>\n","      <td>6.539055</td>\n","      <td>...</td>\n","      <td>0.010126</td>\n","      <td>0.453000</td>\n","      <td>0.516000</td>\n","      <td>0.3910</td>\n","      <td>0.375000</td>\n","      <td>0.441000</td>\n","      <td>0.422000</td>\n","      <td>0.3200</td>\n","      <td>0.468500</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5525</th>\n","      <td>10.427677</td>\n","      <td>4.712157</td>\n","      <td>4.923399</td>\n","      <td>3.956240</td>\n","      <td>4.162726</td>\n","      <td>5.006993</td>\n","      <td>7.153138</td>\n","      <td>1.760024</td>\n","      <td>1.536282</td>\n","      <td>6.597105</td>\n","      <td>...</td>\n","      <td>0.005895</td>\n","      <td>0.000000</td>\n","      <td>0.644500</td>\n","      <td>0.1800</td>\n","      <td>0.750000</td>\n","      <td>0.419750</td>\n","      <td>0.281000</td>\n","      <td>0.3435</td>\n","      <td>0.562500</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5526</th>\n","      <td>8.398520</td>\n","      <td>2.988861</td>\n","      <td>2.252746</td>\n","      <td>3.048421</td>\n","      <td>5.798511</td>\n","      <td>5.158401</td>\n","      <td>4.810905</td>\n","      <td>1.738988</td>\n","      <td>1.730299</td>\n","      <td>4.858199</td>\n","      <td>...</td>\n","      <td>0.006126</td>\n","      <td>0.147000</td>\n","      <td>0.536500</td>\n","      <td>0.1410</td>\n","      <td>0.328000</td>\n","      <td>0.599750</td>\n","      <td>0.172000</td>\n","      <td>0.2890</td>\n","      <td>0.567500</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5527</th>\n","      <td>10.368590</td>\n","      <td>4.730114</td>\n","      <td>5.105931</td>\n","      <td>3.170660</td>\n","      <td>4.135172</td>\n","      <td>4.259716</td>\n","      <td>6.612472</td>\n","      <td>1.772626</td>\n","      <td>1.521696</td>\n","      <td>6.705473</td>\n","      <td>...</td>\n","      <td>0.001772</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5528</th>\n","      <td>9.916215</td>\n","      <td>3.867956</td>\n","      <td>3.379334</td>\n","      <td>3.135911</td>\n","      <td>5.451319</td>\n","      <td>4.645429</td>\n","      <td>5.375762</td>\n","      <td>1.727962</td>\n","      <td>1.417521</td>\n","      <td>5.503230</td>\n","      <td>...</td>\n","      <td>0.000054</td>\n","      <td>0.620500</td>\n","      <td>0.516000</td>\n","      <td>0.2890</td>\n","      <td>0.906000</td>\n","      <td>0.000000</td>\n","      <td>0.433429</td>\n","      <td>0.0000</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5529</th>\n","      <td>9.306035</td>\n","      <td>3.675035</td>\n","      <td>3.137268</td>\n","      <td>2.864412</td>\n","      <td>5.409729</td>\n","      <td>4.707642</td>\n","      <td>5.179926</td>\n","      <td>1.715438</td>\n","      <td>1.629607</td>\n","      <td>5.339017</td>\n","      <td>...</td>\n","      <td>0.013418</td>\n","      <td>0.369000</td>\n","      <td>0.515750</td>\n","      <td>0.1410</td>\n","      <td>0.380000</td>\n","      <td>0.523750</td>\n","      <td>0.424800</td>\n","      <td>0.3590</td>\n","      <td>0.518000</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5530 rows × 183 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a208f8a0-054f-4ea5-9e40-5a727009808e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-ca197688-1885-4c6a-a10a-5bf59b012231\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca197688-1885-4c6a-a10a-5bf59b012231')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-ca197688-1885-4c6a-a10a-5bf59b012231 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a208f8a0-054f-4ea5-9e40-5a727009808e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a208f8a0-054f-4ea5-9e40-5a727009808e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Convert the undersampled data back to a dataframe (if needed)\n","undersampled_df = X_undersampled.copy()\n","undersampled_df['label'] = y_undersampled"],"metadata":{"id":"BC0Mzcfo5vlD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["undersampled_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"qw0tYGsn-AVC","executionInfo":{"status":"ok","timestamp":1689798729007,"user_tz":-330,"elapsed":51,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"ab3fdc4a-3398-4253-d1ac-69e5aa7b281b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     12.319857  6.399014  4.530356  2.865929  4.445246  5.065509  6.307887   \n","1     11.981301  5.855036  4.455084  2.877874  4.566389  4.754900  6.224051   \n","2     12.306263  6.337527  4.459904  2.941618  4.498397  5.115978  6.315321   \n","3     11.924356  5.831368  4.232168  3.345199  4.772059  5.092751  6.328898   \n","4     11.751195  5.696429  4.387308  3.400336  4.734811  4.937661  6.364964   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","5525  10.427677  4.712157  4.923399  3.956240  4.162726  5.006993  7.153138   \n","5526   8.398520  2.988861  2.252746  3.048421  5.798511  5.158401  4.810905   \n","5527  10.368590  4.730114  5.105931  3.170660  4.135172  4.259716  6.612472   \n","5528   9.916215  3.867956  3.379334  3.135911  5.451319  4.645429  5.375762   \n","5529   9.306035  3.675035  3.137268  2.864412  5.409729  4.707642  5.179926   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.829383  1.249323   6.747220  ...         0.278333   \n","1     1.759501  1.451402   6.514011  ...         0.459300   \n","2     1.815021  1.249940   6.686715  ...         0.344000   \n","3     1.776461  1.269352   6.457453  ...         0.294750   \n","4     1.778782  1.192163   6.539055  ...         0.453000   \n","...        ...       ...        ...  ...              ...   \n","5525  1.760024  1.536282   6.597105  ...         0.000000   \n","5526  1.738988  1.730299   4.858199  ...         0.147000   \n","5527  1.772626  1.521696   6.705473  ...         0.000000   \n","5528  1.727962  1.417521   5.503230  ...         0.620500   \n","5529  1.715438  1.629607   5.339017  ...         0.369000   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.583333             0.1410        0.625333   \n","1                   0.560700             0.4735        0.531849   \n","2                   0.609000             0.0000        0.594000   \n","3                   0.590600             0.1410        0.509800   \n","4                   0.516000             0.3910        0.375000   \n","...                      ...                ...             ...   \n","5525                0.644500             0.1800        0.750000   \n","5526                0.536500             0.1410        0.328000   \n","5527                0.000000             0.0000        0.000000   \n","5528                0.516000             0.2890        0.906000   \n","5529                0.515750             0.1410        0.380000   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.671500           0.410250              0.5430         0.518000   \n","1          0.490187           0.535074              0.4725         0.549682   \n","2          0.000000           0.500000              0.0000         0.453000   \n","3          0.523333           0.418000              0.3515         0.492500   \n","4          0.441000           0.422000              0.3200         0.468500   \n","...             ...                ...                 ...              ...   \n","5525       0.419750           0.281000              0.3435         0.562500   \n","5526       0.599750           0.172000              0.2890         0.567500   \n","5527       0.000000           0.000000              0.0000         0.000000   \n","5528       0.000000           0.433429              0.0000         0.000000   \n","5529       0.523750           0.424800              0.3590         0.518000   \n","\n","      symptoms_ext_count  label  \n","0                      0      0  \n","1                      3      0  \n","2                      0      0  \n","3                      1      0  \n","4                      0      0  \n","...                  ...    ...  \n","5525                   0      1  \n","5526                   0      1  \n","5527                   0      1  \n","5528                   1      1  \n","5529                   0      1  \n","\n","[5530 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-e45ccfc0-3f08-4a46-90cc-bca298d0e1b6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12.319857</td>\n","      <td>6.399014</td>\n","      <td>4.530356</td>\n","      <td>2.865929</td>\n","      <td>4.445246</td>\n","      <td>5.065509</td>\n","      <td>6.307887</td>\n","      <td>1.829383</td>\n","      <td>1.249323</td>\n","      <td>6.747220</td>\n","      <td>...</td>\n","      <td>0.278333</td>\n","      <td>0.583333</td>\n","      <td>0.1410</td>\n","      <td>0.625333</td>\n","      <td>0.671500</td>\n","      <td>0.410250</td>\n","      <td>0.5430</td>\n","      <td>0.518000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.981301</td>\n","      <td>5.855036</td>\n","      <td>4.455084</td>\n","      <td>2.877874</td>\n","      <td>4.566389</td>\n","      <td>4.754900</td>\n","      <td>6.224051</td>\n","      <td>1.759501</td>\n","      <td>1.451402</td>\n","      <td>6.514011</td>\n","      <td>...</td>\n","      <td>0.459300</td>\n","      <td>0.560700</td>\n","      <td>0.4735</td>\n","      <td>0.531849</td>\n","      <td>0.490187</td>\n","      <td>0.535074</td>\n","      <td>0.4725</td>\n","      <td>0.549682</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12.306263</td>\n","      <td>6.337527</td>\n","      <td>4.459904</td>\n","      <td>2.941618</td>\n","      <td>4.498397</td>\n","      <td>5.115978</td>\n","      <td>6.315321</td>\n","      <td>1.815021</td>\n","      <td>1.249940</td>\n","      <td>6.686715</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.609000</td>\n","      <td>0.0000</td>\n","      <td>0.594000</td>\n","      <td>0.000000</td>\n","      <td>0.500000</td>\n","      <td>0.0000</td>\n","      <td>0.453000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.924356</td>\n","      <td>5.831368</td>\n","      <td>4.232168</td>\n","      <td>3.345199</td>\n","      <td>4.772059</td>\n","      <td>5.092751</td>\n","      <td>6.328898</td>\n","      <td>1.776461</td>\n","      <td>1.269352</td>\n","      <td>6.457453</td>\n","      <td>...</td>\n","      <td>0.294750</td>\n","      <td>0.590600</td>\n","      <td>0.1410</td>\n","      <td>0.509800</td>\n","      <td>0.523333</td>\n","      <td>0.418000</td>\n","      <td>0.3515</td>\n","      <td>0.492500</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11.751195</td>\n","      <td>5.696429</td>\n","      <td>4.387308</td>\n","      <td>3.400336</td>\n","      <td>4.734811</td>\n","      <td>4.937661</td>\n","      <td>6.364964</td>\n","      <td>1.778782</td>\n","      <td>1.192163</td>\n","      <td>6.539055</td>\n","      <td>...</td>\n","      <td>0.453000</td>\n","      <td>0.516000</td>\n","      <td>0.3910</td>\n","      <td>0.375000</td>\n","      <td>0.441000</td>\n","      <td>0.422000</td>\n","      <td>0.3200</td>\n","      <td>0.468500</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5525</th>\n","      <td>10.427677</td>\n","      <td>4.712157</td>\n","      <td>4.923399</td>\n","      <td>3.956240</td>\n","      <td>4.162726</td>\n","      <td>5.006993</td>\n","      <td>7.153138</td>\n","      <td>1.760024</td>\n","      <td>1.536282</td>\n","      <td>6.597105</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.644500</td>\n","      <td>0.1800</td>\n","      <td>0.750000</td>\n","      <td>0.419750</td>\n","      <td>0.281000</td>\n","      <td>0.3435</td>\n","      <td>0.562500</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5526</th>\n","      <td>8.398520</td>\n","      <td>2.988861</td>\n","      <td>2.252746</td>\n","      <td>3.048421</td>\n","      <td>5.798511</td>\n","      <td>5.158401</td>\n","      <td>4.810905</td>\n","      <td>1.738988</td>\n","      <td>1.730299</td>\n","      <td>4.858199</td>\n","      <td>...</td>\n","      <td>0.147000</td>\n","      <td>0.536500</td>\n","      <td>0.1410</td>\n","      <td>0.328000</td>\n","      <td>0.599750</td>\n","      <td>0.172000</td>\n","      <td>0.2890</td>\n","      <td>0.567500</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5527</th>\n","      <td>10.368590</td>\n","      <td>4.730114</td>\n","      <td>5.105931</td>\n","      <td>3.170660</td>\n","      <td>4.135172</td>\n","      <td>4.259716</td>\n","      <td>6.612472</td>\n","      <td>1.772626</td>\n","      <td>1.521696</td>\n","      <td>6.705473</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5528</th>\n","      <td>9.916215</td>\n","      <td>3.867956</td>\n","      <td>3.379334</td>\n","      <td>3.135911</td>\n","      <td>5.451319</td>\n","      <td>4.645429</td>\n","      <td>5.375762</td>\n","      <td>1.727962</td>\n","      <td>1.417521</td>\n","      <td>5.503230</td>\n","      <td>...</td>\n","      <td>0.620500</td>\n","      <td>0.516000</td>\n","      <td>0.2890</td>\n","      <td>0.906000</td>\n","      <td>0.000000</td>\n","      <td>0.433429</td>\n","      <td>0.0000</td>\n","      <td>0.000000</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5529</th>\n","      <td>9.306035</td>\n","      <td>3.675035</td>\n","      <td>3.137268</td>\n","      <td>2.864412</td>\n","      <td>5.409729</td>\n","      <td>4.707642</td>\n","      <td>5.179926</td>\n","      <td>1.715438</td>\n","      <td>1.629607</td>\n","      <td>5.339017</td>\n","      <td>...</td>\n","      <td>0.369000</td>\n","      <td>0.515750</td>\n","      <td>0.1410</td>\n","      <td>0.380000</td>\n","      <td>0.523750</td>\n","      <td>0.424800</td>\n","      <td>0.3590</td>\n","      <td>0.518000</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5530 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e45ccfc0-3f08-4a46-90cc-bca298d0e1b6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-9d591eb8-8e70-41b7-be8b-420e51aac146\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d591eb8-8e70-41b7-be8b-420e51aac146')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-9d591eb8-8e70-41b7-be8b-420e51aac146 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e45ccfc0-3f08-4a46-90cc-bca298d0e1b6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e45ccfc0-3f08-4a46-90cc-bca298d0e1b6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# Check the class distribution in the undersampled dataframe\n","undersampled_df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MravwUDH5vn0","executionInfo":{"status":"ok","timestamp":1689798729008,"user_tz":-330,"elapsed":47,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"0bfe8d8a-97a7-4c19-cddb-cc565ee74dcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2765\n","1    2765\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":[],"metadata":{"id":"FU7Yeqsc5vug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9_lZawqQ-Qqx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Multilayer DNN"],"metadata":{"id":"LSMISrc8BVGd"}},{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"BnTNvKynBXbf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689798733509,"user_tz":-330,"elapsed":4533,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"fea4de24-1fb8-46a5-f889-88018abadcd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"0xv80zaHBdLD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Concatenate, GlobalMaxPooling1D, Reshape, Dropout, MaxPooling1D, MaxPooling2D, MaxPooling3D\n","from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"xWXP9m3GBbod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaling = MinMaxScaler(feature_range=(0,1))  # (0,1) is default\n","sc = StandardScaler()"],"metadata":{"id":"AbfRKnGpADRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eiY8EYNDBint"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"_2lyD33K-Qtp","executionInfo":{"status":"ok","timestamp":1689798736925,"user_tz":-330,"elapsed":18,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"6d4dbaad-4555-437d-e220-fd342f720166"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-0ede7b35-00a3-42f8-a340-37e9f6bf2f95\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ede7b35-00a3-42f8-a340-37e9f6bf2f95')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-69980646-e07e-42da-99d5-02eb19679528\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69980646-e07e-42da-99d5-02eb19679528')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-69980646-e07e-42da-99d5-02eb19679528 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0ede7b35-00a3-42f8-a340-37e9f6bf2f95 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0ede7b35-00a3-42f8-a340-37e9f6bf2f95');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":[],"metadata":{"id":"suVEyDev-Up5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JjaTIFyuDn2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to register f1 score separately in Keras (working)\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.metrics import Metric\n","\n","class F1Score(Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","        false_positives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * y_pred, 0, 1)))\n","        false_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * (1 - y_pred), 0, 1)))\n","\n","        self.true_positives.assign_add(true_positives)\n","        self.false_positives.assign_add(false_positives)\n","        self.false_negatives.assign_add(false_negatives)\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n","        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","        return f1"],"metadata":{"id":"5kH208hWDn4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S68iNsGi9UJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HGV7uaqx9UNK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fix A. fix part with validation: choose 0.6 for train, 0.2 for validation, 0.2. for test"],"metadata":{"id":"80gnAKiR9UUb"}},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"6J7VGZG_UKqX","executionInfo":{"status":"ok","timestamp":1689798736927,"user_tz":-330,"elapsed":18,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"97fcbdf0-b123-4edd-fca5-b86638a0473e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-2a421400-b869-47d8-b342-a0844d9874bc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a421400-b869-47d8-b342-a0844d9874bc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-447afd7c-1fe5-4644-90fd-fefe2cccb95a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-447afd7c-1fe5-4644-90fd-fefe2cccb95a')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-447afd7c-1fe5-4644-90fd-fefe2cccb95a button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2a421400-b869-47d8-b342-a0844d9874bc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2a421400-b869-47d8-b342-a0844d9874bc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["undersample_seed = [1,2,3,4,5]\n","split_seed = [6,7,8,9,10]"],"metadata":{"id":"Dw3tRF8N-U3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracies = []\n","losses = []\n","f1scores = []"],"metadata":{"id":"CfVb4cJqI1C-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","for i in undersample_seed:\n","    Xtemp = final_df.drop('label', axis=1)\n","    ytemp = final_df['label']\n","    undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","    X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","    final_df = X_undersampled.copy()\n","    final_df['label'] = y_undersampled\n","    #print(final_df.shape, final_df['label'].value_counts())\n","\n","    for j in split_seed:\n","\n","        X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc = final_df.loc[:, 'WC':'Emoji']\n","        X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","        X_emotions = final_df.loc[:, 'admiration':'neutral']\n","        X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y = final_df['label']\n","\n","        # # train test split with stratification\n","        # X_sentemb_train, X_sentemb_test, X_liwc_train, X_liwc_test, X_emotions_train, X_emotions_test, X_intensity_train, X_intensity_test, y_train, y_test = train_test_split(\n","        #     X_sentemb, X_liwc, X_emotions, X_intensity, y, test_size=0.2, random_state=j, stratify=y)\n","\n","        # Split into remaining data and test\n","        X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=j, stratify=y)\n","        X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=j, stratify=y)\n","        X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=j, stratify=y)\n","        X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=j, stratify=y)\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Standardization of train and test LIWC features\n","        train_scaled_liwc = sc.fit_transform(X_liwc_train)\n","        val_scaled_liwc = sc.fit_transform(X_liwc_val)\n","        test_scaled_liwc = sc.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"usvM2AWc5OmI","executionInfo":{"status":"ok","timestamp":1689799349928,"user_tz":-330,"elapsed":613017,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"f60b9349-27b1-43b7-88b3-49f355d6c7c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 22s 36ms/step - loss: 0.5889 - accuracy: 0.6829 - f1_score: 0.6739 - val_loss: 0.5118 - val_accuracy: 0.7468 - val_f1_score: 0.7323\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4806 - accuracy: 0.7779 - f1_score: 0.7703 - val_loss: 0.4925 - val_accuracy: 0.7541 - val_f1_score: 0.7752\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4172 - accuracy: 0.8101 - f1_score: 0.8083 - val_loss: 0.4449 - val_accuracy: 0.8029 - val_f1_score: 0.7985\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3632 - accuracy: 0.8379 - f1_score: 0.8351 - val_loss: 0.4473 - val_accuracy: 0.7929 - val_f1_score: 0.8077\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3076 - accuracy: 0.8719 - f1_score: 0.8693 - val_loss: 0.4228 - val_accuracy: 0.8336 - val_f1_score: 0.8264\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2690 - accuracy: 0.8894 - f1_score: 0.8884 - val_loss: 0.4485 - val_accuracy: 0.8237 - val_f1_score: 0.8318\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2373 - accuracy: 0.9008 - f1_score: 0.8997 - val_loss: 0.4628 - val_accuracy: 0.8255 - val_f1_score: 0.8380\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2158 - accuracy: 0.9129 - f1_score: 0.9123 - val_loss: 0.4218 - val_accuracy: 0.8490 - val_f1_score: 0.8505\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1849 - accuracy: 0.9295 - f1_score: 0.9294 - val_loss: 0.4479 - val_accuracy: 0.8463 - val_f1_score: 0.8432\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1887 - accuracy: 0.9244 - f1_score: 0.9236 - val_loss: 0.4266 - val_accuracy: 0.8517 - val_f1_score: 0.8514\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1465 - accuracy: 0.9473 - f1_score: 0.9470 - val_loss: 0.4724 - val_accuracy: 0.8427 - val_f1_score: 0.8487\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1384 - accuracy: 0.9445 - f1_score: 0.9443 - val_loss: 0.5523 - val_accuracy: 0.8445 - val_f1_score: 0.8464\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1412 - accuracy: 0.9470 - f1_score: 0.9465 - val_loss: 0.5042 - val_accuracy: 0.8373 - val_f1_score: 0.8401\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8590 - f1_score: 0.8574\n","Epoch 1/20\n","104/104 [==============================] - 5s 13ms/step - loss: 0.5933 - accuracy: 0.6748 - f1_score: 0.6757 - val_loss: 0.4944 - val_accuracy: 0.7758 - val_f1_score: 0.7597\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4701 - accuracy: 0.7740 - f1_score: 0.7684 - val_loss: 0.4570 - val_accuracy: 0.7911 - val_f1_score: 0.7711\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3984 - accuracy: 0.8186 - f1_score: 0.8126 - val_loss: 0.4163 - val_accuracy: 0.8255 - val_f1_score: 0.8201\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3425 - accuracy: 0.8487 - f1_score: 0.8456 - val_loss: 0.4121 - val_accuracy: 0.8183 - val_f1_score: 0.8254\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2987 - accuracy: 0.8650 - f1_score: 0.8633 - val_loss: 0.3992 - val_accuracy: 0.8345 - val_f1_score: 0.8255\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2582 - accuracy: 0.8930 - f1_score: 0.8915 - val_loss: 0.4066 - val_accuracy: 0.8363 - val_f1_score: 0.8241\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2315 - accuracy: 0.8999 - f1_score: 0.8985 - val_loss: 0.3889 - val_accuracy: 0.8472 - val_f1_score: 0.8484\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2083 - accuracy: 0.9192 - f1_score: 0.9181 - val_loss: 0.4129 - val_accuracy: 0.8463 - val_f1_score: 0.8393\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1774 - accuracy: 0.9352 - f1_score: 0.9346 - val_loss: 0.4193 - val_accuracy: 0.8526 - val_f1_score: 0.8530\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1573 - accuracy: 0.9397 - f1_score: 0.9393 - val_loss: 0.4522 - val_accuracy: 0.8526 - val_f1_score: 0.8431\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1401 - accuracy: 0.9503 - f1_score: 0.9500 - val_loss: 0.5004 - val_accuracy: 0.8472 - val_f1_score: 0.8351\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1357 - accuracy: 0.9448 - f1_score: 0.9447 - val_loss: 0.5319 - val_accuracy: 0.8553 - val_f1_score: 0.8425\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3833 - accuracy: 0.8571 - f1_score: 0.8571\n","Epoch 1/20\n","104/104 [==============================] - 9s 32ms/step - loss: 0.5851 - accuracy: 0.6929 - f1_score: 0.6838 - val_loss: 0.4828 - val_accuracy: 0.7722 - val_f1_score: 0.7632\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4749 - accuracy: 0.7722 - f1_score: 0.7657 - val_loss: 0.4332 - val_accuracy: 0.7957 - val_f1_score: 0.7810\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4094 - accuracy: 0.8137 - f1_score: 0.8091 - val_loss: 0.4017 - val_accuracy: 0.8156 - val_f1_score: 0.8035\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3587 - accuracy: 0.8394 - f1_score: 0.8347 - val_loss: 0.4037 - val_accuracy: 0.8246 - val_f1_score: 0.8336\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3065 - accuracy: 0.8683 - f1_score: 0.8662 - val_loss: 0.3597 - val_accuracy: 0.8409 - val_f1_score: 0.8394\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2774 - accuracy: 0.8816 - f1_score: 0.8798 - val_loss: 0.3614 - val_accuracy: 0.8499 - val_f1_score: 0.8469\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2443 - accuracy: 0.9020 - f1_score: 0.9006 - val_loss: 0.3592 - val_accuracy: 0.8580 - val_f1_score: 0.8534\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2301 - accuracy: 0.9063 - f1_score: 0.9051 - val_loss: 0.4317 - val_accuracy: 0.8273 - val_f1_score: 0.8380\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2047 - accuracy: 0.9174 - f1_score: 0.9168 - val_loss: 0.3727 - val_accuracy: 0.8626 - val_f1_score: 0.8574\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1868 - accuracy: 0.9274 - f1_score: 0.9268 - val_loss: 0.3947 - val_accuracy: 0.8517 - val_f1_score: 0.8569\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1515 - accuracy: 0.9436 - f1_score: 0.9431 - val_loss: 0.4188 - val_accuracy: 0.8608 - val_f1_score: 0.8654\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1470 - accuracy: 0.9436 - f1_score: 0.9431 - val_loss: 0.4355 - val_accuracy: 0.8590 - val_f1_score: 0.8612\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8427 - f1_score: 0.8352\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5974 - accuracy: 0.6691 - f1_score: 0.6648 - val_loss: 0.5085 - val_accuracy: 0.7495 - val_f1_score: 0.7414\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4682 - accuracy: 0.7785 - f1_score: 0.7734 - val_loss: 0.4737 - val_accuracy: 0.7649 - val_f1_score: 0.7723\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4062 - accuracy: 0.8189 - f1_score: 0.8160 - val_loss: 0.4317 - val_accuracy: 0.7929 - val_f1_score: 0.7897\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3565 - accuracy: 0.8442 - f1_score: 0.8412 - val_loss: 0.3925 - val_accuracy: 0.8318 - val_f1_score: 0.8315\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3027 - accuracy: 0.8794 - f1_score: 0.8773 - val_loss: 0.3781 - val_accuracy: 0.8400 - val_f1_score: 0.8413\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2705 - accuracy: 0.8915 - f1_score: 0.8904 - val_loss: 0.3676 - val_accuracy: 0.8490 - val_f1_score: 0.8449\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2380 - accuracy: 0.9063 - f1_score: 0.9050 - val_loss: 0.3759 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2155 - accuracy: 0.9102 - f1_score: 0.9089 - val_loss: 0.4078 - val_accuracy: 0.8436 - val_f1_score: 0.8550\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1946 - accuracy: 0.9265 - f1_score: 0.9262 - val_loss: 0.3864 - val_accuracy: 0.8526 - val_f1_score: 0.8564\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1727 - accuracy: 0.9313 - f1_score: 0.9304 - val_loss: 0.4230 - val_accuracy: 0.8499 - val_f1_score: 0.8588\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1527 - accuracy: 0.9424 - f1_score: 0.9425 - val_loss: 0.4449 - val_accuracy: 0.8635 - val_f1_score: 0.8663\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3881 - accuracy: 0.8373 - f1_score: 0.8333\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5877 - accuracy: 0.6778 - f1_score: 0.6762 - val_loss: 0.5352 - val_accuracy: 0.7360 - val_f1_score: 0.7240\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4678 - accuracy: 0.7827 - f1_score: 0.7736 - val_loss: 0.5081 - val_accuracy: 0.7586 - val_f1_score: 0.7648\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4128 - accuracy: 0.8159 - f1_score: 0.8117 - val_loss: 0.4733 - val_accuracy: 0.7875 - val_f1_score: 0.7694\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3484 - accuracy: 0.8484 - f1_score: 0.8439 - val_loss: 0.4610 - val_accuracy: 0.8092 - val_f1_score: 0.7985\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2962 - accuracy: 0.8725 - f1_score: 0.8707 - val_loss: 0.4381 - val_accuracy: 0.8110 - val_f1_score: 0.7969\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2497 - accuracy: 0.8939 - f1_score: 0.8923 - val_loss: 0.4365 - val_accuracy: 0.8228 - val_f1_score: 0.8284\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2224 - accuracy: 0.9132 - f1_score: 0.9126 - val_loss: 0.4568 - val_accuracy: 0.8291 - val_f1_score: 0.8184\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1989 - accuracy: 0.9216 - f1_score: 0.9211 - val_loss: 0.4659 - val_accuracy: 0.8382 - val_f1_score: 0.8395\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.1906 - accuracy: 0.9192 - f1_score: 0.9190 - val_loss: 0.4747 - val_accuracy: 0.8427 - val_f1_score: 0.8383\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1669 - accuracy: 0.9370 - f1_score: 0.9366 - val_loss: 0.5175 - val_accuracy: 0.8273 - val_f1_score: 0.8355\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.1489 - accuracy: 0.9388 - f1_score: 0.9387 - val_loss: 0.4977 - val_accuracy: 0.8327 - val_f1_score: 0.8279\n","35/35 [==============================] - 0s 8ms/step - loss: 0.3881 - accuracy: 0.8445 - f1_score: 0.8494\n","Epoch 1/20\n","104/104 [==============================] - 13s 21ms/step - loss: 0.6082 - accuracy: 0.6655 - f1_score: 0.6698 - val_loss: 0.4915 - val_accuracy: 0.7486 - val_f1_score: 0.7367\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4822 - accuracy: 0.7709 - f1_score: 0.7644 - val_loss: 0.4390 - val_accuracy: 0.7993 - val_f1_score: 0.7894\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4140 - accuracy: 0.8083 - f1_score: 0.8035 - val_loss: 0.4199 - val_accuracy: 0.8165 - val_f1_score: 0.8031\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3523 - accuracy: 0.8565 - f1_score: 0.8555 - val_loss: 0.3849 - val_accuracy: 0.8409 - val_f1_score: 0.8382\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3057 - accuracy: 0.8692 - f1_score: 0.8677 - val_loss: 0.3826 - val_accuracy: 0.8427 - val_f1_score: 0.8463\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2635 - accuracy: 0.8933 - f1_score: 0.8917 - val_loss: 0.3772 - val_accuracy: 0.8535 - val_f1_score: 0.8489\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2273 - accuracy: 0.9066 - f1_score: 0.9059 - val_loss: 0.3873 - val_accuracy: 0.8535 - val_f1_score: 0.8551\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2098 - accuracy: 0.9156 - f1_score: 0.9146 - val_loss: 0.3819 - val_accuracy: 0.8544 - val_f1_score: 0.8530\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1674 - accuracy: 0.9406 - f1_score: 0.9403 - val_loss: 0.3973 - val_accuracy: 0.8617 - val_f1_score: 0.8585\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1648 - accuracy: 0.9385 - f1_score: 0.9380 - val_loss: 0.4180 - val_accuracy: 0.8472 - val_f1_score: 0.8482\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1446 - accuracy: 0.9448 - f1_score: 0.9446 - val_loss: 0.4249 - val_accuracy: 0.8662 - val_f1_score: 0.8574\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3818 - accuracy: 0.8526 - f1_score: 0.8434\n","Epoch 1/20\n","104/104 [==============================] - 9s 22ms/step - loss: 0.6007 - accuracy: 0.6688 - f1_score: 0.6628 - val_loss: 0.5005 - val_accuracy: 0.7613 - val_f1_score: 0.7402\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4814 - accuracy: 0.7749 - f1_score: 0.7655 - val_loss: 0.4514 - val_accuracy: 0.7911 - val_f1_score: 0.7925\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4101 - accuracy: 0.8207 - f1_score: 0.8144 - val_loss: 0.4098 - val_accuracy: 0.8128 - val_f1_score: 0.8056\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3491 - accuracy: 0.8463 - f1_score: 0.8439 - val_loss: 0.4433 - val_accuracy: 0.8083 - val_f1_score: 0.7801\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3021 - accuracy: 0.8731 - f1_score: 0.8699 - val_loss: 0.3902 - val_accuracy: 0.8382 - val_f1_score: 0.8365\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2600 - accuracy: 0.8924 - f1_score: 0.8909 - val_loss: 0.3939 - val_accuracy: 0.8481 - val_f1_score: 0.8447\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2350 - accuracy: 0.9039 - f1_score: 0.9025 - val_loss: 0.4055 - val_accuracy: 0.8454 - val_f1_score: 0.8412\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2170 - accuracy: 0.9111 - f1_score: 0.9101 - val_loss: 0.3974 - val_accuracy: 0.8472 - val_f1_score: 0.8413\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1955 - accuracy: 0.9253 - f1_score: 0.9246 - val_loss: 0.4102 - val_accuracy: 0.8508 - val_f1_score: 0.8421\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.1721 - accuracy: 0.9340 - f1_score: 0.9336 - val_loss: 0.4193 - val_accuracy: 0.8608 - val_f1_score: 0.8569\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3886 - accuracy: 0.8373 - f1_score: 0.8375\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.5987 - accuracy: 0.6679 - f1_score: 0.6655 - val_loss: 0.5129 - val_accuracy: 0.7306 - val_f1_score: 0.7330\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4899 - accuracy: 0.7643 - f1_score: 0.7550 - val_loss: 0.4525 - val_accuracy: 0.7893 - val_f1_score: 0.7853\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4472 - accuracy: 0.7990 - f1_score: 0.7943 - val_loss: 0.4276 - val_accuracy: 0.8065 - val_f1_score: 0.7989\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3823 - accuracy: 0.8348 - f1_score: 0.8317 - val_loss: 0.4009 - val_accuracy: 0.8264 - val_f1_score: 0.8129\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3285 - accuracy: 0.8638 - f1_score: 0.8616 - val_loss: 0.4299 - val_accuracy: 0.8074 - val_f1_score: 0.8256\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2902 - accuracy: 0.8831 - f1_score: 0.8814 - val_loss: 0.3599 - val_accuracy: 0.8653 - val_f1_score: 0.8601\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2565 - accuracy: 0.8900 - f1_score: 0.8890 - val_loss: 0.3791 - val_accuracy: 0.8373 - val_f1_score: 0.8472\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2270 - accuracy: 0.9069 - f1_score: 0.9061 - val_loss: 0.3689 - val_accuracy: 0.8562 - val_f1_score: 0.8589\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1984 - accuracy: 0.9244 - f1_score: 0.9239 - val_loss: 0.3849 - val_accuracy: 0.8662 - val_f1_score: 0.8640\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1890 - accuracy: 0.9231 - f1_score: 0.9227 - val_loss: 0.3961 - val_accuracy: 0.8680 - val_f1_score: 0.8607\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1692 - accuracy: 0.9355 - f1_score: 0.9350 - val_loss: 0.4062 - val_accuracy: 0.8662 - val_f1_score: 0.8659\n","35/35 [==============================] - 0s 11ms/step - loss: 0.4404 - accuracy: 0.8354 - f1_score: 0.8299\n","Epoch 1/20\n","104/104 [==============================] - 15s 46ms/step - loss: 0.5898 - accuracy: 0.6808 - f1_score: 0.6732 - val_loss: 0.4992 - val_accuracy: 0.7604 - val_f1_score: 0.7539\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4909 - accuracy: 0.7661 - f1_score: 0.7614 - val_loss: 0.4540 - val_accuracy: 0.7875 - val_f1_score: 0.7930\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4303 - accuracy: 0.8071 - f1_score: 0.8034 - val_loss: 0.4175 - val_accuracy: 0.8128 - val_f1_score: 0.8137\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3617 - accuracy: 0.8430 - f1_score: 0.8395 - val_loss: 0.3879 - val_accuracy: 0.8318 - val_f1_score: 0.8268\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3128 - accuracy: 0.8674 - f1_score: 0.8653 - val_loss: 0.3821 - val_accuracy: 0.8291 - val_f1_score: 0.8335\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2713 - accuracy: 0.8909 - f1_score: 0.8887 - val_loss: 0.3633 - val_accuracy: 0.8508 - val_f1_score: 0.8536\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2534 - accuracy: 0.8936 - f1_score: 0.8926 - val_loss: 0.3836 - val_accuracy: 0.8382 - val_f1_score: 0.8469\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2198 - accuracy: 0.9078 - f1_score: 0.9065 - val_loss: 0.3788 - val_accuracy: 0.8544 - val_f1_score: 0.8576\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.1969 - accuracy: 0.9219 - f1_score: 0.9214 - val_loss: 0.3860 - val_accuracy: 0.8535 - val_f1_score: 0.8576\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.1620 - accuracy: 0.9358 - f1_score: 0.9348 - val_loss: 0.3838 - val_accuracy: 0.8671 - val_f1_score: 0.8670\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.1635 - accuracy: 0.9355 - f1_score: 0.9350 - val_loss: 0.3970 - val_accuracy: 0.8698 - val_f1_score: 0.8664\n","35/35 [==============================] - 0s 8ms/step - loss: 0.3802 - accuracy: 0.8400 - f1_score: 0.8407\n","Epoch 1/20\n","104/104 [==============================] - 10s 36ms/step - loss: 0.5851 - accuracy: 0.6881 - f1_score: 0.6783 - val_loss: 0.5133 - val_accuracy: 0.7414 - val_f1_score: 0.7496\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4724 - accuracy: 0.7722 - f1_score: 0.7643 - val_loss: 0.4579 - val_accuracy: 0.7812 - val_f1_score: 0.7585\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4106 - accuracy: 0.8195 - f1_score: 0.8148 - val_loss: 0.4208 - val_accuracy: 0.8165 - val_f1_score: 0.8069\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3493 - accuracy: 0.8376 - f1_score: 0.8345 - val_loss: 0.3900 - val_accuracy: 0.8382 - val_f1_score: 0.8332\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3091 - accuracy: 0.8671 - f1_score: 0.8651 - val_loss: 0.3768 - val_accuracy: 0.8400 - val_f1_score: 0.8451\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2665 - accuracy: 0.8912 - f1_score: 0.8900 - val_loss: 0.3733 - val_accuracy: 0.8445 - val_f1_score: 0.8404\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2325 - accuracy: 0.9011 - f1_score: 0.9007 - val_loss: 0.3695 - val_accuracy: 0.8508 - val_f1_score: 0.8412\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2066 - accuracy: 0.9180 - f1_score: 0.9173 - val_loss: 0.3637 - val_accuracy: 0.8635 - val_f1_score: 0.8641\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1947 - accuracy: 0.9207 - f1_score: 0.9203 - val_loss: 0.4099 - val_accuracy: 0.8427 - val_f1_score: 0.8291\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1972 - accuracy: 0.9177 - f1_score: 0.9172 - val_loss: 0.3834 - val_accuracy: 0.8508 - val_f1_score: 0.8430\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.1553 - accuracy: 0.9376 - f1_score: 0.9371 - val_loss: 0.3799 - val_accuracy: 0.8671 - val_f1_score: 0.8709\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.1458 - accuracy: 0.9433 - f1_score: 0.9431 - val_loss: 0.4288 - val_accuracy: 0.8571 - val_f1_score: 0.8512\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1253 - accuracy: 0.9503 - f1_score: 0.9499 - val_loss: 0.4392 - val_accuracy: 0.8517 - val_f1_score: 0.8517\n","35/35 [==============================] - 0s 4ms/step - loss: 0.4123 - accuracy: 0.8499 - f1_score: 0.8528\n","Epoch 1/20\n","104/104 [==============================] - 6s 16ms/step - loss: 0.5828 - accuracy: 0.7016 - f1_score: 0.7004 - val_loss: 0.5017 - val_accuracy: 0.7523 - val_f1_score: 0.7405\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4744 - accuracy: 0.7800 - f1_score: 0.7777 - val_loss: 0.4842 - val_accuracy: 0.7568 - val_f1_score: 0.7189\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4161 - accuracy: 0.8146 - f1_score: 0.8122 - val_loss: 0.4301 - val_accuracy: 0.7966 - val_f1_score: 0.7879\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3673 - accuracy: 0.8397 - f1_score: 0.8379 - val_loss: 0.4147 - val_accuracy: 0.8038 - val_f1_score: 0.8068\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3194 - accuracy: 0.8623 - f1_score: 0.8610 - val_loss: 0.4044 - val_accuracy: 0.8183 - val_f1_score: 0.8194\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2835 - accuracy: 0.8837 - f1_score: 0.8827 - val_loss: 0.3994 - val_accuracy: 0.8282 - val_f1_score: 0.8180\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2400 - accuracy: 0.9027 - f1_score: 0.9018 - val_loss: 0.4159 - val_accuracy: 0.8327 - val_f1_score: 0.8236\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2094 - accuracy: 0.9168 - f1_score: 0.9161 - val_loss: 0.4228 - val_accuracy: 0.8291 - val_f1_score: 0.8212\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1962 - accuracy: 0.9204 - f1_score: 0.9200 - val_loss: 0.4322 - val_accuracy: 0.8472 - val_f1_score: 0.8434\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1738 - accuracy: 0.9322 - f1_score: 0.9318 - val_loss: 0.4555 - val_accuracy: 0.8481 - val_f1_score: 0.8447\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1458 - accuracy: 0.9451 - f1_score: 0.9450 - val_loss: 0.5293 - val_accuracy: 0.8273 - val_f1_score: 0.8415\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3694 - accuracy: 0.8354 - f1_score: 0.8240\n","Epoch 1/20\n","104/104 [==============================] - 6s 22ms/step - loss: 0.5789 - accuracy: 0.6926 - f1_score: 0.6790 - val_loss: 0.5108 - val_accuracy: 0.7523 - val_f1_score: 0.7495\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4626 - accuracy: 0.7779 - f1_score: 0.7703 - val_loss: 0.4679 - val_accuracy: 0.7758 - val_f1_score: 0.7656\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4101 - accuracy: 0.8080 - f1_score: 0.8031 - val_loss: 0.4558 - val_accuracy: 0.7966 - val_f1_score: 0.7801\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3392 - accuracy: 0.8529 - f1_score: 0.8503 - val_loss: 0.4454 - val_accuracy: 0.8083 - val_f1_score: 0.8066\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2978 - accuracy: 0.8746 - f1_score: 0.8726 - val_loss: 0.4345 - val_accuracy: 0.8228 - val_f1_score: 0.8192\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2691 - accuracy: 0.8843 - f1_score: 0.8831 - val_loss: 0.4286 - val_accuracy: 0.8210 - val_f1_score: 0.8132\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2242 - accuracy: 0.9075 - f1_score: 0.9066 - val_loss: 0.4264 - val_accuracy: 0.8409 - val_f1_score: 0.8382\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2138 - accuracy: 0.9111 - f1_score: 0.9104 - val_loss: 0.5385 - val_accuracy: 0.8038 - val_f1_score: 0.8234\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1983 - accuracy: 0.9225 - f1_score: 0.9221 - val_loss: 0.4701 - val_accuracy: 0.8318 - val_f1_score: 0.8385\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1746 - accuracy: 0.9331 - f1_score: 0.9325 - val_loss: 0.5048 - val_accuracy: 0.8363 - val_f1_score: 0.8436\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1513 - accuracy: 0.9391 - f1_score: 0.9391 - val_loss: 0.5050 - val_accuracy: 0.8363 - val_f1_score: 0.8397\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1332 - accuracy: 0.9500 - f1_score: 0.9499 - val_loss: 0.5388 - val_accuracy: 0.8363 - val_f1_score: 0.8380\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3811 - accuracy: 0.8535 - f1_score: 0.8460\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5975 - accuracy: 0.6640 - f1_score: 0.6591 - val_loss: 0.5115 - val_accuracy: 0.7423 - val_f1_score: 0.7354\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4787 - accuracy: 0.7673 - f1_score: 0.7584 - val_loss: 0.4826 - val_accuracy: 0.7631 - val_f1_score: 0.7510\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4151 - accuracy: 0.8122 - f1_score: 0.8060 - val_loss: 0.4666 - val_accuracy: 0.7785 - val_f1_score: 0.7717\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3687 - accuracy: 0.8345 - f1_score: 0.8328 - val_loss: 0.4631 - val_accuracy: 0.7875 - val_f1_score: 0.7986\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3184 - accuracy: 0.8632 - f1_score: 0.8618 - val_loss: 0.4408 - val_accuracy: 0.8101 - val_f1_score: 0.8101\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2766 - accuracy: 0.8858 - f1_score: 0.8846 - val_loss: 0.4376 - val_accuracy: 0.8255 - val_f1_score: 0.8231\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2440 - accuracy: 0.8993 - f1_score: 0.8987 - val_loss: 0.4323 - val_accuracy: 0.8418 - val_f1_score: 0.8405\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2146 - accuracy: 0.9156 - f1_score: 0.9149 - val_loss: 0.4521 - val_accuracy: 0.8409 - val_f1_score: 0.8324\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1982 - accuracy: 0.9219 - f1_score: 0.9216 - val_loss: 0.4337 - val_accuracy: 0.8418 - val_f1_score: 0.8357\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1812 - accuracy: 0.9301 - f1_score: 0.9295 - val_loss: 0.4691 - val_accuracy: 0.8436 - val_f1_score: 0.8388\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1516 - accuracy: 0.9412 - f1_score: 0.9411 - val_loss: 0.5193 - val_accuracy: 0.8363 - val_f1_score: 0.8261\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1368 - accuracy: 0.9518 - f1_score: 0.9517 - val_loss: 0.5515 - val_accuracy: 0.8427 - val_f1_score: 0.8330\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8481 - f1_score: 0.8500\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5987 - accuracy: 0.6802 - f1_score: 0.6843 - val_loss: 0.4950 - val_accuracy: 0.7577 - val_f1_score: 0.7161\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4841 - accuracy: 0.7709 - f1_score: 0.7621 - val_loss: 0.4347 - val_accuracy: 0.7975 - val_f1_score: 0.7812\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4130 - accuracy: 0.8207 - f1_score: 0.8158 - val_loss: 0.4128 - val_accuracy: 0.8119 - val_f1_score: 0.8204\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3630 - accuracy: 0.8448 - f1_score: 0.8419 - val_loss: 0.3669 - val_accuracy: 0.8318 - val_f1_score: 0.8309\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3203 - accuracy: 0.8623 - f1_score: 0.8600 - val_loss: 0.3679 - val_accuracy: 0.8373 - val_f1_score: 0.8413\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2668 - accuracy: 0.8924 - f1_score: 0.8909 - val_loss: 0.3561 - val_accuracy: 0.8553 - val_f1_score: 0.8540\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2370 - accuracy: 0.8981 - f1_score: 0.8966 - val_loss: 0.4194 - val_accuracy: 0.8427 - val_f1_score: 0.8510\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2057 - accuracy: 0.9180 - f1_score: 0.9176 - val_loss: 0.3838 - val_accuracy: 0.8526 - val_f1_score: 0.8492\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1880 - accuracy: 0.9225 - f1_score: 0.9215 - val_loss: 0.3797 - val_accuracy: 0.8544 - val_f1_score: 0.8551\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1682 - accuracy: 0.9355 - f1_score: 0.9351 - val_loss: 0.4005 - val_accuracy: 0.8608 - val_f1_score: 0.8597\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1538 - accuracy: 0.9424 - f1_score: 0.9422 - val_loss: 0.4166 - val_accuracy: 0.8626 - val_f1_score: 0.8662\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3596 - accuracy: 0.8526 - f1_score: 0.8489\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5884 - accuracy: 0.6890 - f1_score: 0.6929 - val_loss: 0.5177 - val_accuracy: 0.7378 - val_f1_score: 0.7016\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4744 - accuracy: 0.7706 - f1_score: 0.7603 - val_loss: 0.4793 - val_accuracy: 0.7767 - val_f1_score: 0.7736\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4135 - accuracy: 0.8137 - f1_score: 0.8101 - val_loss: 0.4690 - val_accuracy: 0.7776 - val_f1_score: 0.7598\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3580 - accuracy: 0.8418 - f1_score: 0.8394 - val_loss: 0.4464 - val_accuracy: 0.8038 - val_f1_score: 0.8007\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3096 - accuracy: 0.8686 - f1_score: 0.8667 - val_loss: 0.4449 - val_accuracy: 0.8128 - val_f1_score: 0.7984\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2688 - accuracy: 0.8897 - f1_score: 0.8879 - val_loss: 0.4568 - val_accuracy: 0.8146 - val_f1_score: 0.7915\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2326 - accuracy: 0.9051 - f1_score: 0.9039 - val_loss: 0.4200 - val_accuracy: 0.8363 - val_f1_score: 0.8278\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2072 - accuracy: 0.9126 - f1_score: 0.9120 - val_loss: 0.4402 - val_accuracy: 0.8318 - val_f1_score: 0.8180\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1871 - accuracy: 0.9274 - f1_score: 0.9268 - val_loss: 0.4597 - val_accuracy: 0.8454 - val_f1_score: 0.8292\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1693 - accuracy: 0.9364 - f1_score: 0.9360 - val_loss: 0.4275 - val_accuracy: 0.8463 - val_f1_score: 0.8402\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1407 - accuracy: 0.9464 - f1_score: 0.9461 - val_loss: 0.4861 - val_accuracy: 0.8300 - val_f1_score: 0.8318\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1439 - accuracy: 0.9388 - f1_score: 0.9385 - val_loss: 0.4990 - val_accuracy: 0.8463 - val_f1_score: 0.8343\n","35/35 [==============================] - 0s 4ms/step - loss: 0.4042 - accuracy: 0.8382 - f1_score: 0.8313\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5889 - accuracy: 0.6884 - f1_score: 0.6815 - val_loss: 0.5267 - val_accuracy: 0.7414 - val_f1_score: 0.7276\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4697 - accuracy: 0.7785 - f1_score: 0.7694 - val_loss: 0.4759 - val_accuracy: 0.7685 - val_f1_score: 0.7515\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4174 - accuracy: 0.8110 - f1_score: 0.8068 - val_loss: 0.4823 - val_accuracy: 0.7640 - val_f1_score: 0.7797\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3687 - accuracy: 0.8351 - f1_score: 0.8328 - val_loss: 0.4658 - val_accuracy: 0.7957 - val_f1_score: 0.7762\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3209 - accuracy: 0.8638 - f1_score: 0.8598 - val_loss: 0.4479 - val_accuracy: 0.7993 - val_f1_score: 0.8080\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2816 - accuracy: 0.8807 - f1_score: 0.8786 - val_loss: 0.4262 - val_accuracy: 0.8246 - val_f1_score: 0.8094\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2530 - accuracy: 0.8999 - f1_score: 0.8979 - val_loss: 0.4451 - val_accuracy: 0.8110 - val_f1_score: 0.8236\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2180 - accuracy: 0.9111 - f1_score: 0.9099 - val_loss: 0.4531 - val_accuracy: 0.8201 - val_f1_score: 0.8183\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1855 - accuracy: 0.9268 - f1_score: 0.9255 - val_loss: 0.4744 - val_accuracy: 0.8237 - val_f1_score: 0.8257\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1757 - accuracy: 0.9307 - f1_score: 0.9300 - val_loss: 0.4824 - val_accuracy: 0.8300 - val_f1_score: 0.8175\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1546 - accuracy: 0.9412 - f1_score: 0.9409 - val_loss: 0.4954 - val_accuracy: 0.8345 - val_f1_score: 0.8326\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3666 - accuracy: 0.8336 - f1_score: 0.8164\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5875 - accuracy: 0.6854 - f1_score: 0.6752 - val_loss: 0.4893 - val_accuracy: 0.7568 - val_f1_score: 0.7396\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4827 - accuracy: 0.7640 - f1_score: 0.7534 - val_loss: 0.4474 - val_accuracy: 0.7776 - val_f1_score: 0.7459\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4019 - accuracy: 0.8180 - f1_score: 0.8117 - val_loss: 0.4119 - val_accuracy: 0.8047 - val_f1_score: 0.8075\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3451 - accuracy: 0.8514 - f1_score: 0.8482 - val_loss: 0.4083 - val_accuracy: 0.8174 - val_f1_score: 0.7947\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2892 - accuracy: 0.8773 - f1_score: 0.8742 - val_loss: 0.3688 - val_accuracy: 0.8454 - val_f1_score: 0.8391\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2687 - accuracy: 0.8900 - f1_score: 0.8877 - val_loss: 0.3559 - val_accuracy: 0.8553 - val_f1_score: 0.8462\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2379 - accuracy: 0.9027 - f1_score: 0.9017 - val_loss: 0.3592 - val_accuracy: 0.8427 - val_f1_score: 0.8446\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1975 - accuracy: 0.9237 - f1_score: 0.9229 - val_loss: 0.3564 - val_accuracy: 0.8553 - val_f1_score: 0.8476\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1973 - accuracy: 0.9207 - f1_score: 0.9198 - val_loss: 0.3618 - val_accuracy: 0.8553 - val_f1_score: 0.8566\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1566 - accuracy: 0.9427 - f1_score: 0.9422 - val_loss: 0.4007 - val_accuracy: 0.8526 - val_f1_score: 0.8509\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1459 - accuracy: 0.9400 - f1_score: 0.9395 - val_loss: 0.3833 - val_accuracy: 0.8481 - val_f1_score: 0.8503\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3805 - accuracy: 0.8517 - f1_score: 0.8395\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5891 - accuracy: 0.6832 - f1_score: 0.6833 - val_loss: 0.5255 - val_accuracy: 0.7396 - val_f1_score: 0.7103\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4834 - accuracy: 0.7703 - f1_score: 0.7629 - val_loss: 0.4798 - val_accuracy: 0.7703 - val_f1_score: 0.7622\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4308 - accuracy: 0.8080 - f1_score: 0.8027 - val_loss: 0.4536 - val_accuracy: 0.7920 - val_f1_score: 0.7700\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3749 - accuracy: 0.8309 - f1_score: 0.8283 - val_loss: 0.4516 - val_accuracy: 0.8119 - val_f1_score: 0.7890\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3223 - accuracy: 0.8580 - f1_score: 0.8544 - val_loss: 0.3948 - val_accuracy: 0.8291 - val_f1_score: 0.8277\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2902 - accuracy: 0.8758 - f1_score: 0.8745 - val_loss: 0.3865 - val_accuracy: 0.8436 - val_f1_score: 0.8403\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2386 - accuracy: 0.9014 - f1_score: 0.9002 - val_loss: 0.3818 - val_accuracy: 0.8535 - val_f1_score: 0.8494\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2201 - accuracy: 0.9096 - f1_score: 0.9086 - val_loss: 0.3671 - val_accuracy: 0.8535 - val_f1_score: 0.8503\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1964 - accuracy: 0.9201 - f1_score: 0.9196 - val_loss: 0.3909 - val_accuracy: 0.8617 - val_f1_score: 0.8585\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1784 - accuracy: 0.9262 - f1_score: 0.9252 - val_loss: 0.4326 - val_accuracy: 0.8490 - val_f1_score: 0.8461\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1654 - accuracy: 0.9289 - f1_score: 0.9283 - val_loss: 0.4401 - val_accuracy: 0.8635 - val_f1_score: 0.8624\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1388 - accuracy: 0.9430 - f1_score: 0.9427 - val_loss: 0.4629 - val_accuracy: 0.8508 - val_f1_score: 0.8509\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1490 - accuracy: 0.9373 - f1_score: 0.9369 - val_loss: 0.4473 - val_accuracy: 0.8562 - val_f1_score: 0.8551\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.8562 - f1_score: 0.8569\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6017 - accuracy: 0.6624 - f1_score: 0.6598 - val_loss: 0.4994 - val_accuracy: 0.7523 - val_f1_score: 0.7523\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4910 - accuracy: 0.7634 - f1_score: 0.7546 - val_loss: 0.4509 - val_accuracy: 0.7875 - val_f1_score: 0.7944\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4168 - accuracy: 0.8116 - f1_score: 0.8051 - val_loss: 0.4105 - val_accuracy: 0.8192 - val_f1_score: 0.8073\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3600 - accuracy: 0.8439 - f1_score: 0.8389 - val_loss: 0.4024 - val_accuracy: 0.8201 - val_f1_score: 0.8326\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3141 - accuracy: 0.8710 - f1_score: 0.8681 - val_loss: 0.3654 - val_accuracy: 0.8463 - val_f1_score: 0.8479\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2722 - accuracy: 0.8888 - f1_score: 0.8870 - val_loss: 0.3684 - val_accuracy: 0.8571 - val_f1_score: 0.8509\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2466 - accuracy: 0.9011 - f1_score: 0.9001 - val_loss: 0.3500 - val_accuracy: 0.8553 - val_f1_score: 0.8516\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2206 - accuracy: 0.9114 - f1_score: 0.9104 - val_loss: 0.3426 - val_accuracy: 0.8653 - val_f1_score: 0.8634\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1745 - accuracy: 0.9343 - f1_score: 0.9337 - val_loss: 0.3984 - val_accuracy: 0.8517 - val_f1_score: 0.8605\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1869 - accuracy: 0.9268 - f1_score: 0.9262 - val_loss: 0.4625 - val_accuracy: 0.8237 - val_f1_score: 0.8413\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1529 - accuracy: 0.9394 - f1_score: 0.9392 - val_loss: 0.4000 - val_accuracy: 0.8653 - val_f1_score: 0.8649\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1395 - accuracy: 0.9479 - f1_score: 0.9473 - val_loss: 0.4104 - val_accuracy: 0.8680 - val_f1_score: 0.8670\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1332 - accuracy: 0.9536 - f1_score: 0.9532 - val_loss: 0.4430 - val_accuracy: 0.8544 - val_f1_score: 0.8551\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3956 - accuracy: 0.8454 - f1_score: 0.8409\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5841 - accuracy: 0.6935 - f1_score: 0.6854 - val_loss: 0.5019 - val_accuracy: 0.7495 - val_f1_score: 0.7438\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4831 - accuracy: 0.7673 - f1_score: 0.7563 - val_loss: 0.4783 - val_accuracy: 0.7740 - val_f1_score: 0.7500\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4043 - accuracy: 0.8195 - f1_score: 0.8135 - val_loss: 0.4440 - val_accuracy: 0.8101 - val_f1_score: 0.8023\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3467 - accuracy: 0.8499 - f1_score: 0.8464 - val_loss: 0.4287 - val_accuracy: 0.8146 - val_f1_score: 0.8124\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3073 - accuracy: 0.8698 - f1_score: 0.8675 - val_loss: 0.4077 - val_accuracy: 0.8318 - val_f1_score: 0.8255\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2613 - accuracy: 0.8894 - f1_score: 0.8877 - val_loss: 0.4290 - val_accuracy: 0.8300 - val_f1_score: 0.8250\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2212 - accuracy: 0.9129 - f1_score: 0.9117 - val_loss: 0.4222 - val_accuracy: 0.8436 - val_f1_score: 0.8369\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1994 - accuracy: 0.9231 - f1_score: 0.9231 - val_loss: 0.4545 - val_accuracy: 0.8165 - val_f1_score: 0.8221\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1779 - accuracy: 0.9228 - f1_score: 0.9223 - val_loss: 0.4667 - val_accuracy: 0.8345 - val_f1_score: 0.8285\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1701 - accuracy: 0.9322 - f1_score: 0.9315 - val_loss: 0.5138 - val_accuracy: 0.8282 - val_f1_score: 0.8119\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3786 - accuracy: 0.8354 - f1_score: 0.8351\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5956 - accuracy: 0.6841 - f1_score: 0.6832 - val_loss: 0.4958 - val_accuracy: 0.7577 - val_f1_score: 0.7486\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4813 - accuracy: 0.7676 - f1_score: 0.7636 - val_loss: 0.4452 - val_accuracy: 0.7785 - val_f1_score: 0.7803\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4064 - accuracy: 0.8222 - f1_score: 0.8183 - val_loss: 0.4275 - val_accuracy: 0.7893 - val_f1_score: 0.7976\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3523 - accuracy: 0.8448 - f1_score: 0.8419 - val_loss: 0.4169 - val_accuracy: 0.8165 - val_f1_score: 0.7980\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3124 - accuracy: 0.8695 - f1_score: 0.8666 - val_loss: 0.3978 - val_accuracy: 0.8373 - val_f1_score: 0.8390\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2685 - accuracy: 0.8879 - f1_score: 0.8858 - val_loss: 0.4321 - val_accuracy: 0.8101 - val_f1_score: 0.8247\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2361 - accuracy: 0.9005 - f1_score: 0.8998 - val_loss: 0.4068 - val_accuracy: 0.8373 - val_f1_score: 0.8367\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2096 - accuracy: 0.9183 - f1_score: 0.9173 - val_loss: 0.4025 - val_accuracy: 0.8400 - val_f1_score: 0.8335\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1763 - accuracy: 0.9337 - f1_score: 0.9331 - val_loss: 0.4494 - val_accuracy: 0.8255 - val_f1_score: 0.8309\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1704 - accuracy: 0.9322 - f1_score: 0.9317 - val_loss: 0.4680 - val_accuracy: 0.8300 - val_f1_score: 0.8354\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.8517 - f1_score: 0.8538\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6080 - accuracy: 0.6652 - f1_score: 0.6663 - val_loss: 0.5013 - val_accuracy: 0.7586 - val_f1_score: 0.7592\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4729 - accuracy: 0.7731 - f1_score: 0.7681 - val_loss: 0.4586 - val_accuracy: 0.7785 - val_f1_score: 0.7918\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4027 - accuracy: 0.8149 - f1_score: 0.8135 - val_loss: 0.4171 - val_accuracy: 0.8165 - val_f1_score: 0.8129\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3494 - accuracy: 0.8523 - f1_score: 0.8501 - val_loss: 0.4162 - val_accuracy: 0.8110 - val_f1_score: 0.8227\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3005 - accuracy: 0.8713 - f1_score: 0.8682 - val_loss: 0.4010 - val_accuracy: 0.8246 - val_f1_score: 0.8345\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2693 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3836 - val_accuracy: 0.8363 - val_f1_score: 0.8224\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2366 - accuracy: 0.9011 - f1_score: 0.8999 - val_loss: 0.4157 - val_accuracy: 0.8291 - val_f1_score: 0.8149\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2105 - accuracy: 0.9141 - f1_score: 0.9129 - val_loss: 0.3878 - val_accuracy: 0.8363 - val_f1_score: 0.8307\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1828 - accuracy: 0.9274 - f1_score: 0.9265 - val_loss: 0.3982 - val_accuracy: 0.8490 - val_f1_score: 0.8539\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1765 - accuracy: 0.9316 - f1_score: 0.9312 - val_loss: 0.3931 - val_accuracy: 0.8436 - val_f1_score: 0.8492\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1541 - accuracy: 0.9403 - f1_score: 0.9401 - val_loss: 0.3992 - val_accuracy: 0.8508 - val_f1_score: 0.8457\n","35/35 [==============================] - 0s 4ms/step - loss: 0.4227 - accuracy: 0.8327 - f1_score: 0.8174\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6077 - accuracy: 0.6739 - f1_score: 0.6644 - val_loss: 0.5175 - val_accuracy: 0.7523 - val_f1_score: 0.7580\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4886 - accuracy: 0.7755 - f1_score: 0.7651 - val_loss: 0.4596 - val_accuracy: 0.7866 - val_f1_score: 0.7835\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4160 - accuracy: 0.8062 - f1_score: 0.8006 - val_loss: 0.4032 - val_accuracy: 0.8219 - val_f1_score: 0.8224\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3602 - accuracy: 0.8451 - f1_score: 0.8416 - val_loss: 0.3887 - val_accuracy: 0.8210 - val_f1_score: 0.8251\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3121 - accuracy: 0.8626 - f1_score: 0.8611 - val_loss: 0.3644 - val_accuracy: 0.8526 - val_f1_score: 0.8533\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2751 - accuracy: 0.8879 - f1_score: 0.8867 - val_loss: 0.3873 - val_accuracy: 0.8400 - val_f1_score: 0.8470\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2350 - accuracy: 0.9024 - f1_score: 0.9013 - val_loss: 0.3645 - val_accuracy: 0.8743 - val_f1_score: 0.8695\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2074 - accuracy: 0.9189 - f1_score: 0.9182 - val_loss: 0.3883 - val_accuracy: 0.8644 - val_f1_score: 0.8654\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1884 - accuracy: 0.9253 - f1_score: 0.9247 - val_loss: 0.4074 - val_accuracy: 0.8671 - val_f1_score: 0.8658\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1632 - accuracy: 0.9352 - f1_score: 0.9346 - val_loss: 0.4262 - val_accuracy: 0.8770 - val_f1_score: 0.8743\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3784 - accuracy: 0.8490 - f1_score: 0.8521\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6058 - accuracy: 0.6781 - f1_score: 0.6673 - val_loss: 0.5148 - val_accuracy: 0.7459 - val_f1_score: 0.7226\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4821 - accuracy: 0.7697 - f1_score: 0.7562 - val_loss: 0.4635 - val_accuracy: 0.7776 - val_f1_score: 0.7705\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3971 - accuracy: 0.8207 - f1_score: 0.8127 - val_loss: 0.4233 - val_accuracy: 0.8002 - val_f1_score: 0.7909\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3569 - accuracy: 0.8366 - f1_score: 0.8308 - val_loss: 0.3849 - val_accuracy: 0.8255 - val_f1_score: 0.8188\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3012 - accuracy: 0.8794 - f1_score: 0.8764 - val_loss: 0.3995 - val_accuracy: 0.8128 - val_f1_score: 0.7903\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2654 - accuracy: 0.8891 - f1_score: 0.8870 - val_loss: 0.3695 - val_accuracy: 0.8481 - val_f1_score: 0.8444\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2356 - accuracy: 0.9057 - f1_score: 0.9038 - val_loss: 0.3973 - val_accuracy: 0.8436 - val_f1_score: 0.8319\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2013 - accuracy: 0.9213 - f1_score: 0.9196 - val_loss: 0.4087 - val_accuracy: 0.8472 - val_f1_score: 0.8473\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1864 - accuracy: 0.9289 - f1_score: 0.9278 - val_loss: 0.4051 - val_accuracy: 0.8490 - val_f1_score: 0.8513\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1667 - accuracy: 0.9295 - f1_score: 0.9286 - val_loss: 0.4378 - val_accuracy: 0.8580 - val_f1_score: 0.8599\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1427 - accuracy: 0.9467 - f1_score: 0.9461 - val_loss: 0.4428 - val_accuracy: 0.8490 - val_f1_score: 0.8417\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3485 - accuracy: 0.8608 - f1_score: 0.8579\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5921 - accuracy: 0.6872 - f1_score: 0.6804 - val_loss: 0.5235 - val_accuracy: 0.7369 - val_f1_score: 0.6978\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4766 - accuracy: 0.7694 - f1_score: 0.7560 - val_loss: 0.4679 - val_accuracy: 0.7821 - val_f1_score: 0.7803\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4067 - accuracy: 0.8083 - f1_score: 0.8015 - val_loss: 0.4351 - val_accuracy: 0.8020 - val_f1_score: 0.7963\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3415 - accuracy: 0.8559 - f1_score: 0.8519 - val_loss: 0.4129 - val_accuracy: 0.8246 - val_f1_score: 0.8236\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3046 - accuracy: 0.8716 - f1_score: 0.8699 - val_loss: 0.4209 - val_accuracy: 0.8192 - val_f1_score: 0.8316\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2677 - accuracy: 0.8858 - f1_score: 0.8843 - val_loss: 0.3821 - val_accuracy: 0.8662 - val_f1_score: 0.8647\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2399 - accuracy: 0.8993 - f1_score: 0.8983 - val_loss: 0.3783 - val_accuracy: 0.8580 - val_f1_score: 0.8534\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1967 - accuracy: 0.9177 - f1_score: 0.9171 - val_loss: 0.4781 - val_accuracy: 0.8418 - val_f1_score: 0.8234\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1839 - accuracy: 0.9259 - f1_score: 0.9252 - val_loss: 0.3856 - val_accuracy: 0.8707 - val_f1_score: 0.8699\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1562 - accuracy: 0.9412 - f1_score: 0.9409 - val_loss: 0.4055 - val_accuracy: 0.8680 - val_f1_score: 0.8694\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1646 - accuracy: 0.9367 - f1_score: 0.9366 - val_loss: 0.4155 - val_accuracy: 0.8635 - val_f1_score: 0.8566\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1315 - accuracy: 0.9524 - f1_score: 0.9525 - val_loss: 0.4312 - val_accuracy: 0.8807 - val_f1_score: 0.8764\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3762 - accuracy: 0.8427 - f1_score: 0.8380\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojraqXmTIYke","executionInfo":{"status":"ok","timestamp":1689799349930,"user_tz":-330,"elapsed":118,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1ec15d89-5e34-430f-9d94-fb614bf48206"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8589511513710022, 0.8571428656578064, 0.8426762819290161, 0.8372513651847839, 0.8444846272468567, 0.8526220321655273, 0.8372513651847839, 0.8354430198669434, 0.8399638533592224, 0.8499096035957336, 0.8354430198669434, 0.85352623462677, 0.8481012582778931, 0.8526220321655273, 0.8381555080413818, 0.8336347341537476, 0.8517178893089294, 0.8562387228012085, 0.8453887701034546, 0.8354430198669434, 0.8517178893089294, 0.8327305316925049, 0.849005401134491, 0.8607594966888428, 0.8426762819290161]\n","0.8457142782211303\n","0.008462003751401449\n"]}]},{"cell_type":"code","source":["len(accuracies)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvsgOuzJjq0F","executionInfo":{"status":"ok","timestamp":1689799349931,"user_tz":-330,"elapsed":73,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1fba3975-14ad-41f5-e0b6-3b648417594e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMxaOdIoLZp5","executionInfo":{"status":"ok","timestamp":1689799349932,"user_tz":-330,"elapsed":68,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"5d5dc327-49db-419b-dcb8-878185dab5ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3319709002971649, 0.38325273990631104, 0.3512655794620514, 0.3880942761898041, 0.3880855441093445, 0.3817761242389679, 0.38862714171409607, 0.44042500853538513, 0.38015565276145935, 0.4123273193836212, 0.369392454624176, 0.38108688592910767, 0.3766191005706787, 0.3595510721206665, 0.40423670411109924, 0.3666045069694519, 0.3805488348007202, 0.3413163125514984, 0.39558109641075134, 0.37857839465141296, 0.3361966609954834, 0.4226607084274292, 0.3783993124961853, 0.3485203683376312, 0.37620851397514343]\n","0.3784592485427856\n","0.02512900479410843\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4TgmTOeLZst","executionInfo":{"status":"ok","timestamp":1689799349932,"user_tz":-330,"elapsed":61,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3eed8820-6ba3-4801-b4fa-46ce6aecc490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8574039340019226, 0.8571428060531616, 0.8352272510528564, 0.8333333134651184, 0.849386990070343, 0.8434197306632996, 0.8375450372695923, 0.8299064636230469, 0.8406840562820435, 0.8528367877006531, 0.8239844441413879, 0.8460075259208679, 0.8499999642372131, 0.8489341139793396, 0.831291139125824, 0.8163672089576721, 0.8395302295684814, 0.8568856120109558, 0.8409302234649658, 0.8351448774337769, 0.8538323640823364, 0.817374050617218, 0.8520814180374146, 0.857933521270752, 0.8379887938499451]\n","0.8418068742752075\n","0.01191095317049628\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FuIg5gAeLZvx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AID3ymFX_N1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J2hT-DuKKsUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM CNN LSTM nothing"],"metadata":{"id":"EggUFlYlLH1S"}},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","undersample_seed = [1,2,3,4,5]\n","split_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in undersample_seed:\n","    Xtemp = final_df.drop('label', axis=1)\n","    ytemp = final_df['label']\n","    undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","    X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","    final_df = X_undersampled.copy()\n","    final_df['label'] = y_undersampled\n","    #print(final_df.shape, final_df['label'].value_counts())\n","\n","    for j in split_seed:\n","\n","        X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc = final_df.loc[:, 'WC':'Emoji']\n","        X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","        X_emotions = final_df.loc[:, 'admiration':'neutral']\n","        X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y = final_df['label']\n","\n","        # # train test split with stratification\n","        # X_sentemb_train, X_sentemb_test, X_liwc_train, X_liwc_test, X_emotions_train, X_emotions_test, X_intensity_train, X_intensity_test, y_train, y_test = train_test_split(\n","        #     X_sentemb, X_liwc, X_emotions, X_intensity, y, test_size=0.2, random_state=j, stratify=y)\n","\n","        # Split into remaining data and test\n","        X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=j, stratify=y)\n","        X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=j, stratify=y)\n","        X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=j, stratify=y)\n","        X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=j, stratify=y)\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train val test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Standardization of train val test LIWC features\n","        train_scaled_liwc = sc.fit_transform(X_liwc_train)\n","        val_scaled_liwc = sc.fit_transform(X_liwc_val)\n","        test_scaled_liwc = sc.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, lstm, nothing\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFfMnUlGKsXW","executionInfo":{"status":"ok","timestamp":1689799941736,"user_tz":-330,"elapsed":591854,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d565f770-98f4-4bb2-c1f1-d0657d72eba2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.5710 - accuracy: 0.7031 - f1_score: 0.7027 - val_loss: 0.5145 - val_accuracy: 0.7523 - val_f1_score: 0.7355\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4614 - accuracy: 0.7800 - f1_score: 0.7730 - val_loss: 0.4770 - val_accuracy: 0.7667 - val_f1_score: 0.7580\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3969 - accuracy: 0.8204 - f1_score: 0.8180 - val_loss: 0.4678 - val_accuracy: 0.7929 - val_f1_score: 0.7739\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3383 - accuracy: 0.8541 - f1_score: 0.8523 - val_loss: 0.4542 - val_accuracy: 0.8083 - val_f1_score: 0.8127\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2891 - accuracy: 0.8749 - f1_score: 0.8730 - val_loss: 0.4182 - val_accuracy: 0.8363 - val_f1_score: 0.8284\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2478 - accuracy: 0.8978 - f1_score: 0.8966 - val_loss: 0.4502 - val_accuracy: 0.8201 - val_f1_score: 0.8222\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2315 - accuracy: 0.9075 - f1_score: 0.9061 - val_loss: 0.4975 - val_accuracy: 0.8119 - val_f1_score: 0.8261\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2008 - accuracy: 0.9225 - f1_score: 0.9219 - val_loss: 0.4471 - val_accuracy: 0.8436 - val_f1_score: 0.8459\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1868 - accuracy: 0.9289 - f1_score: 0.9284 - val_loss: 0.4747 - val_accuracy: 0.8391 - val_f1_score: 0.8382\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1566 - accuracy: 0.9394 - f1_score: 0.9390 - val_loss: 0.5190 - val_accuracy: 0.8336 - val_f1_score: 0.8348\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3563 - accuracy: 0.8544 - f1_score: 0.8429\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.5765 - accuracy: 0.7007 - f1_score: 0.6932 - val_loss: 0.4899 - val_accuracy: 0.7613 - val_f1_score: 0.7704\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4774 - accuracy: 0.7743 - f1_score: 0.7680 - val_loss: 0.4482 - val_accuracy: 0.7812 - val_f1_score: 0.7858\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4106 - accuracy: 0.8125 - f1_score: 0.8057 - val_loss: 0.4071 - val_accuracy: 0.8128 - val_f1_score: 0.7996\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3459 - accuracy: 0.8478 - f1_score: 0.8445 - val_loss: 0.3859 - val_accuracy: 0.8237 - val_f1_score: 0.8229\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3004 - accuracy: 0.8822 - f1_score: 0.8796 - val_loss: 0.3899 - val_accuracy: 0.8400 - val_f1_score: 0.8418\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2650 - accuracy: 0.8924 - f1_score: 0.8901 - val_loss: 0.4050 - val_accuracy: 0.8427 - val_f1_score: 0.8284\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2451 - accuracy: 0.8966 - f1_score: 0.8946 - val_loss: 0.3963 - val_accuracy: 0.8409 - val_f1_score: 0.8475\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2218 - accuracy: 0.9150 - f1_score: 0.9140 - val_loss: 0.4201 - val_accuracy: 0.8481 - val_f1_score: 0.8574\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1991 - accuracy: 0.9180 - f1_score: 0.9167 - val_loss: 0.3757 - val_accuracy: 0.8716 - val_f1_score: 0.8711\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1827 - accuracy: 0.9310 - f1_score: 0.9301 - val_loss: 0.3962 - val_accuracy: 0.8517 - val_f1_score: 0.8432\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1643 - accuracy: 0.9403 - f1_score: 0.9397 - val_loss: 0.3868 - val_accuracy: 0.8662 - val_f1_score: 0.8642\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1411 - accuracy: 0.9442 - f1_score: 0.9437 - val_loss: 0.4715 - val_accuracy: 0.8499 - val_f1_score: 0.8363\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1375 - accuracy: 0.9488 - f1_score: 0.9482 - val_loss: 0.4349 - val_accuracy: 0.8644 - val_f1_score: 0.8661\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1264 - accuracy: 0.9560 - f1_score: 0.9558 - val_loss: 0.4384 - val_accuracy: 0.8698 - val_f1_score: 0.8664\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4127 - accuracy: 0.8418 - f1_score: 0.8399\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.5998 - accuracy: 0.6733 - f1_score: 0.6717 - val_loss: 0.5105 - val_accuracy: 0.7477 - val_f1_score: 0.7385\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4792 - accuracy: 0.7709 - f1_score: 0.7679 - val_loss: 0.4812 - val_accuracy: 0.7731 - val_f1_score: 0.7643\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4149 - accuracy: 0.8113 - f1_score: 0.8095 - val_loss: 0.4591 - val_accuracy: 0.7984 - val_f1_score: 0.7812\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3594 - accuracy: 0.8439 - f1_score: 0.8428 - val_loss: 0.4294 - val_accuracy: 0.8056 - val_f1_score: 0.8106\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3157 - accuracy: 0.8680 - f1_score: 0.8665 - val_loss: 0.4026 - val_accuracy: 0.8210 - val_f1_score: 0.8248\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2645 - accuracy: 0.8960 - f1_score: 0.8952 - val_loss: 0.4264 - val_accuracy: 0.8318 - val_f1_score: 0.8158\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2299 - accuracy: 0.9078 - f1_score: 0.9069 - val_loss: 0.4061 - val_accuracy: 0.8409 - val_f1_score: 0.8355\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2081 - accuracy: 0.9192 - f1_score: 0.9190 - val_loss: 0.4566 - val_accuracy: 0.8318 - val_f1_score: 0.8173\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1989 - accuracy: 0.9262 - f1_score: 0.9257 - val_loss: 0.4503 - val_accuracy: 0.8418 - val_f1_score: 0.8259\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1690 - accuracy: 0.9319 - f1_score: 0.9313 - val_loss: 0.4397 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","35/35 [==============================] - 0s 5ms/step - loss: 0.4031 - accuracy: 0.8165 - f1_score: 0.8275\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.5906 - accuracy: 0.6881 - f1_score: 0.6842 - val_loss: 0.4907 - val_accuracy: 0.7649 - val_f1_score: 0.7547\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4818 - accuracy: 0.7725 - f1_score: 0.7644 - val_loss: 0.4543 - val_accuracy: 0.7767 - val_f1_score: 0.7508\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4128 - accuracy: 0.8186 - f1_score: 0.8110 - val_loss: 0.4136 - val_accuracy: 0.8101 - val_f1_score: 0.8023\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3582 - accuracy: 0.8493 - f1_score: 0.8453 - val_loss: 0.3925 - val_accuracy: 0.8210 - val_f1_score: 0.8092\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3160 - accuracy: 0.8668 - f1_score: 0.8639 - val_loss: 0.3837 - val_accuracy: 0.8418 - val_f1_score: 0.8372\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2719 - accuracy: 0.8897 - f1_score: 0.8874 - val_loss: 0.3776 - val_accuracy: 0.8499 - val_f1_score: 0.8416\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2407 - accuracy: 0.9027 - f1_score: 0.9009 - val_loss: 0.4049 - val_accuracy: 0.8463 - val_f1_score: 0.8396\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2149 - accuracy: 0.9156 - f1_score: 0.9143 - val_loss: 0.3760 - val_accuracy: 0.8571 - val_f1_score: 0.8507\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1944 - accuracy: 0.9244 - f1_score: 0.9235 - val_loss: 0.4205 - val_accuracy: 0.8580 - val_f1_score: 0.8636\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1658 - accuracy: 0.9340 - f1_score: 0.9335 - val_loss: 0.3922 - val_accuracy: 0.8599 - val_f1_score: 0.8602\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1463 - accuracy: 0.9412 - f1_score: 0.9409 - val_loss: 0.4605 - val_accuracy: 0.8617 - val_f1_score: 0.8638\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1390 - accuracy: 0.9451 - f1_score: 0.9449 - val_loss: 0.5841 - val_accuracy: 0.8291 - val_f1_score: 0.8450\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1226 - accuracy: 0.9566 - f1_score: 0.9564 - val_loss: 0.5743 - val_accuracy: 0.8391 - val_f1_score: 0.8486\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3971 - accuracy: 0.8382 - f1_score: 0.8260\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6057 - accuracy: 0.6790 - f1_score: 0.6918 - val_loss: 0.5208 - val_accuracy: 0.7369 - val_f1_score: 0.7110\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4853 - accuracy: 0.7722 - f1_score: 0.7595 - val_loss: 0.4887 - val_accuracy: 0.7676 - val_f1_score: 0.7687\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4150 - accuracy: 0.8162 - f1_score: 0.8100 - val_loss: 0.4488 - val_accuracy: 0.7911 - val_f1_score: 0.7688\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3405 - accuracy: 0.8499 - f1_score: 0.8456 - val_loss: 0.4372 - val_accuracy: 0.8210 - val_f1_score: 0.8163\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2976 - accuracy: 0.8722 - f1_score: 0.8703 - val_loss: 0.3910 - val_accuracy: 0.8327 - val_f1_score: 0.8256\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2612 - accuracy: 0.8963 - f1_score: 0.8951 - val_loss: 0.4052 - val_accuracy: 0.8391 - val_f1_score: 0.8405\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2305 - accuracy: 0.9117 - f1_score: 0.9106 - val_loss: 0.4297 - val_accuracy: 0.8327 - val_f1_score: 0.8156\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2093 - accuracy: 0.9123 - f1_score: 0.9115 - val_loss: 0.4172 - val_accuracy: 0.8336 - val_f1_score: 0.8189\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1820 - accuracy: 0.9244 - f1_score: 0.9235 - val_loss: 0.3968 - val_accuracy: 0.8508 - val_f1_score: 0.8512\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1751 - accuracy: 0.9319 - f1_score: 0.9313 - val_loss: 0.3959 - val_accuracy: 0.8517 - val_f1_score: 0.8462\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3749 - accuracy: 0.8363 - f1_score: 0.8335\n","Epoch 1/20\n","104/104 [==============================] - 8s 24ms/step - loss: 0.6013 - accuracy: 0.6787 - f1_score: 0.6816 - val_loss: 0.5282 - val_accuracy: 0.7297 - val_f1_score: 0.7192\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.5019 - accuracy: 0.7595 - f1_score: 0.7531 - val_loss: 0.4884 - val_accuracy: 0.7577 - val_f1_score: 0.7320\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4322 - accuracy: 0.7999 - f1_score: 0.7931 - val_loss: 0.4788 - val_accuracy: 0.7685 - val_f1_score: 0.7852\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3776 - accuracy: 0.8288 - f1_score: 0.8249 - val_loss: 0.4405 - val_accuracy: 0.7920 - val_f1_score: 0.7866\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3172 - accuracy: 0.8602 - f1_score: 0.8589 - val_loss: 0.4339 - val_accuracy: 0.8165 - val_f1_score: 0.7988\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2874 - accuracy: 0.8891 - f1_score: 0.8877 - val_loss: 0.4107 - val_accuracy: 0.8201 - val_f1_score: 0.8081\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2495 - accuracy: 0.9008 - f1_score: 0.8995 - val_loss: 0.3890 - val_accuracy: 0.8382 - val_f1_score: 0.8406\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2115 - accuracy: 0.9126 - f1_score: 0.9120 - val_loss: 0.4396 - val_accuracy: 0.8282 - val_f1_score: 0.8119\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2018 - accuracy: 0.9183 - f1_score: 0.9177 - val_loss: 0.4163 - val_accuracy: 0.8400 - val_f1_score: 0.8410\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1637 - accuracy: 0.9388 - f1_score: 0.9384 - val_loss: 0.4686 - val_accuracy: 0.8309 - val_f1_score: 0.8264\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1602 - accuracy: 0.9355 - f1_score: 0.9350 - val_loss: 0.4326 - val_accuracy: 0.8363 - val_f1_score: 0.8316\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1368 - accuracy: 0.9497 - f1_score: 0.9493 - val_loss: 0.4631 - val_accuracy: 0.8418 - val_f1_score: 0.8387\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3576 - accuracy: 0.8517 - f1_score: 0.8523\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.5964 - accuracy: 0.6769 - f1_score: 0.6752 - val_loss: 0.4889 - val_accuracy: 0.7685 - val_f1_score: 0.7668\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4758 - accuracy: 0.7788 - f1_score: 0.7730 - val_loss: 0.4564 - val_accuracy: 0.7812 - val_f1_score: 0.7627\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4085 - accuracy: 0.8168 - f1_score: 0.8100 - val_loss: 0.4362 - val_accuracy: 0.8101 - val_f1_score: 0.8112\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3614 - accuracy: 0.8388 - f1_score: 0.8342 - val_loss: 0.4158 - val_accuracy: 0.8273 - val_f1_score: 0.8210\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3192 - accuracy: 0.8683 - f1_score: 0.8663 - val_loss: 0.4242 - val_accuracy: 0.8309 - val_f1_score: 0.8221\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2733 - accuracy: 0.8764 - f1_score: 0.8742 - val_loss: 0.4209 - val_accuracy: 0.8409 - val_f1_score: 0.8321\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2422 - accuracy: 0.9002 - f1_score: 0.8987 - val_loss: 0.4650 - val_accuracy: 0.8336 - val_f1_score: 0.8160\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2267 - accuracy: 0.9048 - f1_score: 0.9028 - val_loss: 0.4400 - val_accuracy: 0.8427 - val_f1_score: 0.8330\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1988 - accuracy: 0.9222 - f1_score: 0.9211 - val_loss: 0.4409 - val_accuracy: 0.8562 - val_f1_score: 0.8535\n","35/35 [==============================] - 0s 4ms/step - loss: 0.4165 - accuracy: 0.8165 - f1_score: 0.8112\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.5990 - accuracy: 0.6799 - f1_score: 0.6768 - val_loss: 0.5253 - val_accuracy: 0.7414 - val_f1_score: 0.7250\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4890 - accuracy: 0.7631 - f1_score: 0.7535 - val_loss: 0.4821 - val_accuracy: 0.7703 - val_f1_score: 0.7495\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4240 - accuracy: 0.8104 - f1_score: 0.8042 - val_loss: 0.4368 - val_accuracy: 0.8137 - val_f1_score: 0.8089\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3686 - accuracy: 0.8379 - f1_score: 0.8336 - val_loss: 0.4211 - val_accuracy: 0.8047 - val_f1_score: 0.8138\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3200 - accuracy: 0.8629 - f1_score: 0.8606 - val_loss: 0.3876 - val_accuracy: 0.8391 - val_f1_score: 0.8373\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2890 - accuracy: 0.8885 - f1_score: 0.8870 - val_loss: 0.3904 - val_accuracy: 0.8336 - val_f1_score: 0.8380\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2554 - accuracy: 0.8975 - f1_score: 0.8958 - val_loss: 0.3734 - val_accuracy: 0.8481 - val_f1_score: 0.8489\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2227 - accuracy: 0.9156 - f1_score: 0.9154 - val_loss: 0.3859 - val_accuracy: 0.8544 - val_f1_score: 0.8546\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1913 - accuracy: 0.9256 - f1_score: 0.9250 - val_loss: 0.4068 - val_accuracy: 0.8472 - val_f1_score: 0.8462\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1819 - accuracy: 0.9259 - f1_score: 0.9250 - val_loss: 0.3791 - val_accuracy: 0.8752 - val_f1_score: 0.8713\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1509 - accuracy: 0.9464 - f1_score: 0.9461 - val_loss: 0.4283 - val_accuracy: 0.8644 - val_f1_score: 0.8588\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1560 - accuracy: 0.9412 - f1_score: 0.9407 - val_loss: 0.4355 - val_accuracy: 0.8580 - val_f1_score: 0.8553\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3692 - accuracy: 0.8599 - f1_score: 0.8634\n","Epoch 1/20\n","104/104 [==============================] - 9s 24ms/step - loss: 0.5947 - accuracy: 0.6775 - f1_score: 0.6879 - val_loss: 0.4993 - val_accuracy: 0.7577 - val_f1_score: 0.7413\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4795 - accuracy: 0.7809 - f1_score: 0.7725 - val_loss: 0.4459 - val_accuracy: 0.7776 - val_f1_score: 0.7684\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4041 - accuracy: 0.8279 - f1_score: 0.8229 - val_loss: 0.4107 - val_accuracy: 0.8165 - val_f1_score: 0.8183\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3459 - accuracy: 0.8593 - f1_score: 0.8543 - val_loss: 0.3945 - val_accuracy: 0.8300 - val_f1_score: 0.8351\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2953 - accuracy: 0.8797 - f1_score: 0.8771 - val_loss: 0.3890 - val_accuracy: 0.8445 - val_f1_score: 0.8459\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2634 - accuracy: 0.8948 - f1_score: 0.8926 - val_loss: 0.4077 - val_accuracy: 0.8400 - val_f1_score: 0.8454\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2404 - accuracy: 0.8996 - f1_score: 0.8979 - val_loss: 0.3885 - val_accuracy: 0.8508 - val_f1_score: 0.8474\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2159 - accuracy: 0.9111 - f1_score: 0.9094 - val_loss: 0.3772 - val_accuracy: 0.8626 - val_f1_score: 0.8648\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1984 - accuracy: 0.9222 - f1_score: 0.9212 - val_loss: 0.4004 - val_accuracy: 0.8562 - val_f1_score: 0.8587\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1759 - accuracy: 0.9304 - f1_score: 0.9293 - val_loss: 0.4213 - val_accuracy: 0.8526 - val_f1_score: 0.8533\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1662 - accuracy: 0.9331 - f1_score: 0.9322 - val_loss: 0.4165 - val_accuracy: 0.8662 - val_f1_score: 0.8679\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1477 - accuracy: 0.9433 - f1_score: 0.9432 - val_loss: 0.5141 - val_accuracy: 0.8436 - val_f1_score: 0.8497\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1278 - accuracy: 0.9479 - f1_score: 0.9477 - val_loss: 0.5098 - val_accuracy: 0.8508 - val_f1_score: 0.8531\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3958 - accuracy: 0.8445 - f1_score: 0.8419\n","Epoch 1/20\n","104/104 [==============================] - 8s 28ms/step - loss: 0.5958 - accuracy: 0.6851 - f1_score: 0.6863 - val_loss: 0.5167 - val_accuracy: 0.7532 - val_f1_score: 0.7520\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4797 - accuracy: 0.7773 - f1_score: 0.7670 - val_loss: 0.4808 - val_accuracy: 0.7884 - val_f1_score: 0.7947\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4194 - accuracy: 0.8168 - f1_score: 0.8118 - val_loss: 0.4491 - val_accuracy: 0.7984 - val_f1_score: 0.7986\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3601 - accuracy: 0.8418 - f1_score: 0.8363 - val_loss: 0.4364 - val_accuracy: 0.8156 - val_f1_score: 0.8012\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3185 - accuracy: 0.8671 - f1_score: 0.8646 - val_loss: 0.4428 - val_accuracy: 0.8219 - val_f1_score: 0.8276\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2805 - accuracy: 0.8822 - f1_score: 0.8803 - val_loss: 0.3977 - val_accuracy: 0.8373 - val_f1_score: 0.8324\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2433 - accuracy: 0.8963 - f1_score: 0.8948 - val_loss: 0.4085 - val_accuracy: 0.8400 - val_f1_score: 0.8395\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2193 - accuracy: 0.9129 - f1_score: 0.9120 - val_loss: 0.4050 - val_accuracy: 0.8490 - val_f1_score: 0.8494\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1935 - accuracy: 0.9231 - f1_score: 0.9222 - val_loss: 0.4273 - val_accuracy: 0.8427 - val_f1_score: 0.8380\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1760 - accuracy: 0.9328 - f1_score: 0.9324 - val_loss: 0.4502 - val_accuracy: 0.8490 - val_f1_score: 0.8500\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1685 - accuracy: 0.9352 - f1_score: 0.9349 - val_loss: 0.4601 - val_accuracy: 0.8481 - val_f1_score: 0.8427\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8454 - f1_score: 0.8485\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.5960 - accuracy: 0.6860 - f1_score: 0.6846 - val_loss: 0.5097 - val_accuracy: 0.7514 - val_f1_score: 0.7285\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4941 - accuracy: 0.7715 - f1_score: 0.7675 - val_loss: 0.4629 - val_accuracy: 0.7911 - val_f1_score: 0.7819\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4263 - accuracy: 0.8098 - f1_score: 0.8060 - val_loss: 0.4360 - val_accuracy: 0.8002 - val_f1_score: 0.8066\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3583 - accuracy: 0.8436 - f1_score: 0.8420 - val_loss: 0.4074 - val_accuracy: 0.8110 - val_f1_score: 0.7965\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3233 - accuracy: 0.8662 - f1_score: 0.8641 - val_loss: 0.4096 - val_accuracy: 0.8354 - val_f1_score: 0.8363\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2788 - accuracy: 0.8918 - f1_score: 0.8903 - val_loss: 0.4004 - val_accuracy: 0.8409 - val_f1_score: 0.8291\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2529 - accuracy: 0.8957 - f1_score: 0.8952 - val_loss: 0.3979 - val_accuracy: 0.8409 - val_f1_score: 0.8376\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2088 - accuracy: 0.9147 - f1_score: 0.9144 - val_loss: 0.4206 - val_accuracy: 0.8526 - val_f1_score: 0.8509\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1939 - accuracy: 0.9253 - f1_score: 0.9246 - val_loss: 0.4566 - val_accuracy: 0.8282 - val_f1_score: 0.8313\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1703 - accuracy: 0.9331 - f1_score: 0.9327 - val_loss: 0.4556 - val_accuracy: 0.8454 - val_f1_score: 0.8403\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1589 - accuracy: 0.9355 - f1_score: 0.9351 - val_loss: 0.4324 - val_accuracy: 0.8517 - val_f1_score: 0.8484\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1415 - accuracy: 0.9442 - f1_score: 0.9440 - val_loss: 0.4816 - val_accuracy: 0.8508 - val_f1_score: 0.8490\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3623 - accuracy: 0.8445 - f1_score: 0.8393\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6012 - accuracy: 0.6766 - f1_score: 0.6759 - val_loss: 0.5023 - val_accuracy: 0.7586 - val_f1_score: 0.7553\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4768 - accuracy: 0.7767 - f1_score: 0.7707 - val_loss: 0.4580 - val_accuracy: 0.7893 - val_f1_score: 0.7824\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4108 - accuracy: 0.8080 - f1_score: 0.8043 - val_loss: 0.4308 - val_accuracy: 0.8083 - val_f1_score: 0.7917\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3505 - accuracy: 0.8463 - f1_score: 0.8440 - val_loss: 0.4178 - val_accuracy: 0.8165 - val_f1_score: 0.7976\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2967 - accuracy: 0.8746 - f1_score: 0.8730 - val_loss: 0.4361 - val_accuracy: 0.8065 - val_f1_score: 0.8208\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2659 - accuracy: 0.8870 - f1_score: 0.8858 - val_loss: 0.3889 - val_accuracy: 0.8373 - val_f1_score: 0.8330\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2282 - accuracy: 0.9075 - f1_score: 0.9061 - val_loss: 0.3894 - val_accuracy: 0.8472 - val_f1_score: 0.8459\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2107 - accuracy: 0.9141 - f1_score: 0.9131 - val_loss: 0.3988 - val_accuracy: 0.8418 - val_f1_score: 0.8464\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1834 - accuracy: 0.9322 - f1_score: 0.9321 - val_loss: 0.3870 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1747 - accuracy: 0.9343 - f1_score: 0.9340 - val_loss: 0.4579 - val_accuracy: 0.8237 - val_f1_score: 0.8371\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1550 - accuracy: 0.9388 - f1_score: 0.9386 - val_loss: 0.4268 - val_accuracy: 0.8472 - val_f1_score: 0.8519\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1431 - accuracy: 0.9464 - f1_score: 0.9459 - val_loss: 0.4527 - val_accuracy: 0.8427 - val_f1_score: 0.8330\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1228 - accuracy: 0.9524 - f1_score: 0.9522 - val_loss: 0.4792 - val_accuracy: 0.8409 - val_f1_score: 0.8459\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1189 - accuracy: 0.9593 - f1_score: 0.9593 - val_loss: 0.4656 - val_accuracy: 0.8490 - val_f1_score: 0.8513\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4164 - accuracy: 0.8499 - f1_score: 0.8513\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.5960 - accuracy: 0.6860 - f1_score: 0.6835 - val_loss: 0.4980 - val_accuracy: 0.7477 - val_f1_score: 0.7173\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4852 - accuracy: 0.7734 - f1_score: 0.7653 - val_loss: 0.4457 - val_accuracy: 0.7848 - val_f1_score: 0.7812\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4063 - accuracy: 0.8168 - f1_score: 0.8134 - val_loss: 0.4253 - val_accuracy: 0.8029 - val_f1_score: 0.7859\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3490 - accuracy: 0.8562 - f1_score: 0.8533 - val_loss: 0.3872 - val_accuracy: 0.8291 - val_f1_score: 0.8352\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2941 - accuracy: 0.8713 - f1_score: 0.8687 - val_loss: 0.3673 - val_accuracy: 0.8553 - val_f1_score: 0.8459\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2605 - accuracy: 0.8924 - f1_score: 0.8904 - val_loss: 0.3716 - val_accuracy: 0.8454 - val_f1_score: 0.8370\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2291 - accuracy: 0.9057 - f1_score: 0.9041 - val_loss: 0.4461 - val_accuracy: 0.8300 - val_f1_score: 0.8433\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2178 - accuracy: 0.9174 - f1_score: 0.9166 - val_loss: 0.3975 - val_accuracy: 0.8580 - val_f1_score: 0.8462\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1752 - accuracy: 0.9319 - f1_score: 0.9311 - val_loss: 0.4200 - val_accuracy: 0.8626 - val_f1_score: 0.8544\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1685 - accuracy: 0.9364 - f1_score: 0.9356 - val_loss: 0.4033 - val_accuracy: 0.8734 - val_f1_score: 0.8720\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3853 - accuracy: 0.8472 - f1_score: 0.8377\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6082 - accuracy: 0.6658 - f1_score: 0.6612 - val_loss: 0.5249 - val_accuracy: 0.7405 - val_f1_score: 0.7398\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4930 - accuracy: 0.7526 - f1_score: 0.7440 - val_loss: 0.4737 - val_accuracy: 0.7703 - val_f1_score: 0.7548\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4388 - accuracy: 0.7945 - f1_score: 0.7867 - val_loss: 0.4441 - val_accuracy: 0.7920 - val_f1_score: 0.7677\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3741 - accuracy: 0.8382 - f1_score: 0.8324 - val_loss: 0.4289 - val_accuracy: 0.8128 - val_f1_score: 0.7996\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3237 - accuracy: 0.8671 - f1_score: 0.8622 - val_loss: 0.4074 - val_accuracy: 0.8327 - val_f1_score: 0.8379\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2804 - accuracy: 0.8788 - f1_score: 0.8755 - val_loss: 0.3918 - val_accuracy: 0.8490 - val_f1_score: 0.8452\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2593 - accuracy: 0.8936 - f1_score: 0.8912 - val_loss: 0.3861 - val_accuracy: 0.8599 - val_f1_score: 0.8632\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2161 - accuracy: 0.9084 - f1_score: 0.9069 - val_loss: 0.4135 - val_accuracy: 0.8544 - val_f1_score: 0.8540\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1923 - accuracy: 0.9213 - f1_score: 0.9204 - val_loss: 0.4017 - val_accuracy: 0.8662 - val_f1_score: 0.8676\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1665 - accuracy: 0.9361 - f1_score: 0.9352 - val_loss: 0.4267 - val_accuracy: 0.8770 - val_f1_score: 0.8781\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1578 - accuracy: 0.9415 - f1_score: 0.9407 - val_loss: 0.4746 - val_accuracy: 0.8590 - val_f1_score: 0.8610\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1428 - accuracy: 0.9448 - f1_score: 0.9443 - val_loss: 0.5073 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3579 - accuracy: 0.8526 - f1_score: 0.8535\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.5928 - accuracy: 0.6869 - f1_score: 0.6816 - val_loss: 0.5171 - val_accuracy: 0.7459 - val_f1_score: 0.7331\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4777 - accuracy: 0.7703 - f1_score: 0.7604 - val_loss: 0.4802 - val_accuracy: 0.7703 - val_f1_score: 0.7470\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4205 - accuracy: 0.8101 - f1_score: 0.8040 - val_loss: 0.4553 - val_accuracy: 0.8002 - val_f1_score: 0.8025\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3520 - accuracy: 0.8469 - f1_score: 0.8425 - val_loss: 0.4213 - val_accuracy: 0.8165 - val_f1_score: 0.8119\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3105 - accuracy: 0.8692 - f1_score: 0.8661 - val_loss: 0.4107 - val_accuracy: 0.8219 - val_f1_score: 0.8104\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2570 - accuracy: 0.8939 - f1_score: 0.8926 - val_loss: 0.4176 - val_accuracy: 0.8309 - val_f1_score: 0.8270\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2319 - accuracy: 0.9051 - f1_score: 0.9039 - val_loss: 0.4447 - val_accuracy: 0.8192 - val_f1_score: 0.7972\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2178 - accuracy: 0.9144 - f1_score: 0.9130 - val_loss: 0.4249 - val_accuracy: 0.8400 - val_f1_score: 0.8378\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1986 - accuracy: 0.9228 - f1_score: 0.9218 - val_loss: 0.4506 - val_accuracy: 0.8327 - val_f1_score: 0.8181\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1778 - accuracy: 0.9301 - f1_score: 0.9296 - val_loss: 0.4551 - val_accuracy: 0.8300 - val_f1_score: 0.8168\n","35/35 [==============================] - 0s 4ms/step - loss: 0.3518 - accuracy: 0.8454 - f1_score: 0.8409\n","Epoch 1/20\n","104/104 [==============================] - 9s 23ms/step - loss: 0.5917 - accuracy: 0.6784 - f1_score: 0.6762 - val_loss: 0.5062 - val_accuracy: 0.7405 - val_f1_score: 0.7519\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4688 - accuracy: 0.7815 - f1_score: 0.7763 - val_loss: 0.4639 - val_accuracy: 0.7758 - val_f1_score: 0.7851\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3964 - accuracy: 0.8198 - f1_score: 0.8155 - val_loss: 0.4470 - val_accuracy: 0.7920 - val_f1_score: 0.8014\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3434 - accuracy: 0.8514 - f1_score: 0.8475 - val_loss: 0.4092 - val_accuracy: 0.8210 - val_f1_score: 0.8242\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2956 - accuracy: 0.8770 - f1_score: 0.8744 - val_loss: 0.4084 - val_accuracy: 0.8327 - val_f1_score: 0.8219\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2674 - accuracy: 0.8849 - f1_score: 0.8830 - val_loss: 0.4057 - val_accuracy: 0.8300 - val_f1_score: 0.8153\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2323 - accuracy: 0.9063 - f1_score: 0.9049 - val_loss: 0.4102 - val_accuracy: 0.8336 - val_f1_score: 0.8397\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2053 - accuracy: 0.9183 - f1_score: 0.9175 - val_loss: 0.4278 - val_accuracy: 0.8409 - val_f1_score: 0.8483\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1910 - accuracy: 0.9253 - f1_score: 0.9243 - val_loss: 0.4553 - val_accuracy: 0.8409 - val_f1_score: 0.8464\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1652 - accuracy: 0.9364 - f1_score: 0.9358 - val_loss: 0.4575 - val_accuracy: 0.8517 - val_f1_score: 0.8517\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1582 - accuracy: 0.9394 - f1_score: 0.9389 - val_loss: 0.4494 - val_accuracy: 0.8490 - val_f1_score: 0.8420\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4055 - accuracy: 0.8363 - f1_score: 0.8203\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.5950 - accuracy: 0.6802 - f1_score: 0.6820 - val_loss: 0.4832 - val_accuracy: 0.7685 - val_f1_score: 0.7500\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4686 - accuracy: 0.7728 - f1_score: 0.7614 - val_loss: 0.4271 - val_accuracy: 0.7957 - val_f1_score: 0.7860\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4016 - accuracy: 0.8222 - f1_score: 0.8168 - val_loss: 0.4040 - val_accuracy: 0.8165 - val_f1_score: 0.8218\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3522 - accuracy: 0.8448 - f1_score: 0.8415 - val_loss: 0.3772 - val_accuracy: 0.8345 - val_f1_score: 0.8282\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3021 - accuracy: 0.8713 - f1_score: 0.8674 - val_loss: 0.3652 - val_accuracy: 0.8544 - val_f1_score: 0.8553\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2653 - accuracy: 0.8939 - f1_score: 0.8918 - val_loss: 0.3820 - val_accuracy: 0.8463 - val_f1_score: 0.8511\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2443 - accuracy: 0.8999 - f1_score: 0.8987 - val_loss: 0.3792 - val_accuracy: 0.8508 - val_f1_score: 0.8536\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2028 - accuracy: 0.9237 - f1_score: 0.9226 - val_loss: 0.3792 - val_accuracy: 0.8580 - val_f1_score: 0.8517\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1954 - accuracy: 0.9256 - f1_score: 0.9248 - val_loss: 0.3798 - val_accuracy: 0.8599 - val_f1_score: 0.8566\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1794 - accuracy: 0.9289 - f1_score: 0.9280 - val_loss: 0.3752 - val_accuracy: 0.8626 - val_f1_score: 0.8628\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.8409 - f1_score: 0.8370\n","Epoch 1/20\n","104/104 [==============================] - 9s 27ms/step - loss: 0.5947 - accuracy: 0.6802 - f1_score: 0.6728 - val_loss: 0.4978 - val_accuracy: 0.7559 - val_f1_score: 0.7523\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4812 - accuracy: 0.7664 - f1_score: 0.7604 - val_loss: 0.4557 - val_accuracy: 0.7794 - val_f1_score: 0.7702\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4146 - accuracy: 0.8080 - f1_score: 0.8027 - val_loss: 0.4187 - val_accuracy: 0.8110 - val_f1_score: 0.8000\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3606 - accuracy: 0.8385 - f1_score: 0.8357 - val_loss: 0.4006 - val_accuracy: 0.8255 - val_f1_score: 0.8266\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3138 - accuracy: 0.8668 - f1_score: 0.8653 - val_loss: 0.3813 - val_accuracy: 0.8445 - val_f1_score: 0.8343\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2776 - accuracy: 0.8791 - f1_score: 0.8764 - val_loss: 0.4156 - val_accuracy: 0.8228 - val_f1_score: 0.8339\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2474 - accuracy: 0.9017 - f1_score: 0.9007 - val_loss: 0.3896 - val_accuracy: 0.8553 - val_f1_score: 0.8493\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2034 - accuracy: 0.9150 - f1_score: 0.9141 - val_loss: 0.4167 - val_accuracy: 0.8463 - val_f1_score: 0.8534\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1936 - accuracy: 0.9204 - f1_score: 0.9195 - val_loss: 0.3943 - val_accuracy: 0.8553 - val_f1_score: 0.8488\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1583 - accuracy: 0.9394 - f1_score: 0.9389 - val_loss: 0.4327 - val_accuracy: 0.8535 - val_f1_score: 0.8564\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3671 - accuracy: 0.8391 - f1_score: 0.8336\n","Epoch 1/20\n","104/104 [==============================] - 7s 24ms/step - loss: 0.6198 - accuracy: 0.6582 - f1_score: 0.6564 - val_loss: 0.5343 - val_accuracy: 0.7432 - val_f1_score: 0.7060\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4894 - accuracy: 0.7631 - f1_score: 0.7511 - val_loss: 0.4841 - val_accuracy: 0.7658 - val_f1_score: 0.7333\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4376 - accuracy: 0.8020 - f1_score: 0.7950 - val_loss: 0.4333 - val_accuracy: 0.8038 - val_f1_score: 0.7993\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3681 - accuracy: 0.8430 - f1_score: 0.8385 - val_loss: 0.4227 - val_accuracy: 0.8210 - val_f1_score: 0.8308\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3231 - accuracy: 0.8599 - f1_score: 0.8573 - val_loss: 0.3961 - val_accuracy: 0.8363 - val_f1_score: 0.8278\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2752 - accuracy: 0.8945 - f1_score: 0.8935 - val_loss: 0.3962 - val_accuracy: 0.8454 - val_f1_score: 0.8472\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2430 - accuracy: 0.9036 - f1_score: 0.9020 - val_loss: 0.3987 - val_accuracy: 0.8472 - val_f1_score: 0.8434\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2042 - accuracy: 0.9210 - f1_score: 0.9205 - val_loss: 0.4534 - val_accuracy: 0.8363 - val_f1_score: 0.8438\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1990 - accuracy: 0.9250 - f1_score: 0.9241 - val_loss: 0.4280 - val_accuracy: 0.8526 - val_f1_score: 0.8528\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1805 - accuracy: 0.9304 - f1_score: 0.9299 - val_loss: 0.5281 - val_accuracy: 0.8282 - val_f1_score: 0.8081\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3964 - accuracy: 0.8255 - f1_score: 0.8146\n","Epoch 1/20\n","104/104 [==============================] - 9s 26ms/step - loss: 0.6128 - accuracy: 0.6624 - f1_score: 0.6368 - val_loss: 0.5155 - val_accuracy: 0.7559 - val_f1_score: 0.7343\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4879 - accuracy: 0.7725 - f1_score: 0.7624 - val_loss: 0.4646 - val_accuracy: 0.7767 - val_f1_score: 0.7557\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4191 - accuracy: 0.8086 - f1_score: 0.8014 - val_loss: 0.4285 - val_accuracy: 0.8020 - val_f1_score: 0.8039\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3502 - accuracy: 0.8457 - f1_score: 0.8426 - val_loss: 0.4266 - val_accuracy: 0.8165 - val_f1_score: 0.8245\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3039 - accuracy: 0.8755 - f1_score: 0.8740 - val_loss: 0.3799 - val_accuracy: 0.8454 - val_f1_score: 0.8385\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2626 - accuracy: 0.8909 - f1_score: 0.8896 - val_loss: 0.3745 - val_accuracy: 0.8481 - val_f1_score: 0.8406\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2352 - accuracy: 0.9039 - f1_score: 0.9034 - val_loss: 0.3676 - val_accuracy: 0.8526 - val_f1_score: 0.8500\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2099 - accuracy: 0.9174 - f1_score: 0.9169 - val_loss: 0.4418 - val_accuracy: 0.8219 - val_f1_score: 0.8365\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1910 - accuracy: 0.9216 - f1_score: 0.9213 - val_loss: 0.4042 - val_accuracy: 0.8481 - val_f1_score: 0.8475\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1838 - accuracy: 0.9307 - f1_score: 0.9300 - val_loss: 0.4706 - val_accuracy: 0.8137 - val_f1_score: 0.8242\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1536 - accuracy: 0.9403 - f1_score: 0.9401 - val_loss: 0.4231 - val_accuracy: 0.8499 - val_f1_score: 0.8471\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1348 - accuracy: 0.9503 - f1_score: 0.9503 - val_loss: 0.4821 - val_accuracy: 0.8553 - val_f1_score: 0.8513\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3896 - accuracy: 0.8445 - f1_score: 0.8467\n","Epoch 1/20\n","104/104 [==============================] - 8s 27ms/step - loss: 0.5789 - accuracy: 0.7080 - f1_score: 0.7065 - val_loss: 0.5278 - val_accuracy: 0.7297 - val_f1_score: 0.7219\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4854 - accuracy: 0.7688 - f1_score: 0.7620 - val_loss: 0.4908 - val_accuracy: 0.7550 - val_f1_score: 0.7215\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4169 - accuracy: 0.8128 - f1_score: 0.8072 - val_loss: 0.4510 - val_accuracy: 0.7984 - val_f1_score: 0.8000\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3597 - accuracy: 0.8451 - f1_score: 0.8418 - val_loss: 0.4150 - val_accuracy: 0.8201 - val_f1_score: 0.8096\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3100 - accuracy: 0.8638 - f1_score: 0.8610 - val_loss: 0.4007 - val_accuracy: 0.8373 - val_f1_score: 0.8330\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2675 - accuracy: 0.8855 - f1_score: 0.8846 - val_loss: 0.4033 - val_accuracy: 0.8327 - val_f1_score: 0.8233\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2388 - accuracy: 0.9024 - f1_score: 0.9006 - val_loss: 0.4256 - val_accuracy: 0.8345 - val_f1_score: 0.8405\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2007 - accuracy: 0.9147 - f1_score: 0.9135 - val_loss: 0.4372 - val_accuracy: 0.8490 - val_f1_score: 0.8414\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1772 - accuracy: 0.9253 - f1_score: 0.9247 - val_loss: 0.4289 - val_accuracy: 0.8553 - val_f1_score: 0.8491\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1569 - accuracy: 0.9379 - f1_score: 0.9377 - val_loss: 0.4504 - val_accuracy: 0.8445 - val_f1_score: 0.8467\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3582 - accuracy: 0.8373 - f1_score: 0.8249\n","Epoch 1/20\n","104/104 [==============================] - 7s 22ms/step - loss: 0.5812 - accuracy: 0.6896 - f1_score: 0.6922 - val_loss: 0.4917 - val_accuracy: 0.7604 - val_f1_score: 0.7512\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4783 - accuracy: 0.7839 - f1_score: 0.7775 - val_loss: 0.4557 - val_accuracy: 0.7749 - val_f1_score: 0.7537\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4166 - accuracy: 0.8125 - f1_score: 0.8064 - val_loss: 0.4204 - val_accuracy: 0.8074 - val_f1_score: 0.7989\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3509 - accuracy: 0.8421 - f1_score: 0.8394 - val_loss: 0.4008 - val_accuracy: 0.8237 - val_f1_score: 0.8105\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3126 - accuracy: 0.8719 - f1_score: 0.8701 - val_loss: 0.3824 - val_accuracy: 0.8300 - val_f1_score: 0.8223\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2804 - accuracy: 0.8858 - f1_score: 0.8836 - val_loss: 0.3625 - val_accuracy: 0.8409 - val_f1_score: 0.8336\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2442 - accuracy: 0.9027 - f1_score: 0.9011 - val_loss: 0.3661 - val_accuracy: 0.8472 - val_f1_score: 0.8413\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2239 - accuracy: 0.9066 - f1_score: 0.9055 - val_loss: 0.3948 - val_accuracy: 0.8454 - val_f1_score: 0.8435\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2028 - accuracy: 0.9198 - f1_score: 0.9190 - val_loss: 0.3579 - val_accuracy: 0.8608 - val_f1_score: 0.8579\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1688 - accuracy: 0.9373 - f1_score: 0.9365 - val_loss: 0.3868 - val_accuracy: 0.8508 - val_f1_score: 0.8387\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1617 - accuracy: 0.9391 - f1_score: 0.9386 - val_loss: 0.3910 - val_accuracy: 0.8553 - val_f1_score: 0.8513\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1527 - accuracy: 0.9403 - f1_score: 0.9400 - val_loss: 0.3993 - val_accuracy: 0.8580 - val_f1_score: 0.8584\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1176 - accuracy: 0.9587 - f1_score: 0.9584 - val_loss: 0.4057 - val_accuracy: 0.8626 - val_f1_score: 0.8603\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1153 - accuracy: 0.9575 - f1_score: 0.9573 - val_loss: 0.4929 - val_accuracy: 0.8409 - val_f1_score: 0.8271\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3706 - accuracy: 0.8590 - f1_score: 0.8561\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.5853 - accuracy: 0.6829 - f1_score: 0.6789 - val_loss: 0.5089 - val_accuracy: 0.7477 - val_f1_score: 0.7400\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4736 - accuracy: 0.7773 - f1_score: 0.7656 - val_loss: 0.4679 - val_accuracy: 0.7839 - val_f1_score: 0.7739\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4146 - accuracy: 0.8122 - f1_score: 0.8056 - val_loss: 0.4351 - val_accuracy: 0.8065 - val_f1_score: 0.7847\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3652 - accuracy: 0.8324 - f1_score: 0.8288 - val_loss: 0.3917 - val_accuracy: 0.8273 - val_f1_score: 0.8193\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3211 - accuracy: 0.8596 - f1_score: 0.8568 - val_loss: 0.4307 - val_accuracy: 0.8011 - val_f1_score: 0.8185\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2724 - accuracy: 0.8870 - f1_score: 0.8853 - val_loss: 0.3747 - val_accuracy: 0.8436 - val_f1_score: 0.8476\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2441 - accuracy: 0.8957 - f1_score: 0.8941 - val_loss: 0.3866 - val_accuracy: 0.8418 - val_f1_score: 0.8487\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2014 - accuracy: 0.9219 - f1_score: 0.9215 - val_loss: 0.3913 - val_accuracy: 0.8445 - val_f1_score: 0.8320\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2070 - accuracy: 0.9228 - f1_score: 0.9220 - val_loss: 0.3693 - val_accuracy: 0.8617 - val_f1_score: 0.8569\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1554 - accuracy: 0.9379 - f1_score: 0.9373 - val_loss: 0.4563 - val_accuracy: 0.8264 - val_f1_score: 0.8392\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1517 - accuracy: 0.9421 - f1_score: 0.9417 - val_loss: 0.4461 - val_accuracy: 0.8653 - val_f1_score: 0.8614\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1476 - accuracy: 0.9436 - f1_score: 0.9433 - val_loss: 0.4460 - val_accuracy: 0.8508 - val_f1_score: 0.8439\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1278 - accuracy: 0.9518 - f1_score: 0.9516 - val_loss: 0.5009 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1118 - accuracy: 0.9614 - f1_score: 0.9613 - val_loss: 0.5912 - val_accuracy: 0.8165 - val_f1_score: 0.8307\n","35/35 [==============================] - 0s 5ms/step - loss: 0.3892 - accuracy: 0.8571 - f1_score: 0.8540\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6087 - accuracy: 0.6727 - f1_score: 0.6715 - val_loss: 0.5167 - val_accuracy: 0.7523 - val_f1_score: 0.7198\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5079 - accuracy: 0.7523 - f1_score: 0.7450 - val_loss: 0.4596 - val_accuracy: 0.7740 - val_f1_score: 0.7596\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4388 - accuracy: 0.7963 - f1_score: 0.7897 - val_loss: 0.4381 - val_accuracy: 0.7993 - val_f1_score: 0.8049\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3927 - accuracy: 0.8297 - f1_score: 0.8252 - val_loss: 0.4150 - val_accuracy: 0.8165 - val_f1_score: 0.8163\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3347 - accuracy: 0.8541 - f1_score: 0.8512 - val_loss: 0.4141 - val_accuracy: 0.8192 - val_f1_score: 0.8239\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2891 - accuracy: 0.8831 - f1_score: 0.8803 - val_loss: 0.3938 - val_accuracy: 0.8535 - val_f1_score: 0.8561\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2563 - accuracy: 0.8951 - f1_score: 0.8934 - val_loss: 0.4102 - val_accuracy: 0.8409 - val_f1_score: 0.8295\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2195 - accuracy: 0.9147 - f1_score: 0.9137 - val_loss: 0.4186 - val_accuracy: 0.8499 - val_f1_score: 0.8446\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2038 - accuracy: 0.9192 - f1_score: 0.9185 - val_loss: 0.4336 - val_accuracy: 0.8580 - val_f1_score: 0.8609\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1854 - accuracy: 0.9201 - f1_score: 0.9196 - val_loss: 0.4353 - val_accuracy: 0.8599 - val_f1_score: 0.8569\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1475 - accuracy: 0.9433 - f1_score: 0.9432 - val_loss: 0.5505 - val_accuracy: 0.8382 - val_f1_score: 0.8208\n","35/35 [==============================] - 0s 9ms/step - loss: 0.3801 - accuracy: 0.8481 - f1_score: 0.8492\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.5831 - accuracy: 0.7004 - f1_score: 0.6993 - val_loss: 0.5178 - val_accuracy: 0.7495 - val_f1_score: 0.7409\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4731 - accuracy: 0.7737 - f1_score: 0.7648 - val_loss: 0.5013 - val_accuracy: 0.7577 - val_f1_score: 0.7649\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4119 - accuracy: 0.8119 - f1_score: 0.8063 - val_loss: 0.4800 - val_accuracy: 0.7758 - val_f1_score: 0.7592\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3588 - accuracy: 0.8466 - f1_score: 0.8424 - val_loss: 0.4492 - val_accuracy: 0.7884 - val_f1_score: 0.7715\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3132 - accuracy: 0.8689 - f1_score: 0.8660 - val_loss: 0.4303 - val_accuracy: 0.8156 - val_f1_score: 0.8061\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2878 - accuracy: 0.8785 - f1_score: 0.8756 - val_loss: 0.4269 - val_accuracy: 0.8092 - val_f1_score: 0.7941\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2430 - accuracy: 0.8975 - f1_score: 0.8961 - val_loss: 0.4176 - val_accuracy: 0.8336 - val_f1_score: 0.8296\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2104 - accuracy: 0.9138 - f1_score: 0.9121 - val_loss: 0.4327 - val_accuracy: 0.8373 - val_f1_score: 0.8336\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2009 - accuracy: 0.9216 - f1_score: 0.9209 - val_loss: 0.4106 - val_accuracy: 0.8400 - val_f1_score: 0.8384\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1796 - accuracy: 0.9286 - f1_score: 0.9279 - val_loss: 0.4473 - val_accuracy: 0.8418 - val_f1_score: 0.8269\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1589 - accuracy: 0.9394 - f1_score: 0.9388 - val_loss: 0.4407 - val_accuracy: 0.8409 - val_f1_score: 0.8400\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1489 - accuracy: 0.9409 - f1_score: 0.9406 - val_loss: 0.4572 - val_accuracy: 0.8363 - val_f1_score: 0.8251\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1317 - accuracy: 0.9506 - f1_score: 0.9504 - val_loss: 0.4848 - val_accuracy: 0.8553 - val_f1_score: 0.8521\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1058 - accuracy: 0.9626 - f1_score: 0.9625 - val_loss: 0.4646 - val_accuracy: 0.8436 - val_f1_score: 0.8420\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4060 - accuracy: 0.8445 - f1_score: 0.8470\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjABEI6qLr5T","executionInfo":{"status":"ok","timestamp":1689799941737,"user_tz":-330,"elapsed":77,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"67f9e77c-4ed6-4ba7-c6cf-5e15cd38ab85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8544303774833679, 0.8417721390724182, 0.8164557218551636, 0.8381555080413818, 0.836347222328186, 0.8517178893089294, 0.8164557218551636, 0.8598553538322449, 0.8444846272468567, 0.8453887701034546, 0.8444846272468567, 0.8499096035957336, 0.8471971154212952, 0.8526220321655273, 0.8453887701034546, 0.836347222328186, 0.8408679962158203, 0.8390596508979797, 0.8254972696304321, 0.8444846272468567, 0.8372513651847839, 0.8589511513710022, 0.8571428656578064, 0.8481012582778931, 0.8444846272468567]\n","0.8430741405487061\n","0.011011687255871469\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uiahc2vrLs-u","executionInfo":{"status":"ok","timestamp":1689799941739,"user_tz":-330,"elapsed":35,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"59e5d156-70f9-45bf-d4ac-56d37142d0ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.35627761483192444, 0.4127177596092224, 0.403129518032074, 0.397052139043808, 0.37489116191864014, 0.35760244727134705, 0.41652023792266846, 0.3691675364971161, 0.39579859375953674, 0.33659103512763977, 0.36234626173973083, 0.41644659638404846, 0.3852972984313965, 0.35785314440727234, 0.35181668400764465, 0.4055047631263733, 0.3663147985935211, 0.3670716881752014, 0.39640843868255615, 0.3895745873451233, 0.3581677973270416, 0.37061578035354614, 0.3891719877719879, 0.3801259696483612, 0.40599003434181213]\n","0.38089815497398377\n","0.022085790724384633\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKqV7KpzLtBK","executionInfo":{"status":"ok","timestamp":1689799941740,"user_tz":-330,"elapsed":30,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"4aff8d5d-0f82-4696-b29f-46805489d119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8429268002510071, 0.8398901224136353, 0.8275275826454163, 0.8260446190834045, 0.8334866166114807, 0.8522521257400513, 0.8111627697944641, 0.8634359836578369, 0.8419117331504822, 0.8485384583473206, 0.8392522931098938, 0.851254403591156, 0.8376560211181641, 0.8535488843917847, 0.8409302234649658, 0.8202581405639648, 0.8370370268821716, 0.8336447477340698, 0.8146013617515564, 0.8467022180557251, 0.8249027132987976, 0.8560884594917297, 0.8539741039276123, 0.8491920232772827, 0.8469750285148621]\n","0.8397277784347534\n","0.013066335938889595\n"]}]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing  -\n","1. Accuracy - 0.845/0.008\n","2. Loss - 0.378/0.025\n","3. F1 score - 0.841/0.011\n","\n","LSTM CNN LSTM nothing  -\n","1. Accuracy - 0.843/0.011\n","2. Loss - 0.380/0.022\n","3. F1 score - 0.839/0.013"],"metadata":{"id":"m774qhlwOTZ7"}},{"cell_type":"code","source":[],"metadata":{"id":"i1C9nlrPM1Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Nbgpm0mrUdEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PFReTIQYUsU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bcroaP6CUsXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s0wJUdqYUsZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SKbSWQuEUscd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MZifIAQnUse_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zTrzSX7LUdHF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h2Dmp2bDUdJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8LitKrhZUdL9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# B. fix the loops so that you first (outer loop) choose test set and then in the inner loop you do undersampling and stratification for train/validation (0.6/0.2)"],"metadata":{"id":"CyZIEr399beq"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/IDSIA Biomedical Texts/AllSource_Intensity_ThirdJuly.csv', low_memory=False)\n","df.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"eRViIdKrGRnw","executionInfo":{"status":"ok","timestamp":1689799942243,"user_tz":-330,"elapsed":519,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"206a34fe-d30f-459b-8fa0-30f6e2ef2251"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                urls  \\\n","0  https://www.quora.com/What-are-panic-attacks-l...   \n","\n","                                                text source  label   WC  \\\n","0  i have been dealing with these for quite some ...  Quora      1  607   \n","\n","   Analytic  Clout  Authentic  Tone    WPS  ...  \\\n","0     55.22  35.35      48.82   1.0  26.39  ...   \n","\n","                                      all_emo_labels  \\\n","0  ['fear', 'nervousness', 'confusion', 'curiosit...   \n","\n","                                  all_emo_label_rank  anger_intensity  \\\n","0  {'fear': 1, 'nervousness': 2, 'confusion': 3, ...         0.415048   \n","\n","   anticipation_intensity  disgust_intensity  fear_intensity  joy_intensity  \\\n","0                0.553423           0.272333        0.568205         0.4095   \n","\n","   sadness_intensity  surprise_intensity  trust_intensity  \n","0           0.467625              0.4345         0.522773  \n","\n","[1 rows x 165 columns]"],"text/html":["\n","\n","  <div id=\"df-d09573e3-0f3f-4888-8758-01a76d6c36f9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>urls</th>\n","      <th>text</th>\n","      <th>source</th>\n","      <th>label</th>\n","      <th>WC</th>\n","      <th>Analytic</th>\n","      <th>Clout</th>\n","      <th>Authentic</th>\n","      <th>Tone</th>\n","      <th>WPS</th>\n","      <th>...</th>\n","      <th>all_emo_labels</th>\n","      <th>all_emo_label_rank</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>https://www.quora.com/What-are-panic-attacks-l...</td>\n","      <td>i have been dealing with these for quite some ...</td>\n","      <td>Quora</td>\n","      <td>1</td>\n","      <td>607</td>\n","      <td>55.22</td>\n","      <td>35.35</td>\n","      <td>48.82</td>\n","      <td>1.0</td>\n","      <td>26.39</td>\n","      <td>...</td>\n","      <td>['fear', 'nervousness', 'confusion', 'curiosit...</td>\n","      <td>{'fear': 1, 'nervousness': 2, 'confusion': 3, ...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.4095</td>\n","      <td>0.467625</td>\n","      <td>0.4345</td>\n","      <td>0.522773</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows × 165 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d09573e3-0f3f-4888-8758-01a76d6c36f9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-26f5afc1-d09f-4b57-a0c0-5940a5ba810d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26f5afc1-d09f-4b57-a0c0-5940a5ba810d')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-26f5afc1-d09f-4b57-a0c0-5940a5ba810d button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d09573e3-0f3f-4888-8758-01a76d6c36f9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d09573e3-0f3f-4888-8758-01a76d6c36f9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["import regex as re\n","\n","panic_symptoms = [\"Palpitations\", \"Pounding heart\", \"Accelerated heart rate\", \"Sweating\", \"Trembling\", \"Shaking\", \"Shortness of breath\",\n","\"Smothering\", \"Feelings of choking\", \"Chest pain\", \"Discomfort\", \"Abdominal distress\", \"Nausea\", \"Dizziness\", \"Unsteadiness\", \"Lightheadedness\",\n","\"Faintness\", \"Chills\", \"Heat flashes\", \"Paresthesia\", \"Numbness\", \"Tingling sensations\", \"Derealization\", \"Depersonalization\", \"Fear of losing control\",\n","\"Fear of going crazy\", \"Fear of dying\", \"Mental images of dying\", \"Mental images of collapsing\", \"Agoraphobia\", \"Need to escape\"]\n","\n","\n","panic_symptoms_ext = [\"Palpitations\", \"Pounding heart\", \"Accelerated heart rate\", \"Sweating\", \"Trembling\", \"Shaking\", \"Shortness of breath\",\n","\"Smothering\", \"Feelings of choking\", \"Chest pain\", \"Discomfort\", \"Abdominal distress\", \"Nausea\", \"Dizziness\", \"Unsteadiness\", \"Lightheadedness\",\n","\"Faintness\", \"Chills\", \"Heat flashes\", \"Paresthesia\", \"Numbness\", \"Tingling sensations\", \"Derealization\", \"Depersonalization\", \"Fear of losing control\",\n","\"Fear of going crazy\", \"Fear of dying\", \"Mental images of dying\", \"Mental images of collapsing\", \"Agoraphobia\", \"Need to escape\"\n","\"Sweat\", \"Tremble\", \"Shake\", \"Shortage of breath\",\"Feeling of choking\",\"Dizzy\",\n","\"Faint\",\"Fainted\",\"Chill\",\"Heat flash\", \"Numb\", \"Tingling sensation\",\"Mental image of dying\", \"Mental image of collapsing\"]\n","\n","def count_symptoms_in_text(text, panic_list):\n","    if isinstance(text, str):\n","        count = 0\n","        for symptom in panic_list:\n","            match = re.search(r'\\b{}\\b'.format(symptom), text, re.IGNORECASE)\n","            if match:\n","                count += 1\n","        return count\n","\n","df['symptoms_count'] = df['text'].apply(lambda x: count_symptoms_in_text(x,panic_symptoms))\n","df['symptoms_ext_count'] = df['text'].apply(lambda x: count_symptoms_in_text(x,panic_symptoms_ext))"],"metadata":{"id":"Pd8tEyCLGRvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pickle_in = open(\"/content/drive/MyDrive/IDSIA Biomedical Texts/Sentence Embeddings/AllSource_alldistilrobertav1_via_UMAP_SHORTembeddings.pickle\", 'rb')\n","sentence_embeddings = pickle.load(pickle_in)\n","sentence_embeddings"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcjFZhkjGRya","executionInfo":{"status":"ok","timestamp":1689799946739,"user_tz":-330,"elapsed":119,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1f5a23d8-7c7e-4a2c-ecf1-f0998d5c0cb8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[10.64194  ,  5.0430765,  5.6824026, ...,  4.9058275,  6.8707986,\n","         4.538727 ],\n","       [11.312859 ,  5.364349 ,  4.41365  , ...,  4.92234  ,  6.8475184,\n","         4.5590596],\n","       [10.531799 ,  4.894456 ,  5.387705 , ...,  4.8968716,  6.8360796,\n","         4.530069 ],\n","       ...,\n","       [10.346373 ,  4.4247556,  3.5815325, ...,  5.0401225,  6.552696 ,\n","         4.490976 ],\n","       [10.454275 ,  4.5640407,  3.6035635, ...,  5.0320673,  6.564847 ,\n","         4.4918733],\n","       [11.222271 ,  5.1468487,  4.0054016, ...,  5.079988 ,  6.6341186,\n","         4.5597043]], dtype=float32)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["sentemb_column_names = [\"sentemb\" + str(i+1) for i in range(28)]"],"metadata":{"id":"qZksvGNRGR1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentembdf = pd.DataFrame(sentence_embeddings, columns=sentemb_column_names)\n","stdliwcdf = df.loc[:, 'WC':'Emoji']\n","emodf = df.loc[:, 'admiration':'neutral']\n","intensitydf = df.loc[:, 'anger_intensity':'trust_intensity']\n","final_df = pd.concat([sentembdf, stdliwcdf, emodf, intensitydf, df['symptoms_ext_count']], axis=1)\n","final_df['label'] = df['label']\n","final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"fYCkRTe5GR3n","executionInfo":{"status":"ok","timestamp":1689799946741,"user_tz":-330,"elapsed":113,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7d549eed-3de3-4652-87ae-442667eedc5c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-b8ecff7d-39e2-46f7-813d-d5a589bc20c6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8ecff7d-39e2-46f7-813d-d5a589bc20c6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-38ee47a5-d2bc-47c9-99a6-d1b98eb74d4b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38ee47a5-d2bc-47c9-99a6-d1b98eb74d4b')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-38ee47a5-d2bc-47c9-99a6-d1b98eb74d4b button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b8ecff7d-39e2-46f7-813d-d5a589bc20c6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b8ecff7d-39e2-46f7-813d-d5a589bc20c6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":[],"metadata":{"id":"EmSBgaFBGR6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"AdWT-bhTKvF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIpdMOBoLE5Y","executionInfo":{"status":"ok","timestamp":1689799946746,"user_tz":-330,"elapsed":113,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"5f9a2549-9a66-4e56-f929-16f8299b9516"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    4640\n","0    2765\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":[],"metadata":{"id":"rmcMbhw5SCNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### trying splitting into testing first and then undersamping remanining data and then split again into train val with stratify"],"metadata":{"id":"2NewSrnWSCUC"}},{"cell_type":"code","source":["0.2*4640, 0.2*2765"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVQYDENfLJ7h","executionInfo":{"status":"ok","timestamp":1689799946749,"user_tz":-330,"elapsed":104,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"b5219a4b-38d6-4026-d7ad-b5bdbe55cca2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(928.0, 553.0)"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["# Split into remaining data and test\n","X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=1, stratify=y)\n","X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=1, stratify=y)\n","X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=1, stratify=y)\n","X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=1, stratify=y)"],"metadata":{"id":"UxIdsoR-GR8q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GmVJJzPLbp6","executionInfo":{"status":"ok","timestamp":1689799946752,"user_tz":-330,"elapsed":99,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"176e357c-151f-4316-b3e3-a84b1610e90f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    928\n","0    553\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["7405-0.2*7405"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbCwSobiOiaf","executionInfo":{"status":"ok","timestamp":1689799946753,"user_tz":-330,"elapsed":93,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"5c850424-a362-418b-d6ce-f39c7c26b9a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5924.0"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["X_liwc_remaining.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZsBvSrqGOZDJ","executionInfo":{"status":"ok","timestamp":1689799946755,"user_tz":-330,"elapsed":89,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"734e8753-8df0-453f-bdca-0670e29d3180"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5924, 119)"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"T_EVINe3OAzC","executionInfo":{"status":"ok","timestamp":1689799946756,"user_tz":-330,"elapsed":83,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"b45303ce-58ab-40f9-da82-9d1211bbf281"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","2458   7.575323  2.200696  3.096856  2.900266  5.277917  4.001982  5.428308   \n","1665  10.756251  4.886991  4.574633  3.796134  4.353689  5.205556  6.904582   \n","2529   8.027141  2.072421  2.965644  3.051319  5.493457  4.136465  5.178690   \n","6219  11.417689  5.390684  4.665149  2.582741  4.314730  4.603600  6.142970   \n","6059   9.424075  4.235147  3.038955  3.291266  5.294016  4.973732  5.672050   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","1378  12.357803  6.282046  4.435970  2.665469  4.394099  5.089520  6.256873   \n","5727  11.063497  4.318204  4.369992  1.049330  4.070986  4.288837  5.296537   \n","1062  11.666553  5.678929  4.617547  1.947428  4.241860  4.401950  5.816283   \n","5507   8.162202  2.367997  2.811537  3.122674  5.579625  4.600275  5.148420   \n","1668  10.890951  5.019511  4.159523  3.364110  4.638531  5.071639  6.378169   \n","\n","      sentemb8  sentemb9  sentemb10  ...  AllPunc  Period  Comma  QMark  \\\n","2458  1.371828  2.516677   5.128863  ...    13.70    5.48   4.11    0.0   \n","1665  1.752458  1.522096   6.413411  ...    19.61    6.86   4.90    0.0   \n","2529  1.490584  2.040954   4.939975  ...    10.71    0.00   0.00    0.0   \n","6219  1.749087  1.412425   6.490743  ...    16.67    8.33   8.33    0.0   \n","6059  1.663040  1.937158   5.665931  ...    27.66   17.02   0.00    0.0   \n","...        ...       ...        ...  ...      ...     ...    ...    ...   \n","1378  1.785521  1.451136   6.556712  ...    16.06    3.11   7.77    0.0   \n","5727  1.639845  2.033876   5.621100  ...    23.38    5.19   2.60    1.3   \n","1062  1.774453  1.630935   6.475672  ...    10.87    0.00   4.35    0.0   \n","5507  1.529511  1.877481   4.742156  ...    29.41    8.82   8.82    0.0   \n","1668  1.748661  1.563693   6.232338  ...     6.25    0.00   0.00    0.0   \n","\n","      Exclam  Apostro  OtherP  Emoji  symptoms_ext_count  label  \n","2458    0.00     1.37    2.74    0.0                   0      1  \n","1665    0.00     4.90    2.94    0.0                   0      1  \n","2529    7.14     3.57    0.00    0.0                   0      1  \n","6219    0.00     0.00    0.00    0.0                   0      0  \n","6059    2.13     6.38    2.13    0.0                   0      0  \n","...      ...      ...     ...    ...                 ...    ...  \n","1378    0.00     2.07    3.11    0.0                   0      0  \n","5727    0.00     9.09    5.19    0.0                   0      1  \n","1062    0.00     0.00    6.52    0.0                   2      0  \n","5507    5.15     2.94    3.68    0.0                   0      1  \n","1668    0.00     0.00    6.25    0.0                   0      1  \n","\n","[5924 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-344f4c59-0d0f-4e5c-8c1b-6fe89628bca2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>AllPunc</th>\n","      <th>Period</th>\n","      <th>Comma</th>\n","      <th>QMark</th>\n","      <th>Exclam</th>\n","      <th>Apostro</th>\n","      <th>OtherP</th>\n","      <th>Emoji</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2458</th>\n","      <td>7.575323</td>\n","      <td>2.200696</td>\n","      <td>3.096856</td>\n","      <td>2.900266</td>\n","      <td>5.277917</td>\n","      <td>4.001982</td>\n","      <td>5.428308</td>\n","      <td>1.371828</td>\n","      <td>2.516677</td>\n","      <td>5.128863</td>\n","      <td>...</td>\n","      <td>13.70</td>\n","      <td>5.48</td>\n","      <td>4.11</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>1.37</td>\n","      <td>2.74</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1665</th>\n","      <td>10.756251</td>\n","      <td>4.886991</td>\n","      <td>4.574633</td>\n","      <td>3.796134</td>\n","      <td>4.353689</td>\n","      <td>5.205556</td>\n","      <td>6.904582</td>\n","      <td>1.752458</td>\n","      <td>1.522096</td>\n","      <td>6.413411</td>\n","      <td>...</td>\n","      <td>19.61</td>\n","      <td>6.86</td>\n","      <td>4.90</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>4.90</td>\n","      <td>2.94</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2529</th>\n","      <td>8.027141</td>\n","      <td>2.072421</td>\n","      <td>2.965644</td>\n","      <td>3.051319</td>\n","      <td>5.493457</td>\n","      <td>4.136465</td>\n","      <td>5.178690</td>\n","      <td>1.490584</td>\n","      <td>2.040954</td>\n","      <td>4.939975</td>\n","      <td>...</td>\n","      <td>10.71</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>7.14</td>\n","      <td>3.57</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6219</th>\n","      <td>11.417689</td>\n","      <td>5.390684</td>\n","      <td>4.665149</td>\n","      <td>2.582741</td>\n","      <td>4.314730</td>\n","      <td>4.603600</td>\n","      <td>6.142970</td>\n","      <td>1.749087</td>\n","      <td>1.412425</td>\n","      <td>6.490743</td>\n","      <td>...</td>\n","      <td>16.67</td>\n","      <td>8.33</td>\n","      <td>8.33</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6059</th>\n","      <td>9.424075</td>\n","      <td>4.235147</td>\n","      <td>3.038955</td>\n","      <td>3.291266</td>\n","      <td>5.294016</td>\n","      <td>4.973732</td>\n","      <td>5.672050</td>\n","      <td>1.663040</td>\n","      <td>1.937158</td>\n","      <td>5.665931</td>\n","      <td>...</td>\n","      <td>27.66</td>\n","      <td>17.02</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>2.13</td>\n","      <td>6.38</td>\n","      <td>2.13</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1378</th>\n","      <td>12.357803</td>\n","      <td>6.282046</td>\n","      <td>4.435970</td>\n","      <td>2.665469</td>\n","      <td>4.394099</td>\n","      <td>5.089520</td>\n","      <td>6.256873</td>\n","      <td>1.785521</td>\n","      <td>1.451136</td>\n","      <td>6.556712</td>\n","      <td>...</td>\n","      <td>16.06</td>\n","      <td>3.11</td>\n","      <td>7.77</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>2.07</td>\n","      <td>3.11</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5727</th>\n","      <td>11.063497</td>\n","      <td>4.318204</td>\n","      <td>4.369992</td>\n","      <td>1.049330</td>\n","      <td>4.070986</td>\n","      <td>4.288837</td>\n","      <td>5.296537</td>\n","      <td>1.639845</td>\n","      <td>2.033876</td>\n","      <td>5.621100</td>\n","      <td>...</td>\n","      <td>23.38</td>\n","      <td>5.19</td>\n","      <td>2.60</td>\n","      <td>1.3</td>\n","      <td>0.00</td>\n","      <td>9.09</td>\n","      <td>5.19</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1062</th>\n","      <td>11.666553</td>\n","      <td>5.678929</td>\n","      <td>4.617547</td>\n","      <td>1.947428</td>\n","      <td>4.241860</td>\n","      <td>4.401950</td>\n","      <td>5.816283</td>\n","      <td>1.774453</td>\n","      <td>1.630935</td>\n","      <td>6.475672</td>\n","      <td>...</td>\n","      <td>10.87</td>\n","      <td>0.00</td>\n","      <td>4.35</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>6.52</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5507</th>\n","      <td>8.162202</td>\n","      <td>2.367997</td>\n","      <td>2.811537</td>\n","      <td>3.122674</td>\n","      <td>5.579625</td>\n","      <td>4.600275</td>\n","      <td>5.148420</td>\n","      <td>1.529511</td>\n","      <td>1.877481</td>\n","      <td>4.742156</td>\n","      <td>...</td>\n","      <td>29.41</td>\n","      <td>8.82</td>\n","      <td>8.82</td>\n","      <td>0.0</td>\n","      <td>5.15</td>\n","      <td>2.94</td>\n","      <td>3.68</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1668</th>\n","      <td>10.890951</td>\n","      <td>5.019511</td>\n","      <td>4.159523</td>\n","      <td>3.364110</td>\n","      <td>4.638531</td>\n","      <td>5.071639</td>\n","      <td>6.378169</td>\n","      <td>1.748661</td>\n","      <td>1.563693</td>\n","      <td>6.232338</td>\n","      <td>...</td>\n","      <td>6.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>6.25</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5924 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-344f4c59-0d0f-4e5c-8c1b-6fe89628bca2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-584e29fb-658b-4528-b1e6-2163eee71517\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-584e29fb-658b-4528-b1e6-2163eee71517')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-584e29fb-658b-4528-b1e6-2163eee71517 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-344f4c59-0d0f-4e5c-8c1b-6fe89628bca2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-344f4c59-0d0f-4e5c-8c1b-6fe89628bca2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)"],"metadata":{"id":"kuQrF1L9OxG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtemp = final_rem_df.drop('label', axis=1)\n","ytemp = final_rem_df['label']\n","undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","final_rem_df = X_undersampled.copy()\n","final_rem_df['label'] = y_undersampled"],"metadata":{"id":"2neE-v4_Liu9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_rem_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHb6vAzLPasQ","executionInfo":{"status":"ok","timestamp":1689799947279,"user_tz":-330,"elapsed":74,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8fce4fe0-637f-4191-8c0c-46303bfe470f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4424, 184)"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["final_rem_df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vj19BlHBPWjy","executionInfo":{"status":"ok","timestamp":1689799947280,"user_tz":-330,"elapsed":70,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"dff58f60-08ab-4c5d-a343-4095f2517fb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2212\n","1    2212\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","y_remaining = final_rem_df['label']"],"metadata":{"id":"qr8xTrwGPi8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split remaining data into train and validation sets\n","X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=2, stratify=y_remaining)\n","X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=2, stratify=y_remaining)\n","X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=2, stratify=y_remaining)\n","X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=2, stratify=y_remaining)"],"metadata":{"id":"4pIfTuA4GHHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_sentemb_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0n4BsHbqOk9W","executionInfo":{"status":"ok","timestamp":1689799947299,"user_tz":-330,"elapsed":76,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"6f7c84eb-2f1f-467a-96d0-d6d008d2553f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3318, 28)"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":[],"metadata":{"id":"XfP5lOToR3Ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to register f1 score separately in Keras (working)\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.metrics import Metric\n","\n","class F1Score(Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","        false_positives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * y_pred, 0, 1)))\n","        false_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * (1 - y_pred), 0, 1)))\n","\n","        self.true_positives.assign_add(true_positives)\n","        self.false_positives.assign_add(false_positives)\n","        self.false_negatives.assign_add(false_negatives)\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n","        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","        return f1"],"metadata":{"id":"YK55gmNVRVtp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LzdVJeNLTrVL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM CNN nothing nothing"],"metadata":{"id":"SNBMPvkhRV1B"}},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"avidq0OGQKz7","executionInfo":{"status":"ok","timestamp":1689799947310,"user_tz":-330,"elapsed":69,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"17e04ce8-1b05-48d9-9c2f-7cd555bb9f9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-89420fd7-a77a-44c2-99c4-a57fb3c699b0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89420fd7-a77a-44c2-99c4-a57fb3c699b0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-1f0f965f-7dc9-415b-8aa3-3edf0d429632\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f0f965f-7dc9-415b-8aa3-3edf0d429632')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-1f0f965f-7dc9-415b-8aa3-3edf0d429632 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-89420fd7-a77a-44c2-99c4-a57fb3c699b0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-89420fd7-a77a-44c2-99c4-a57fb3c699b0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"eT0OdvUqQ5CG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Standardization of train and test LIWC features\n","        train_scaled_liwc = sc.fit_transform(X_liwc_train)\n","        val_scaled_liwc = sc.fit_transform(X_liwc_val)\n","        test_scaled_liwc = sc.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tC33aixRX-6","executionInfo":{"status":"ok","timestamp":1689800478154,"user_tz":-330,"elapsed":530909,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"ee8517bd-c482-44b6-b109-e31fd6bdd2c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5955 - accuracy: 0.6739 - f1_score: 0.6600 - val_loss: 0.4847 - val_accuracy: 0.7758 - val_f1_score: 0.7708\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4838 - accuracy: 0.7658 - f1_score: 0.7509 - val_loss: 0.4258 - val_accuracy: 0.8201 - val_f1_score: 0.8142\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4143 - accuracy: 0.8080 - f1_score: 0.7986 - val_loss: 0.3926 - val_accuracy: 0.8092 - val_f1_score: 0.8228\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3588 - accuracy: 0.8442 - f1_score: 0.8391 - val_loss: 0.3411 - val_accuracy: 0.8436 - val_f1_score: 0.8366\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3109 - accuracy: 0.8689 - f1_score: 0.8654 - val_loss: 0.3247 - val_accuracy: 0.8499 - val_f1_score: 0.8520\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2678 - accuracy: 0.8897 - f1_score: 0.8868 - val_loss: 0.3194 - val_accuracy: 0.8553 - val_f1_score: 0.8548\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2458 - accuracy: 0.9020 - f1_score: 0.9005 - val_loss: 0.3289 - val_accuracy: 0.8617 - val_f1_score: 0.8642\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2253 - accuracy: 0.9060 - f1_score: 0.9046 - val_loss: 0.3104 - val_accuracy: 0.8779 - val_f1_score: 0.8785\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1966 - accuracy: 0.9225 - f1_score: 0.9215 - val_loss: 0.3369 - val_accuracy: 0.8734 - val_f1_score: 0.8752\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1908 - accuracy: 0.9231 - f1_score: 0.9226 - val_loss: 0.3211 - val_accuracy: 0.8689 - val_f1_score: 0.8656\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1651 - accuracy: 0.9352 - f1_score: 0.9346 - val_loss: 0.3682 - val_accuracy: 0.8635 - val_f1_score: 0.8711\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1367 - accuracy: 0.9458 - f1_score: 0.9452 - val_loss: 0.3957 - val_accuracy: 0.8644 - val_f1_score: 0.8702\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1314 - accuracy: 0.9506 - f1_score: 0.9504 - val_loss: 0.4501 - val_accuracy: 0.8526 - val_f1_score: 0.8622\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8393 - f1_score: 0.8632\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5759 - accuracy: 0.6926 - f1_score: 0.6953 - val_loss: 0.5359 - val_accuracy: 0.7351 - val_f1_score: 0.7090\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4711 - accuracy: 0.7719 - f1_score: 0.7673 - val_loss: 0.4904 - val_accuracy: 0.7667 - val_f1_score: 0.7650\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4015 - accuracy: 0.8146 - f1_score: 0.8102 - val_loss: 0.4723 - val_accuracy: 0.7749 - val_f1_score: 0.7409\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3448 - accuracy: 0.8478 - f1_score: 0.8452 - val_loss: 0.4478 - val_accuracy: 0.8110 - val_f1_score: 0.8197\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3062 - accuracy: 0.8680 - f1_score: 0.8665 - val_loss: 0.4200 - val_accuracy: 0.8246 - val_f1_score: 0.8298\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2694 - accuracy: 0.8816 - f1_score: 0.8799 - val_loss: 0.4214 - val_accuracy: 0.8336 - val_f1_score: 0.8210\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2398 - accuracy: 0.9014 - f1_score: 0.9002 - val_loss: 0.3996 - val_accuracy: 0.8300 - val_f1_score: 0.8351\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2134 - accuracy: 0.9105 - f1_score: 0.9099 - val_loss: 0.4484 - val_accuracy: 0.8273 - val_f1_score: 0.8361\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1900 - accuracy: 0.9231 - f1_score: 0.9227 - val_loss: 0.4438 - val_accuracy: 0.8427 - val_f1_score: 0.8383\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1663 - accuracy: 0.9328 - f1_score: 0.9324 - val_loss: 0.4443 - val_accuracy: 0.8400 - val_f1_score: 0.8446\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1541 - accuracy: 0.9436 - f1_score: 0.9433 - val_loss: 0.4755 - val_accuracy: 0.8436 - val_f1_score: 0.8481\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1360 - accuracy: 0.9488 - f1_score: 0.9488 - val_loss: 0.4579 - val_accuracy: 0.8562 - val_f1_score: 0.8537\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3729 - accuracy: 0.8515 - f1_score: 0.8768\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5839 - accuracy: 0.6869 - f1_score: 0.6810 - val_loss: 0.5301 - val_accuracy: 0.7306 - val_f1_score: 0.7189\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4694 - accuracy: 0.7679 - f1_score: 0.7591 - val_loss: 0.4965 - val_accuracy: 0.7468 - val_f1_score: 0.7403\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3940 - accuracy: 0.8207 - f1_score: 0.8178 - val_loss: 0.4678 - val_accuracy: 0.7767 - val_f1_score: 0.7843\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3380 - accuracy: 0.8553 - f1_score: 0.8526 - val_loss: 0.4853 - val_accuracy: 0.7812 - val_f1_score: 0.7855\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2897 - accuracy: 0.8776 - f1_score: 0.8758 - val_loss: 0.5328 - val_accuracy: 0.7767 - val_f1_score: 0.7994\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2668 - accuracy: 0.8903 - f1_score: 0.8890 - val_loss: 0.4676 - val_accuracy: 0.7975 - val_f1_score: 0.8072\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2290 - accuracy: 0.9060 - f1_score: 0.9053 - val_loss: 0.4817 - val_accuracy: 0.8101 - val_f1_score: 0.7969\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2087 - accuracy: 0.9204 - f1_score: 0.9198 - val_loss: 0.4917 - val_accuracy: 0.8156 - val_f1_score: 0.8128\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1933 - accuracy: 0.9250 - f1_score: 0.9241 - val_loss: 0.5121 - val_accuracy: 0.8128 - val_f1_score: 0.8150\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1640 - accuracy: 0.9379 - f1_score: 0.9378 - val_loss: 0.5243 - val_accuracy: 0.8165 - val_f1_score: 0.8122\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1436 - accuracy: 0.9430 - f1_score: 0.9431 - val_loss: 0.5251 - val_accuracy: 0.8183 - val_f1_score: 0.8130\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3716 - accuracy: 0.8406 - f1_score: 0.8708\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5968 - accuracy: 0.6739 - f1_score: 0.6782 - val_loss: 0.4964 - val_accuracy: 0.7667 - val_f1_score: 0.7490\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4797 - accuracy: 0.7673 - f1_score: 0.7546 - val_loss: 0.4536 - val_accuracy: 0.7848 - val_f1_score: 0.7629\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4104 - accuracy: 0.8192 - f1_score: 0.8129 - val_loss: 0.4165 - val_accuracy: 0.8192 - val_f1_score: 0.8211\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3593 - accuracy: 0.8403 - f1_score: 0.8374 - val_loss: 0.3911 - val_accuracy: 0.8336 - val_f1_score: 0.8224\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3041 - accuracy: 0.8704 - f1_score: 0.8678 - val_loss: 0.3862 - val_accuracy: 0.8345 - val_f1_score: 0.8295\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2664 - accuracy: 0.8909 - f1_score: 0.8899 - val_loss: 0.3752 - val_accuracy: 0.8508 - val_f1_score: 0.8412\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2420 - accuracy: 0.9017 - f1_score: 0.9002 - val_loss: 0.3728 - val_accuracy: 0.8517 - val_f1_score: 0.8530\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2075 - accuracy: 0.9219 - f1_score: 0.9211 - val_loss: 0.3989 - val_accuracy: 0.8562 - val_f1_score: 0.8564\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1842 - accuracy: 0.9259 - f1_score: 0.9254 - val_loss: 0.4035 - val_accuracy: 0.8544 - val_f1_score: 0.8540\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1790 - accuracy: 0.9328 - f1_score: 0.9322 - val_loss: 0.4375 - val_accuracy: 0.8571 - val_f1_score: 0.8495\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1560 - accuracy: 0.9331 - f1_score: 0.9328 - val_loss: 0.4457 - val_accuracy: 0.8626 - val_f1_score: 0.8603\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1384 - accuracy: 0.9473 - f1_score: 0.9471 - val_loss: 0.4997 - val_accuracy: 0.8336 - val_f1_score: 0.8417\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3807 - accuracy: 0.8515 - f1_score: 0.8772\n","Epoch 1/20\n","104/104 [==============================] - 6s 22ms/step - loss: 0.5905 - accuracy: 0.6730 - f1_score: 0.6658 - val_loss: 0.5227 - val_accuracy: 0.7378 - val_f1_score: 0.7469\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4890 - accuracy: 0.7640 - f1_score: 0.7587 - val_loss: 0.4682 - val_accuracy: 0.7749 - val_f1_score: 0.7675\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4308 - accuracy: 0.8047 - f1_score: 0.8017 - val_loss: 0.4221 - val_accuracy: 0.8020 - val_f1_score: 0.7932\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3756 - accuracy: 0.8297 - f1_score: 0.8258 - val_loss: 0.3878 - val_accuracy: 0.8336 - val_f1_score: 0.8274\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3256 - accuracy: 0.8547 - f1_score: 0.8533 - val_loss: 0.3745 - val_accuracy: 0.8481 - val_f1_score: 0.8366\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2908 - accuracy: 0.8800 - f1_score: 0.8787 - val_loss: 0.3508 - val_accuracy: 0.8562 - val_f1_score: 0.8513\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2488 - accuracy: 0.8936 - f1_score: 0.8931 - val_loss: 0.3416 - val_accuracy: 0.8535 - val_f1_score: 0.8569\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2276 - accuracy: 0.9060 - f1_score: 0.9058 - val_loss: 0.3582 - val_accuracy: 0.8562 - val_f1_score: 0.8628\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1753 - accuracy: 0.9280 - f1_score: 0.9274 - val_loss: 0.3755 - val_accuracy: 0.8626 - val_f1_score: 0.8692\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1659 - accuracy: 0.9340 - f1_score: 0.9337 - val_loss: 0.3664 - val_accuracy: 0.8743 - val_f1_score: 0.8719\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1636 - accuracy: 0.9376 - f1_score: 0.9373 - val_loss: 0.3885 - val_accuracy: 0.8698 - val_f1_score: 0.8696\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1340 - accuracy: 0.9494 - f1_score: 0.9494 - val_loss: 0.4252 - val_accuracy: 0.8535 - val_f1_score: 0.8581\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4066 - accuracy: 0.8413 - f1_score: 0.8670\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.5897 - accuracy: 0.6793 - f1_score: 0.6835 - val_loss: 0.4951 - val_accuracy: 0.7722 - val_f1_score: 0.7510\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4704 - accuracy: 0.7860 - f1_score: 0.7791 - val_loss: 0.4536 - val_accuracy: 0.7929 - val_f1_score: 0.7905\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4144 - accuracy: 0.8237 - f1_score: 0.8204 - val_loss: 0.4180 - val_accuracy: 0.8128 - val_f1_score: 0.7969\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3573 - accuracy: 0.8424 - f1_score: 0.8395 - val_loss: 0.4045 - val_accuracy: 0.8174 - val_f1_score: 0.8231\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3171 - accuracy: 0.8647 - f1_score: 0.8639 - val_loss: 0.3524 - val_accuracy: 0.8454 - val_f1_score: 0.8430\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2697 - accuracy: 0.8864 - f1_score: 0.8850 - val_loss: 0.3643 - val_accuracy: 0.8508 - val_f1_score: 0.8591\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2359 - accuracy: 0.9033 - f1_score: 0.9028 - val_loss: 0.3514 - val_accuracy: 0.8608 - val_f1_score: 0.8547\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2139 - accuracy: 0.9126 - f1_score: 0.9118 - val_loss: 0.3978 - val_accuracy: 0.8382 - val_f1_score: 0.8450\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1843 - accuracy: 0.9292 - f1_score: 0.9287 - val_loss: 0.3599 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1576 - accuracy: 0.9427 - f1_score: 0.9423 - val_loss: 0.3864 - val_accuracy: 0.8653 - val_f1_score: 0.8712\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1467 - accuracy: 0.9436 - f1_score: 0.9435 - val_loss: 0.3552 - val_accuracy: 0.8725 - val_f1_score: 0.8710\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1356 - accuracy: 0.9491 - f1_score: 0.9489 - val_loss: 0.3949 - val_accuracy: 0.8734 - val_f1_score: 0.8752\n","47/47 [==============================] - 0s 4ms/step - loss: 0.5059 - accuracy: 0.8042 - f1_score: 0.8280\n","Epoch 1/20\n","104/104 [==============================] - 6s 18ms/step - loss: 0.5868 - accuracy: 0.6739 - f1_score: 0.6842 - val_loss: 0.5217 - val_accuracy: 0.7405 - val_f1_score: 0.7254\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4715 - accuracy: 0.7803 - f1_score: 0.7759 - val_loss: 0.4675 - val_accuracy: 0.7812 - val_f1_score: 0.7990\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4021 - accuracy: 0.8231 - f1_score: 0.8197 - val_loss: 0.4208 - val_accuracy: 0.8074 - val_f1_score: 0.8079\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3544 - accuracy: 0.8382 - f1_score: 0.8373 - val_loss: 0.3780 - val_accuracy: 0.8373 - val_f1_score: 0.8273\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3034 - accuracy: 0.8731 - f1_score: 0.8709 - val_loss: 0.3589 - val_accuracy: 0.8490 - val_f1_score: 0.8483\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2609 - accuracy: 0.8873 - f1_score: 0.8865 - val_loss: 0.3508 - val_accuracy: 0.8599 - val_f1_score: 0.8558\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2248 - accuracy: 0.9096 - f1_score: 0.9089 - val_loss: 0.3587 - val_accuracy: 0.8571 - val_f1_score: 0.8504\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2089 - accuracy: 0.9102 - f1_score: 0.9097 - val_loss: 0.3888 - val_accuracy: 0.8590 - val_f1_score: 0.8602\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1759 - accuracy: 0.9277 - f1_score: 0.9274 - val_loss: 0.3682 - val_accuracy: 0.8689 - val_f1_score: 0.8676\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1509 - accuracy: 0.9388 - f1_score: 0.9387 - val_loss: 0.3787 - val_accuracy: 0.8635 - val_f1_score: 0.8571\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1321 - accuracy: 0.9530 - f1_score: 0.9528 - val_loss: 0.3819 - val_accuracy: 0.8689 - val_f1_score: 0.8688\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4211 - accuracy: 0.8258 - f1_score: 0.8526\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5802 - accuracy: 0.6914 - f1_score: 0.6841 - val_loss: 0.4931 - val_accuracy: 0.7731 - val_f1_score: 0.7652\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4638 - accuracy: 0.7905 - f1_score: 0.7846 - val_loss: 0.4443 - val_accuracy: 0.7929 - val_f1_score: 0.7800\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3889 - accuracy: 0.8222 - f1_score: 0.8182 - val_loss: 0.4031 - val_accuracy: 0.8273 - val_f1_score: 0.8262\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3315 - accuracy: 0.8580 - f1_score: 0.8555 - val_loss: 0.3813 - val_accuracy: 0.8409 - val_f1_score: 0.8275\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2849 - accuracy: 0.8725 - f1_score: 0.8712 - val_loss: 0.3685 - val_accuracy: 0.8382 - val_f1_score: 0.8453\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2378 - accuracy: 0.9120 - f1_score: 0.9113 - val_loss: 0.3490 - val_accuracy: 0.8599 - val_f1_score: 0.8558\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2086 - accuracy: 0.9135 - f1_score: 0.9128 - val_loss: 0.3643 - val_accuracy: 0.8526 - val_f1_score: 0.8434\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1872 - accuracy: 0.9292 - f1_score: 0.9285 - val_loss: 0.3738 - val_accuracy: 0.8680 - val_f1_score: 0.8675\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1681 - accuracy: 0.9361 - f1_score: 0.9357 - val_loss: 0.3857 - val_accuracy: 0.8644 - val_f1_score: 0.8606\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1450 - accuracy: 0.9451 - f1_score: 0.9451 - val_loss: 0.4080 - val_accuracy: 0.8671 - val_f1_score: 0.8700\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1337 - accuracy: 0.9503 - f1_score: 0.9501 - val_loss: 0.4077 - val_accuracy: 0.8707 - val_f1_score: 0.8667\n","47/47 [==============================] - 0s 6ms/step - loss: 0.4427 - accuracy: 0.8305 - f1_score: 0.8538\n","Epoch 1/20\n","104/104 [==============================] - 6s 16ms/step - loss: 0.5774 - accuracy: 0.6923 - f1_score: 0.6903 - val_loss: 0.5116 - val_accuracy: 0.7613 - val_f1_score: 0.7476\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4729 - accuracy: 0.7827 - f1_score: 0.7757 - val_loss: 0.4579 - val_accuracy: 0.7957 - val_f1_score: 0.7945\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4024 - accuracy: 0.8216 - f1_score: 0.8183 - val_loss: 0.4225 - val_accuracy: 0.8219 - val_f1_score: 0.8082\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3453 - accuracy: 0.8469 - f1_score: 0.8441 - val_loss: 0.3908 - val_accuracy: 0.8300 - val_f1_score: 0.8230\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3035 - accuracy: 0.8716 - f1_score: 0.8692 - val_loss: 0.3689 - val_accuracy: 0.8463 - val_f1_score: 0.8405\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2646 - accuracy: 0.8921 - f1_score: 0.8910 - val_loss: 0.3540 - val_accuracy: 0.8671 - val_f1_score: 0.8630\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2425 - accuracy: 0.9008 - f1_score: 0.8998 - val_loss: 0.3538 - val_accuracy: 0.8662 - val_f1_score: 0.8642\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2092 - accuracy: 0.9159 - f1_score: 0.9153 - val_loss: 0.3594 - val_accuracy: 0.8680 - val_f1_score: 0.8696\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1770 - accuracy: 0.9277 - f1_score: 0.9272 - val_loss: 0.3863 - val_accuracy: 0.8580 - val_f1_score: 0.8584\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1607 - accuracy: 0.9385 - f1_score: 0.9384 - val_loss: 0.3996 - val_accuracy: 0.8662 - val_f1_score: 0.8630\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1537 - accuracy: 0.9382 - f1_score: 0.9380 - val_loss: 0.3744 - val_accuracy: 0.8662 - val_f1_score: 0.8662\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1390 - accuracy: 0.9485 - f1_score: 0.9484 - val_loss: 0.3848 - val_accuracy: 0.8707 - val_f1_score: 0.8667\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4088 - accuracy: 0.8298 - f1_score: 0.8567\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.5849 - accuracy: 0.6832 - f1_score: 0.6721 - val_loss: 0.4939 - val_accuracy: 0.7586 - val_f1_score: 0.7327\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4732 - accuracy: 0.7734 - f1_score: 0.7625 - val_loss: 0.4476 - val_accuracy: 0.7794 - val_f1_score: 0.7694\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4114 - accuracy: 0.8083 - f1_score: 0.8042 - val_loss: 0.4273 - val_accuracy: 0.7929 - val_f1_score: 0.7675\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3578 - accuracy: 0.8442 - f1_score: 0.8387 - val_loss: 0.4072 - val_accuracy: 0.8156 - val_f1_score: 0.8128\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3210 - accuracy: 0.8596 - f1_score: 0.8564 - val_loss: 0.3821 - val_accuracy: 0.8219 - val_f1_score: 0.8082\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2760 - accuracy: 0.8797 - f1_score: 0.8778 - val_loss: 0.3738 - val_accuracy: 0.8336 - val_f1_score: 0.8321\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2370 - accuracy: 0.9054 - f1_score: 0.9037 - val_loss: 0.3710 - val_accuracy: 0.8336 - val_f1_score: 0.8261\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2185 - accuracy: 0.9105 - f1_score: 0.9094 - val_loss: 0.4436 - val_accuracy: 0.8219 - val_f1_score: 0.8000\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1973 - accuracy: 0.9168 - f1_score: 0.9157 - val_loss: 0.3829 - val_accuracy: 0.8373 - val_f1_score: 0.8443\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1797 - accuracy: 0.9289 - f1_score: 0.9285 - val_loss: 0.3639 - val_accuracy: 0.8454 - val_f1_score: 0.8475\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1435 - accuracy: 0.9482 - f1_score: 0.9478 - val_loss: 0.4356 - val_accuracy: 0.8472 - val_f1_score: 0.8542\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1269 - accuracy: 0.9521 - f1_score: 0.9521 - val_loss: 0.3733 - val_accuracy: 0.8490 - val_f1_score: 0.8444\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1121 - accuracy: 0.9575 - f1_score: 0.9573 - val_loss: 0.4135 - val_accuracy: 0.8445 - val_f1_score: 0.8442\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.0993 - accuracy: 0.9620 - f1_score: 0.9619 - val_loss: 0.4774 - val_accuracy: 0.8454 - val_f1_score: 0.8472\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1076 - accuracy: 0.9614 - f1_score: 0.9615 - val_loss: 0.4693 - val_accuracy: 0.8562 - val_f1_score: 0.8501\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4556 - accuracy: 0.8244 - f1_score: 0.8573\n","Epoch 1/20\n","104/104 [==============================] - 6s 20ms/step - loss: 0.5939 - accuracy: 0.6745 - f1_score: 0.6717 - val_loss: 0.5214 - val_accuracy: 0.7595 - val_f1_score: 0.7671\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4736 - accuracy: 0.7719 - f1_score: 0.7669 - val_loss: 0.4780 - val_accuracy: 0.7803 - val_f1_score: 0.7710\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4147 - accuracy: 0.8080 - f1_score: 0.8035 - val_loss: 0.4406 - val_accuracy: 0.8038 - val_f1_score: 0.7974\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3508 - accuracy: 0.8409 - f1_score: 0.8366 - val_loss: 0.4418 - val_accuracy: 0.8156 - val_f1_score: 0.7976\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3164 - accuracy: 0.8614 - f1_score: 0.8588 - val_loss: 0.4208 - val_accuracy: 0.8327 - val_f1_score: 0.8223\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2609 - accuracy: 0.8900 - f1_score: 0.8880 - val_loss: 0.4111 - val_accuracy: 0.8382 - val_f1_score: 0.8397\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2289 - accuracy: 0.9054 - f1_score: 0.9043 - val_loss: 0.4270 - val_accuracy: 0.8463 - val_f1_score: 0.8346\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2138 - accuracy: 0.9168 - f1_score: 0.9163 - val_loss: 0.4185 - val_accuracy: 0.8363 - val_f1_score: 0.8388\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1749 - accuracy: 0.9256 - f1_score: 0.9253 - val_loss: 0.4403 - val_accuracy: 0.8544 - val_f1_score: 0.8494\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1787 - accuracy: 0.9337 - f1_score: 0.9333 - val_loss: 0.4354 - val_accuracy: 0.8445 - val_f1_score: 0.8515\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1602 - accuracy: 0.9403 - f1_score: 0.9400 - val_loss: 0.4304 - val_accuracy: 0.8463 - val_f1_score: 0.8501\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3760 - accuracy: 0.8447 - f1_score: 0.8722\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.5894 - accuracy: 0.6926 - f1_score: 0.6807 - val_loss: 0.5058 - val_accuracy: 0.7514 - val_f1_score: 0.7418\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4841 - accuracy: 0.7661 - f1_score: 0.7631 - val_loss: 0.4571 - val_accuracy: 0.7785 - val_f1_score: 0.7682\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4177 - accuracy: 0.8095 - f1_score: 0.8045 - val_loss: 0.4353 - val_accuracy: 0.8047 - val_f1_score: 0.7958\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3647 - accuracy: 0.8373 - f1_score: 0.8352 - val_loss: 0.4047 - val_accuracy: 0.8174 - val_f1_score: 0.8069\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3087 - accuracy: 0.8626 - f1_score: 0.8604 - val_loss: 0.3961 - val_accuracy: 0.8345 - val_f1_score: 0.8291\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2653 - accuracy: 0.8882 - f1_score: 0.8875 - val_loss: 0.4046 - val_accuracy: 0.8309 - val_f1_score: 0.8317\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2376 - accuracy: 0.9027 - f1_score: 0.9015 - val_loss: 0.3864 - val_accuracy: 0.8400 - val_f1_score: 0.8443\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2113 - accuracy: 0.9120 - f1_score: 0.9120 - val_loss: 0.3885 - val_accuracy: 0.8454 - val_f1_score: 0.8433\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1907 - accuracy: 0.9216 - f1_score: 0.9216 - val_loss: 0.4046 - val_accuracy: 0.8490 - val_f1_score: 0.8472\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1582 - accuracy: 0.9400 - f1_score: 0.9399 - val_loss: 0.4365 - val_accuracy: 0.8508 - val_f1_score: 0.8523\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1471 - accuracy: 0.9430 - f1_score: 0.9429 - val_loss: 0.4565 - val_accuracy: 0.8454 - val_f1_score: 0.8455\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1324 - accuracy: 0.9488 - f1_score: 0.9488 - val_loss: 0.4593 - val_accuracy: 0.8463 - val_f1_score: 0.8501\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3812 - accuracy: 0.8440 - f1_score: 0.8729\n","Epoch 1/20\n","104/104 [==============================] - 7s 14ms/step - loss: 0.5884 - accuracy: 0.6793 - f1_score: 0.6687 - val_loss: 0.5175 - val_accuracy: 0.7387 - val_f1_score: 0.7218\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4837 - accuracy: 0.7839 - f1_score: 0.7754 - val_loss: 0.4774 - val_accuracy: 0.7676 - val_f1_score: 0.7526\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4211 - accuracy: 0.8089 - f1_score: 0.8036 - val_loss: 0.4626 - val_accuracy: 0.7722 - val_f1_score: 0.7423\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3542 - accuracy: 0.8502 - f1_score: 0.8473 - val_loss: 0.4075 - val_accuracy: 0.8165 - val_f1_score: 0.8149\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2995 - accuracy: 0.8791 - f1_score: 0.8765 - val_loss: 0.3940 - val_accuracy: 0.8445 - val_f1_score: 0.8442\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2635 - accuracy: 0.8915 - f1_score: 0.8904 - val_loss: 0.3885 - val_accuracy: 0.8400 - val_f1_score: 0.8389\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2246 - accuracy: 0.9078 - f1_score: 0.9066 - val_loss: 0.3968 - val_accuracy: 0.8517 - val_f1_score: 0.8538\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2113 - accuracy: 0.9171 - f1_score: 0.9163 - val_loss: 0.4027 - val_accuracy: 0.8517 - val_f1_score: 0.8549\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1795 - accuracy: 0.9262 - f1_score: 0.9258 - val_loss: 0.4047 - val_accuracy: 0.8517 - val_f1_score: 0.8561\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1521 - accuracy: 0.9394 - f1_score: 0.9387 - val_loss: 0.4108 - val_accuracy: 0.8580 - val_f1_score: 0.8579\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1492 - accuracy: 0.9442 - f1_score: 0.9440 - val_loss: 0.4470 - val_accuracy: 0.8562 - val_f1_score: 0.8592\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8420 - f1_score: 0.8699\n","Epoch 1/20\n","104/104 [==============================] - 6s 20ms/step - loss: 0.5983 - accuracy: 0.6772 - f1_score: 0.6756 - val_loss: 0.5082 - val_accuracy: 0.7514 - val_f1_score: 0.7301\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4839 - accuracy: 0.7637 - f1_score: 0.7525 - val_loss: 0.4574 - val_accuracy: 0.7984 - val_f1_score: 0.7890\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4232 - accuracy: 0.8020 - f1_score: 0.7960 - val_loss: 0.4288 - val_accuracy: 0.8110 - val_f1_score: 0.8149\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3620 - accuracy: 0.8376 - f1_score: 0.8332 - val_loss: 0.4051 - val_accuracy: 0.8201 - val_f1_score: 0.8085\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3072 - accuracy: 0.8719 - f1_score: 0.8690 - val_loss: 0.3896 - val_accuracy: 0.8409 - val_f1_score: 0.8311\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2646 - accuracy: 0.8870 - f1_score: 0.8848 - val_loss: 0.3860 - val_accuracy: 0.8246 - val_f1_score: 0.8286\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2302 - accuracy: 0.8993 - f1_score: 0.8979 - val_loss: 0.3616 - val_accuracy: 0.8472 - val_f1_score: 0.8398\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1995 - accuracy: 0.9228 - f1_score: 0.9224 - val_loss: 0.3832 - val_accuracy: 0.8508 - val_f1_score: 0.8520\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1797 - accuracy: 0.9271 - f1_score: 0.9270 - val_loss: 0.3888 - val_accuracy: 0.8544 - val_f1_score: 0.8456\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1532 - accuracy: 0.9391 - f1_score: 0.9386 - val_loss: 0.4152 - val_accuracy: 0.8309 - val_f1_score: 0.8352\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1266 - accuracy: 0.9521 - f1_score: 0.9518 - val_loss: 0.4465 - val_accuracy: 0.8490 - val_f1_score: 0.8466\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1201 - accuracy: 0.9551 - f1_score: 0.9548 - val_loss: 0.4734 - val_accuracy: 0.8608 - val_f1_score: 0.8620\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4262 - accuracy: 0.8258 - f1_score: 0.8517\n","Epoch 1/20\n","104/104 [==============================] - 7s 14ms/step - loss: 0.5837 - accuracy: 0.6832 - f1_score: 0.6793 - val_loss: 0.5093 - val_accuracy: 0.7532 - val_f1_score: 0.7278\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4798 - accuracy: 0.7691 - f1_score: 0.7587 - val_loss: 0.4736 - val_accuracy: 0.7685 - val_f1_score: 0.7445\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4256 - accuracy: 0.8044 - f1_score: 0.7973 - val_loss: 0.4495 - val_accuracy: 0.7794 - val_f1_score: 0.7882\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3621 - accuracy: 0.8376 - f1_score: 0.8342 - val_loss: 0.4128 - val_accuracy: 0.8092 - val_f1_score: 0.8090\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3074 - accuracy: 0.8692 - f1_score: 0.8669 - val_loss: 0.3810 - val_accuracy: 0.8237 - val_f1_score: 0.8306\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2901 - accuracy: 0.8734 - f1_score: 0.8722 - val_loss: 0.3802 - val_accuracy: 0.8246 - val_f1_score: 0.8325\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2477 - accuracy: 0.9027 - f1_score: 0.9014 - val_loss: 0.3840 - val_accuracy: 0.8273 - val_f1_score: 0.8278\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2141 - accuracy: 0.9153 - f1_score: 0.9146 - val_loss: 0.3695 - val_accuracy: 0.8373 - val_f1_score: 0.8289\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9180 - f1_score: 0.9171 - val_loss: 0.3819 - val_accuracy: 0.8336 - val_f1_score: 0.8354\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1739 - accuracy: 0.9334 - f1_score: 0.9330 - val_loss: 0.3849 - val_accuracy: 0.8517 - val_f1_score: 0.8538\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1453 - accuracy: 0.9415 - f1_score: 0.9412 - val_loss: 0.4156 - val_accuracy: 0.8409 - val_f1_score: 0.8448\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1504 - accuracy: 0.9364 - f1_score: 0.9361 - val_loss: 0.4030 - val_accuracy: 0.8463 - val_f1_score: 0.8509\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1213 - accuracy: 0.9512 - f1_score: 0.9511 - val_loss: 0.4672 - val_accuracy: 0.8373 - val_f1_score: 0.8464\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.8271 - f1_score: 0.8494\n","Epoch 1/20\n","104/104 [==============================] - 6s 22ms/step - loss: 0.5869 - accuracy: 0.6887 - f1_score: 0.6954 - val_loss: 0.4895 - val_accuracy: 0.7749 - val_f1_score: 0.7617\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4706 - accuracy: 0.7782 - f1_score: 0.7669 - val_loss: 0.4413 - val_accuracy: 0.8029 - val_f1_score: 0.8015\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3963 - accuracy: 0.8195 - f1_score: 0.8135 - val_loss: 0.4086 - val_accuracy: 0.8373 - val_f1_score: 0.8355\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3485 - accuracy: 0.8451 - f1_score: 0.8410 - val_loss: 0.3670 - val_accuracy: 0.8391 - val_f1_score: 0.8349\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2947 - accuracy: 0.8767 - f1_score: 0.8755 - val_loss: 0.3780 - val_accuracy: 0.8309 - val_f1_score: 0.8121\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2718 - accuracy: 0.8876 - f1_score: 0.8861 - val_loss: 0.3623 - val_accuracy: 0.8526 - val_f1_score: 0.8425\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2411 - accuracy: 0.9045 - f1_score: 0.9034 - val_loss: 0.3422 - val_accuracy: 0.8644 - val_f1_score: 0.8598\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2162 - accuracy: 0.9120 - f1_score: 0.9108 - val_loss: 0.3395 - val_accuracy: 0.8653 - val_f1_score: 0.8661\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1876 - accuracy: 0.9247 - f1_score: 0.9240 - val_loss: 0.3447 - val_accuracy: 0.8698 - val_f1_score: 0.8723\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1628 - accuracy: 0.9382 - f1_score: 0.9376 - val_loss: 0.3968 - val_accuracy: 0.8553 - val_f1_score: 0.8532\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1443 - accuracy: 0.9442 - f1_score: 0.9440 - val_loss: 0.3920 - val_accuracy: 0.8626 - val_f1_score: 0.8628\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1291 - accuracy: 0.9515 - f1_score: 0.9513 - val_loss: 0.3879 - val_accuracy: 0.8617 - val_f1_score: 0.8558\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1294 - accuracy: 0.9512 - f1_score: 0.9509 - val_loss: 0.5046 - val_accuracy: 0.8418 - val_f1_score: 0.8252\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3747 - accuracy: 0.8508 - f1_score: 0.8759\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6185 - accuracy: 0.6721 - f1_score: 0.6699 - val_loss: 0.5302 - val_accuracy: 0.7468 - val_f1_score: 0.7535\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4731 - accuracy: 0.7788 - f1_score: 0.7735 - val_loss: 0.4747 - val_accuracy: 0.7785 - val_f1_score: 0.7822\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4047 - accuracy: 0.8128 - f1_score: 0.8104 - val_loss: 0.4417 - val_accuracy: 0.8119 - val_f1_score: 0.8163\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3399 - accuracy: 0.8484 - f1_score: 0.8476 - val_loss: 0.4460 - val_accuracy: 0.8128 - val_f1_score: 0.8004\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3042 - accuracy: 0.8656 - f1_score: 0.8639 - val_loss: 0.4172 - val_accuracy: 0.8282 - val_f1_score: 0.8339\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2710 - accuracy: 0.8885 - f1_score: 0.8873 - val_loss: 0.4173 - val_accuracy: 0.8454 - val_f1_score: 0.8430\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2254 - accuracy: 0.9072 - f1_score: 0.9064 - val_loss: 0.4082 - val_accuracy: 0.8553 - val_f1_score: 0.8524\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1999 - accuracy: 0.9216 - f1_score: 0.9214 - val_loss: 0.4166 - val_accuracy: 0.8599 - val_f1_score: 0.8605\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1672 - accuracy: 0.9301 - f1_score: 0.9301 - val_loss: 0.4629 - val_accuracy: 0.8571 - val_f1_score: 0.8501\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1617 - accuracy: 0.9391 - f1_score: 0.9390 - val_loss: 0.4828 - val_accuracy: 0.8336 - val_f1_score: 0.8435\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1257 - accuracy: 0.9536 - f1_score: 0.9536 - val_loss: 0.5083 - val_accuracy: 0.8544 - val_f1_score: 0.8556\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1133 - accuracy: 0.9566 - f1_score: 0.9565 - val_loss: 0.5235 - val_accuracy: 0.8599 - val_f1_score: 0.8597\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4345 - accuracy: 0.8244 - f1_score: 0.8501\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.5927 - accuracy: 0.6887 - f1_score: 0.6836 - val_loss: 0.4960 - val_accuracy: 0.7703 - val_f1_score: 0.7639\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4689 - accuracy: 0.7833 - f1_score: 0.7747 - val_loss: 0.4488 - val_accuracy: 0.7839 - val_f1_score: 0.7768\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4073 - accuracy: 0.8228 - f1_score: 0.8177 - val_loss: 0.4135 - val_accuracy: 0.8165 - val_f1_score: 0.8227\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3437 - accuracy: 0.8559 - f1_score: 0.8531 - val_loss: 0.3808 - val_accuracy: 0.8300 - val_f1_score: 0.8368\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2934 - accuracy: 0.8755 - f1_score: 0.8744 - val_loss: 0.3604 - val_accuracy: 0.8454 - val_f1_score: 0.8435\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2580 - accuracy: 0.8948 - f1_score: 0.8944 - val_loss: 0.3687 - val_accuracy: 0.8580 - val_f1_score: 0.8561\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2229 - accuracy: 0.9117 - f1_score: 0.9114 - val_loss: 0.3696 - val_accuracy: 0.8517 - val_f1_score: 0.8493\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1929 - accuracy: 0.9241 - f1_score: 0.9238 - val_loss: 0.3562 - val_accuracy: 0.8553 - val_f1_score: 0.8566\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1672 - accuracy: 0.9367 - f1_score: 0.9367 - val_loss: 0.3858 - val_accuracy: 0.8580 - val_f1_score: 0.8561\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1637 - accuracy: 0.9361 - f1_score: 0.9358 - val_loss: 0.3737 - val_accuracy: 0.8608 - val_f1_score: 0.8592\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1395 - accuracy: 0.9485 - f1_score: 0.9483 - val_loss: 0.4228 - val_accuracy: 0.8590 - val_f1_score: 0.8520\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1326 - accuracy: 0.9536 - f1_score: 0.9533 - val_loss: 0.4215 - val_accuracy: 0.8662 - val_f1_score: 0.8637\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1247 - accuracy: 0.9530 - f1_score: 0.9530 - val_loss: 0.4353 - val_accuracy: 0.8689 - val_f1_score: 0.8678\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3788 - accuracy: 0.8555 - f1_score: 0.8807\n","Epoch 1/20\n","104/104 [==============================] - 7s 23ms/step - loss: 0.5761 - accuracy: 0.6974 - f1_score: 0.6894 - val_loss: 0.5013 - val_accuracy: 0.7622 - val_f1_score: 0.7439\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4569 - accuracy: 0.7890 - f1_score: 0.7830 - val_loss: 0.4694 - val_accuracy: 0.7884 - val_f1_score: 0.7754\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3964 - accuracy: 0.8228 - f1_score: 0.8189 - val_loss: 0.4352 - val_accuracy: 0.7884 - val_f1_score: 0.8027\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3400 - accuracy: 0.8596 - f1_score: 0.8576 - val_loss: 0.3938 - val_accuracy: 0.8291 - val_f1_score: 0.8181\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2980 - accuracy: 0.8770 - f1_score: 0.8746 - val_loss: 0.4002 - val_accuracy: 0.8201 - val_f1_score: 0.8262\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2608 - accuracy: 0.8960 - f1_score: 0.8947 - val_loss: 0.3970 - val_accuracy: 0.8183 - val_f1_score: 0.8269\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2429 - accuracy: 0.8951 - f1_score: 0.8939 - val_loss: 0.3816 - val_accuracy: 0.8318 - val_f1_score: 0.8397\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2163 - accuracy: 0.9120 - f1_score: 0.9116 - val_loss: 0.3871 - val_accuracy: 0.8517 - val_f1_score: 0.8444\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1827 - accuracy: 0.9253 - f1_score: 0.9246 - val_loss: 0.3848 - val_accuracy: 0.8571 - val_f1_score: 0.8592\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1690 - accuracy: 0.9307 - f1_score: 0.9303 - val_loss: 0.3907 - val_accuracy: 0.8608 - val_f1_score: 0.8595\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1441 - accuracy: 0.9485 - f1_score: 0.9482 - val_loss: 0.4252 - val_accuracy: 0.8472 - val_f1_score: 0.8398\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1223 - accuracy: 0.9566 - f1_score: 0.9563 - val_loss: 0.4357 - val_accuracy: 0.8517 - val_f1_score: 0.8538\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3723 - accuracy: 0.8379 - f1_score: 0.8701\n","Epoch 1/20\n","104/104 [==============================] - 5s 15ms/step - loss: 0.5983 - accuracy: 0.6706 - f1_score: 0.6695 - val_loss: 0.4811 - val_accuracy: 0.7812 - val_f1_score: 0.7768\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4786 - accuracy: 0.7712 - f1_score: 0.7606 - val_loss: 0.4464 - val_accuracy: 0.8002 - val_f1_score: 0.8060\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4312 - accuracy: 0.7948 - f1_score: 0.7856 - val_loss: 0.4078 - val_accuracy: 0.8156 - val_f1_score: 0.8086\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3787 - accuracy: 0.8264 - f1_score: 0.8195 - val_loss: 0.3815 - val_accuracy: 0.8363 - val_f1_score: 0.8335\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3350 - accuracy: 0.8614 - f1_score: 0.8573 - val_loss: 0.3803 - val_accuracy: 0.8427 - val_f1_score: 0.8352\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2920 - accuracy: 0.8767 - f1_score: 0.8747 - val_loss: 0.3622 - val_accuracy: 0.8481 - val_f1_score: 0.8475\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2486 - accuracy: 0.9005 - f1_score: 0.8991 - val_loss: 0.3588 - val_accuracy: 0.8617 - val_f1_score: 0.8600\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2265 - accuracy: 0.9045 - f1_score: 0.9030 - val_loss: 0.3809 - val_accuracy: 0.8590 - val_f1_score: 0.8550\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2043 - accuracy: 0.9144 - f1_score: 0.9133 - val_loss: 0.4051 - val_accuracy: 0.8580 - val_f1_score: 0.8553\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1822 - accuracy: 0.9286 - f1_score: 0.9278 - val_loss: 0.4150 - val_accuracy: 0.8626 - val_f1_score: 0.8655\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1489 - accuracy: 0.9406 - f1_score: 0.9406 - val_loss: 0.4361 - val_accuracy: 0.8716 - val_f1_score: 0.8707\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1385 - accuracy: 0.9461 - f1_score: 0.9453 - val_loss: 0.5061 - val_accuracy: 0.8490 - val_f1_score: 0.8591\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4054 - accuracy: 0.8305 - f1_score: 0.8547\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5942 - accuracy: 0.6754 - f1_score: 0.6805 - val_loss: 0.5041 - val_accuracy: 0.7550 - val_f1_score: 0.7366\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4735 - accuracy: 0.7794 - f1_score: 0.7750 - val_loss: 0.4462 - val_accuracy: 0.7975 - val_f1_score: 0.7838\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4070 - accuracy: 0.8125 - f1_score: 0.8094 - val_loss: 0.4079 - val_accuracy: 0.8201 - val_f1_score: 0.8206\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3406 - accuracy: 0.8565 - f1_score: 0.8558 - val_loss: 0.4080 - val_accuracy: 0.8264 - val_f1_score: 0.8068\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3022 - accuracy: 0.8758 - f1_score: 0.8743 - val_loss: 0.3757 - val_accuracy: 0.8481 - val_f1_score: 0.8456\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2730 - accuracy: 0.8861 - f1_score: 0.8850 - val_loss: 0.3818 - val_accuracy: 0.8553 - val_f1_score: 0.8545\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2364 - accuracy: 0.9066 - f1_score: 0.9064 - val_loss: 0.3594 - val_accuracy: 0.8617 - val_f1_score: 0.8603\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2174 - accuracy: 0.9165 - f1_score: 0.9159 - val_loss: 0.3886 - val_accuracy: 0.8580 - val_f1_score: 0.8599\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1846 - accuracy: 0.9277 - f1_score: 0.9275 - val_loss: 0.4027 - val_accuracy: 0.8454 - val_f1_score: 0.8504\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1676 - accuracy: 0.9349 - f1_score: 0.9348 - val_loss: 0.4053 - val_accuracy: 0.8590 - val_f1_score: 0.8569\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1568 - accuracy: 0.9367 - f1_score: 0.9364 - val_loss: 0.4226 - val_accuracy: 0.8454 - val_f1_score: 0.8485\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1313 - accuracy: 0.9473 - f1_score: 0.9469 - val_loss: 0.4431 - val_accuracy: 0.8562 - val_f1_score: 0.8592\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3872 - accuracy: 0.8359 - f1_score: 0.8607\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.5857 - accuracy: 0.6884 - f1_score: 0.6826 - val_loss: 0.5096 - val_accuracy: 0.7514 - val_f1_score: 0.7542\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4761 - accuracy: 0.7734 - f1_score: 0.7706 - val_loss: 0.4757 - val_accuracy: 0.7821 - val_f1_score: 0.7823\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4188 - accuracy: 0.8159 - f1_score: 0.8135 - val_loss: 0.4535 - val_accuracy: 0.7975 - val_f1_score: 0.7996\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3541 - accuracy: 0.8418 - f1_score: 0.8398 - val_loss: 0.4333 - val_accuracy: 0.8174 - val_f1_score: 0.8196\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3010 - accuracy: 0.8749 - f1_score: 0.8733 - val_loss: 0.4738 - val_accuracy: 0.8092 - val_f1_score: 0.8263\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2511 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.4073 - val_accuracy: 0.8418 - val_f1_score: 0.8372\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2241 - accuracy: 0.9084 - f1_score: 0.9074 - val_loss: 0.4788 - val_accuracy: 0.8228 - val_f1_score: 0.8369\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1932 - accuracy: 0.9216 - f1_score: 0.9216 - val_loss: 0.4909 - val_accuracy: 0.8300 - val_f1_score: 0.8420\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1728 - accuracy: 0.9334 - f1_score: 0.9332 - val_loss: 0.4716 - val_accuracy: 0.8472 - val_f1_score: 0.8521\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1477 - accuracy: 0.9454 - f1_score: 0.9453 - val_loss: 0.4917 - val_accuracy: 0.8535 - val_f1_score: 0.8556\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1477 - accuracy: 0.9424 - f1_score: 0.9424 - val_loss: 0.5586 - val_accuracy: 0.8219 - val_f1_score: 0.8360\n","47/47 [==============================] - 0s 6ms/step - loss: 0.4008 - accuracy: 0.8265 - f1_score: 0.8519\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5984 - accuracy: 0.6778 - f1_score: 0.6781 - val_loss: 0.5053 - val_accuracy: 0.7586 - val_f1_score: 0.7385\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4830 - accuracy: 0.7761 - f1_score: 0.7693 - val_loss: 0.4721 - val_accuracy: 0.7694 - val_f1_score: 0.7401\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4277 - accuracy: 0.8077 - f1_score: 0.8014 - val_loss: 0.4140 - val_accuracy: 0.8183 - val_f1_score: 0.8120\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3723 - accuracy: 0.8360 - f1_score: 0.8334 - val_loss: 0.3950 - val_accuracy: 0.8345 - val_f1_score: 0.8228\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3302 - accuracy: 0.8583 - f1_score: 0.8543 - val_loss: 0.3850 - val_accuracy: 0.8472 - val_f1_score: 0.8539\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2771 - accuracy: 0.8903 - f1_score: 0.8891 - val_loss: 0.3589 - val_accuracy: 0.8599 - val_f1_score: 0.8600\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2530 - accuracy: 0.8942 - f1_score: 0.8928 - val_loss: 0.3469 - val_accuracy: 0.8698 - val_f1_score: 0.8669\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2069 - accuracy: 0.9147 - f1_score: 0.9140 - val_loss: 0.4072 - val_accuracy: 0.8481 - val_f1_score: 0.8554\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1948 - accuracy: 0.9219 - f1_score: 0.9213 - val_loss: 0.3815 - val_accuracy: 0.8617 - val_f1_score: 0.8522\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1656 - accuracy: 0.9355 - f1_score: 0.9351 - val_loss: 0.4511 - val_accuracy: 0.8472 - val_f1_score: 0.8571\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1375 - accuracy: 0.9470 - f1_score: 0.9469 - val_loss: 0.4267 - val_accuracy: 0.8689 - val_f1_score: 0.8602\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1479 - accuracy: 0.9433 - f1_score: 0.9429 - val_loss: 0.4324 - val_accuracy: 0.8517 - val_f1_score: 0.8586\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4191 - accuracy: 0.8190 - f1_score: 0.8433\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.5868 - accuracy: 0.6890 - f1_score: 0.6873 - val_loss: 0.5058 - val_accuracy: 0.7541 - val_f1_score: 0.7385\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4696 - accuracy: 0.7827 - f1_score: 0.7768 - val_loss: 0.4903 - val_accuracy: 0.7749 - val_f1_score: 0.7436\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4083 - accuracy: 0.8116 - f1_score: 0.8054 - val_loss: 0.4494 - val_accuracy: 0.7929 - val_f1_score: 0.7950\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3564 - accuracy: 0.8466 - f1_score: 0.8433 - val_loss: 0.4316 - val_accuracy: 0.8137 - val_f1_score: 0.7984\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3057 - accuracy: 0.8710 - f1_score: 0.8675 - val_loss: 0.4095 - val_accuracy: 0.8327 - val_f1_score: 0.8311\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2666 - accuracy: 0.8879 - f1_score: 0.8854 - val_loss: 0.4158 - val_accuracy: 0.8409 - val_f1_score: 0.8423\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2444 - accuracy: 0.8975 - f1_score: 0.8965 - val_loss: 0.4027 - val_accuracy: 0.8472 - val_f1_score: 0.8419\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2037 - accuracy: 0.9244 - f1_score: 0.9233 - val_loss: 0.4434 - val_accuracy: 0.8418 - val_f1_score: 0.8286\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1908 - accuracy: 0.9228 - f1_score: 0.9220 - val_loss: 0.4392 - val_accuracy: 0.8517 - val_f1_score: 0.8435\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1855 - accuracy: 0.9250 - f1_score: 0.9244 - val_loss: 0.4370 - val_accuracy: 0.8535 - val_f1_score: 0.8525\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1462 - accuracy: 0.9451 - f1_score: 0.9448 - val_loss: 0.4535 - val_accuracy: 0.8562 - val_f1_score: 0.8526\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1347 - accuracy: 0.9451 - f1_score: 0.9450 - val_loss: 0.4911 - val_accuracy: 0.8680 - val_f1_score: 0.8625\n","47/47 [==============================] - 0s 6ms/step - loss: 0.4165 - accuracy: 0.8224 - f1_score: 0.8488\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.5941 - accuracy: 0.6887 - f1_score: 0.6821 - val_loss: 0.5060 - val_accuracy: 0.7541 - val_f1_score: 0.7458\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4696 - accuracy: 0.7806 - f1_score: 0.7749 - val_loss: 0.4606 - val_accuracy: 0.7812 - val_f1_score: 0.7784\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4023 - accuracy: 0.8192 - f1_score: 0.8155 - val_loss: 0.4271 - val_accuracy: 0.7993 - val_f1_score: 0.7832\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3329 - accuracy: 0.8602 - f1_score: 0.8573 - val_loss: 0.4120 - val_accuracy: 0.8219 - val_f1_score: 0.8122\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2993 - accuracy: 0.8797 - f1_score: 0.8782 - val_loss: 0.3886 - val_accuracy: 0.8391 - val_f1_score: 0.8441\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2561 - accuracy: 0.8936 - f1_score: 0.8923 - val_loss: 0.3873 - val_accuracy: 0.8363 - val_f1_score: 0.8347\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2217 - accuracy: 0.9120 - f1_score: 0.9109 - val_loss: 0.3795 - val_accuracy: 0.8427 - val_f1_score: 0.8362\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1985 - accuracy: 0.9244 - f1_score: 0.9238 - val_loss: 0.4177 - val_accuracy: 0.8382 - val_f1_score: 0.8236\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1766 - accuracy: 0.9340 - f1_score: 0.9336 - val_loss: 0.3956 - val_accuracy: 0.8472 - val_f1_score: 0.8428\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1614 - accuracy: 0.9340 - f1_score: 0.9337 - val_loss: 0.3954 - val_accuracy: 0.8400 - val_f1_score: 0.8353\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1545 - accuracy: 0.9451 - f1_score: 0.9450 - val_loss: 0.4057 - val_accuracy: 0.8499 - val_f1_score: 0.8413\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1182 - accuracy: 0.9596 - f1_score: 0.9593 - val_loss: 0.4283 - val_accuracy: 0.8427 - val_f1_score: 0.8343\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4342 - accuracy: 0.8332 - f1_score: 0.8546\n"]}]},{"cell_type":"code","source":["X_sentemb_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CiIxDq88TtfY","executionInfo":{"status":"ok","timestamp":1689800478155,"user_tz":-330,"elapsed":164,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3d78b339-401e-43ca-d9e0-ee8d12eb0d19"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1106, 28)"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjS1XHW3WJ4j","executionInfo":{"status":"ok","timestamp":1689800478157,"user_tz":-330,"elapsed":115,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"51ebed74-dc55-435a-ed96-2a12e71cd1b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8392977714538574, 0.8514516949653625, 0.8406482338905334, 0.8514516949653625, 0.8413234353065491, 0.8041863441467285, 0.8257933855056763, 0.8305199146270752, 0.8298447132110596, 0.8244429230690002, 0.844699501991272, 0.8440243005752563, 0.8419986367225647, 0.8257933855056763, 0.8271438479423523, 0.8507764935493469, 0.8244429230690002, 0.8555030226707458, 0.8379473090171814, 0.8305199146270752, 0.8359216451644897, 0.8264685869216919, 0.8190411925315857, 0.8224172592163086, 0.8332207798957825]\n","0.8343551564216614\n","0.011804903697468953\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WQ-Xp6VWJ7T","executionInfo":{"status":"ok","timestamp":1689800478158,"user_tz":-330,"elapsed":109,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"61cc6b0e-44bc-48a1-80d4-f1cf3d2fa7b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.40846267342567444, 0.37287411093711853, 0.37155482172966003, 0.38073039054870605, 0.4066043496131897, 0.5059119462966919, 0.4211104214191437, 0.4427274167537689, 0.4087826907634735, 0.4556005597114563, 0.3759515881538391, 0.3811666667461395, 0.3721076548099518, 0.4261902868747711, 0.4347975254058838, 0.37465900182724, 0.4344838857650757, 0.3787654638290405, 0.37226828932762146, 0.40540042519569397, 0.38722866773605347, 0.4007585346698761, 0.4191334545612335, 0.4165077805519104, 0.4341703951358795]\n","0.4075179600715637\n","0.03220794951716746\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uTVa4qJWJ-O","executionInfo":{"status":"ok","timestamp":1689800478159,"user_tz":-330,"elapsed":102,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"2d725024-35ed-40ee-f7d9-6c6f117f6dba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8632183074951172, 0.8768196702003479, 0.8707557320594788, 0.877232015132904, 0.8670061826705933, 0.8279951810836792, 0.8525713682174683, 0.8538147211074829, 0.8566552400588989, 0.8572996258735657, 0.8722221255302429, 0.8728672862052917, 0.8698552846908569, 0.8517241477966309, 0.8494117259979248, 0.8759123086929321, 0.8500576019287109, 0.8807134032249451, 0.8701298832893372, 0.854661226272583, 0.8607448935508728, 0.8518730998039246, 0.843274712562561, 0.8487635850906372, 0.8546202778816223]\n","0.8604079842567444\n","0.012479531537493743\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dpekPoR6UVaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM CNN LSTM nothing"],"metadata":{"id":"MoQ40JtTV3iB"}},{"cell_type":"code","source":["test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Standardization of train and test LIWC features\n","        train_scaled_liwc = sc.fit_transform(X_liwc_train)\n","        val_scaled_liwc = sc.fit_transform(X_liwc_val)\n","        test_scaled_liwc = sc.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ywKJuR27V5Ni","executionInfo":{"status":"ok","timestamp":1689801069236,"user_tz":-330,"elapsed":591171,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3ad81013-d069-45f8-ec45-e81e7d9e5861"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.5848 - accuracy: 0.6899 - f1_score: 0.6829 - val_loss: 0.4810 - val_accuracy: 0.7975 - val_f1_score: 0.7934\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4847 - accuracy: 0.7676 - f1_score: 0.7570 - val_loss: 0.4362 - val_accuracy: 0.8047 - val_f1_score: 0.7978\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4263 - accuracy: 0.8068 - f1_score: 0.7981 - val_loss: 0.4162 - val_accuracy: 0.8110 - val_f1_score: 0.8102\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3763 - accuracy: 0.8351 - f1_score: 0.8320 - val_loss: 0.3823 - val_accuracy: 0.8291 - val_f1_score: 0.8242\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3281 - accuracy: 0.8590 - f1_score: 0.8557 - val_loss: 0.3642 - val_accuracy: 0.8418 - val_f1_score: 0.8341\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2747 - accuracy: 0.8803 - f1_score: 0.8776 - val_loss: 0.3846 - val_accuracy: 0.8318 - val_f1_score: 0.8413\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2442 - accuracy: 0.8981 - f1_score: 0.8973 - val_loss: 0.3432 - val_accuracy: 0.8626 - val_f1_score: 0.8587\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2237 - accuracy: 0.9066 - f1_score: 0.9057 - val_loss: 0.3669 - val_accuracy: 0.8617 - val_f1_score: 0.8642\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2002 - accuracy: 0.9186 - f1_score: 0.9179 - val_loss: 0.3694 - val_accuracy: 0.8571 - val_f1_score: 0.8592\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1760 - accuracy: 0.9298 - f1_score: 0.9291 - val_loss: 0.3877 - val_accuracy: 0.8590 - val_f1_score: 0.8545\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1614 - accuracy: 0.9361 - f1_score: 0.9358 - val_loss: 0.3982 - val_accuracy: 0.8662 - val_f1_score: 0.8709\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1415 - accuracy: 0.9454 - f1_score: 0.9452 - val_loss: 0.4027 - val_accuracy: 0.8608 - val_f1_score: 0.8679\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4623 - accuracy: 0.8123 - f1_score: 0.8353\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.5764 - accuracy: 0.6947 - f1_score: 0.6894 - val_loss: 0.5294 - val_accuracy: 0.7423 - val_f1_score: 0.7278\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4665 - accuracy: 0.7773 - f1_score: 0.7678 - val_loss: 0.4887 - val_accuracy: 0.7667 - val_f1_score: 0.7543\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4038 - accuracy: 0.8180 - f1_score: 0.8111 - val_loss: 0.4696 - val_accuracy: 0.7893 - val_f1_score: 0.7860\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3654 - accuracy: 0.8366 - f1_score: 0.8331 - val_loss: 0.4407 - val_accuracy: 0.8029 - val_f1_score: 0.7900\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3140 - accuracy: 0.8608 - f1_score: 0.8577 - val_loss: 0.4612 - val_accuracy: 0.8029 - val_f1_score: 0.8159\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2797 - accuracy: 0.8740 - f1_score: 0.8715 - val_loss: 0.4077 - val_accuracy: 0.8255 - val_f1_score: 0.8266\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2358 - accuracy: 0.8990 - f1_score: 0.8973 - val_loss: 0.4047 - val_accuracy: 0.8409 - val_f1_score: 0.8382\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2044 - accuracy: 0.9183 - f1_score: 0.9177 - val_loss: 0.4340 - val_accuracy: 0.8418 - val_f1_score: 0.8430\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1826 - accuracy: 0.9313 - f1_score: 0.9311 - val_loss: 0.4652 - val_accuracy: 0.8409 - val_f1_score: 0.8394\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1552 - accuracy: 0.9382 - f1_score: 0.9378 - val_loss: 0.4703 - val_accuracy: 0.8463 - val_f1_score: 0.8432\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1569 - accuracy: 0.9406 - f1_score: 0.9402 - val_loss: 0.4633 - val_accuracy: 0.8526 - val_f1_score: 0.8514\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1219 - accuracy: 0.9521 - f1_score: 0.9518 - val_loss: 0.5173 - val_accuracy: 0.8544 - val_f1_score: 0.8532\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4147 - accuracy: 0.8238 - f1_score: 0.8490\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.5842 - accuracy: 0.6983 - f1_score: 0.7039 - val_loss: 0.5342 - val_accuracy: 0.7324 - val_f1_score: 0.7058\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4679 - accuracy: 0.7800 - f1_score: 0.7736 - val_loss: 0.5140 - val_accuracy: 0.7514 - val_f1_score: 0.7465\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4023 - accuracy: 0.8234 - f1_score: 0.8198 - val_loss: 0.4787 - val_accuracy: 0.7712 - val_f1_score: 0.7672\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3427 - accuracy: 0.8505 - f1_score: 0.8484 - val_loss: 0.5022 - val_accuracy: 0.7749 - val_f1_score: 0.7833\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2934 - accuracy: 0.8831 - f1_score: 0.8813 - val_loss: 0.4692 - val_accuracy: 0.7957 - val_f1_score: 0.7835\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2641 - accuracy: 0.8951 - f1_score: 0.8938 - val_loss: 0.4471 - val_accuracy: 0.8165 - val_f1_score: 0.8046\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2253 - accuracy: 0.9111 - f1_score: 0.9100 - val_loss: 0.5048 - val_accuracy: 0.8074 - val_f1_score: 0.8103\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2049 - accuracy: 0.9174 - f1_score: 0.9169 - val_loss: 0.5251 - val_accuracy: 0.7939 - val_f1_score: 0.8034\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1851 - accuracy: 0.9241 - f1_score: 0.9232 - val_loss: 0.4671 - val_accuracy: 0.8192 - val_f1_score: 0.8172\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1583 - accuracy: 0.9373 - f1_score: 0.9370 - val_loss: 0.5204 - val_accuracy: 0.8146 - val_f1_score: 0.8114\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1347 - accuracy: 0.9515 - f1_score: 0.9512 - val_loss: 0.5635 - val_accuracy: 0.8119 - val_f1_score: 0.8163\n","47/47 [==============================] - 0s 7ms/step - loss: 0.4258 - accuracy: 0.8298 - f1_score: 0.8509\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6052 - accuracy: 0.6615 - f1_score: 0.6388 - val_loss: 0.4939 - val_accuracy: 0.7676 - val_f1_score: 0.7526\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4960 - accuracy: 0.7607 - f1_score: 0.7463 - val_loss: 0.4612 - val_accuracy: 0.8011 - val_f1_score: 0.8046\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4297 - accuracy: 0.7917 - f1_score: 0.7835 - val_loss: 0.4303 - val_accuracy: 0.8047 - val_f1_score: 0.8018\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3695 - accuracy: 0.8342 - f1_score: 0.8296 - val_loss: 0.4199 - val_accuracy: 0.8237 - val_f1_score: 0.8209\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3149 - accuracy: 0.8605 - f1_score: 0.8583 - val_loss: 0.4114 - val_accuracy: 0.8228 - val_f1_score: 0.8284\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2872 - accuracy: 0.8734 - f1_score: 0.8718 - val_loss: 0.4299 - val_accuracy: 0.8237 - val_f1_score: 0.8273\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2396 - accuracy: 0.9036 - f1_score: 0.9030 - val_loss: 0.4196 - val_accuracy: 0.8454 - val_f1_score: 0.8458\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2053 - accuracy: 0.9183 - f1_score: 0.9177 - val_loss: 0.4407 - val_accuracy: 0.8445 - val_f1_score: 0.8410\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1769 - accuracy: 0.9295 - f1_score: 0.9290 - val_loss: 0.4717 - val_accuracy: 0.8571 - val_f1_score: 0.8507\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1601 - accuracy: 0.9385 - f1_score: 0.9383 - val_loss: 0.4563 - val_accuracy: 0.8526 - val_f1_score: 0.8525\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3912 - accuracy: 0.8346 - f1_score: 0.8627\n","Epoch 1/20\n","104/104 [==============================] - 8s 24ms/step - loss: 0.5869 - accuracy: 0.6911 - f1_score: 0.6820 - val_loss: 0.5287 - val_accuracy: 0.7459 - val_f1_score: 0.7193\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4886 - accuracy: 0.7622 - f1_score: 0.7580 - val_loss: 0.4749 - val_accuracy: 0.7893 - val_f1_score: 0.7816\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4274 - accuracy: 0.8074 - f1_score: 0.8037 - val_loss: 0.4246 - val_accuracy: 0.8146 - val_f1_score: 0.8046\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3617 - accuracy: 0.8403 - f1_score: 0.8371 - val_loss: 0.3995 - val_accuracy: 0.8246 - val_f1_score: 0.8348\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3099 - accuracy: 0.8647 - f1_score: 0.8631 - val_loss: 0.3602 - val_accuracy: 0.8562 - val_f1_score: 0.8529\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2728 - accuracy: 0.8906 - f1_score: 0.8898 - val_loss: 0.3496 - val_accuracy: 0.8644 - val_f1_score: 0.8651\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2280 - accuracy: 0.9063 - f1_score: 0.9053 - val_loss: 0.3945 - val_accuracy: 0.8391 - val_f1_score: 0.8494\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2216 - accuracy: 0.9126 - f1_score: 0.9122 - val_loss: 0.4035 - val_accuracy: 0.8264 - val_f1_score: 0.8416\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2058 - accuracy: 0.9156 - f1_score: 0.9152 - val_loss: 0.3890 - val_accuracy: 0.8535 - val_f1_score: 0.8594\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1762 - accuracy: 0.9316 - f1_score: 0.9316 - val_loss: 0.3882 - val_accuracy: 0.8635 - val_f1_score: 0.8655\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1529 - accuracy: 0.9427 - f1_score: 0.9427 - val_loss: 0.4068 - val_accuracy: 0.8626 - val_f1_score: 0.8631\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3909 - accuracy: 0.8433 - f1_score: 0.8677\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.5929 - accuracy: 0.6820 - f1_score: 0.6786 - val_loss: 0.4815 - val_accuracy: 0.7676 - val_f1_score: 0.7618\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4723 - accuracy: 0.7782 - f1_score: 0.7707 - val_loss: 0.4141 - val_accuracy: 0.8083 - val_f1_score: 0.8004\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3929 - accuracy: 0.8228 - f1_score: 0.8189 - val_loss: 0.3734 - val_accuracy: 0.8373 - val_f1_score: 0.8273\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3276 - accuracy: 0.8602 - f1_score: 0.8578 - val_loss: 0.3333 - val_accuracy: 0.8571 - val_f1_score: 0.8542\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2872 - accuracy: 0.8770 - f1_score: 0.8762 - val_loss: 0.3375 - val_accuracy: 0.8571 - val_f1_score: 0.8561\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2670 - accuracy: 0.8876 - f1_score: 0.8860 - val_loss: 0.3144 - val_accuracy: 0.8707 - val_f1_score: 0.8727\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2111 - accuracy: 0.9108 - f1_score: 0.9099 - val_loss: 0.3069 - val_accuracy: 0.8752 - val_f1_score: 0.8752\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1995 - accuracy: 0.9204 - f1_score: 0.9204 - val_loss: 0.3326 - val_accuracy: 0.8743 - val_f1_score: 0.8744\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1725 - accuracy: 0.9334 - f1_score: 0.9329 - val_loss: 0.3432 - val_accuracy: 0.8725 - val_f1_score: 0.8712\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1676 - accuracy: 0.9361 - f1_score: 0.9359 - val_loss: 0.3658 - val_accuracy: 0.8707 - val_f1_score: 0.8736\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1405 - accuracy: 0.9488 - f1_score: 0.9486 - val_loss: 0.3545 - val_accuracy: 0.8843 - val_f1_score: 0.8824\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1265 - accuracy: 0.9536 - f1_score: 0.9538 - val_loss: 0.3648 - val_accuracy: 0.8788 - val_f1_score: 0.8780\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4368 - accuracy: 0.8427 - f1_score: 0.8696\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.5760 - accuracy: 0.6998 - f1_score: 0.6973 - val_loss: 0.4895 - val_accuracy: 0.7559 - val_f1_score: 0.7429\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4620 - accuracy: 0.7881 - f1_score: 0.7811 - val_loss: 0.4419 - val_accuracy: 0.7875 - val_f1_score: 0.7842\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3947 - accuracy: 0.8282 - f1_score: 0.8228 - val_loss: 0.4233 - val_accuracy: 0.8065 - val_f1_score: 0.7803\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3468 - accuracy: 0.8469 - f1_score: 0.8448 - val_loss: 0.3782 - val_accuracy: 0.8246 - val_f1_score: 0.8102\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2972 - accuracy: 0.8716 - f1_score: 0.8696 - val_loss: 0.3878 - val_accuracy: 0.8345 - val_f1_score: 0.8450\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2646 - accuracy: 0.8894 - f1_score: 0.8881 - val_loss: 0.3311 - val_accuracy: 0.8571 - val_f1_score: 0.8515\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2321 - accuracy: 0.9075 - f1_score: 0.9062 - val_loss: 0.3621 - val_accuracy: 0.8490 - val_f1_score: 0.8423\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2055 - accuracy: 0.9195 - f1_score: 0.9189 - val_loss: 0.3571 - val_accuracy: 0.8544 - val_f1_score: 0.8502\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1764 - accuracy: 0.9301 - f1_score: 0.9290 - val_loss: 0.3829 - val_accuracy: 0.8653 - val_f1_score: 0.8624\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1645 - accuracy: 0.9403 - f1_score: 0.9402 - val_loss: 0.3616 - val_accuracy: 0.8653 - val_f1_score: 0.8685\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1448 - accuracy: 0.9482 - f1_score: 0.9478 - val_loss: 0.3943 - val_accuracy: 0.8707 - val_f1_score: 0.8717\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4089 - accuracy: 0.8224 - f1_score: 0.8475\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.5687 - accuracy: 0.7025 - f1_score: 0.7024 - val_loss: 0.5012 - val_accuracy: 0.7613 - val_f1_score: 0.7505\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4675 - accuracy: 0.7815 - f1_score: 0.7767 - val_loss: 0.4637 - val_accuracy: 0.7893 - val_f1_score: 0.7740\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4064 - accuracy: 0.8189 - f1_score: 0.8150 - val_loss: 0.4433 - val_accuracy: 0.7966 - val_f1_score: 0.8028\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3440 - accuracy: 0.8511 - f1_score: 0.8503 - val_loss: 0.4209 - val_accuracy: 0.8273 - val_f1_score: 0.8176\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2859 - accuracy: 0.8797 - f1_score: 0.8785 - val_loss: 0.4269 - val_accuracy: 0.8318 - val_f1_score: 0.8208\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2611 - accuracy: 0.8858 - f1_score: 0.8843 - val_loss: 0.3789 - val_accuracy: 0.8499 - val_f1_score: 0.8474\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2213 - accuracy: 0.9108 - f1_score: 0.9094 - val_loss: 0.3958 - val_accuracy: 0.8517 - val_f1_score: 0.8506\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1956 - accuracy: 0.9244 - f1_score: 0.9237 - val_loss: 0.4497 - val_accuracy: 0.8445 - val_f1_score: 0.8478\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1773 - accuracy: 0.9277 - f1_score: 0.9271 - val_loss: 0.4255 - val_accuracy: 0.8526 - val_f1_score: 0.8419\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1512 - accuracy: 0.9412 - f1_score: 0.9409 - val_loss: 0.4465 - val_accuracy: 0.8617 - val_f1_score: 0.8595\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1246 - accuracy: 0.9530 - f1_score: 0.9528 - val_loss: 0.5060 - val_accuracy: 0.8535 - val_f1_score: 0.8574\n","47/47 [==============================] - 0s 7ms/step - loss: 0.4427 - accuracy: 0.8163 - f1_score: 0.8417\n","Epoch 1/20\n","104/104 [==============================] - 10s 26ms/step - loss: 0.5649 - accuracy: 0.7071 - f1_score: 0.6994 - val_loss: 0.5115 - val_accuracy: 0.7631 - val_f1_score: 0.7673\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4639 - accuracy: 0.7812 - f1_score: 0.7759 - val_loss: 0.4737 - val_accuracy: 0.7821 - val_f1_score: 0.7707\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4016 - accuracy: 0.8159 - f1_score: 0.8111 - val_loss: 0.4426 - val_accuracy: 0.8029 - val_f1_score: 0.7974\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3572 - accuracy: 0.8475 - f1_score: 0.8429 - val_loss: 0.4011 - val_accuracy: 0.8327 - val_f1_score: 0.8298\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3045 - accuracy: 0.8644 - f1_score: 0.8620 - val_loss: 0.4148 - val_accuracy: 0.8237 - val_f1_score: 0.8094\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2615 - accuracy: 0.8840 - f1_score: 0.8824 - val_loss: 0.3786 - val_accuracy: 0.8463 - val_f1_score: 0.8482\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2364 - accuracy: 0.9036 - f1_score: 0.9033 - val_loss: 0.3794 - val_accuracy: 0.8535 - val_f1_score: 0.8492\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1946 - accuracy: 0.9195 - f1_score: 0.9192 - val_loss: 0.4050 - val_accuracy: 0.8490 - val_f1_score: 0.8562\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1866 - accuracy: 0.9292 - f1_score: 0.9290 - val_loss: 0.4105 - val_accuracy: 0.8526 - val_f1_score: 0.8425\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1729 - accuracy: 0.9289 - f1_score: 0.9285 - val_loss: 0.4331 - val_accuracy: 0.8517 - val_f1_score: 0.8484\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1426 - accuracy: 0.9412 - f1_score: 0.9412 - val_loss: 0.4552 - val_accuracy: 0.8553 - val_f1_score: 0.8473\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8292 - f1_score: 0.8587\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.5883 - accuracy: 0.6778 - f1_score: 0.6802 - val_loss: 0.4943 - val_accuracy: 0.7785 - val_f1_score: 0.7660\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4761 - accuracy: 0.7818 - f1_score: 0.7798 - val_loss: 0.4552 - val_accuracy: 0.7920 - val_f1_score: 0.7946\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4034 - accuracy: 0.8189 - f1_score: 0.8169 - val_loss: 0.4129 - val_accuracy: 0.8119 - val_f1_score: 0.8126\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3376 - accuracy: 0.8520 - f1_score: 0.8510 - val_loss: 0.3928 - val_accuracy: 0.8282 - val_f1_score: 0.8263\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2923 - accuracy: 0.8749 - f1_score: 0.8735 - val_loss: 0.3800 - val_accuracy: 0.8327 - val_f1_score: 0.8292\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2421 - accuracy: 0.9033 - f1_score: 0.9026 - val_loss: 0.3631 - val_accuracy: 0.8363 - val_f1_score: 0.8307\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2318 - accuracy: 0.9027 - f1_score: 0.9023 - val_loss: 0.3861 - val_accuracy: 0.8327 - val_f1_score: 0.8393\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2033 - accuracy: 0.9168 - f1_score: 0.9163 - val_loss: 0.3745 - val_accuracy: 0.8454 - val_f1_score: 0.8466\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1637 - accuracy: 0.9367 - f1_score: 0.9362 - val_loss: 0.4095 - val_accuracy: 0.8481 - val_f1_score: 0.8444\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1589 - accuracy: 0.9391 - f1_score: 0.9388 - val_loss: 0.3980 - val_accuracy: 0.8508 - val_f1_score: 0.8454\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1336 - accuracy: 0.9461 - f1_score: 0.9461 - val_loss: 0.4127 - val_accuracy: 0.8481 - val_f1_score: 0.8453\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.8298 - f1_score: 0.8548\n","Epoch 1/20\n","104/104 [==============================] - 9s 20ms/step - loss: 0.5969 - accuracy: 0.6715 - f1_score: 0.6667 - val_loss: 0.5222 - val_accuracy: 0.7468 - val_f1_score: 0.7393\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4692 - accuracy: 0.7737 - f1_score: 0.7644 - val_loss: 0.5003 - val_accuracy: 0.7740 - val_f1_score: 0.7444\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4084 - accuracy: 0.8134 - f1_score: 0.8072 - val_loss: 0.4497 - val_accuracy: 0.8047 - val_f1_score: 0.7874\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3340 - accuracy: 0.8547 - f1_score: 0.8510 - val_loss: 0.4397 - val_accuracy: 0.8011 - val_f1_score: 0.8084\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2907 - accuracy: 0.8788 - f1_score: 0.8762 - val_loss: 0.4084 - val_accuracy: 0.8336 - val_f1_score: 0.8324\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2516 - accuracy: 0.9011 - f1_score: 0.9000 - val_loss: 0.4153 - val_accuracy: 0.8327 - val_f1_score: 0.8240\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2310 - accuracy: 0.9036 - f1_score: 0.9026 - val_loss: 0.4194 - val_accuracy: 0.8427 - val_f1_score: 0.8358\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2038 - accuracy: 0.9219 - f1_score: 0.9212 - val_loss: 0.4296 - val_accuracy: 0.8454 - val_f1_score: 0.8450\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1805 - accuracy: 0.9298 - f1_score: 0.9289 - val_loss: 0.4338 - val_accuracy: 0.8517 - val_f1_score: 0.8538\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1610 - accuracy: 0.9400 - f1_score: 0.9395 - val_loss: 0.4715 - val_accuracy: 0.8463 - val_f1_score: 0.8384\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3563 - accuracy: 0.8440 - f1_score: 0.8703\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6015 - accuracy: 0.6670 - f1_score: 0.6677 - val_loss: 0.5116 - val_accuracy: 0.7414 - val_f1_score: 0.7008\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4850 - accuracy: 0.7685 - f1_score: 0.7576 - val_loss: 0.4539 - val_accuracy: 0.7803 - val_f1_score: 0.7606\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4228 - accuracy: 0.8116 - f1_score: 0.8030 - val_loss: 0.4150 - val_accuracy: 0.8056 - val_f1_score: 0.8102\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3575 - accuracy: 0.8421 - f1_score: 0.8388 - val_loss: 0.3942 - val_accuracy: 0.8137 - val_f1_score: 0.8218\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3055 - accuracy: 0.8746 - f1_score: 0.8723 - val_loss: 0.3656 - val_accuracy: 0.8336 - val_f1_score: 0.8357\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2704 - accuracy: 0.8873 - f1_score: 0.8856 - val_loss: 0.3563 - val_accuracy: 0.8499 - val_f1_score: 0.8474\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2347 - accuracy: 0.9081 - f1_score: 0.9073 - val_loss: 0.3594 - val_accuracy: 0.8517 - val_f1_score: 0.8525\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1942 - accuracy: 0.9222 - f1_score: 0.9212 - val_loss: 0.3898 - val_accuracy: 0.8454 - val_f1_score: 0.8385\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1928 - accuracy: 0.9147 - f1_score: 0.9141 - val_loss: 0.3978 - val_accuracy: 0.8400 - val_f1_score: 0.8316\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1658 - accuracy: 0.9382 - f1_score: 0.9380 - val_loss: 0.4104 - val_accuracy: 0.8544 - val_f1_score: 0.8511\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1425 - accuracy: 0.9436 - f1_score: 0.9434 - val_loss: 0.4100 - val_accuracy: 0.8526 - val_f1_score: 0.8551\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3937 - accuracy: 0.8346 - f1_score: 0.8609\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.5997 - accuracy: 0.6748 - f1_score: 0.6669 - val_loss: 0.5247 - val_accuracy: 0.7306 - val_f1_score: 0.7026\n","Epoch 2/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.4828 - accuracy: 0.7673 - f1_score: 0.7560 - val_loss: 0.4876 - val_accuracy: 0.7532 - val_f1_score: 0.7240\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4189 - accuracy: 0.8068 - f1_score: 0.7996 - val_loss: 0.4599 - val_accuracy: 0.7875 - val_f1_score: 0.7707\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3670 - accuracy: 0.8409 - f1_score: 0.8368 - val_loss: 0.4296 - val_accuracy: 0.8128 - val_f1_score: 0.8110\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3250 - accuracy: 0.8593 - f1_score: 0.8551 - val_loss: 0.4202 - val_accuracy: 0.8201 - val_f1_score: 0.8074\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2728 - accuracy: 0.8813 - f1_score: 0.8790 - val_loss: 0.4275 - val_accuracy: 0.8309 - val_f1_score: 0.8204\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2437 - accuracy: 0.9008 - f1_score: 0.8995 - val_loss: 0.4009 - val_accuracy: 0.8472 - val_f1_score: 0.8442\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2171 - accuracy: 0.9126 - f1_score: 0.9120 - val_loss: 0.4105 - val_accuracy: 0.8499 - val_f1_score: 0.8454\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1826 - accuracy: 0.9289 - f1_score: 0.9287 - val_loss: 0.4347 - val_accuracy: 0.8427 - val_f1_score: 0.8371\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1654 - accuracy: 0.9364 - f1_score: 0.9363 - val_loss: 0.4773 - val_accuracy: 0.8454 - val_f1_score: 0.8472\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1431 - accuracy: 0.9442 - f1_score: 0.9441 - val_loss: 0.4784 - val_accuracy: 0.8508 - val_f1_score: 0.8566\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1313 - accuracy: 0.9506 - f1_score: 0.9504 - val_loss: 0.4924 - val_accuracy: 0.8427 - val_f1_score: 0.8320\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4052 - accuracy: 0.8447 - f1_score: 0.8686\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6017 - accuracy: 0.6754 - f1_score: 0.6778 - val_loss: 0.5036 - val_accuracy: 0.7685 - val_f1_score: 0.7656\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4657 - accuracy: 0.7800 - f1_score: 0.7707 - val_loss: 0.4772 - val_accuracy: 0.7559 - val_f1_score: 0.7039\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3980 - accuracy: 0.8225 - f1_score: 0.8155 - val_loss: 0.4018 - val_accuracy: 0.8092 - val_f1_score: 0.8118\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3345 - accuracy: 0.8596 - f1_score: 0.8568 - val_loss: 0.3605 - val_accuracy: 0.8463 - val_f1_score: 0.8359\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3030 - accuracy: 0.8677 - f1_score: 0.8649 - val_loss: 0.3447 - val_accuracy: 0.8535 - val_f1_score: 0.8494\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2665 - accuracy: 0.8888 - f1_score: 0.8866 - val_loss: 0.3599 - val_accuracy: 0.8490 - val_f1_score: 0.8396\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2361 - accuracy: 0.9024 - f1_score: 0.9013 - val_loss: 0.3756 - val_accuracy: 0.8472 - val_f1_score: 0.8345\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2218 - accuracy: 0.9102 - f1_score: 0.9093 - val_loss: 0.3373 - val_accuracy: 0.8635 - val_f1_score: 0.8601\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1784 - accuracy: 0.9316 - f1_score: 0.9313 - val_loss: 0.3765 - val_accuracy: 0.8617 - val_f1_score: 0.8577\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1593 - accuracy: 0.9349 - f1_score: 0.9345 - val_loss: 0.3657 - val_accuracy: 0.8580 - val_f1_score: 0.8607\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1424 - accuracy: 0.9473 - f1_score: 0.9470 - val_loss: 0.3936 - val_accuracy: 0.8580 - val_f1_score: 0.8550\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1318 - accuracy: 0.9482 - f1_score: 0.9478 - val_loss: 0.3687 - val_accuracy: 0.8689 - val_f1_score: 0.8704\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1283 - accuracy: 0.9530 - f1_score: 0.9528 - val_loss: 0.4142 - val_accuracy: 0.8535 - val_f1_score: 0.8454\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3884 - accuracy: 0.8515 - f1_score: 0.8763\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.5956 - accuracy: 0.6814 - f1_score: 0.6813 - val_loss: 0.5443 - val_accuracy: 0.7306 - val_f1_score: 0.6830\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4901 - accuracy: 0.7625 - f1_score: 0.7487 - val_loss: 0.4739 - val_accuracy: 0.7631 - val_f1_score: 0.7481\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4161 - accuracy: 0.8041 - f1_score: 0.7971 - val_loss: 0.4267 - val_accuracy: 0.8011 - val_f1_score: 0.7967\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3505 - accuracy: 0.8490 - f1_score: 0.8460 - val_loss: 0.3905 - val_accuracy: 0.8110 - val_f1_score: 0.8015\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3002 - accuracy: 0.8743 - f1_score: 0.8723 - val_loss: 0.4162 - val_accuracy: 0.8119 - val_f1_score: 0.8287\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2684 - accuracy: 0.8876 - f1_score: 0.8868 - val_loss: 0.3650 - val_accuracy: 0.8336 - val_f1_score: 0.8244\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2385 - accuracy: 0.8993 - f1_score: 0.8979 - val_loss: 0.3688 - val_accuracy: 0.8391 - val_f1_score: 0.8333\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2110 - accuracy: 0.9174 - f1_score: 0.9166 - val_loss: 0.3968 - val_accuracy: 0.8327 - val_f1_score: 0.8174\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2013 - accuracy: 0.9180 - f1_score: 0.9177 - val_loss: 0.3671 - val_accuracy: 0.8445 - val_f1_score: 0.8431\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1699 - accuracy: 0.9334 - f1_score: 0.9334 - val_loss: 0.4423 - val_accuracy: 0.8382 - val_f1_score: 0.8215\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1510 - accuracy: 0.9397 - f1_score: 0.9397 - val_loss: 0.3955 - val_accuracy: 0.8481 - val_f1_score: 0.8531\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4164 - accuracy: 0.8386 - f1_score: 0.8610\n","Epoch 1/20\n","104/104 [==============================] - 9s 18ms/step - loss: 0.5797 - accuracy: 0.6923 - f1_score: 0.6924 - val_loss: 0.5244 - val_accuracy: 0.7450 - val_f1_score: 0.7662\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4768 - accuracy: 0.7731 - f1_score: 0.7652 - val_loss: 0.4613 - val_accuracy: 0.7911 - val_f1_score: 0.7823\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4083 - accuracy: 0.8165 - f1_score: 0.8138 - val_loss: 0.4283 - val_accuracy: 0.8146 - val_f1_score: 0.8023\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3453 - accuracy: 0.8532 - f1_score: 0.8497 - val_loss: 0.4115 - val_accuracy: 0.8291 - val_f1_score: 0.8344\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3011 - accuracy: 0.8716 - f1_score: 0.8695 - val_loss: 0.3915 - val_accuracy: 0.8363 - val_f1_score: 0.8313\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2622 - accuracy: 0.8888 - f1_score: 0.8872 - val_loss: 0.3915 - val_accuracy: 0.8400 - val_f1_score: 0.8468\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2385 - accuracy: 0.9042 - f1_score: 0.9035 - val_loss: 0.3795 - val_accuracy: 0.8526 - val_f1_score: 0.8603\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2022 - accuracy: 0.9247 - f1_score: 0.9239 - val_loss: 0.3761 - val_accuracy: 0.8436 - val_f1_score: 0.8411\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1694 - accuracy: 0.9337 - f1_score: 0.9335 - val_loss: 0.4062 - val_accuracy: 0.8517 - val_f1_score: 0.8453\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1687 - accuracy: 0.9289 - f1_score: 0.9287 - val_loss: 0.4023 - val_accuracy: 0.8590 - val_f1_score: 0.8655\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1411 - accuracy: 0.9467 - f1_score: 0.9466 - val_loss: 0.4058 - val_accuracy: 0.8571 - val_f1_score: 0.8571\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1283 - accuracy: 0.9506 - f1_score: 0.9503 - val_loss: 0.5088 - val_accuracy: 0.8400 - val_f1_score: 0.8266\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1221 - accuracy: 0.9527 - f1_score: 0.9526 - val_loss: 0.4772 - val_accuracy: 0.8499 - val_f1_score: 0.8541\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3950 - accuracy: 0.8339 - f1_score: 0.8573\n","Epoch 1/20\n","104/104 [==============================] - 8s 27ms/step - loss: 0.5775 - accuracy: 0.6965 - f1_score: 0.6883 - val_loss: 0.5210 - val_accuracy: 0.7550 - val_f1_score: 0.7366\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4520 - accuracy: 0.7878 - f1_score: 0.7783 - val_loss: 0.5042 - val_accuracy: 0.7722 - val_f1_score: 0.7857\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3870 - accuracy: 0.8261 - f1_score: 0.8201 - val_loss: 0.4239 - val_accuracy: 0.8192 - val_f1_score: 0.8134\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3241 - accuracy: 0.8641 - f1_score: 0.8624 - val_loss: 0.4210 - val_accuracy: 0.8183 - val_f1_score: 0.7988\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2807 - accuracy: 0.8828 - f1_score: 0.8821 - val_loss: 0.3770 - val_accuracy: 0.8535 - val_f1_score: 0.8477\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2439 - accuracy: 0.9011 - f1_score: 0.8999 - val_loss: 0.4164 - val_accuracy: 0.8499 - val_f1_score: 0.8395\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2164 - accuracy: 0.9144 - f1_score: 0.9138 - val_loss: 0.3982 - val_accuracy: 0.8689 - val_f1_score: 0.8685\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1936 - accuracy: 0.9237 - f1_score: 0.9229 - val_loss: 0.4076 - val_accuracy: 0.8680 - val_f1_score: 0.8615\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1669 - accuracy: 0.9334 - f1_score: 0.9330 - val_loss: 0.4281 - val_accuracy: 0.8725 - val_f1_score: 0.8722\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1557 - accuracy: 0.9379 - f1_score: 0.9371 - val_loss: 0.4264 - val_accuracy: 0.8671 - val_f1_score: 0.8599\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4186 - accuracy: 0.8177 - f1_score: 0.8397\n","Epoch 1/20\n","104/104 [==============================] - 9s 27ms/step - loss: 0.5861 - accuracy: 0.6893 - f1_score: 0.6877 - val_loss: 0.4865 - val_accuracy: 0.7631 - val_f1_score: 0.7528\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4711 - accuracy: 0.7791 - f1_score: 0.7736 - val_loss: 0.4622 - val_accuracy: 0.7893 - val_f1_score: 0.8003\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4155 - accuracy: 0.8119 - f1_score: 0.8102 - val_loss: 0.4181 - val_accuracy: 0.8101 - val_f1_score: 0.8115\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3517 - accuracy: 0.8457 - f1_score: 0.8427 - val_loss: 0.4088 - val_accuracy: 0.8192 - val_f1_score: 0.8000\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3048 - accuracy: 0.8737 - f1_score: 0.8719 - val_loss: 0.3662 - val_accuracy: 0.8481 - val_f1_score: 0.8467\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2580 - accuracy: 0.8945 - f1_score: 0.8938 - val_loss: 0.3622 - val_accuracy: 0.8472 - val_f1_score: 0.8410\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2369 - accuracy: 0.9033 - f1_score: 0.9027 - val_loss: 0.3614 - val_accuracy: 0.8526 - val_f1_score: 0.8446\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2027 - accuracy: 0.9231 - f1_score: 0.9228 - val_loss: 0.3410 - val_accuracy: 0.8653 - val_f1_score: 0.8642\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1819 - accuracy: 0.9349 - f1_score: 0.9343 - val_loss: 0.3733 - val_accuracy: 0.8608 - val_f1_score: 0.8585\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1716 - accuracy: 0.9328 - f1_score: 0.9327 - val_loss: 0.3698 - val_accuracy: 0.8689 - val_f1_score: 0.8693\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1382 - accuracy: 0.9485 - f1_score: 0.9485 - val_loss: 0.4129 - val_accuracy: 0.8544 - val_f1_score: 0.8581\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1303 - accuracy: 0.9506 - f1_score: 0.9504 - val_loss: 0.4318 - val_accuracy: 0.8635 - val_f1_score: 0.8690\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1255 - accuracy: 0.9512 - f1_score: 0.9510 - val_loss: 0.4348 - val_accuracy: 0.8716 - val_f1_score: 0.8734\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.8542 - f1_score: 0.8781\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.5870 - accuracy: 0.6872 - f1_score: 0.6870 - val_loss: 0.5261 - val_accuracy: 0.7423 - val_f1_score: 0.7118\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4647 - accuracy: 0.7848 - f1_score: 0.7730 - val_loss: 0.4781 - val_accuracy: 0.7785 - val_f1_score: 0.7656\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4020 - accuracy: 0.8189 - f1_score: 0.8114 - val_loss: 0.4280 - val_accuracy: 0.8056 - val_f1_score: 0.7826\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3363 - accuracy: 0.8529 - f1_score: 0.8496 - val_loss: 0.4158 - val_accuracy: 0.8083 - val_f1_score: 0.8194\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2941 - accuracy: 0.8764 - f1_score: 0.8745 - val_loss: 0.3694 - val_accuracy: 0.8373 - val_f1_score: 0.8311\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2641 - accuracy: 0.8906 - f1_score: 0.8890 - val_loss: 0.3758 - val_accuracy: 0.8300 - val_f1_score: 0.8127\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2255 - accuracy: 0.9102 - f1_score: 0.9092 - val_loss: 0.4004 - val_accuracy: 0.8318 - val_f1_score: 0.8399\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2003 - accuracy: 0.9234 - f1_score: 0.9233 - val_loss: 0.3528 - val_accuracy: 0.8590 - val_f1_score: 0.8528\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1795 - accuracy: 0.9322 - f1_score: 0.9318 - val_loss: 0.3857 - val_accuracy: 0.8571 - val_f1_score: 0.8584\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1608 - accuracy: 0.9364 - f1_score: 0.9361 - val_loss: 0.3998 - val_accuracy: 0.8544 - val_f1_score: 0.8576\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1342 - accuracy: 0.9476 - f1_score: 0.9472 - val_loss: 0.4067 - val_accuracy: 0.8617 - val_f1_score: 0.8582\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1332 - accuracy: 0.9509 - f1_score: 0.9506 - val_loss: 0.4136 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1280 - accuracy: 0.9500 - f1_score: 0.9499 - val_loss: 0.4628 - val_accuracy: 0.8562 - val_f1_score: 0.8584\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4148 - accuracy: 0.8298 - f1_score: 0.8538\n","Epoch 1/20\n","104/104 [==============================] - 9s 20ms/step - loss: 0.5853 - accuracy: 0.6893 - f1_score: 0.6970 - val_loss: 0.4726 - val_accuracy: 0.7667 - val_f1_score: 0.7461\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4847 - accuracy: 0.7688 - f1_score: 0.7580 - val_loss: 0.4275 - val_accuracy: 0.8128 - val_f1_score: 0.8179\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4173 - accuracy: 0.8062 - f1_score: 0.7986 - val_loss: 0.3878 - val_accuracy: 0.8363 - val_f1_score: 0.8275\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3591 - accuracy: 0.8409 - f1_score: 0.8369 - val_loss: 0.3601 - val_accuracy: 0.8445 - val_f1_score: 0.8442\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3119 - accuracy: 0.8577 - f1_score: 0.8549 - val_loss: 0.3602 - val_accuracy: 0.8436 - val_f1_score: 0.8289\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2799 - accuracy: 0.8822 - f1_score: 0.8804 - val_loss: 0.3506 - val_accuracy: 0.8553 - val_f1_score: 0.8527\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2535 - accuracy: 0.8951 - f1_score: 0.8936 - val_loss: 0.3448 - val_accuracy: 0.8707 - val_f1_score: 0.8599\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2168 - accuracy: 0.9075 - f1_score: 0.9060 - val_loss: 0.3519 - val_accuracy: 0.8716 - val_f1_score: 0.8778\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1973 - accuracy: 0.9201 - f1_score: 0.9191 - val_loss: 0.3857 - val_accuracy: 0.8653 - val_f1_score: 0.8740\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1714 - accuracy: 0.9268 - f1_score: 0.9261 - val_loss: 0.3657 - val_accuracy: 0.8698 - val_f1_score: 0.8691\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1561 - accuracy: 0.9388 - f1_score: 0.9384 - val_loss: 0.4048 - val_accuracy: 0.8761 - val_f1_score: 0.8820\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1369 - accuracy: 0.9473 - f1_score: 0.9471 - val_loss: 0.4058 - val_accuracy: 0.8797 - val_f1_score: 0.8814\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4776 - accuracy: 0.7974 - f1_score: 0.8150\n","Epoch 1/20\n","104/104 [==============================] - 9s 18ms/step - loss: 0.5755 - accuracy: 0.7004 - f1_score: 0.6992 - val_loss: 0.4981 - val_accuracy: 0.7685 - val_f1_score: 0.7562\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4699 - accuracy: 0.7800 - f1_score: 0.7750 - val_loss: 0.4608 - val_accuracy: 0.7830 - val_f1_score: 0.7727\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4109 - accuracy: 0.8131 - f1_score: 0.8116 - val_loss: 0.4360 - val_accuracy: 0.8065 - val_f1_score: 0.8116\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3460 - accuracy: 0.8547 - f1_score: 0.8530 - val_loss: 0.4056 - val_accuracy: 0.8219 - val_f1_score: 0.8264\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3027 - accuracy: 0.8719 - f1_score: 0.8709 - val_loss: 0.3954 - val_accuracy: 0.8336 - val_f1_score: 0.8342\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2616 - accuracy: 0.8936 - f1_score: 0.8933 - val_loss: 0.3817 - val_accuracy: 0.8382 - val_f1_score: 0.8329\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2263 - accuracy: 0.9033 - f1_score: 0.9027 - val_loss: 0.3930 - val_accuracy: 0.8490 - val_f1_score: 0.8432\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2127 - accuracy: 0.9147 - f1_score: 0.9141 - val_loss: 0.3906 - val_accuracy: 0.8508 - val_f1_score: 0.8523\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1834 - accuracy: 0.9265 - f1_score: 0.9262 - val_loss: 0.4430 - val_accuracy: 0.8436 - val_f1_score: 0.8397\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1603 - accuracy: 0.9397 - f1_score: 0.9395 - val_loss: 0.4431 - val_accuracy: 0.8445 - val_f1_score: 0.8483\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1537 - accuracy: 0.9427 - f1_score: 0.9426 - val_loss: 0.4300 - val_accuracy: 0.8490 - val_f1_score: 0.8475\n","47/47 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.8096 - f1_score: 0.8351\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.5832 - accuracy: 0.6965 - f1_score: 0.6946 - val_loss: 0.5027 - val_accuracy: 0.7604 - val_f1_score: 0.7653\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4709 - accuracy: 0.7815 - f1_score: 0.7756 - val_loss: 0.4487 - val_accuracy: 0.8020 - val_f1_score: 0.7996\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3992 - accuracy: 0.8294 - f1_score: 0.8257 - val_loss: 0.4180 - val_accuracy: 0.8183 - val_f1_score: 0.8134\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3361 - accuracy: 0.8583 - f1_score: 0.8556 - val_loss: 0.4061 - val_accuracy: 0.8363 - val_f1_score: 0.8402\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2810 - accuracy: 0.8846 - f1_score: 0.8830 - val_loss: 0.4015 - val_accuracy: 0.8400 - val_f1_score: 0.8360\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2337 - accuracy: 0.9066 - f1_score: 0.9061 - val_loss: 0.4260 - val_accuracy: 0.8391 - val_f1_score: 0.8336\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2136 - accuracy: 0.9177 - f1_score: 0.9173 - val_loss: 0.4227 - val_accuracy: 0.8445 - val_f1_score: 0.8472\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1874 - accuracy: 0.9340 - f1_score: 0.9337 - val_loss: 0.4729 - val_accuracy: 0.8454 - val_f1_score: 0.8472\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1568 - accuracy: 0.9433 - f1_score: 0.9430 - val_loss: 0.5059 - val_accuracy: 0.8291 - val_f1_score: 0.8364\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1598 - accuracy: 0.9367 - f1_score: 0.9364 - val_loss: 0.5138 - val_accuracy: 0.8400 - val_f1_score: 0.8329\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3890 - accuracy: 0.8224 - f1_score: 0.8475\n","Epoch 1/20\n","104/104 [==============================] - 10s 20ms/step - loss: 0.6012 - accuracy: 0.6715 - f1_score: 0.6695 - val_loss: 0.5168 - val_accuracy: 0.7459 - val_f1_score: 0.7259\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4898 - accuracy: 0.7679 - f1_score: 0.7535 - val_loss: 0.4756 - val_accuracy: 0.7875 - val_f1_score: 0.7944\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4242 - accuracy: 0.8089 - f1_score: 0.8029 - val_loss: 0.4118 - val_accuracy: 0.8101 - val_f1_score: 0.7953\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3555 - accuracy: 0.8457 - f1_score: 0.8410 - val_loss: 0.3970 - val_accuracy: 0.8318 - val_f1_score: 0.8363\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3135 - accuracy: 0.8686 - f1_score: 0.8671 - val_loss: 0.3844 - val_accuracy: 0.8436 - val_f1_score: 0.8292\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2710 - accuracy: 0.8882 - f1_score: 0.8865 - val_loss: 0.3502 - val_accuracy: 0.8571 - val_f1_score: 0.8619\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2315 - accuracy: 0.9060 - f1_score: 0.9051 - val_loss: 0.3582 - val_accuracy: 0.8662 - val_f1_score: 0.8695\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1992 - accuracy: 0.9198 - f1_score: 0.9194 - val_loss: 0.3904 - val_accuracy: 0.8644 - val_f1_score: 0.8673\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1705 - accuracy: 0.9337 - f1_score: 0.9333 - val_loss: 0.3959 - val_accuracy: 0.8617 - val_f1_score: 0.8684\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1700 - accuracy: 0.9322 - f1_score: 0.9319 - val_loss: 0.3796 - val_accuracy: 0.8689 - val_f1_score: 0.8738\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1441 - accuracy: 0.9479 - f1_score: 0.9480 - val_loss: 0.4363 - val_accuracy: 0.8716 - val_f1_score: 0.8739\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3586 - accuracy: 0.8406 - f1_score: 0.8690\n","Epoch 1/20\n","104/104 [==============================] - 9s 36ms/step - loss: 0.5885 - accuracy: 0.6841 - f1_score: 0.6824 - val_loss: 0.5092 - val_accuracy: 0.7495 - val_f1_score: 0.7384\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4614 - accuracy: 0.7899 - f1_score: 0.7827 - val_loss: 0.4553 - val_accuracy: 0.7939 - val_f1_score: 0.7861\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3937 - accuracy: 0.8252 - f1_score: 0.8211 - val_loss: 0.4350 - val_accuracy: 0.8074 - val_f1_score: 0.8076\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3397 - accuracy: 0.8526 - f1_score: 0.8503 - val_loss: 0.4141 - val_accuracy: 0.8219 - val_f1_score: 0.8150\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2868 - accuracy: 0.8852 - f1_score: 0.8832 - val_loss: 0.3830 - val_accuracy: 0.8472 - val_f1_score: 0.8451\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2641 - accuracy: 0.8918 - f1_score: 0.8898 - val_loss: 0.3901 - val_accuracy: 0.8373 - val_f1_score: 0.8346\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2268 - accuracy: 0.9093 - f1_score: 0.9081 - val_loss: 0.3793 - val_accuracy: 0.8553 - val_f1_score: 0.8545\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1957 - accuracy: 0.9195 - f1_score: 0.9184 - val_loss: 0.4608 - val_accuracy: 0.8282 - val_f1_score: 0.8077\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1769 - accuracy: 0.9292 - f1_score: 0.9285 - val_loss: 0.4108 - val_accuracy: 0.8662 - val_f1_score: 0.8647\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1507 - accuracy: 0.9415 - f1_score: 0.9411 - val_loss: 0.4315 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1569 - accuracy: 0.9424 - f1_score: 0.9422 - val_loss: 0.4254 - val_accuracy: 0.8571 - val_f1_score: 0.8616\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1197 - accuracy: 0.9554 - f1_score: 0.9553 - val_loss: 0.4523 - val_accuracy: 0.8644 - val_f1_score: 0.8614\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3960 - accuracy: 0.8339 - f1_score: 0.8609\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6026 - accuracy: 0.6712 - f1_score: 0.6640 - val_loss: 0.5019 - val_accuracy: 0.7432 - val_f1_score: 0.7568\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4695 - accuracy: 0.7827 - f1_score: 0.7782 - val_loss: 0.4604 - val_accuracy: 0.7803 - val_f1_score: 0.7932\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3997 - accuracy: 0.8210 - f1_score: 0.8182 - val_loss: 0.4269 - val_accuracy: 0.8110 - val_f1_score: 0.8136\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3388 - accuracy: 0.8599 - f1_score: 0.8578 - val_loss: 0.4078 - val_accuracy: 0.8146 - val_f1_score: 0.8004\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2961 - accuracy: 0.8822 - f1_score: 0.8808 - val_loss: 0.3943 - val_accuracy: 0.8300 - val_f1_score: 0.8199\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2620 - accuracy: 0.8924 - f1_score: 0.8916 - val_loss: 0.3835 - val_accuracy: 0.8291 - val_f1_score: 0.8225\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2214 - accuracy: 0.9123 - f1_score: 0.9118 - val_loss: 0.3919 - val_accuracy: 0.8363 - val_f1_score: 0.8278\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2011 - accuracy: 0.9174 - f1_score: 0.9172 - val_loss: 0.4095 - val_accuracy: 0.8373 - val_f1_score: 0.8343\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1796 - accuracy: 0.9328 - f1_score: 0.9321 - val_loss: 0.4002 - val_accuracy: 0.8382 - val_f1_score: 0.8383\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1647 - accuracy: 0.9352 - f1_score: 0.9353 - val_loss: 0.4149 - val_accuracy: 0.8436 - val_f1_score: 0.8492\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1341 - accuracy: 0.9488 - f1_score: 0.9486 - val_loss: 0.4548 - val_accuracy: 0.8418 - val_f1_score: 0.8402\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4083 - accuracy: 0.8258 - f1_score: 0.8495\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZk-AqJmWITf","executionInfo":{"status":"ok","timestamp":1689801069238,"user_tz":-330,"elapsed":132,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3e30f598-4f46-4bf0-b9c3-85f88a6cad04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8122889995574951, 0.8237677216529846, 0.8298447132110596, 0.8345712423324585, 0.8433490991592407, 0.8426738977432251, 0.8224172592163086, 0.8163403272628784, 0.8291694521903992, 0.8298447132110596, 0.8440243005752563, 0.8345712423324585, 0.844699501991272, 0.8514516949653625, 0.8386225700378418, 0.8338960409164429, 0.8176907300949097, 0.8541526198387146, 0.8298447132110596, 0.7974341511726379, 0.8095881342887878, 0.8224172592163086, 0.8406482338905334, 0.8338960409164429, 0.8257933855056763]\n","0.8305199217796325\n","0.013190134547092943\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5q2vC9jhWIV1","executionInfo":{"status":"ok","timestamp":1689801069239,"user_tz":-330,"elapsed":83,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3f7cf0ea-1adc-4348-93cd-a6c8993f5e9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.4623211622238159, 0.4147168695926666, 0.4257839322090149, 0.39118531346321106, 0.3908918499946594, 0.43682464957237244, 0.40893155336380005, 0.4426589012145996, 0.4313533306121826, 0.4294588565826416, 0.3562535047531128, 0.39370712637901306, 0.40520215034484863, 0.38836315274238586, 0.416375070810318, 0.3950478434562683, 0.4185805320739746, 0.38696765899658203, 0.4148384928703308, 0.477586567401886, 0.4326673150062561, 0.389005571603775, 0.35863128304481506, 0.3959800601005554, 0.40832197666168213]\n","0.4108661890029907\n","0.0277336242339917\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtkSQV9wWIYc","executionInfo":{"status":"ok","timestamp":1689801069240,"user_tz":-330,"elapsed":78,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"07a91729-fe70-4db5-f9d6-f827e27d42b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8353080153465271, 0.8490456342697144, 0.8508875370025635, 0.8627450466156006, 0.8677307963371277, 0.8696138858795166, 0.8475361466407776, 0.8416762948036194, 0.8587380647659302, 0.8548386693000793, 0.8702974915504456, 0.8608744740486145, 0.8685713410377502, 0.8762654066085815, 0.8609656095504761, 0.8573085069656372, 0.8396673798561096, 0.8781037926673889, 0.8538282513618469, 0.8150430917739868, 0.8350877165794373, 0.8475361466407776, 0.8690344095230103, 0.8608596324920654, 0.8494748473167419]\n","0.855241527557373\n","0.014350210102853474\n"]}]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing  -\n","1. Accuracy - 0.834/0.011\n","2. Loss - 0.407/0.032\n","3. F1 score - 0.860/0.012\n","\n","LSTM CNN LSTM nothing  -\n","1. Accuracy - 0.830/0.013\n","2. Loss - 0.410/0.027\n","3. F1 score - 0.855/0.014"],"metadata":{"id":"hxyrFZ9oOOs-"}},{"cell_type":"code","source":[],"metadata":{"id":"dZZGWeY7XFwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NjuBDZIV2KYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y3LfB6nx2KbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JIZGID0u2uLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4U42Tv4-2uOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D5LG5AqbAC7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OvFkPnHEADAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# C. Change LIWC part to be preprocessed in the same way as the rest of the features"],"metadata":{"id":"vgvHwjyX2KjL"}},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"_Xwqm9wy2wL7","executionInfo":{"status":"ok","timestamp":1689801071265,"user_tz":-330,"elapsed":15,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d8e7d2a5-9f14-42cd-dc45-36fea43a49ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-0f624bb2-4912-434c-802c-bae205c797f1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f624bb2-4912-434c-802c-bae205c797f1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-a3f6b1e9-8730-47bf-84b9-55796a0efd51\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a3f6b1e9-8730-47bf-84b9-55796a0efd51')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-a3f6b1e9-8730-47bf-84b9-55796a0efd51 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0f624bb2-4912-434c-802c-bae205c797f1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0f624bb2-4912-434c-802c-bae205c797f1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"mRqVXvclQ-D1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM CNN nothing nothing"],"metadata":{"id":"F_FZLSpw22vw"}},{"cell_type":"code","source":["test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDKfEjh021Pj","executionInfo":{"status":"ok","timestamp":1689801685471,"user_tz":-330,"elapsed":613794,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"9d4846d8-c1e0-4883-cc0f-49bc3163a717"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 5s 13ms/step - loss: 0.6340 - accuracy: 0.6302 - f1_score: 0.6188 - val_loss: 0.4955 - val_accuracy: 0.7758 - val_f1_score: 0.7660\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5185 - accuracy: 0.7441 - f1_score: 0.7369 - val_loss: 0.4128 - val_accuracy: 0.8146 - val_f1_score: 0.7956\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4449 - accuracy: 0.7899 - f1_score: 0.7821 - val_loss: 0.4101 - val_accuracy: 0.8345 - val_f1_score: 0.8516\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3837 - accuracy: 0.8366 - f1_score: 0.8333 - val_loss: 0.3811 - val_accuracy: 0.8246 - val_f1_score: 0.7975\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3604 - accuracy: 0.8406 - f1_score: 0.8366 - val_loss: 0.3121 - val_accuracy: 0.8698 - val_f1_score: 0.8714\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3348 - accuracy: 0.8596 - f1_score: 0.8577 - val_loss: 0.3241 - val_accuracy: 0.8671 - val_f1_score: 0.8723\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3210 - accuracy: 0.8698 - f1_score: 0.8684 - val_loss: 0.3256 - val_accuracy: 0.8553 - val_f1_score: 0.8403\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3127 - accuracy: 0.8755 - f1_score: 0.8733 - val_loss: 0.2940 - val_accuracy: 0.8716 - val_f1_score: 0.8678\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3163 - accuracy: 0.8719 - f1_score: 0.8696 - val_loss: 0.2976 - val_accuracy: 0.8761 - val_f1_score: 0.8747\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3001 - accuracy: 0.8788 - f1_score: 0.8768 - val_loss: 0.3097 - val_accuracy: 0.8770 - val_f1_score: 0.8817\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2831 - accuracy: 0.8861 - f1_score: 0.8844 - val_loss: 0.3436 - val_accuracy: 0.8662 - val_f1_score: 0.8744\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2839 - accuracy: 0.8846 - f1_score: 0.8832 - val_loss: 0.3067 - val_accuracy: 0.8716 - val_f1_score: 0.8730\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2729 - accuracy: 0.8945 - f1_score: 0.8927 - val_loss: 0.3128 - val_accuracy: 0.8671 - val_f1_score: 0.8643\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3787 - accuracy: 0.8454 - f1_score: 0.8660\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6276 - accuracy: 0.6462 - f1_score: 0.6334 - val_loss: 0.5521 - val_accuracy: 0.7260 - val_f1_score: 0.7055\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5098 - accuracy: 0.7577 - f1_score: 0.7526 - val_loss: 0.5147 - val_accuracy: 0.7514 - val_f1_score: 0.6968\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4415 - accuracy: 0.7948 - f1_score: 0.7888 - val_loss: 0.4549 - val_accuracy: 0.7839 - val_f1_score: 0.8068\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3939 - accuracy: 0.8279 - f1_score: 0.8254 - val_loss: 0.4335 - val_accuracy: 0.7902 - val_f1_score: 0.8114\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3597 - accuracy: 0.8394 - f1_score: 0.8388 - val_loss: 0.3747 - val_accuracy: 0.8327 - val_f1_score: 0.8376\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3366 - accuracy: 0.8550 - f1_score: 0.8538 - val_loss: 0.3623 - val_accuracy: 0.8363 - val_f1_score: 0.8394\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3156 - accuracy: 0.8680 - f1_score: 0.8665 - val_loss: 0.3622 - val_accuracy: 0.8382 - val_f1_score: 0.8466\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3026 - accuracy: 0.8701 - f1_score: 0.8692 - val_loss: 0.3550 - val_accuracy: 0.8409 - val_f1_score: 0.8483\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2995 - accuracy: 0.8719 - f1_score: 0.8715 - val_loss: 0.3423 - val_accuracy: 0.8716 - val_f1_score: 0.8700\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2899 - accuracy: 0.8779 - f1_score: 0.8775 - val_loss: 0.3391 - val_accuracy: 0.8662 - val_f1_score: 0.8596\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2692 - accuracy: 0.8885 - f1_score: 0.8881 - val_loss: 0.3516 - val_accuracy: 0.8562 - val_f1_score: 0.8537\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2730 - accuracy: 0.8861 - f1_score: 0.8856 - val_loss: 0.3693 - val_accuracy: 0.8599 - val_f1_score: 0.8505\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2854 - accuracy: 0.8825 - f1_score: 0.8825 - val_loss: 0.3517 - val_accuracy: 0.8698 - val_f1_score: 0.8644\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2620 - accuracy: 0.8900 - f1_score: 0.8902 - val_loss: 0.4175 - val_accuracy: 0.8382 - val_f1_score: 0.8186\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2417 - accuracy: 0.9057 - f1_score: 0.9052 - val_loss: 0.3633 - val_accuracy: 0.8635 - val_f1_score: 0.8595\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3492 - accuracy: 0.8521 - f1_score: 0.8732\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6193 - accuracy: 0.6591 - f1_score: 0.6540 - val_loss: 0.5382 - val_accuracy: 0.7432 - val_f1_score: 0.7017\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4904 - accuracy: 0.7679 - f1_score: 0.7610 - val_loss: 0.4674 - val_accuracy: 0.7776 - val_f1_score: 0.7535\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4223 - accuracy: 0.8056 - f1_score: 0.8012 - val_loss: 0.4395 - val_accuracy: 0.7911 - val_f1_score: 0.7669\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3841 - accuracy: 0.8297 - f1_score: 0.8276 - val_loss: 0.4108 - val_accuracy: 0.8101 - val_f1_score: 0.8148\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3544 - accuracy: 0.8478 - f1_score: 0.8460 - val_loss: 0.4004 - val_accuracy: 0.8165 - val_f1_score: 0.8143\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3282 - accuracy: 0.8590 - f1_score: 0.8571 - val_loss: 0.3956 - val_accuracy: 0.8327 - val_f1_score: 0.8347\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3143 - accuracy: 0.8692 - f1_score: 0.8682 - val_loss: 0.4096 - val_accuracy: 0.8273 - val_f1_score: 0.8122\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3008 - accuracy: 0.8788 - f1_score: 0.8773 - val_loss: 0.3786 - val_accuracy: 0.8382 - val_f1_score: 0.8368\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2782 - accuracy: 0.8933 - f1_score: 0.8925 - val_loss: 0.3825 - val_accuracy: 0.8345 - val_f1_score: 0.8335\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2733 - accuracy: 0.8903 - f1_score: 0.8894 - val_loss: 0.4268 - val_accuracy: 0.8201 - val_f1_score: 0.8283\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2578 - accuracy: 0.8981 - f1_score: 0.8975 - val_loss: 0.4198 - val_accuracy: 0.8237 - val_f1_score: 0.8086\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2553 - accuracy: 0.9020 - f1_score: 0.9014 - val_loss: 0.4555 - val_accuracy: 0.8128 - val_f1_score: 0.7894\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2460 - accuracy: 0.9020 - f1_score: 0.9016 - val_loss: 0.4388 - val_accuracy: 0.8237 - val_f1_score: 0.8048\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.8542 - f1_score: 0.8774\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6334 - accuracy: 0.6371 - f1_score: 0.6156 - val_loss: 0.5757 - val_accuracy: 0.6980 - val_f1_score: 0.7481\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5197 - accuracy: 0.7495 - f1_score: 0.7393 - val_loss: 0.4358 - val_accuracy: 0.7939 - val_f1_score: 0.7833\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4280 - accuracy: 0.8005 - f1_score: 0.7918 - val_loss: 0.4753 - val_accuracy: 0.7966 - val_f1_score: 0.8249\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3897 - accuracy: 0.8294 - f1_score: 0.8256 - val_loss: 0.3766 - val_accuracy: 0.8418 - val_f1_score: 0.8498\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3429 - accuracy: 0.8538 - f1_score: 0.8514 - val_loss: 0.4743 - val_accuracy: 0.7957 - val_f1_score: 0.8223\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3256 - accuracy: 0.8635 - f1_score: 0.8614 - val_loss: 0.4424 - val_accuracy: 0.8237 - val_f1_score: 0.8426\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3160 - accuracy: 0.8686 - f1_score: 0.8668 - val_loss: 0.3385 - val_accuracy: 0.8617 - val_f1_score: 0.8638\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3201 - accuracy: 0.8683 - f1_score: 0.8662 - val_loss: 0.3414 - val_accuracy: 0.8599 - val_f1_score: 0.8632\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2972 - accuracy: 0.8837 - f1_score: 0.8823 - val_loss: 0.3526 - val_accuracy: 0.8571 - val_f1_score: 0.8640\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2918 - accuracy: 0.8828 - f1_score: 0.8806 - val_loss: 0.4848 - val_accuracy: 0.7993 - val_f1_score: 0.8255\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2911 - accuracy: 0.8794 - f1_score: 0.8783 - val_loss: 0.3416 - val_accuracy: 0.8608 - val_f1_score: 0.8590\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2681 - accuracy: 0.8933 - f1_score: 0.8918 - val_loss: 0.4183 - val_accuracy: 0.8382 - val_f1_score: 0.8519\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3460 - accuracy: 0.8535 - f1_score: 0.8759\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6311 - accuracy: 0.6414 - f1_score: 0.6183 - val_loss: 0.5164 - val_accuracy: 0.7505 - val_f1_score: 0.7240\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4865 - accuracy: 0.7715 - f1_score: 0.7627 - val_loss: 0.3982 - val_accuracy: 0.8174 - val_f1_score: 0.8240\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4160 - accuracy: 0.8180 - f1_score: 0.8132 - val_loss: 0.3641 - val_accuracy: 0.8409 - val_f1_score: 0.8431\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3587 - accuracy: 0.8469 - f1_score: 0.8444 - val_loss: 0.3685 - val_accuracy: 0.8472 - val_f1_score: 0.8542\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3342 - accuracy: 0.8635 - f1_score: 0.8626 - val_loss: 0.3460 - val_accuracy: 0.8571 - val_f1_score: 0.8498\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3279 - accuracy: 0.8626 - f1_score: 0.8609 - val_loss: 0.3311 - val_accuracy: 0.8671 - val_f1_score: 0.8679\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3144 - accuracy: 0.8740 - f1_score: 0.8728 - val_loss: 0.3256 - val_accuracy: 0.8689 - val_f1_score: 0.8673\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3132 - accuracy: 0.8740 - f1_score: 0.8723 - val_loss: 0.3297 - val_accuracy: 0.8689 - val_f1_score: 0.8697\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3024 - accuracy: 0.8749 - f1_score: 0.8725 - val_loss: 0.3250 - val_accuracy: 0.8752 - val_f1_score: 0.8713\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2896 - accuracy: 0.8906 - f1_score: 0.8895 - val_loss: 0.3304 - val_accuracy: 0.8707 - val_f1_score: 0.8711\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2822 - accuracy: 0.8825 - f1_score: 0.8808 - val_loss: 0.3392 - val_accuracy: 0.8743 - val_f1_score: 0.8742\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2677 - accuracy: 0.8954 - f1_score: 0.8942 - val_loss: 0.3603 - val_accuracy: 0.8698 - val_f1_score: 0.8659\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2616 - accuracy: 0.8942 - f1_score: 0.8931 - val_loss: 0.3357 - val_accuracy: 0.8734 - val_f1_score: 0.8706\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2670 - accuracy: 0.8936 - f1_score: 0.8922 - val_loss: 0.3305 - val_accuracy: 0.8743 - val_f1_score: 0.8767\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.8400 - f1_score: 0.8615\n","Epoch 1/20\n","104/104 [==============================] - 7s 14ms/step - loss: 0.6236 - accuracy: 0.6615 - f1_score: 0.6500 - val_loss: 0.5178 - val_accuracy: 0.7613 - val_f1_score: 0.7495\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5007 - accuracy: 0.7598 - f1_score: 0.7505 - val_loss: 0.4562 - val_accuracy: 0.7875 - val_f1_score: 0.7969\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4215 - accuracy: 0.8083 - f1_score: 0.8021 - val_loss: 0.3636 - val_accuracy: 0.8436 - val_f1_score: 0.8408\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3794 - accuracy: 0.8351 - f1_score: 0.8320 - val_loss: 0.3893 - val_accuracy: 0.8282 - val_f1_score: 0.8448\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3551 - accuracy: 0.8484 - f1_score: 0.8464 - val_loss: 0.3133 - val_accuracy: 0.8743 - val_f1_score: 0.8726\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3223 - accuracy: 0.8665 - f1_score: 0.8647 - val_loss: 0.3038 - val_accuracy: 0.8779 - val_f1_score: 0.8819\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3050 - accuracy: 0.8731 - f1_score: 0.8727 - val_loss: 0.3166 - val_accuracy: 0.8653 - val_f1_score: 0.8517\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3079 - accuracy: 0.8725 - f1_score: 0.8715 - val_loss: 0.2816 - val_accuracy: 0.8834 - val_f1_score: 0.8802\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2902 - accuracy: 0.8867 - f1_score: 0.8856 - val_loss: 0.3421 - val_accuracy: 0.8472 - val_f1_score: 0.8590\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2941 - accuracy: 0.8761 - f1_score: 0.8759 - val_loss: 0.2921 - val_accuracy: 0.8788 - val_f1_score: 0.8704\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2665 - accuracy: 0.8966 - f1_score: 0.8955 - val_loss: 0.2885 - val_accuracy: 0.8770 - val_f1_score: 0.8784\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2735 - accuracy: 0.8891 - f1_score: 0.8884 - val_loss: 0.2811 - val_accuracy: 0.8816 - val_f1_score: 0.8772\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2507 - accuracy: 0.9011 - f1_score: 0.9002 - val_loss: 0.3302 - val_accuracy: 0.8571 - val_f1_score: 0.8401\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2433 - accuracy: 0.9005 - f1_score: 0.8994 - val_loss: 0.2848 - val_accuracy: 0.8788 - val_f1_score: 0.8775\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2246 - accuracy: 0.9168 - f1_score: 0.9162 - val_loss: 0.2923 - val_accuracy: 0.8788 - val_f1_score: 0.8804\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2229 - accuracy: 0.9153 - f1_score: 0.9152 - val_loss: 0.3667 - val_accuracy: 0.8463 - val_f1_score: 0.8255\n","Epoch 17/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2250 - accuracy: 0.9177 - f1_score: 0.9169 - val_loss: 0.2994 - val_accuracy: 0.8761 - val_f1_score: 0.8767\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.8528 - f1_score: 0.8771\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6329 - accuracy: 0.6377 - f1_score: 0.6324 - val_loss: 0.5464 - val_accuracy: 0.7233 - val_f1_score: 0.6958\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4953 - accuracy: 0.7631 - f1_score: 0.7570 - val_loss: 0.4349 - val_accuracy: 0.7893 - val_f1_score: 0.8010\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4171 - accuracy: 0.8116 - f1_score: 0.8082 - val_loss: 0.3767 - val_accuracy: 0.8363 - val_f1_score: 0.8244\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3796 - accuracy: 0.8406 - f1_score: 0.8378 - val_loss: 0.3432 - val_accuracy: 0.8499 - val_f1_score: 0.8526\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3550 - accuracy: 0.8433 - f1_score: 0.8419 - val_loss: 0.3303 - val_accuracy: 0.8716 - val_f1_score: 0.8683\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3255 - accuracy: 0.8632 - f1_score: 0.8616 - val_loss: 0.3283 - val_accuracy: 0.8671 - val_f1_score: 0.8714\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3306 - accuracy: 0.8623 - f1_score: 0.8615 - val_loss: 0.3330 - val_accuracy: 0.8562 - val_f1_score: 0.8626\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3037 - accuracy: 0.8797 - f1_score: 0.8787 - val_loss: 0.3241 - val_accuracy: 0.8590 - val_f1_score: 0.8503\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3057 - accuracy: 0.8758 - f1_score: 0.8748 - val_loss: 0.3493 - val_accuracy: 0.8544 - val_f1_score: 0.8644\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2788 - accuracy: 0.8903 - f1_score: 0.8893 - val_loss: 0.3095 - val_accuracy: 0.8743 - val_f1_score: 0.8740\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2749 - accuracy: 0.8918 - f1_score: 0.8912 - val_loss: 0.3362 - val_accuracy: 0.8580 - val_f1_score: 0.8453\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2735 - accuracy: 0.8873 - f1_score: 0.8867 - val_loss: 0.3306 - val_accuracy: 0.8617 - val_f1_score: 0.8675\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2505 - accuracy: 0.9005 - f1_score: 0.8998 - val_loss: 0.3069 - val_accuracy: 0.8797 - val_f1_score: 0.8809\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2519 - accuracy: 0.8969 - f1_score: 0.8967 - val_loss: 0.3115 - val_accuracy: 0.8770 - val_f1_score: 0.8784\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2343 - accuracy: 0.9099 - f1_score: 0.9087 - val_loss: 0.3365 - val_accuracy: 0.8707 - val_f1_score: 0.8724\n","Epoch 16/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2390 - accuracy: 0.9048 - f1_score: 0.9045 - val_loss: 0.3083 - val_accuracy: 0.8770 - val_f1_score: 0.8741\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2145 - accuracy: 0.9183 - f1_score: 0.9176 - val_loss: 0.3232 - val_accuracy: 0.8752 - val_f1_score: 0.8774\n","Epoch 18/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2047 - accuracy: 0.9237 - f1_score: 0.9229 - val_loss: 0.3319 - val_accuracy: 0.8797 - val_f1_score: 0.8822\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8636 - f1_score: 0.8883\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6179 - accuracy: 0.6609 - f1_score: 0.6418 - val_loss: 0.5396 - val_accuracy: 0.7369 - val_f1_score: 0.7605\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4840 - accuracy: 0.7782 - f1_score: 0.7669 - val_loss: 0.4432 - val_accuracy: 0.7920 - val_f1_score: 0.7894\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4072 - accuracy: 0.8171 - f1_score: 0.8106 - val_loss: 0.3915 - val_accuracy: 0.8336 - val_f1_score: 0.8318\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3743 - accuracy: 0.8339 - f1_score: 0.8307 - val_loss: 0.4426 - val_accuracy: 0.7911 - val_f1_score: 0.8180\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3388 - accuracy: 0.8511 - f1_score: 0.8488 - val_loss: 0.4171 - val_accuracy: 0.8201 - val_f1_score: 0.8376\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3142 - accuracy: 0.8665 - f1_score: 0.8659 - val_loss: 0.3317 - val_accuracy: 0.8680 - val_f1_score: 0.8696\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3183 - accuracy: 0.8656 - f1_score: 0.8649 - val_loss: 0.4417 - val_accuracy: 0.7749 - val_f1_score: 0.8086\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2949 - accuracy: 0.8755 - f1_score: 0.8747 - val_loss: 0.3411 - val_accuracy: 0.8653 - val_f1_score: 0.8705\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2918 - accuracy: 0.8794 - f1_score: 0.8785 - val_loss: 0.3384 - val_accuracy: 0.8716 - val_f1_score: 0.8640\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2769 - accuracy: 0.8870 - f1_score: 0.8864 - val_loss: 0.3438 - val_accuracy: 0.8571 - val_f1_score: 0.8647\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2655 - accuracy: 0.8939 - f1_score: 0.8936 - val_loss: 0.3299 - val_accuracy: 0.8689 - val_f1_score: 0.8623\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2506 - accuracy: 0.9014 - f1_score: 0.9011 - val_loss: 0.3449 - val_accuracy: 0.8617 - val_f1_score: 0.8666\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2480 - accuracy: 0.8999 - f1_score: 0.8993 - val_loss: 0.3381 - val_accuracy: 0.8617 - val_f1_score: 0.8530\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2347 - accuracy: 0.9081 - f1_score: 0.9077 - val_loss: 0.3279 - val_accuracy: 0.8752 - val_f1_score: 0.8755\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2242 - accuracy: 0.9099 - f1_score: 0.9100 - val_loss: 0.3522 - val_accuracy: 0.8608 - val_f1_score: 0.8469\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2252 - accuracy: 0.9105 - f1_score: 0.9096 - val_loss: 0.3274 - val_accuracy: 0.8761 - val_f1_score: 0.8756\n","Epoch 17/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2144 - accuracy: 0.9177 - f1_score: 0.9170 - val_loss: 0.3831 - val_accuracy: 0.8454 - val_f1_score: 0.8547\n","Epoch 18/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1950 - accuracy: 0.9289 - f1_score: 0.9285 - val_loss: 0.3421 - val_accuracy: 0.8779 - val_f1_score: 0.8780\n","Epoch 19/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2031 - accuracy: 0.9225 - f1_score: 0.9222 - val_loss: 0.3429 - val_accuracy: 0.8743 - val_f1_score: 0.8687\n","Epoch 20/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.1935 - accuracy: 0.9253 - f1_score: 0.9250 - val_loss: 0.3502 - val_accuracy: 0.8716 - val_f1_score: 0.8685\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3785 - accuracy: 0.8521 - f1_score: 0.8773\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.6294 - accuracy: 0.6612 - f1_score: 0.6501 - val_loss: 0.5796 - val_accuracy: 0.7034 - val_f1_score: 0.6114\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5016 - accuracy: 0.7652 - f1_score: 0.7565 - val_loss: 0.4353 - val_accuracy: 0.8056 - val_f1_score: 0.7974\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4276 - accuracy: 0.8032 - f1_score: 0.7989 - val_loss: 0.4022 - val_accuracy: 0.8255 - val_f1_score: 0.8076\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3769 - accuracy: 0.8330 - f1_score: 0.8295 - val_loss: 0.3435 - val_accuracy: 0.8517 - val_f1_score: 0.8517\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3531 - accuracy: 0.8574 - f1_score: 0.8560 - val_loss: 0.3276 - val_accuracy: 0.8617 - val_f1_score: 0.8652\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3332 - accuracy: 0.8565 - f1_score: 0.8563 - val_loss: 0.4683 - val_accuracy: 0.7866 - val_f1_score: 0.7366\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3166 - accuracy: 0.8659 - f1_score: 0.8648 - val_loss: 0.3023 - val_accuracy: 0.8807 - val_f1_score: 0.8809\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3031 - accuracy: 0.8776 - f1_score: 0.8764 - val_loss: 0.3138 - val_accuracy: 0.8680 - val_f1_score: 0.8726\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2880 - accuracy: 0.8797 - f1_score: 0.8790 - val_loss: 0.3809 - val_accuracy: 0.8264 - val_f1_score: 0.8447\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2801 - accuracy: 0.8843 - f1_score: 0.8836 - val_loss: 0.3019 - val_accuracy: 0.8825 - val_f1_score: 0.8816\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2687 - accuracy: 0.8927 - f1_score: 0.8926 - val_loss: 0.3081 - val_accuracy: 0.8788 - val_f1_score: 0.8757\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2543 - accuracy: 0.8966 - f1_score: 0.8964 - val_loss: 0.3349 - val_accuracy: 0.8653 - val_f1_score: 0.8552\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2517 - accuracy: 0.9036 - f1_score: 0.9030 - val_loss: 0.3577 - val_accuracy: 0.8599 - val_f1_score: 0.8674\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2398 - accuracy: 0.9063 - f1_score: 0.9063 - val_loss: 0.3325 - val_accuracy: 0.8707 - val_f1_score: 0.8652\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2231 - accuracy: 0.9108 - f1_score: 0.9105 - val_loss: 0.3295 - val_accuracy: 0.8734 - val_f1_score: 0.8732\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3394 - accuracy: 0.8589 - f1_score: 0.8837\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6402 - accuracy: 0.6423 - f1_score: 0.6426 - val_loss: 0.5468 - val_accuracy: 0.7405 - val_f1_score: 0.7007\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5045 - accuracy: 0.7562 - f1_score: 0.7504 - val_loss: 0.4772 - val_accuracy: 0.7694 - val_f1_score: 0.7908\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4395 - accuracy: 0.7993 - f1_score: 0.7928 - val_loss: 0.3977 - val_accuracy: 0.8246 - val_f1_score: 0.8079\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3740 - accuracy: 0.8312 - f1_score: 0.8262 - val_loss: 0.3653 - val_accuracy: 0.8400 - val_f1_score: 0.8273\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3430 - accuracy: 0.8541 - f1_score: 0.8521 - val_loss: 0.3402 - val_accuracy: 0.8490 - val_f1_score: 0.8464\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3370 - accuracy: 0.8529 - f1_score: 0.8503 - val_loss: 0.3418 - val_accuracy: 0.8553 - val_f1_score: 0.8434\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3079 - accuracy: 0.8794 - f1_score: 0.8783 - val_loss: 0.3667 - val_accuracy: 0.8409 - val_f1_score: 0.8516\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3115 - accuracy: 0.8764 - f1_score: 0.8753 - val_loss: 0.3199 - val_accuracy: 0.8716 - val_f1_score: 0.8663\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2788 - accuracy: 0.8888 - f1_score: 0.8885 - val_loss: 0.3265 - val_accuracy: 0.8580 - val_f1_score: 0.8629\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2847 - accuracy: 0.8870 - f1_score: 0.8864 - val_loss: 0.3112 - val_accuracy: 0.8653 - val_f1_score: 0.8673\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2652 - accuracy: 0.8954 - f1_score: 0.8953 - val_loss: 0.3025 - val_accuracy: 0.8671 - val_f1_score: 0.8648\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2536 - accuracy: 0.9033 - f1_score: 0.9029 - val_loss: 0.3619 - val_accuracy: 0.8445 - val_f1_score: 0.8535\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2524 - accuracy: 0.9030 - f1_score: 0.9025 - val_loss: 0.3142 - val_accuracy: 0.8734 - val_f1_score: 0.8736\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2588 - accuracy: 0.8987 - f1_score: 0.8981 - val_loss: 0.3230 - val_accuracy: 0.8626 - val_f1_score: 0.8660\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2437 - accuracy: 0.9042 - f1_score: 0.9034 - val_loss: 0.3157 - val_accuracy: 0.8662 - val_f1_score: 0.8697\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2155 - accuracy: 0.9174 - f1_score: 0.9170 - val_loss: 0.3488 - val_accuracy: 0.8671 - val_f1_score: 0.8571\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8670 - f1_score: 0.8904\n","Epoch 1/20\n","104/104 [==============================] - 7s 21ms/step - loss: 0.6373 - accuracy: 0.6420 - f1_score: 0.6192 - val_loss: 0.5572 - val_accuracy: 0.7260 - val_f1_score: 0.6645\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5017 - accuracy: 0.7592 - f1_score: 0.7521 - val_loss: 0.4446 - val_accuracy: 0.7993 - val_f1_score: 0.8144\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3954 - accuracy: 0.8312 - f1_score: 0.8275 - val_loss: 0.3781 - val_accuracy: 0.8354 - val_f1_score: 0.8409\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3553 - accuracy: 0.8463 - f1_score: 0.8462 - val_loss: 0.3923 - val_accuracy: 0.8454 - val_f1_score: 0.8271\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3181 - accuracy: 0.8749 - f1_score: 0.8742 - val_loss: 0.3436 - val_accuracy: 0.8590 - val_f1_score: 0.8561\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3124 - accuracy: 0.8728 - f1_score: 0.8719 - val_loss: 0.3343 - val_accuracy: 0.8671 - val_f1_score: 0.8635\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2863 - accuracy: 0.8819 - f1_score: 0.8815 - val_loss: 0.3324 - val_accuracy: 0.8680 - val_f1_score: 0.8641\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2920 - accuracy: 0.8834 - f1_score: 0.8830 - val_loss: 0.3259 - val_accuracy: 0.8644 - val_f1_score: 0.8626\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2744 - accuracy: 0.8918 - f1_score: 0.8907 - val_loss: 0.3418 - val_accuracy: 0.8698 - val_f1_score: 0.8649\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2711 - accuracy: 0.8891 - f1_score: 0.8886 - val_loss: 0.4204 - val_accuracy: 0.8382 - val_f1_score: 0.8153\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2572 - accuracy: 0.8936 - f1_score: 0.8933 - val_loss: 0.3809 - val_accuracy: 0.8617 - val_f1_score: 0.8493\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2540 - accuracy: 0.8981 - f1_score: 0.8973 - val_loss: 0.3548 - val_accuracy: 0.8698 - val_f1_score: 0.8644\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2505 - accuracy: 0.9011 - f1_score: 0.9008 - val_loss: 0.3529 - val_accuracy: 0.8608 - val_f1_score: 0.8625\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3073 - accuracy: 0.8710 - f1_score: 0.8963\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6277 - accuracy: 0.6513 - f1_score: 0.6017 - val_loss: 0.4895 - val_accuracy: 0.7785 - val_f1_score: 0.7758\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4595 - accuracy: 0.7878 - f1_score: 0.7835 - val_loss: 0.4966 - val_accuracy: 0.7468 - val_f1_score: 0.7907\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3955 - accuracy: 0.8204 - f1_score: 0.8191 - val_loss: 0.4141 - val_accuracy: 0.8056 - val_f1_score: 0.8279\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3601 - accuracy: 0.8445 - f1_score: 0.8437 - val_loss: 0.3263 - val_accuracy: 0.8752 - val_f1_score: 0.8734\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3389 - accuracy: 0.8574 - f1_score: 0.8564 - val_loss: 0.3626 - val_accuracy: 0.8562 - val_f1_score: 0.8408\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3370 - accuracy: 0.8535 - f1_score: 0.8521 - val_loss: 0.3172 - val_accuracy: 0.8716 - val_f1_score: 0.8658\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3075 - accuracy: 0.8758 - f1_score: 0.8745 - val_loss: 0.3078 - val_accuracy: 0.8797 - val_f1_score: 0.8772\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3081 - accuracy: 0.8656 - f1_score: 0.8639 - val_loss: 0.3064 - val_accuracy: 0.8797 - val_f1_score: 0.8760\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2820 - accuracy: 0.8876 - f1_score: 0.8861 - val_loss: 0.3504 - val_accuracy: 0.8725 - val_f1_score: 0.8632\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2746 - accuracy: 0.8861 - f1_score: 0.8850 - val_loss: 0.3386 - val_accuracy: 0.8635 - val_f1_score: 0.8515\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2689 - accuracy: 0.8945 - f1_score: 0.8932 - val_loss: 0.3259 - val_accuracy: 0.8680 - val_f1_score: 0.8599\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2754 - accuracy: 0.8876 - f1_score: 0.8867 - val_loss: 0.3363 - val_accuracy: 0.8716 - val_f1_score: 0.8627\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2713 - accuracy: 0.8903 - f1_score: 0.8890 - val_loss: 0.3156 - val_accuracy: 0.8707 - val_f1_score: 0.8626\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8616 - f1_score: 0.8888\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6184 - accuracy: 0.6534 - f1_score: 0.6314 - val_loss: 0.5176 - val_accuracy: 0.7495 - val_f1_score: 0.7428\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4792 - accuracy: 0.7722 - f1_score: 0.7602 - val_loss: 0.4231 - val_accuracy: 0.8029 - val_f1_score: 0.7936\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4165 - accuracy: 0.8119 - f1_score: 0.8054 - val_loss: 0.4215 - val_accuracy: 0.8119 - val_f1_score: 0.8210\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3742 - accuracy: 0.8369 - f1_score: 0.8328 - val_loss: 0.4072 - val_accuracy: 0.8273 - val_f1_score: 0.8383\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3438 - accuracy: 0.8529 - f1_score: 0.8502 - val_loss: 0.4023 - val_accuracy: 0.8409 - val_f1_score: 0.8506\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3197 - accuracy: 0.8583 - f1_score: 0.8569 - val_loss: 0.3812 - val_accuracy: 0.8517 - val_f1_score: 0.8566\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3045 - accuracy: 0.8647 - f1_score: 0.8620 - val_loss: 0.3677 - val_accuracy: 0.8571 - val_f1_score: 0.8548\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3056 - accuracy: 0.8710 - f1_score: 0.8687 - val_loss: 0.4102 - val_accuracy: 0.8409 - val_f1_score: 0.8498\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2873 - accuracy: 0.8837 - f1_score: 0.8818 - val_loss: 0.4746 - val_accuracy: 0.8020 - val_f1_score: 0.8261\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2777 - accuracy: 0.8876 - f1_score: 0.8859 - val_loss: 0.3765 - val_accuracy: 0.8526 - val_f1_score: 0.8589\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2630 - accuracy: 0.8924 - f1_score: 0.8909 - val_loss: 0.3811 - val_accuracy: 0.8526 - val_f1_score: 0.8584\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2523 - accuracy: 0.8954 - f1_score: 0.8938 - val_loss: 0.3765 - val_accuracy: 0.8590 - val_f1_score: 0.8577\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3353 - accuracy: 0.8609 - f1_score: 0.8861\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6293 - accuracy: 0.6368 - f1_score: 0.6307 - val_loss: 0.5104 - val_accuracy: 0.7477 - val_f1_score: 0.7121\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4891 - accuracy: 0.7628 - f1_score: 0.7524 - val_loss: 0.4305 - val_accuracy: 0.8002 - val_f1_score: 0.7724\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4324 - accuracy: 0.7996 - f1_score: 0.7892 - val_loss: 0.4199 - val_accuracy: 0.7929 - val_f1_score: 0.8137\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3659 - accuracy: 0.8345 - f1_score: 0.8312 - val_loss: 0.3515 - val_accuracy: 0.8490 - val_f1_score: 0.8383\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3429 - accuracy: 0.8508 - f1_score: 0.8487 - val_loss: 0.3998 - val_accuracy: 0.8174 - val_f1_score: 0.8363\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3209 - accuracy: 0.8626 - f1_score: 0.8601 - val_loss: 0.4730 - val_accuracy: 0.7848 - val_f1_score: 0.8169\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3004 - accuracy: 0.8701 - f1_score: 0.8687 - val_loss: 0.3415 - val_accuracy: 0.8608 - val_f1_score: 0.8651\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2913 - accuracy: 0.8797 - f1_score: 0.8784 - val_loss: 0.3822 - val_accuracy: 0.8246 - val_f1_score: 0.8420\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2924 - accuracy: 0.8773 - f1_score: 0.8762 - val_loss: 0.3105 - val_accuracy: 0.8734 - val_f1_score: 0.8699\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2842 - accuracy: 0.8885 - f1_score: 0.8859 - val_loss: 0.3919 - val_accuracy: 0.8237 - val_f1_score: 0.8416\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2646 - accuracy: 0.8924 - f1_score: 0.8911 - val_loss: 0.3227 - val_accuracy: 0.8707 - val_f1_score: 0.8677\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2586 - accuracy: 0.8978 - f1_score: 0.8967 - val_loss: 0.3430 - val_accuracy: 0.8517 - val_f1_score: 0.8622\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2623 - accuracy: 0.8924 - f1_score: 0.8909 - val_loss: 0.4157 - val_accuracy: 0.8300 - val_f1_score: 0.8472\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2441 - accuracy: 0.9027 - f1_score: 0.9016 - val_loss: 0.3315 - val_accuracy: 0.8689 - val_f1_score: 0.8753\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.8582 - f1_score: 0.8808\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6298 - accuracy: 0.6549 - f1_score: 0.6308 - val_loss: 0.5419 - val_accuracy: 0.7242 - val_f1_score: 0.7260\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5304 - accuracy: 0.7411 - f1_score: 0.7272 - val_loss: 0.4918 - val_accuracy: 0.7676 - val_f1_score: 0.7210\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4470 - accuracy: 0.7966 - f1_score: 0.7862 - val_loss: 0.4051 - val_accuracy: 0.8183 - val_f1_score: 0.8217\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3848 - accuracy: 0.8348 - f1_score: 0.8320 - val_loss: 0.3721 - val_accuracy: 0.8391 - val_f1_score: 0.8428\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3515 - accuracy: 0.8571 - f1_score: 0.8549 - val_loss: 0.3972 - val_accuracy: 0.8146 - val_f1_score: 0.8332\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3244 - accuracy: 0.8659 - f1_score: 0.8655 - val_loss: 0.3417 - val_accuracy: 0.8608 - val_f1_score: 0.8561\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3110 - accuracy: 0.8716 - f1_score: 0.8698 - val_loss: 0.3652 - val_accuracy: 0.8273 - val_f1_score: 0.8438\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2971 - accuracy: 0.8834 - f1_score: 0.8835 - val_loss: 0.3244 - val_accuracy: 0.8662 - val_f1_score: 0.8601\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2843 - accuracy: 0.8900 - f1_score: 0.8892 - val_loss: 0.4219 - val_accuracy: 0.8128 - val_f1_score: 0.8361\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2891 - accuracy: 0.8761 - f1_score: 0.8763 - val_loss: 0.3154 - val_accuracy: 0.8716 - val_f1_score: 0.8739\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2752 - accuracy: 0.8882 - f1_score: 0.8879 - val_loss: 0.3563 - val_accuracy: 0.8544 - val_f1_score: 0.8625\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2824 - accuracy: 0.8855 - f1_score: 0.8863 - val_loss: 0.3287 - val_accuracy: 0.8716 - val_f1_score: 0.8772\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2540 - accuracy: 0.9027 - f1_score: 0.9026 - val_loss: 0.3667 - val_accuracy: 0.8517 - val_f1_score: 0.8392\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2458 - accuracy: 0.9057 - f1_score: 0.9056 - val_loss: 0.3254 - val_accuracy: 0.8689 - val_f1_score: 0.8676\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2447 - accuracy: 0.9033 - f1_score: 0.9028 - val_loss: 0.3876 - val_accuracy: 0.8264 - val_f1_score: 0.8439\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8663 - f1_score: 0.8925\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6206 - accuracy: 0.6573 - f1_score: 0.6293 - val_loss: 0.5321 - val_accuracy: 0.7423 - val_f1_score: 0.7677\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4854 - accuracy: 0.7712 - f1_score: 0.7617 - val_loss: 0.5325 - val_accuracy: 0.7559 - val_f1_score: 0.7897\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4037 - accuracy: 0.8137 - f1_score: 0.8071 - val_loss: 0.4375 - val_accuracy: 0.8029 - val_f1_score: 0.8239\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3674 - accuracy: 0.8357 - f1_score: 0.8336 - val_loss: 0.3816 - val_accuracy: 0.8300 - val_f1_score: 0.8420\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3414 - accuracy: 0.8523 - f1_score: 0.8501 - val_loss: 0.4673 - val_accuracy: 0.7758 - val_f1_score: 0.8086\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3134 - accuracy: 0.8635 - f1_score: 0.8629 - val_loss: 0.3388 - val_accuracy: 0.8517 - val_f1_score: 0.8551\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3010 - accuracy: 0.8746 - f1_score: 0.8734 - val_loss: 0.3322 - val_accuracy: 0.8544 - val_f1_score: 0.8599\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2813 - accuracy: 0.8861 - f1_score: 0.8857 - val_loss: 0.3264 - val_accuracy: 0.8608 - val_f1_score: 0.8635\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2838 - accuracy: 0.8837 - f1_score: 0.8828 - val_loss: 0.3698 - val_accuracy: 0.8391 - val_f1_score: 0.8206\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2732 - accuracy: 0.8885 - f1_score: 0.8871 - val_loss: 0.3596 - val_accuracy: 0.8481 - val_f1_score: 0.8327\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2610 - accuracy: 0.8954 - f1_score: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.8689 - val_f1_score: 0.8631\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2530 - accuracy: 0.8996 - f1_score: 0.8987 - val_loss: 0.3312 - val_accuracy: 0.8635 - val_f1_score: 0.8663\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2532 - accuracy: 0.9011 - f1_score: 0.8998 - val_loss: 0.3826 - val_accuracy: 0.8354 - val_f1_score: 0.8476\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3477 - accuracy: 0.8548 - f1_score: 0.8787\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6039 - accuracy: 0.6712 - f1_score: 0.6839 - val_loss: 0.5211 - val_accuracy: 0.7577 - val_f1_score: 0.7572\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4902 - accuracy: 0.7737 - f1_score: 0.7617 - val_loss: 0.4578 - val_accuracy: 0.7875 - val_f1_score: 0.7734\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4210 - accuracy: 0.8101 - f1_score: 0.8013 - val_loss: 0.4129 - val_accuracy: 0.8056 - val_f1_score: 0.8112\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3870 - accuracy: 0.8273 - f1_score: 0.8222 - val_loss: 0.3765 - val_accuracy: 0.8291 - val_f1_score: 0.8323\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3475 - accuracy: 0.8490 - f1_score: 0.8468 - val_loss: 0.3438 - val_accuracy: 0.8562 - val_f1_score: 0.8501\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3161 - accuracy: 0.8677 - f1_score: 0.8664 - val_loss: 0.3583 - val_accuracy: 0.8427 - val_f1_score: 0.8246\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3030 - accuracy: 0.8767 - f1_score: 0.8756 - val_loss: 0.3406 - val_accuracy: 0.8526 - val_f1_score: 0.8425\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3042 - accuracy: 0.8746 - f1_score: 0.8731 - val_loss: 0.3198 - val_accuracy: 0.8653 - val_f1_score: 0.8598\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2673 - accuracy: 0.8924 - f1_score: 0.8917 - val_loss: 0.3573 - val_accuracy: 0.8544 - val_f1_score: 0.8375\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2810 - accuracy: 0.8894 - f1_score: 0.8883 - val_loss: 0.3179 - val_accuracy: 0.8680 - val_f1_score: 0.8670\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2589 - accuracy: 0.8948 - f1_score: 0.8939 - val_loss: 0.3292 - val_accuracy: 0.8716 - val_f1_score: 0.8688\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2519 - accuracy: 0.8948 - f1_score: 0.8939 - val_loss: 0.3295 - val_accuracy: 0.8671 - val_f1_score: 0.8604\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2386 - accuracy: 0.9048 - f1_score: 0.9037 - val_loss: 0.3503 - val_accuracy: 0.8580 - val_f1_score: 0.8480\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2355 - accuracy: 0.9072 - f1_score: 0.9060 - val_loss: 0.3441 - val_accuracy: 0.8707 - val_f1_score: 0.8724\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2319 - accuracy: 0.9081 - f1_score: 0.9065 - val_loss: 0.3630 - val_accuracy: 0.8617 - val_f1_score: 0.8642\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3443 - accuracy: 0.8609 - f1_score: 0.8839\n","Epoch 1/20\n","104/104 [==============================] - 6s 16ms/step - loss: 0.6279 - accuracy: 0.6465 - f1_score: 0.6489 - val_loss: 0.5237 - val_accuracy: 0.7523 - val_f1_score: 0.7175\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5075 - accuracy: 0.7595 - f1_score: 0.7429 - val_loss: 0.4217 - val_accuracy: 0.8119 - val_f1_score: 0.7961\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4292 - accuracy: 0.7987 - f1_score: 0.7901 - val_loss: 0.3988 - val_accuracy: 0.8174 - val_f1_score: 0.7913\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3774 - accuracy: 0.8339 - f1_score: 0.8299 - val_loss: 0.3527 - val_accuracy: 0.8463 - val_f1_score: 0.8327\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3383 - accuracy: 0.8605 - f1_score: 0.8586 - val_loss: 0.3274 - val_accuracy: 0.8571 - val_f1_score: 0.8621\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3072 - accuracy: 0.8701 - f1_score: 0.8690 - val_loss: 0.4371 - val_accuracy: 0.8101 - val_f1_score: 0.7727\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3145 - accuracy: 0.8713 - f1_score: 0.8702 - val_loss: 0.3413 - val_accuracy: 0.8526 - val_f1_score: 0.8385\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3029 - accuracy: 0.8803 - f1_score: 0.8794 - val_loss: 0.3365 - val_accuracy: 0.8544 - val_f1_score: 0.8411\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3056 - accuracy: 0.8810 - f1_score: 0.8800 - val_loss: 0.3748 - val_accuracy: 0.8237 - val_f1_score: 0.7945\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2896 - accuracy: 0.8819 - f1_score: 0.8803 - val_loss: 0.3017 - val_accuracy: 0.8680 - val_f1_score: 0.8668\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2700 - accuracy: 0.8948 - f1_score: 0.8936 - val_loss: 0.3116 - val_accuracy: 0.8761 - val_f1_score: 0.8760\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2661 - accuracy: 0.8909 - f1_score: 0.8899 - val_loss: 0.3414 - val_accuracy: 0.8617 - val_f1_score: 0.8527\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2526 - accuracy: 0.9027 - f1_score: 0.9018 - val_loss: 0.3629 - val_accuracy: 0.8363 - val_f1_score: 0.8166\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2470 - accuracy: 0.9002 - f1_score: 0.8989 - val_loss: 0.3031 - val_accuracy: 0.8779 - val_f1_score: 0.8751\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2430 - accuracy: 0.9051 - f1_score: 0.9046 - val_loss: 0.3841 - val_accuracy: 0.8373 - val_f1_score: 0.8163\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3383 - accuracy: 0.8575 - f1_score: 0.8859\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6107 - accuracy: 0.6709 - f1_score: 0.6814 - val_loss: 0.5123 - val_accuracy: 0.7550 - val_f1_score: 0.7412\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4766 - accuracy: 0.7803 - f1_score: 0.7713 - val_loss: 0.4551 - val_accuracy: 0.7785 - val_f1_score: 0.7487\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3995 - accuracy: 0.8228 - f1_score: 0.8171 - val_loss: 0.3980 - val_accuracy: 0.8228 - val_f1_score: 0.8130\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3676 - accuracy: 0.8388 - f1_score: 0.8371 - val_loss: 0.3914 - val_accuracy: 0.8300 - val_f1_score: 0.8135\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3250 - accuracy: 0.8626 - f1_score: 0.8615 - val_loss: 0.3784 - val_accuracy: 0.8445 - val_f1_score: 0.8340\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3282 - accuracy: 0.8629 - f1_score: 0.8613 - val_loss: 0.3748 - val_accuracy: 0.8391 - val_f1_score: 0.8268\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3101 - accuracy: 0.8701 - f1_score: 0.8682 - val_loss: 0.3548 - val_accuracy: 0.8499 - val_f1_score: 0.8422\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3028 - accuracy: 0.8734 - f1_score: 0.8725 - val_loss: 0.3659 - val_accuracy: 0.8445 - val_f1_score: 0.8327\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2849 - accuracy: 0.8843 - f1_score: 0.8825 - val_loss: 0.3824 - val_accuracy: 0.8427 - val_f1_score: 0.8297\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2776 - accuracy: 0.8891 - f1_score: 0.8877 - val_loss: 0.3553 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2599 - accuracy: 0.8996 - f1_score: 0.8981 - val_loss: 0.3796 - val_accuracy: 0.8580 - val_f1_score: 0.8582\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2452 - accuracy: 0.9030 - f1_score: 0.9022 - val_loss: 0.3634 - val_accuracy: 0.8490 - val_f1_score: 0.8371\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3532 - accuracy: 0.8528 - f1_score: 0.8764\n","Epoch 1/20\n","104/104 [==============================] - 6s 18ms/step - loss: 0.6270 - accuracy: 0.6404 - f1_score: 0.6317 - val_loss: 0.4830 - val_accuracy: 0.7749 - val_f1_score: 0.7613\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.5108 - accuracy: 0.7613 - f1_score: 0.7537 - val_loss: 0.4277 - val_accuracy: 0.8101 - val_f1_score: 0.8073\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4306 - accuracy: 0.8023 - f1_score: 0.7951 - val_loss: 0.3605 - val_accuracy: 0.8391 - val_f1_score: 0.8275\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4104 - accuracy: 0.8128 - f1_score: 0.8105 - val_loss: 0.3748 - val_accuracy: 0.8327 - val_f1_score: 0.8490\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3518 - accuracy: 0.8520 - f1_score: 0.8505 - val_loss: 0.3250 - val_accuracy: 0.8553 - val_f1_score: 0.8637\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3365 - accuracy: 0.8541 - f1_score: 0.8530 - val_loss: 0.3733 - val_accuracy: 0.8327 - val_f1_score: 0.8512\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3145 - accuracy: 0.8713 - f1_score: 0.8706 - val_loss: 0.2806 - val_accuracy: 0.8879 - val_f1_score: 0.8871\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3197 - accuracy: 0.8650 - f1_score: 0.8639 - val_loss: 0.2806 - val_accuracy: 0.8951 - val_f1_score: 0.8906\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3073 - accuracy: 0.8707 - f1_score: 0.8691 - val_loss: 0.3150 - val_accuracy: 0.8653 - val_f1_score: 0.8751\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2915 - accuracy: 0.8794 - f1_score: 0.8789 - val_loss: 0.2705 - val_accuracy: 0.8969 - val_f1_score: 0.8975\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2949 - accuracy: 0.8810 - f1_score: 0.8796 - val_loss: 0.2688 - val_accuracy: 0.8933 - val_f1_score: 0.8923\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2666 - accuracy: 0.8957 - f1_score: 0.8945 - val_loss: 0.2785 - val_accuracy: 0.8897 - val_f1_score: 0.8934\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2655 - accuracy: 0.8960 - f1_score: 0.8956 - val_loss: 0.2753 - val_accuracy: 0.8942 - val_f1_score: 0.8953\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2498 - accuracy: 0.9063 - f1_score: 0.9057 - val_loss: 0.3214 - val_accuracy: 0.8761 - val_f1_score: 0.8838\n","Epoch 15/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2392 - accuracy: 0.9048 - f1_score: 0.9045 - val_loss: 0.2780 - val_accuracy: 0.8978 - val_f1_score: 0.8977\n","Epoch 16/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2425 - accuracy: 0.9072 - f1_score: 0.9061 - val_loss: 0.3614 - val_accuracy: 0.8517 - val_f1_score: 0.8645\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3404 - accuracy: 0.8575 - f1_score: 0.8814\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6211 - accuracy: 0.6585 - f1_score: 0.6673 - val_loss: 0.5324 - val_accuracy: 0.7378 - val_f1_score: 0.7555\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4657 - accuracy: 0.7869 - f1_score: 0.7821 - val_loss: 0.4720 - val_accuracy: 0.7812 - val_f1_score: 0.8091\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4085 - accuracy: 0.8159 - f1_score: 0.8147 - val_loss: 0.4028 - val_accuracy: 0.8192 - val_f1_score: 0.8328\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3587 - accuracy: 0.8484 - f1_score: 0.8475 - val_loss: 0.3593 - val_accuracy: 0.8445 - val_f1_score: 0.8462\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3315 - accuracy: 0.8629 - f1_score: 0.8612 - val_loss: 0.3465 - val_accuracy: 0.8562 - val_f1_score: 0.8571\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3249 - accuracy: 0.8692 - f1_score: 0.8682 - val_loss: 0.3451 - val_accuracy: 0.8535 - val_f1_score: 0.8497\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3093 - accuracy: 0.8731 - f1_score: 0.8730 - val_loss: 0.3377 - val_accuracy: 0.8553 - val_f1_score: 0.8505\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3043 - accuracy: 0.8767 - f1_score: 0.8756 - val_loss: 0.3325 - val_accuracy: 0.8617 - val_f1_score: 0.8605\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2855 - accuracy: 0.8864 - f1_score: 0.8850 - val_loss: 0.3706 - val_accuracy: 0.8544 - val_f1_score: 0.8414\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2895 - accuracy: 0.8800 - f1_score: 0.8786 - val_loss: 0.3286 - val_accuracy: 0.8671 - val_f1_score: 0.8640\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2699 - accuracy: 0.8939 - f1_score: 0.8926 - val_loss: 0.3319 - val_accuracy: 0.8680 - val_f1_score: 0.8607\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2621 - accuracy: 0.8915 - f1_score: 0.8902 - val_loss: 0.3414 - val_accuracy: 0.8698 - val_f1_score: 0.8705\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2667 - accuracy: 0.8957 - f1_score: 0.8952 - val_loss: 0.3420 - val_accuracy: 0.8716 - val_f1_score: 0.8692\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2407 - accuracy: 0.9054 - f1_score: 0.9047 - val_loss: 0.3451 - val_accuracy: 0.8743 - val_f1_score: 0.8714\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2548 - accuracy: 0.8990 - f1_score: 0.8982 - val_loss: 0.3341 - val_accuracy: 0.8680 - val_f1_score: 0.8656\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3397 - accuracy: 0.8643 - f1_score: 0.8851\n","Epoch 1/20\n","104/104 [==============================] - 7s 15ms/step - loss: 0.6268 - accuracy: 0.6480 - f1_score: 0.6285 - val_loss: 0.5799 - val_accuracy: 0.6754 - val_f1_score: 0.7343\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4837 - accuracy: 0.7728 - f1_score: 0.7693 - val_loss: 0.4203 - val_accuracy: 0.8165 - val_f1_score: 0.8236\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4079 - accuracy: 0.8195 - f1_score: 0.8165 - val_loss: 0.3894 - val_accuracy: 0.8445 - val_f1_score: 0.8507\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3505 - accuracy: 0.8502 - f1_score: 0.8471 - val_loss: 0.4008 - val_accuracy: 0.8409 - val_f1_score: 0.8496\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3213 - accuracy: 0.8674 - f1_score: 0.8663 - val_loss: 0.4083 - val_accuracy: 0.8400 - val_f1_score: 0.8481\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3186 - accuracy: 0.8641 - f1_score: 0.8625 - val_loss: 0.3664 - val_accuracy: 0.8553 - val_f1_score: 0.8556\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3052 - accuracy: 0.8740 - f1_score: 0.8733 - val_loss: 0.3575 - val_accuracy: 0.8617 - val_f1_score: 0.8587\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2863 - accuracy: 0.8882 - f1_score: 0.8872 - val_loss: 0.4290 - val_accuracy: 0.8219 - val_f1_score: 0.8376\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2846 - accuracy: 0.8900 - f1_score: 0.8890 - val_loss: 0.3757 - val_accuracy: 0.8535 - val_f1_score: 0.8599\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2775 - accuracy: 0.8873 - f1_score: 0.8865 - val_loss: 0.3541 - val_accuracy: 0.8617 - val_f1_score: 0.8598\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2601 - accuracy: 0.9005 - f1_score: 0.8997 - val_loss: 0.3614 - val_accuracy: 0.8580 - val_f1_score: 0.8602\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2547 - accuracy: 0.9030 - f1_score: 0.9017 - val_loss: 0.3885 - val_accuracy: 0.8481 - val_f1_score: 0.8531\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2525 - accuracy: 0.9048 - f1_score: 0.9041 - val_loss: 0.3932 - val_accuracy: 0.8354 - val_f1_score: 0.8473\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2369 - accuracy: 0.9120 - f1_score: 0.9110 - val_loss: 0.3840 - val_accuracy: 0.8580 - val_f1_score: 0.8638\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2342 - accuracy: 0.9060 - f1_score: 0.9053 - val_loss: 0.3805 - val_accuracy: 0.8517 - val_f1_score: 0.8464\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3525 - accuracy: 0.8629 - f1_score: 0.8833\n","Epoch 1/20\n","104/104 [==============================] - 5s 15ms/step - loss: 0.6225 - accuracy: 0.6558 - f1_score: 0.6503 - val_loss: 0.5502 - val_accuracy: 0.7297 - val_f1_score: 0.7567\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4943 - accuracy: 0.7697 - f1_score: 0.7608 - val_loss: 0.4414 - val_accuracy: 0.8101 - val_f1_score: 0.8161\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4150 - accuracy: 0.8174 - f1_score: 0.8097 - val_loss: 0.5651 - val_accuracy: 0.7432 - val_f1_score: 0.7884\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3699 - accuracy: 0.8327 - f1_score: 0.8303 - val_loss: 0.3594 - val_accuracy: 0.8653 - val_f1_score: 0.8690\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3571 - accuracy: 0.8460 - f1_score: 0.8448 - val_loss: 0.3497 - val_accuracy: 0.8644 - val_f1_score: 0.8707\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3230 - accuracy: 0.8674 - f1_score: 0.8660 - val_loss: 0.3208 - val_accuracy: 0.8689 - val_f1_score: 0.8683\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3098 - accuracy: 0.8743 - f1_score: 0.8727 - val_loss: 0.3159 - val_accuracy: 0.8707 - val_f1_score: 0.8720\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3010 - accuracy: 0.8788 - f1_score: 0.8774 - val_loss: 0.3300 - val_accuracy: 0.8680 - val_f1_score: 0.8746\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3090 - accuracy: 0.8734 - f1_score: 0.8725 - val_loss: 0.3957 - val_accuracy: 0.8427 - val_f1_score: 0.8571\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2928 - accuracy: 0.8828 - f1_score: 0.8816 - val_loss: 0.3285 - val_accuracy: 0.8689 - val_f1_score: 0.8757\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2868 - accuracy: 0.8819 - f1_score: 0.8809 - val_loss: 0.3744 - val_accuracy: 0.8454 - val_f1_score: 0.8579\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2641 - accuracy: 0.8930 - f1_score: 0.8920 - val_loss: 0.3159 - val_accuracy: 0.8689 - val_f1_score: 0.8713\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2708 - accuracy: 0.8909 - f1_score: 0.8900 - val_loss: 0.3214 - val_accuracy: 0.8725 - val_f1_score: 0.8698\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2665 - accuracy: 0.8966 - f1_score: 0.8955 - val_loss: 0.3273 - val_accuracy: 0.8671 - val_f1_score: 0.8718\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2474 - accuracy: 0.9008 - f1_score: 0.8999 - val_loss: 0.3804 - val_accuracy: 0.8499 - val_f1_score: 0.8607\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2437 - accuracy: 0.9005 - f1_score: 0.8996 - val_loss: 0.3255 - val_accuracy: 0.8752 - val_f1_score: 0.8743\n","Epoch 17/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2406 - accuracy: 0.9093 - f1_score: 0.9083 - val_loss: 0.3333 - val_accuracy: 0.8716 - val_f1_score: 0.8714\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3195 - accuracy: 0.8697 - f1_score: 0.8921\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6195 - accuracy: 0.6621 - f1_score: 0.6483 - val_loss: 0.5560 - val_accuracy: 0.7134 - val_f1_score: 0.7421\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4736 - accuracy: 0.7752 - f1_score: 0.7705 - val_loss: 0.4278 - val_accuracy: 0.8029 - val_f1_score: 0.7871\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3982 - accuracy: 0.8216 - f1_score: 0.8190 - val_loss: 0.3927 - val_accuracy: 0.8345 - val_f1_score: 0.8435\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3562 - accuracy: 0.8433 - f1_score: 0.8413 - val_loss: 0.3651 - val_accuracy: 0.8499 - val_f1_score: 0.8569\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3399 - accuracy: 0.8571 - f1_score: 0.8550 - val_loss: 0.3702 - val_accuracy: 0.8463 - val_f1_score: 0.8303\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3190 - accuracy: 0.8641 - f1_score: 0.8628 - val_loss: 0.3349 - val_accuracy: 0.8734 - val_f1_score: 0.8669\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3073 - accuracy: 0.8686 - f1_score: 0.8665 - val_loss: 0.3270 - val_accuracy: 0.8689 - val_f1_score: 0.8666\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2989 - accuracy: 0.8776 - f1_score: 0.8767 - val_loss: 0.3261 - val_accuracy: 0.8716 - val_f1_score: 0.8655\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2858 - accuracy: 0.8849 - f1_score: 0.8836 - val_loss: 0.3215 - val_accuracy: 0.8743 - val_f1_score: 0.8717\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2734 - accuracy: 0.8912 - f1_score: 0.8898 - val_loss: 0.3235 - val_accuracy: 0.8698 - val_f1_score: 0.8721\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2602 - accuracy: 0.8969 - f1_score: 0.8960 - val_loss: 0.3153 - val_accuracy: 0.8797 - val_f1_score: 0.8805\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2597 - accuracy: 0.8978 - f1_score: 0.8974 - val_loss: 0.3231 - val_accuracy: 0.8725 - val_f1_score: 0.8719\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2490 - accuracy: 0.9017 - f1_score: 0.9012 - val_loss: 0.3511 - val_accuracy: 0.8580 - val_f1_score: 0.8655\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2368 - accuracy: 0.9075 - f1_score: 0.9067 - val_loss: 0.3325 - val_accuracy: 0.8662 - val_f1_score: 0.8604\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2276 - accuracy: 0.9093 - f1_score: 0.9085 - val_loss: 0.3297 - val_accuracy: 0.8770 - val_f1_score: 0.8745\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2427 - accuracy: 0.9005 - f1_score: 0.8992 - val_loss: 0.3732 - val_accuracy: 0.8499 - val_f1_score: 0.8350\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3020 - accuracy: 0.8751 - f1_score: 0.8986\n","Epoch 1/20\n","104/104 [==============================] - 8s 29ms/step - loss: 0.6126 - accuracy: 0.6606 - f1_score: 0.6582 - val_loss: 0.5113 - val_accuracy: 0.7414 - val_f1_score: 0.7697\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4836 - accuracy: 0.7770 - f1_score: 0.7706 - val_loss: 0.4173 - val_accuracy: 0.8083 - val_f1_score: 0.8262\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4120 - accuracy: 0.8128 - f1_score: 0.8079 - val_loss: 0.3808 - val_accuracy: 0.8255 - val_f1_score: 0.8393\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3549 - accuracy: 0.8508 - f1_score: 0.8487 - val_loss: 0.3379 - val_accuracy: 0.8644 - val_f1_score: 0.8670\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3221 - accuracy: 0.8623 - f1_score: 0.8602 - val_loss: 0.3362 - val_accuracy: 0.8617 - val_f1_score: 0.8525\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3062 - accuracy: 0.8758 - f1_score: 0.8743 - val_loss: 0.3226 - val_accuracy: 0.8680 - val_f1_score: 0.8610\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2990 - accuracy: 0.8779 - f1_score: 0.8763 - val_loss: 0.3295 - val_accuracy: 0.8671 - val_f1_score: 0.8560\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2888 - accuracy: 0.8834 - f1_score: 0.8822 - val_loss: 0.3206 - val_accuracy: 0.8689 - val_f1_score: 0.8615\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2730 - accuracy: 0.8957 - f1_score: 0.8948 - val_loss: 0.3211 - val_accuracy: 0.8653 - val_f1_score: 0.8552\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2758 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.3094 - val_accuracy: 0.8743 - val_f1_score: 0.8769\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2593 - accuracy: 0.8981 - f1_score: 0.8978 - val_loss: 0.3606 - val_accuracy: 0.8427 - val_f1_score: 0.8224\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2587 - accuracy: 0.8978 - f1_score: 0.8972 - val_loss: 0.3014 - val_accuracy: 0.8770 - val_f1_score: 0.8768\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2379 - accuracy: 0.9051 - f1_score: 0.9044 - val_loss: 0.3167 - val_accuracy: 0.8680 - val_f1_score: 0.8612\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2288 - accuracy: 0.9114 - f1_score: 0.9106 - val_loss: 0.3368 - val_accuracy: 0.8463 - val_f1_score: 0.8310\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2108 - accuracy: 0.9216 - f1_score: 0.9208 - val_loss: 0.3319 - val_accuracy: 0.8707 - val_f1_score: 0.8677\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2027 - accuracy: 0.9247 - f1_score: 0.9239 - val_loss: 0.3478 - val_accuracy: 0.8590 - val_f1_score: 0.8474\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1912 - accuracy: 0.9331 - f1_score: 0.9325 - val_loss: 0.3349 - val_accuracy: 0.8626 - val_f1_score: 0.8621\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3131 - accuracy: 0.8724 - f1_score: 0.8953\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vjpumrfy3QZK","executionInfo":{"status":"ok","timestamp":1689801685473,"user_tz":-330,"elapsed":136,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"01284360-a920-4712-99d9-fa39bd9d9be9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8453747630119324, 0.852126955986023, 0.8541526198387146, 0.8534773588180542, 0.839972972869873, 0.8528021574020386, 0.8636056780815125, 0.852126955986023, 0.8588791489601135, 0.8669817447662354, 0.8710330724716187, 0.8615800142288208, 0.8609048128128052, 0.8582038879394531, 0.8663065433502197, 0.8548278212547302, 0.8609048128128052, 0.8575286865234375, 0.8528021574020386, 0.8575286865234375, 0.8642808794975281, 0.8629304766654968, 0.8696826696395874, 0.875084400177002, 0.8723835349082947]\n","0.8594193124771118\n","0.008205514324934424\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaocgRfh3Qbw","executionInfo":{"status":"ok","timestamp":1689801685474,"user_tz":-330,"elapsed":44,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"190d55b9-9119-4822-9bbc-1294873e64d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3787480294704437, 0.349211722612381, 0.33591899275779724, 0.34599825739860535, 0.35947033762931824, 0.3412804901599884, 0.35609298944473267, 0.3785141110420227, 0.3393889367580414, 0.3309958875179291, 0.3072587549686432, 0.31737303733825684, 0.3352953791618347, 0.3254651427268982, 0.32043686509132385, 0.34770047664642334, 0.34434524178504944, 0.3382985293865204, 0.3532254993915558, 0.34041187167167664, 0.3397105932235718, 0.35246530175209045, 0.3194606602191925, 0.30201107263565063, 0.3130916953086853]\n","0.3388867950439453\n","0.018987580821432884\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBzObuWq3Qek","executionInfo":{"status":"ok","timestamp":1689801685474,"user_tz":-330,"elapsed":36,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"9b276ddb-b7db-4d1e-86d7-074962e5f0fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8660034537315369, 0.8731904625892639, 0.877411961555481, 0.875929057598114, 0.8614844083786011, 0.8771137595176697, 0.8882743120193481, 0.8773108720779419, 0.8836950063705444, 0.8903727531433105, 0.8962520360946655, 0.8887682557106018, 0.8860619068145752, 0.8808171153068542, 0.8925080895423889, 0.8787365555763245, 0.8838781714439392, 0.8858842253684998, 0.8764171600341797, 0.8813940286636353, 0.8850771188735962, 0.883266270160675, 0.8921184539794922, 0.8986300826072693, 0.8952907919883728]\n","0.8830354523658752\n","0.008828145502304775\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"1YAiPn893W2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TDqlC31t3W7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LSTM CNN LSTM nothing"],"metadata":{"id":"dh5QRVSk3XB1"}},{"cell_type":"code","source":["test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LAp-FmyA3aFz","executionInfo":{"status":"ok","timestamp":1689802423802,"user_tz":-330,"elapsed":738354,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"349fa703-d431-4dac-ea3c-11788dde5be5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6256 - accuracy: 0.6450 - f1_score: 0.6517 - val_loss: 0.5058 - val_accuracy: 0.7667 - val_f1_score: 0.7480\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5029 - accuracy: 0.7468 - f1_score: 0.7348 - val_loss: 0.3962 - val_accuracy: 0.8282 - val_f1_score: 0.8322\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4259 - accuracy: 0.8077 - f1_score: 0.8011 - val_loss: 0.3831 - val_accuracy: 0.8291 - val_f1_score: 0.8432\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3804 - accuracy: 0.8376 - f1_score: 0.8349 - val_loss: 0.3705 - val_accuracy: 0.8382 - val_f1_score: 0.8539\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3504 - accuracy: 0.8505 - f1_score: 0.8480 - val_loss: 0.3051 - val_accuracy: 0.8788 - val_f1_score: 0.8766\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3390 - accuracy: 0.8623 - f1_score: 0.8615 - val_loss: 0.3071 - val_accuracy: 0.8626 - val_f1_score: 0.8558\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3330 - accuracy: 0.8629 - f1_score: 0.8599 - val_loss: 0.3395 - val_accuracy: 0.8662 - val_f1_score: 0.8750\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3244 - accuracy: 0.8644 - f1_score: 0.8626 - val_loss: 0.3226 - val_accuracy: 0.8716 - val_f1_score: 0.8786\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3073 - accuracy: 0.8764 - f1_score: 0.8738 - val_loss: 0.3444 - val_accuracy: 0.8400 - val_f1_score: 0.8225\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2969 - accuracy: 0.8819 - f1_score: 0.8793 - val_loss: 0.3072 - val_accuracy: 0.8662 - val_f1_score: 0.8580\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3581 - accuracy: 0.8413 - f1_score: 0.8636\n","Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6298 - accuracy: 0.6429 - f1_score: 0.6548 - val_loss: 0.5553 - val_accuracy: 0.7233 - val_f1_score: 0.7320\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5050 - accuracy: 0.7619 - f1_score: 0.7519 - val_loss: 0.5013 - val_accuracy: 0.7631 - val_f1_score: 0.7856\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4479 - accuracy: 0.7972 - f1_score: 0.7913 - val_loss: 0.4389 - val_accuracy: 0.8002 - val_f1_score: 0.7729\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3974 - accuracy: 0.8171 - f1_score: 0.8125 - val_loss: 0.4717 - val_accuracy: 0.7722 - val_f1_score: 0.8019\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3736 - accuracy: 0.8300 - f1_score: 0.8272 - val_loss: 0.3946 - val_accuracy: 0.8156 - val_f1_score: 0.8280\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3407 - accuracy: 0.8562 - f1_score: 0.8557 - val_loss: 0.3523 - val_accuracy: 0.8608 - val_f1_score: 0.8587\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3258 - accuracy: 0.8602 - f1_score: 0.8591 - val_loss: 0.3454 - val_accuracy: 0.8662 - val_f1_score: 0.8642\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3040 - accuracy: 0.8767 - f1_score: 0.8761 - val_loss: 0.3458 - val_accuracy: 0.8590 - val_f1_score: 0.8592\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3103 - accuracy: 0.8698 - f1_score: 0.8696 - val_loss: 0.3500 - val_accuracy: 0.8526 - val_f1_score: 0.8543\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2886 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.3708 - val_accuracy: 0.8291 - val_f1_score: 0.8405\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2809 - accuracy: 0.8825 - f1_score: 0.8825 - val_loss: 0.3426 - val_accuracy: 0.8608 - val_f1_score: 0.8550\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2653 - accuracy: 0.8954 - f1_score: 0.8949 - val_loss: 0.3561 - val_accuracy: 0.8608 - val_f1_score: 0.8561\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2503 - accuracy: 0.8990 - f1_score: 0.8991 - val_loss: 0.3789 - val_accuracy: 0.8499 - val_f1_score: 0.8569\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2655 - accuracy: 0.8915 - f1_score: 0.8912 - val_loss: 0.3734 - val_accuracy: 0.8490 - val_f1_score: 0.8541\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2395 - accuracy: 0.9045 - f1_score: 0.9045 - val_loss: 0.3892 - val_accuracy: 0.8580 - val_f1_score: 0.8480\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2325 - accuracy: 0.9057 - f1_score: 0.9057 - val_loss: 0.3657 - val_accuracy: 0.8671 - val_f1_score: 0.8650\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3488 - accuracy: 0.8562 - f1_score: 0.8778\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6099 - accuracy: 0.6775 - f1_score: 0.6646 - val_loss: 0.5388 - val_accuracy: 0.7450 - val_f1_score: 0.7152\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4665 - accuracy: 0.7854 - f1_score: 0.7794 - val_loss: 0.4722 - val_accuracy: 0.7758 - val_f1_score: 0.7427\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3855 - accuracy: 0.8312 - f1_score: 0.8279 - val_loss: 0.3916 - val_accuracy: 0.8183 - val_f1_score: 0.8229\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3457 - accuracy: 0.8590 - f1_score: 0.8575 - val_loss: 0.4700 - val_accuracy: 0.7866 - val_f1_score: 0.7418\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3326 - accuracy: 0.8608 - f1_score: 0.8590 - val_loss: 0.3790 - val_accuracy: 0.8354 - val_f1_score: 0.8378\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3069 - accuracy: 0.8785 - f1_score: 0.8784 - val_loss: 0.3848 - val_accuracy: 0.8354 - val_f1_score: 0.8240\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2986 - accuracy: 0.8785 - f1_score: 0.8777 - val_loss: 0.3716 - val_accuracy: 0.8463 - val_f1_score: 0.8443\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2933 - accuracy: 0.8834 - f1_score: 0.8816 - val_loss: 0.3823 - val_accuracy: 0.8336 - val_f1_score: 0.8280\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2774 - accuracy: 0.8888 - f1_score: 0.8881 - val_loss: 0.4447 - val_accuracy: 0.8228 - val_f1_score: 0.8008\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2785 - accuracy: 0.8918 - f1_score: 0.8906 - val_loss: 0.4202 - val_accuracy: 0.8246 - val_f1_score: 0.8032\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2668 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3972 - val_accuracy: 0.8382 - val_f1_score: 0.8335\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2552 - accuracy: 0.9027 - f1_score: 0.9016 - val_loss: 0.4119 - val_accuracy: 0.8382 - val_f1_score: 0.8431\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.8582 - f1_score: 0.8808\n","Epoch 1/20\n","104/104 [==============================] - 8s 24ms/step - loss: 0.6350 - accuracy: 0.6456 - f1_score: 0.6517 - val_loss: 0.5220 - val_accuracy: 0.7514 - val_f1_score: 0.7275\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5074 - accuracy: 0.7523 - f1_score: 0.7405 - val_loss: 0.4931 - val_accuracy: 0.7812 - val_f1_score: 0.8130\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4058 - accuracy: 0.8156 - f1_score: 0.8119 - val_loss: 0.4291 - val_accuracy: 0.8110 - val_f1_score: 0.8324\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3560 - accuracy: 0.8433 - f1_score: 0.8422 - val_loss: 0.3613 - val_accuracy: 0.8508 - val_f1_score: 0.8554\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3493 - accuracy: 0.8487 - f1_score: 0.8474 - val_loss: 0.4268 - val_accuracy: 0.8156 - val_f1_score: 0.8365\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3374 - accuracy: 0.8638 - f1_score: 0.8625 - val_loss: 0.3514 - val_accuracy: 0.8553 - val_f1_score: 0.8482\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3218 - accuracy: 0.8707 - f1_score: 0.8692 - val_loss: 0.3614 - val_accuracy: 0.8535 - val_f1_score: 0.8611\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2995 - accuracy: 0.8822 - f1_score: 0.8810 - val_loss: 0.3549 - val_accuracy: 0.8590 - val_f1_score: 0.8627\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2960 - accuracy: 0.8822 - f1_score: 0.8815 - val_loss: 0.3403 - val_accuracy: 0.8617 - val_f1_score: 0.8638\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2911 - accuracy: 0.8849 - f1_score: 0.8837 - val_loss: 0.3694 - val_accuracy: 0.8526 - val_f1_score: 0.8617\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2864 - accuracy: 0.8855 - f1_score: 0.8846 - val_loss: 0.3617 - val_accuracy: 0.8599 - val_f1_score: 0.8644\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2730 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3644 - val_accuracy: 0.8571 - val_f1_score: 0.8607\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2823 - accuracy: 0.8882 - f1_score: 0.8873 - val_loss: 0.3587 - val_accuracy: 0.8608 - val_f1_score: 0.8582\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2539 - accuracy: 0.9011 - f1_score: 0.9004 - val_loss: 0.3918 - val_accuracy: 0.8490 - val_f1_score: 0.8586\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.8609 - f1_score: 0.8834\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6314 - accuracy: 0.6368 - f1_score: 0.6572 - val_loss: 0.5264 - val_accuracy: 0.7523 - val_f1_score: 0.7495\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4958 - accuracy: 0.7649 - f1_score: 0.7485 - val_loss: 0.4264 - val_accuracy: 0.8083 - val_f1_score: 0.8153\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4191 - accuracy: 0.8083 - f1_score: 0.8008 - val_loss: 0.3671 - val_accuracy: 0.8418 - val_f1_score: 0.8453\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3829 - accuracy: 0.8303 - f1_score: 0.8262 - val_loss: 0.3544 - val_accuracy: 0.8571 - val_f1_score: 0.8469\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3650 - accuracy: 0.8376 - f1_score: 0.8335 - val_loss: 0.3385 - val_accuracy: 0.8635 - val_f1_score: 0.8595\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3259 - accuracy: 0.8608 - f1_score: 0.8581 - val_loss: 0.3612 - val_accuracy: 0.8436 - val_f1_score: 0.8518\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3245 - accuracy: 0.8635 - f1_score: 0.8618 - val_loss: 0.3431 - val_accuracy: 0.8526 - val_f1_score: 0.8422\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3026 - accuracy: 0.8734 - f1_score: 0.8704 - val_loss: 0.3207 - val_accuracy: 0.8662 - val_f1_score: 0.8657\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2962 - accuracy: 0.8776 - f1_score: 0.8752 - val_loss: 0.3235 - val_accuracy: 0.8644 - val_f1_score: 0.8631\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2898 - accuracy: 0.8840 - f1_score: 0.8815 - val_loss: 0.3399 - val_accuracy: 0.8590 - val_f1_score: 0.8480\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2832 - accuracy: 0.8822 - f1_score: 0.8802 - val_loss: 0.3454 - val_accuracy: 0.8743 - val_f1_score: 0.8687\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2795 - accuracy: 0.8846 - f1_score: 0.8821 - val_loss: 0.3461 - val_accuracy: 0.8635 - val_f1_score: 0.8549\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2733 - accuracy: 0.8888 - f1_score: 0.8854 - val_loss: 0.3497 - val_accuracy: 0.8635 - val_f1_score: 0.8677\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8596 - f1_score: 0.8822\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6209 - accuracy: 0.6558 - f1_score: 0.6326 - val_loss: 0.5042 - val_accuracy: 0.7595 - val_f1_score: 0.7532\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4944 - accuracy: 0.7610 - f1_score: 0.7577 - val_loss: 0.4623 - val_accuracy: 0.7857 - val_f1_score: 0.8078\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4180 - accuracy: 0.8128 - f1_score: 0.8105 - val_loss: 0.3822 - val_accuracy: 0.8391 - val_f1_score: 0.8241\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3814 - accuracy: 0.8318 - f1_score: 0.8295 - val_loss: 0.3449 - val_accuracy: 0.8562 - val_f1_score: 0.8529\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3573 - accuracy: 0.8433 - f1_score: 0.8407 - val_loss: 0.3273 - val_accuracy: 0.8671 - val_f1_score: 0.8712\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3369 - accuracy: 0.8544 - f1_score: 0.8535 - val_loss: 0.3106 - val_accuracy: 0.8779 - val_f1_score: 0.8744\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3160 - accuracy: 0.8722 - f1_score: 0.8713 - val_loss: 0.2965 - val_accuracy: 0.8779 - val_f1_score: 0.8783\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3100 - accuracy: 0.8713 - f1_score: 0.8710 - val_loss: 0.3001 - val_accuracy: 0.8888 - val_f1_score: 0.8845\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2942 - accuracy: 0.8767 - f1_score: 0.8775 - val_loss: 0.2987 - val_accuracy: 0.8888 - val_f1_score: 0.8852\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2845 - accuracy: 0.8858 - f1_score: 0.8854 - val_loss: 0.3096 - val_accuracy: 0.8698 - val_f1_score: 0.8571\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2725 - accuracy: 0.8894 - f1_score: 0.8881 - val_loss: 0.2873 - val_accuracy: 0.8852 - val_f1_score: 0.8796\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2710 - accuracy: 0.8900 - f1_score: 0.8899 - val_loss: 0.2908 - val_accuracy: 0.8797 - val_f1_score: 0.8834\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2564 - accuracy: 0.8969 - f1_score: 0.8967 - val_loss: 0.2896 - val_accuracy: 0.8879 - val_f1_score: 0.8841\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2356 - accuracy: 0.9084 - f1_score: 0.9080 - val_loss: 0.3130 - val_accuracy: 0.8797 - val_f1_score: 0.8727\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2294 - accuracy: 0.9081 - f1_score: 0.9080 - val_loss: 0.3260 - val_accuracy: 0.8698 - val_f1_score: 0.8599\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2111 - accuracy: 0.9171 - f1_score: 0.9168 - val_loss: 0.3528 - val_accuracy: 0.8373 - val_f1_score: 0.8520\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3385 - accuracy: 0.8582 - f1_score: 0.8822\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6118 - accuracy: 0.6721 - f1_score: 0.6689 - val_loss: 0.5419 - val_accuracy: 0.7369 - val_f1_score: 0.7565\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4760 - accuracy: 0.7806 - f1_score: 0.7721 - val_loss: 0.5288 - val_accuracy: 0.7441 - val_f1_score: 0.7867\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4102 - accuracy: 0.8162 - f1_score: 0.8132 - val_loss: 0.3953 - val_accuracy: 0.8264 - val_f1_score: 0.8381\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3793 - accuracy: 0.8406 - f1_score: 0.8386 - val_loss: 0.4460 - val_accuracy: 0.7939 - val_f1_score: 0.8238\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3402 - accuracy: 0.8586 - f1_score: 0.8573 - val_loss: 0.4784 - val_accuracy: 0.7830 - val_f1_score: 0.8171\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3188 - accuracy: 0.8668 - f1_score: 0.8661 - val_loss: 0.3764 - val_accuracy: 0.8373 - val_f1_score: 0.8541\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3116 - accuracy: 0.8722 - f1_score: 0.8718 - val_loss: 0.3273 - val_accuracy: 0.8635 - val_f1_score: 0.8547\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3069 - accuracy: 0.8716 - f1_score: 0.8699 - val_loss: 0.3109 - val_accuracy: 0.8752 - val_f1_score: 0.8768\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2956 - accuracy: 0.8761 - f1_score: 0.8755 - val_loss: 0.3151 - val_accuracy: 0.8671 - val_f1_score: 0.8593\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2830 - accuracy: 0.8843 - f1_score: 0.8838 - val_loss: 0.3227 - val_accuracy: 0.8626 - val_f1_score: 0.8710\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2593 - accuracy: 0.8927 - f1_score: 0.8922 - val_loss: 0.4249 - val_accuracy: 0.8174 - val_f1_score: 0.8414\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2528 - accuracy: 0.9045 - f1_score: 0.9043 - val_loss: 0.3302 - val_accuracy: 0.8698 - val_f1_score: 0.8776\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2395 - accuracy: 0.9078 - f1_score: 0.9073 - val_loss: 0.3006 - val_accuracy: 0.8816 - val_f1_score: 0.8823\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2324 - accuracy: 0.9126 - f1_score: 0.9123 - val_loss: 0.3231 - val_accuracy: 0.8707 - val_f1_score: 0.8731\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2150 - accuracy: 0.9150 - f1_score: 0.9148 - val_loss: 0.3165 - val_accuracy: 0.8743 - val_f1_score: 0.8792\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2142 - accuracy: 0.9174 - f1_score: 0.9172 - val_loss: 0.3705 - val_accuracy: 0.8562 - val_f1_score: 0.8680\n","Epoch 17/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2083 - accuracy: 0.9159 - f1_score: 0.9155 - val_loss: 0.3049 - val_accuracy: 0.8825 - val_f1_score: 0.8796\n","Epoch 18/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.1911 - accuracy: 0.9253 - f1_score: 0.9249 - val_loss: 0.3382 - val_accuracy: 0.8816 - val_f1_score: 0.8858\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3494 - accuracy: 0.8575 - f1_score: 0.8825\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6297 - accuracy: 0.6438 - f1_score: 0.6463 - val_loss: 0.6081 - val_accuracy: 0.6618 - val_f1_score: 0.7278\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5112 - accuracy: 0.7601 - f1_score: 0.7500 - val_loss: 0.4787 - val_accuracy: 0.7731 - val_f1_score: 0.7383\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4280 - accuracy: 0.8002 - f1_score: 0.7921 - val_loss: 0.4183 - val_accuracy: 0.8038 - val_f1_score: 0.7828\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3760 - accuracy: 0.8345 - f1_score: 0.8293 - val_loss: 0.4778 - val_accuracy: 0.7911 - val_f1_score: 0.8180\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3489 - accuracy: 0.8517 - f1_score: 0.8482 - val_loss: 0.3571 - val_accuracy: 0.8499 - val_f1_score: 0.8488\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3273 - accuracy: 0.8611 - f1_score: 0.8590 - val_loss: 0.3501 - val_accuracy: 0.8571 - val_f1_score: 0.8489\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3357 - accuracy: 0.8517 - f1_score: 0.8490 - val_loss: 0.3649 - val_accuracy: 0.8463 - val_f1_score: 0.8290\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3001 - accuracy: 0.8788 - f1_score: 0.8778 - val_loss: 0.3281 - val_accuracy: 0.8653 - val_f1_score: 0.8624\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2826 - accuracy: 0.8837 - f1_score: 0.8827 - val_loss: 0.3262 - val_accuracy: 0.8725 - val_f1_score: 0.8735\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2684 - accuracy: 0.8930 - f1_score: 0.8917 - val_loss: 0.4325 - val_accuracy: 0.8083 - val_f1_score: 0.8309\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2847 - accuracy: 0.8828 - f1_score: 0.8818 - val_loss: 0.3462 - val_accuracy: 0.8544 - val_f1_score: 0.8618\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2535 - accuracy: 0.9008 - f1_score: 0.9008 - val_loss: 0.3321 - val_accuracy: 0.8653 - val_f1_score: 0.8593\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2565 - accuracy: 0.8939 - f1_score: 0.8929 - val_loss: 0.3379 - val_accuracy: 0.8707 - val_f1_score: 0.8720\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2359 - accuracy: 0.9063 - f1_score: 0.9061 - val_loss: 0.4136 - val_accuracy: 0.8336 - val_f1_score: 0.8115\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3462 - accuracy: 0.8521 - f1_score: 0.8787\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6154 - accuracy: 0.6643 - f1_score: 0.6492 - val_loss: 0.5197 - val_accuracy: 0.7577 - val_f1_score: 0.7248\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4895 - accuracy: 0.7719 - f1_score: 0.7647 - val_loss: 0.4381 - val_accuracy: 0.7939 - val_f1_score: 0.7747\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4274 - accuracy: 0.8050 - f1_score: 0.7979 - val_loss: 0.3933 - val_accuracy: 0.8273 - val_f1_score: 0.8366\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3768 - accuracy: 0.8379 - f1_score: 0.8351 - val_loss: 0.3491 - val_accuracy: 0.8454 - val_f1_score: 0.8469\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3539 - accuracy: 0.8511 - f1_score: 0.8491 - val_loss: 0.3974 - val_accuracy: 0.8282 - val_f1_score: 0.8453\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3326 - accuracy: 0.8580 - f1_score: 0.8562 - val_loss: 0.3247 - val_accuracy: 0.8680 - val_f1_score: 0.8722\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3071 - accuracy: 0.8722 - f1_score: 0.8713 - val_loss: 0.3176 - val_accuracy: 0.8734 - val_f1_score: 0.8669\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2894 - accuracy: 0.8785 - f1_score: 0.8772 - val_loss: 0.3441 - val_accuracy: 0.8617 - val_f1_score: 0.8702\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2915 - accuracy: 0.8797 - f1_score: 0.8787 - val_loss: 0.3107 - val_accuracy: 0.8752 - val_f1_score: 0.8722\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2802 - accuracy: 0.8870 - f1_score: 0.8859 - val_loss: 0.3009 - val_accuracy: 0.8843 - val_f1_score: 0.8821\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2715 - accuracy: 0.8855 - f1_score: 0.8844 - val_loss: 0.3063 - val_accuracy: 0.8797 - val_f1_score: 0.8779\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2662 - accuracy: 0.8876 - f1_score: 0.8866 - val_loss: 0.3453 - val_accuracy: 0.8526 - val_f1_score: 0.8615\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2639 - accuracy: 0.8897 - f1_score: 0.8888 - val_loss: 0.5107 - val_accuracy: 0.8020 - val_f1_score: 0.7607\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2353 - accuracy: 0.9075 - f1_score: 0.9072 - val_loss: 0.3903 - val_accuracy: 0.8427 - val_f1_score: 0.8560\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2356 - accuracy: 0.9066 - f1_score: 0.9067 - val_loss: 0.3410 - val_accuracy: 0.8671 - val_f1_score: 0.8596\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3551 - accuracy: 0.8535 - f1_score: 0.8783\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6212 - accuracy: 0.6498 - f1_score: 0.6464 - val_loss: 0.5124 - val_accuracy: 0.7631 - val_f1_score: 0.7730\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4753 - accuracy: 0.7824 - f1_score: 0.7699 - val_loss: 0.4149 - val_accuracy: 0.8038 - val_f1_score: 0.7923\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3990 - accuracy: 0.8225 - f1_score: 0.8183 - val_loss: 0.3762 - val_accuracy: 0.8345 - val_f1_score: 0.8298\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3578 - accuracy: 0.8454 - f1_score: 0.8418 - val_loss: 0.3820 - val_accuracy: 0.8363 - val_f1_score: 0.8195\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3437 - accuracy: 0.8565 - f1_score: 0.8548 - val_loss: 0.3566 - val_accuracy: 0.8517 - val_f1_score: 0.8426\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3156 - accuracy: 0.8716 - f1_score: 0.8700 - val_loss: 0.3341 - val_accuracy: 0.8635 - val_f1_score: 0.8626\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3160 - accuracy: 0.8641 - f1_score: 0.8631 - val_loss: 0.3397 - val_accuracy: 0.8562 - val_f1_score: 0.8584\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2916 - accuracy: 0.8803 - f1_score: 0.8796 - val_loss: 0.4459 - val_accuracy: 0.7966 - val_f1_score: 0.8227\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2961 - accuracy: 0.8794 - f1_score: 0.8791 - val_loss: 0.3225 - val_accuracy: 0.8662 - val_f1_score: 0.8659\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2780 - accuracy: 0.8891 - f1_score: 0.8877 - val_loss: 0.3404 - val_accuracy: 0.8671 - val_f1_score: 0.8609\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2640 - accuracy: 0.9020 - f1_score: 0.9017 - val_loss: 0.3263 - val_accuracy: 0.8644 - val_f1_score: 0.8585\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2667 - accuracy: 0.8927 - f1_score: 0.8921 - val_loss: 0.3107 - val_accuracy: 0.8653 - val_f1_score: 0.8661\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2476 - accuracy: 0.9017 - f1_score: 0.9013 - val_loss: 0.3294 - val_accuracy: 0.8571 - val_f1_score: 0.8548\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2379 - accuracy: 0.9129 - f1_score: 0.9125 - val_loss: 0.3370 - val_accuracy: 0.8662 - val_f1_score: 0.8657\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2319 - accuracy: 0.9099 - f1_score: 0.9097 - val_loss: 0.3269 - val_accuracy: 0.8707 - val_f1_score: 0.8645\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2157 - accuracy: 0.9192 - f1_score: 0.9194 - val_loss: 0.5225 - val_accuracy: 0.8119 - val_f1_score: 0.7744\n","Epoch 17/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2133 - accuracy: 0.9186 - f1_score: 0.9178 - val_loss: 0.3391 - val_accuracy: 0.8599 - val_f1_score: 0.8632\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8582 - f1_score: 0.8860\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6224 - accuracy: 0.6646 - f1_score: 0.6741 - val_loss: 0.5100 - val_accuracy: 0.7722 - val_f1_score: 0.7623\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4977 - accuracy: 0.7601 - f1_score: 0.7455 - val_loss: 0.5185 - val_accuracy: 0.7523 - val_f1_score: 0.6851\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4249 - accuracy: 0.8038 - f1_score: 0.7933 - val_loss: 0.4105 - val_accuracy: 0.8137 - val_f1_score: 0.7964\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3847 - accuracy: 0.8222 - f1_score: 0.8175 - val_loss: 0.3738 - val_accuracy: 0.8418 - val_f1_score: 0.8366\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3598 - accuracy: 0.8415 - f1_score: 0.8363 - val_loss: 0.3950 - val_accuracy: 0.8228 - val_f1_score: 0.8364\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3297 - accuracy: 0.8568 - f1_score: 0.8545 - val_loss: 0.4002 - val_accuracy: 0.8336 - val_f1_score: 0.8145\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3169 - accuracy: 0.8701 - f1_score: 0.8686 - val_loss: 0.3477 - val_accuracy: 0.8580 - val_f1_score: 0.8582\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2875 - accuracy: 0.8819 - f1_score: 0.8805 - val_loss: 0.3491 - val_accuracy: 0.8535 - val_f1_score: 0.8546\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2774 - accuracy: 0.8891 - f1_score: 0.8879 - val_loss: 0.3624 - val_accuracy: 0.8562 - val_f1_score: 0.8467\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2667 - accuracy: 0.8891 - f1_score: 0.8882 - val_loss: 0.3961 - val_accuracy: 0.8327 - val_f1_score: 0.8114\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2582 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3413 - val_accuracy: 0.8644 - val_f1_score: 0.8580\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2446 - accuracy: 0.9048 - f1_score: 0.9042 - val_loss: 0.4685 - val_accuracy: 0.8101 - val_f1_score: 0.7756\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2527 - accuracy: 0.8984 - f1_score: 0.8972 - val_loss: 0.3898 - val_accuracy: 0.8391 - val_f1_score: 0.8486\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2267 - accuracy: 0.9093 - f1_score: 0.9087 - val_loss: 0.3965 - val_accuracy: 0.8400 - val_f1_score: 0.8214\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2222 - accuracy: 0.9093 - f1_score: 0.9089 - val_loss: 0.3572 - val_accuracy: 0.8644 - val_f1_score: 0.8626\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2244 - accuracy: 0.9048 - f1_score: 0.9040 - val_loss: 0.4027 - val_accuracy: 0.8635 - val_f1_score: 0.8638\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8683 - f1_score: 0.8935\n","Epoch 1/20\n","104/104 [==============================] - 8s 24ms/step - loss: 0.6286 - accuracy: 0.6504 - f1_score: 0.6382 - val_loss: 0.5265 - val_accuracy: 0.7568 - val_f1_score: 0.7269\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4914 - accuracy: 0.7634 - f1_score: 0.7524 - val_loss: 0.4342 - val_accuracy: 0.7957 - val_f1_score: 0.7646\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4117 - accuracy: 0.8213 - f1_score: 0.8156 - val_loss: 0.3786 - val_accuracy: 0.8454 - val_f1_score: 0.8530\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3733 - accuracy: 0.8333 - f1_score: 0.8301 - val_loss: 0.3417 - val_accuracy: 0.8544 - val_f1_score: 0.8584\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3379 - accuracy: 0.8577 - f1_score: 0.8556 - val_loss: 0.3445 - val_accuracy: 0.8562 - val_f1_score: 0.8619\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3236 - accuracy: 0.8656 - f1_score: 0.8636 - val_loss: 0.3209 - val_accuracy: 0.8698 - val_f1_score: 0.8647\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3114 - accuracy: 0.8755 - f1_score: 0.8734 - val_loss: 0.3771 - val_accuracy: 0.8418 - val_f1_score: 0.8201\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3036 - accuracy: 0.8752 - f1_score: 0.8737 - val_loss: 0.3089 - val_accuracy: 0.8816 - val_f1_score: 0.8756\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2876 - accuracy: 0.8831 - f1_score: 0.8816 - val_loss: 0.3051 - val_accuracy: 0.8788 - val_f1_score: 0.8750\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2887 - accuracy: 0.8816 - f1_score: 0.8801 - val_loss: 0.3112 - val_accuracy: 0.8734 - val_f1_score: 0.8679\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2780 - accuracy: 0.8867 - f1_score: 0.8850 - val_loss: 0.3086 - val_accuracy: 0.8734 - val_f1_score: 0.8743\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2696 - accuracy: 0.8915 - f1_score: 0.8902 - val_loss: 0.3060 - val_accuracy: 0.8879 - val_f1_score: 0.8864\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2597 - accuracy: 0.8924 - f1_score: 0.8907 - val_loss: 0.3335 - val_accuracy: 0.8752 - val_f1_score: 0.8652\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2557 - accuracy: 0.8969 - f1_score: 0.8956 - val_loss: 0.3625 - val_accuracy: 0.8689 - val_f1_score: 0.8554\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8629 - f1_score: 0.8914\n","Epoch 1/20\n","104/104 [==============================] - 8s 30ms/step - loss: 0.6302 - accuracy: 0.6453 - f1_score: 0.6404 - val_loss: 0.5224 - val_accuracy: 0.7459 - val_f1_score: 0.7366\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4791 - accuracy: 0.7719 - f1_score: 0.7585 - val_loss: 0.5807 - val_accuracy: 0.7269 - val_f1_score: 0.7786\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3947 - accuracy: 0.8255 - f1_score: 0.8200 - val_loss: 0.5042 - val_accuracy: 0.7902 - val_f1_score: 0.8182\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3483 - accuracy: 0.8499 - f1_score: 0.8472 - val_loss: 0.4076 - val_accuracy: 0.8418 - val_f1_score: 0.8508\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3371 - accuracy: 0.8565 - f1_score: 0.8541 - val_loss: 0.3578 - val_accuracy: 0.8608 - val_f1_score: 0.8640\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3098 - accuracy: 0.8737 - f1_score: 0.8728 - val_loss: 0.3536 - val_accuracy: 0.8508 - val_f1_score: 0.8465\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3153 - accuracy: 0.8623 - f1_score: 0.8591 - val_loss: 0.4229 - val_accuracy: 0.8291 - val_f1_score: 0.8444\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2835 - accuracy: 0.8870 - f1_score: 0.8855 - val_loss: 0.3942 - val_accuracy: 0.8463 - val_f1_score: 0.8559\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2698 - accuracy: 0.8933 - f1_score: 0.8917 - val_loss: 0.4294 - val_accuracy: 0.8499 - val_f1_score: 0.8576\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2687 - accuracy: 0.8933 - f1_score: 0.8919 - val_loss: 0.3528 - val_accuracy: 0.8617 - val_f1_score: 0.8610\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2695 - accuracy: 0.8873 - f1_score: 0.8865 - val_loss: 0.3622 - val_accuracy: 0.8608 - val_f1_score: 0.8642\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2432 - accuracy: 0.8987 - f1_score: 0.8977 - val_loss: 0.3865 - val_accuracy: 0.8617 - val_f1_score: 0.8638\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2423 - accuracy: 0.9005 - f1_score: 0.8996 - val_loss: 0.3835 - val_accuracy: 0.8571 - val_f1_score: 0.8518\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2413 - accuracy: 0.8993 - f1_score: 0.8988 - val_loss: 0.3628 - val_accuracy: 0.8626 - val_f1_score: 0.8608\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2304 - accuracy: 0.9060 - f1_score: 0.9046 - val_loss: 0.4139 - val_accuracy: 0.8662 - val_f1_score: 0.8709\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.8670 - f1_score: 0.8913\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6399 - accuracy: 0.6435 - f1_score: 0.6115 - val_loss: 0.5417 - val_accuracy: 0.7324 - val_f1_score: 0.6897\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5174 - accuracy: 0.7559 - f1_score: 0.7478 - val_loss: 0.4787 - val_accuracy: 0.7622 - val_f1_score: 0.7107\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4356 - accuracy: 0.8017 - f1_score: 0.7940 - val_loss: 0.4502 - val_accuracy: 0.7857 - val_f1_score: 0.7393\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3963 - accuracy: 0.8297 - f1_score: 0.8258 - val_loss: 0.3670 - val_accuracy: 0.8436 - val_f1_score: 0.8528\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3471 - accuracy: 0.8559 - f1_score: 0.8536 - val_loss: 0.3648 - val_accuracy: 0.8427 - val_f1_score: 0.8548\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3401 - accuracy: 0.8520 - f1_score: 0.8496 - val_loss: 0.3562 - val_accuracy: 0.8472 - val_f1_score: 0.8583\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3082 - accuracy: 0.8704 - f1_score: 0.8691 - val_loss: 0.3211 - val_accuracy: 0.8752 - val_f1_score: 0.8748\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2939 - accuracy: 0.8861 - f1_score: 0.8849 - val_loss: 0.3689 - val_accuracy: 0.8445 - val_f1_score: 0.8555\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2833 - accuracy: 0.8867 - f1_score: 0.8853 - val_loss: 0.3244 - val_accuracy: 0.8825 - val_f1_score: 0.8814\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3050 - accuracy: 0.8737 - f1_score: 0.8731 - val_loss: 0.3292 - val_accuracy: 0.8716 - val_f1_score: 0.8765\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2692 - accuracy: 0.8918 - f1_score: 0.8908 - val_loss: 0.3343 - val_accuracy: 0.8635 - val_f1_score: 0.8695\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2616 - accuracy: 0.8957 - f1_score: 0.8946 - val_loss: 0.4813 - val_accuracy: 0.7839 - val_f1_score: 0.8160\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8623 - f1_score: 0.8867\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6360 - accuracy: 0.6459 - f1_score: 0.6141 - val_loss: 0.5539 - val_accuracy: 0.7278 - val_f1_score: 0.7416\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.5056 - accuracy: 0.7532 - f1_score: 0.7459 - val_loss: 0.4522 - val_accuracy: 0.7902 - val_f1_score: 0.7925\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4259 - accuracy: 0.8092 - f1_score: 0.8041 - val_loss: 0.4468 - val_accuracy: 0.7866 - val_f1_score: 0.8094\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3768 - accuracy: 0.8391 - f1_score: 0.8382 - val_loss: 0.4647 - val_accuracy: 0.7866 - val_f1_score: 0.8159\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3497 - accuracy: 0.8526 - f1_score: 0.8530 - val_loss: 0.3428 - val_accuracy: 0.8608 - val_f1_score: 0.8574\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3203 - accuracy: 0.8680 - f1_score: 0.8670 - val_loss: 0.3504 - val_accuracy: 0.8562 - val_f1_score: 0.8638\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3097 - accuracy: 0.8680 - f1_score: 0.8669 - val_loss: 0.3901 - val_accuracy: 0.8246 - val_f1_score: 0.8420\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2974 - accuracy: 0.8831 - f1_score: 0.8827 - val_loss: 0.3539 - val_accuracy: 0.8562 - val_f1_score: 0.8443\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2977 - accuracy: 0.8816 - f1_score: 0.8812 - val_loss: 0.3247 - val_accuracy: 0.8644 - val_f1_score: 0.8585\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2858 - accuracy: 0.8855 - f1_score: 0.8846 - val_loss: 0.3157 - val_accuracy: 0.8644 - val_f1_score: 0.8675\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2718 - accuracy: 0.8906 - f1_score: 0.8900 - val_loss: 0.3654 - val_accuracy: 0.8445 - val_f1_score: 0.8569\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2770 - accuracy: 0.8900 - f1_score: 0.8903 - val_loss: 0.3299 - val_accuracy: 0.8671 - val_f1_score: 0.8601\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2466 - accuracy: 0.9093 - f1_score: 0.9092 - val_loss: 0.3174 - val_accuracy: 0.8725 - val_f1_score: 0.8738\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2478 - accuracy: 0.8999 - f1_score: 0.8995 - val_loss: 0.3233 - val_accuracy: 0.8644 - val_f1_score: 0.8709\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2321 - accuracy: 0.9108 - f1_score: 0.9106 - val_loss: 0.3429 - val_accuracy: 0.8698 - val_f1_score: 0.8703\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3302 - accuracy: 0.8629 - f1_score: 0.8901\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6197 - accuracy: 0.6558 - f1_score: 0.6676 - val_loss: 0.5139 - val_accuracy: 0.7676 - val_f1_score: 0.7569\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4936 - accuracy: 0.7685 - f1_score: 0.7540 - val_loss: 0.4841 - val_accuracy: 0.7749 - val_f1_score: 0.7947\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4331 - accuracy: 0.8035 - f1_score: 0.7945 - val_loss: 0.4240 - val_accuracy: 0.8101 - val_f1_score: 0.8247\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3759 - accuracy: 0.8324 - f1_score: 0.8289 - val_loss: 0.3801 - val_accuracy: 0.8318 - val_f1_score: 0.8388\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3392 - accuracy: 0.8577 - f1_score: 0.8541 - val_loss: 0.3772 - val_accuracy: 0.8400 - val_f1_score: 0.8488\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3377 - accuracy: 0.8517 - f1_score: 0.8503 - val_loss: 0.3439 - val_accuracy: 0.8517 - val_f1_score: 0.8495\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3042 - accuracy: 0.8704 - f1_score: 0.8685 - val_loss: 0.3384 - val_accuracy: 0.8644 - val_f1_score: 0.8670\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2879 - accuracy: 0.8785 - f1_score: 0.8766 - val_loss: 0.3456 - val_accuracy: 0.8608 - val_f1_score: 0.8666\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2741 - accuracy: 0.8855 - f1_score: 0.8850 - val_loss: 0.3344 - val_accuracy: 0.8599 - val_f1_score: 0.8644\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2662 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3364 - val_accuracy: 0.8662 - val_f1_score: 0.8601\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2652 - accuracy: 0.8888 - f1_score: 0.8880 - val_loss: 0.4430 - val_accuracy: 0.8029 - val_f1_score: 0.8286\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2557 - accuracy: 0.8987 - f1_score: 0.8976 - val_loss: 0.3285 - val_accuracy: 0.8698 - val_f1_score: 0.8703\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2370 - accuracy: 0.9057 - f1_score: 0.9052 - val_loss: 0.3296 - val_accuracy: 0.8698 - val_f1_score: 0.8698\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2350 - accuracy: 0.9072 - f1_score: 0.9061 - val_loss: 0.3464 - val_accuracy: 0.8580 - val_f1_score: 0.8636\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2335 - accuracy: 0.9060 - f1_score: 0.9056 - val_loss: 0.3448 - val_accuracy: 0.8662 - val_f1_score: 0.8647\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2246 - accuracy: 0.9153 - f1_score: 0.9143 - val_loss: 0.4511 - val_accuracy: 0.8056 - val_f1_score: 0.8298\n","Epoch 17/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2068 - accuracy: 0.9189 - f1_score: 0.9184 - val_loss: 0.4033 - val_accuracy: 0.8571 - val_f1_score: 0.8626\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3565 - accuracy: 0.8602 - f1_score: 0.8840\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6167 - accuracy: 0.6582 - f1_score: 0.6166 - val_loss: 0.5175 - val_accuracy: 0.7586 - val_f1_score: 0.7502\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4506 - accuracy: 0.7863 - f1_score: 0.7811 - val_loss: 0.4097 - val_accuracy: 0.8137 - val_f1_score: 0.8187\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3933 - accuracy: 0.8231 - f1_score: 0.8208 - val_loss: 0.3637 - val_accuracy: 0.8354 - val_f1_score: 0.8280\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3555 - accuracy: 0.8532 - f1_score: 0.8521 - val_loss: 0.3443 - val_accuracy: 0.8562 - val_f1_score: 0.8487\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3391 - accuracy: 0.8562 - f1_score: 0.8545 - val_loss: 0.3243 - val_accuracy: 0.8671 - val_f1_score: 0.8630\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3171 - accuracy: 0.8674 - f1_score: 0.8664 - val_loss: 0.3322 - val_accuracy: 0.8644 - val_f1_score: 0.8580\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3005 - accuracy: 0.8782 - f1_score: 0.8769 - val_loss: 0.3127 - val_accuracy: 0.8671 - val_f1_score: 0.8682\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2844 - accuracy: 0.8807 - f1_score: 0.8794 - val_loss: 0.3266 - val_accuracy: 0.8716 - val_f1_score: 0.8650\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2783 - accuracy: 0.8870 - f1_score: 0.8864 - val_loss: 0.3286 - val_accuracy: 0.8635 - val_f1_score: 0.8677\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2614 - accuracy: 0.8945 - f1_score: 0.8937 - val_loss: 0.3261 - val_accuracy: 0.8707 - val_f1_score: 0.8727\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2508 - accuracy: 0.8996 - f1_score: 0.8991 - val_loss: 0.3345 - val_accuracy: 0.8725 - val_f1_score: 0.8671\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2384 - accuracy: 0.9045 - f1_score: 0.9033 - val_loss: 0.3886 - val_accuracy: 0.8445 - val_f1_score: 0.8557\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3430 - accuracy: 0.8521 - f1_score: 0.8780\n","Epoch 1/20\n","104/104 [==============================] - 9s 20ms/step - loss: 0.6215 - accuracy: 0.6513 - f1_score: 0.6437 - val_loss: 0.5600 - val_accuracy: 0.7179 - val_f1_score: 0.6355\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4785 - accuracy: 0.7779 - f1_score: 0.7687 - val_loss: 0.4049 - val_accuracy: 0.8336 - val_f1_score: 0.8345\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4050 - accuracy: 0.8156 - f1_score: 0.8110 - val_loss: 0.3690 - val_accuracy: 0.8382 - val_f1_score: 0.8453\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3734 - accuracy: 0.8373 - f1_score: 0.8359 - val_loss: 0.3509 - val_accuracy: 0.8562 - val_f1_score: 0.8440\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3367 - accuracy: 0.8586 - f1_score: 0.8577 - val_loss: 0.3208 - val_accuracy: 0.8571 - val_f1_score: 0.8602\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3291 - accuracy: 0.8674 - f1_score: 0.8664 - val_loss: 0.3119 - val_accuracy: 0.8752 - val_f1_score: 0.8715\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3107 - accuracy: 0.8788 - f1_score: 0.8769 - val_loss: 0.3041 - val_accuracy: 0.8788 - val_f1_score: 0.8748\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3050 - accuracy: 0.8731 - f1_score: 0.8722 - val_loss: 0.3336 - val_accuracy: 0.8626 - val_f1_score: 0.8507\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2907 - accuracy: 0.8864 - f1_score: 0.8850 - val_loss: 0.2984 - val_accuracy: 0.8807 - val_f1_score: 0.8773\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2731 - accuracy: 0.8939 - f1_score: 0.8932 - val_loss: 0.3073 - val_accuracy: 0.8761 - val_f1_score: 0.8709\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2689 - accuracy: 0.8924 - f1_score: 0.8915 - val_loss: 0.3020 - val_accuracy: 0.8797 - val_f1_score: 0.8772\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2563 - accuracy: 0.8978 - f1_score: 0.8969 - val_loss: 0.3371 - val_accuracy: 0.8635 - val_f1_score: 0.8535\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2623 - accuracy: 0.8963 - f1_score: 0.8956 - val_loss: 0.3676 - val_accuracy: 0.8309 - val_f1_score: 0.8078\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2395 - accuracy: 0.9054 - f1_score: 0.9045 - val_loss: 0.3280 - val_accuracy: 0.8770 - val_f1_score: 0.8715\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3392 - accuracy: 0.8623 - f1_score: 0.8859\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.6251 - accuracy: 0.6459 - f1_score: 0.6375 - val_loss: 0.5260 - val_accuracy: 0.7333 - val_f1_score: 0.7133\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4693 - accuracy: 0.7866 - f1_score: 0.7776 - val_loss: 0.4419 - val_accuracy: 0.7821 - val_f1_score: 0.7588\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4054 - accuracy: 0.8204 - f1_score: 0.8129 - val_loss: 0.3898 - val_accuracy: 0.8291 - val_f1_score: 0.8225\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3493 - accuracy: 0.8520 - f1_score: 0.8499 - val_loss: 0.3735 - val_accuracy: 0.8418 - val_f1_score: 0.8366\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3228 - accuracy: 0.8656 - f1_score: 0.8635 - val_loss: 0.3707 - val_accuracy: 0.8490 - val_f1_score: 0.8371\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3099 - accuracy: 0.8813 - f1_score: 0.8805 - val_loss: 0.3769 - val_accuracy: 0.8463 - val_f1_score: 0.8359\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2906 - accuracy: 0.8843 - f1_score: 0.8830 - val_loss: 0.3893 - val_accuracy: 0.8481 - val_f1_score: 0.8353\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2877 - accuracy: 0.8822 - f1_score: 0.8811 - val_loss: 0.3567 - val_accuracy: 0.8590 - val_f1_score: 0.8636\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3093 - accuracy: 0.8740 - f1_score: 0.8727 - val_loss: 0.3474 - val_accuracy: 0.8662 - val_f1_score: 0.8619\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2874 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.3559 - val_accuracy: 0.8599 - val_f1_score: 0.8563\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2616 - accuracy: 0.8990 - f1_score: 0.8981 - val_loss: 0.3777 - val_accuracy: 0.8517 - val_f1_score: 0.8450\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2478 - accuracy: 0.9017 - f1_score: 0.9006 - val_loss: 0.3601 - val_accuracy: 0.8671 - val_f1_score: 0.8700\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2506 - accuracy: 0.9011 - f1_score: 0.9005 - val_loss: 0.3634 - val_accuracy: 0.8680 - val_f1_score: 0.8617\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2477 - accuracy: 0.9020 - f1_score: 0.9008 - val_loss: 0.3653 - val_accuracy: 0.8580 - val_f1_score: 0.8626\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3422 - accuracy: 0.8555 - f1_score: 0.8800\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6214 - accuracy: 0.6570 - f1_score: 0.6627 - val_loss: 0.5274 - val_accuracy: 0.7351 - val_f1_score: 0.6636\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4845 - accuracy: 0.7758 - f1_score: 0.7674 - val_loss: 0.4249 - val_accuracy: 0.7948 - val_f1_score: 0.7603\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4310 - accuracy: 0.8020 - f1_score: 0.7935 - val_loss: 0.3547 - val_accuracy: 0.8508 - val_f1_score: 0.8571\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3879 - accuracy: 0.8189 - f1_score: 0.8154 - val_loss: 0.3552 - val_accuracy: 0.8508 - val_f1_score: 0.8617\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3550 - accuracy: 0.8484 - f1_score: 0.8468 - val_loss: 0.4299 - val_accuracy: 0.8047 - val_f1_score: 0.8320\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3469 - accuracy: 0.8508 - f1_score: 0.8488 - val_loss: 0.3122 - val_accuracy: 0.8689 - val_f1_score: 0.8580\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3205 - accuracy: 0.8701 - f1_score: 0.8680 - val_loss: 0.2871 - val_accuracy: 0.8888 - val_f1_score: 0.8891\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3114 - accuracy: 0.8707 - f1_score: 0.8695 - val_loss: 0.2871 - val_accuracy: 0.8888 - val_f1_score: 0.8901\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2988 - accuracy: 0.8722 - f1_score: 0.8703 - val_loss: 0.4053 - val_accuracy: 0.8228 - val_f1_score: 0.8457\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2962 - accuracy: 0.8713 - f1_score: 0.8703 - val_loss: 0.2822 - val_accuracy: 0.8861 - val_f1_score: 0.8885\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2810 - accuracy: 0.8870 - f1_score: 0.8855 - val_loss: 0.2768 - val_accuracy: 0.9005 - val_f1_score: 0.8983\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2761 - accuracy: 0.8888 - f1_score: 0.8873 - val_loss: 0.2822 - val_accuracy: 0.8861 - val_f1_score: 0.8899\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2786 - accuracy: 0.8864 - f1_score: 0.8861 - val_loss: 0.2787 - val_accuracy: 0.8942 - val_f1_score: 0.8914\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2671 - accuracy: 0.8921 - f1_score: 0.8907 - val_loss: 0.2840 - val_accuracy: 0.8924 - val_f1_score: 0.8903\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2594 - accuracy: 0.8939 - f1_score: 0.8931 - val_loss: 0.2894 - val_accuracy: 0.8834 - val_f1_score: 0.8877\n","Epoch 16/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2398 - accuracy: 0.9072 - f1_score: 0.9066 - val_loss: 0.2917 - val_accuracy: 0.8915 - val_f1_score: 0.8944\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3460 - accuracy: 0.8602 - f1_score: 0.8829\n","Epoch 1/20\n","104/104 [==============================] - 9s 18ms/step - loss: 0.6517 - accuracy: 0.6136 - f1_score: 0.6383 - val_loss: 0.5518 - val_accuracy: 0.7378 - val_f1_score: 0.7469\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5222 - accuracy: 0.7511 - f1_score: 0.7401 - val_loss: 0.4680 - val_accuracy: 0.7821 - val_f1_score: 0.7476\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4253 - accuracy: 0.8074 - f1_score: 0.8010 - val_loss: 0.4217 - val_accuracy: 0.8011 - val_f1_score: 0.8182\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3844 - accuracy: 0.8333 - f1_score: 0.8312 - val_loss: 0.3663 - val_accuracy: 0.8418 - val_f1_score: 0.8422\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3452 - accuracy: 0.8559 - f1_score: 0.8552 - val_loss: 0.3647 - val_accuracy: 0.8508 - val_f1_score: 0.8559\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3285 - accuracy: 0.8692 - f1_score: 0.8678 - val_loss: 0.3601 - val_accuracy: 0.8544 - val_f1_score: 0.8594\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3082 - accuracy: 0.8776 - f1_score: 0.8768 - val_loss: 0.3433 - val_accuracy: 0.8553 - val_f1_score: 0.8476\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3087 - accuracy: 0.8785 - f1_score: 0.8770 - val_loss: 0.3703 - val_accuracy: 0.8418 - val_f1_score: 0.8513\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2823 - accuracy: 0.8861 - f1_score: 0.8844 - val_loss: 0.3335 - val_accuracy: 0.8680 - val_f1_score: 0.8670\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2889 - accuracy: 0.8873 - f1_score: 0.8861 - val_loss: 0.3455 - val_accuracy: 0.8599 - val_f1_score: 0.8511\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2645 - accuracy: 0.8993 - f1_score: 0.8984 - val_loss: 0.3460 - val_accuracy: 0.8635 - val_f1_score: 0.8674\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2763 - accuracy: 0.8897 - f1_score: 0.8889 - val_loss: 0.3623 - val_accuracy: 0.8553 - val_f1_score: 0.8437\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2688 - accuracy: 0.8963 - f1_score: 0.8949 - val_loss: 0.3605 - val_accuracy: 0.8526 - val_f1_score: 0.8391\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2412 - accuracy: 0.9087 - f1_score: 0.9076 - val_loss: 0.3439 - val_accuracy: 0.8734 - val_f1_score: 0.8752\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3325 - accuracy: 0.8650 - f1_score: 0.8881\n","Epoch 1/20\n","104/104 [==============================] - 8s 21ms/step - loss: 0.6117 - accuracy: 0.6664 - f1_score: 0.6520 - val_loss: 0.5014 - val_accuracy: 0.7685 - val_f1_score: 0.7739\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4484 - accuracy: 0.7939 - f1_score: 0.7872 - val_loss: 0.4759 - val_accuracy: 0.7640 - val_f1_score: 0.7963\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3754 - accuracy: 0.8354 - f1_score: 0.8330 - val_loss: 0.3805 - val_accuracy: 0.8400 - val_f1_score: 0.8329\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3435 - accuracy: 0.8553 - f1_score: 0.8529 - val_loss: 0.4494 - val_accuracy: 0.8101 - val_f1_score: 0.8295\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3124 - accuracy: 0.8728 - f1_score: 0.8724 - val_loss: 0.3746 - val_accuracy: 0.8562 - val_f1_score: 0.8566\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3037 - accuracy: 0.8761 - f1_score: 0.8752 - val_loss: 0.3556 - val_accuracy: 0.8571 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3051 - accuracy: 0.8794 - f1_score: 0.8780 - val_loss: 0.3484 - val_accuracy: 0.8644 - val_f1_score: 0.8631\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2822 - accuracy: 0.8882 - f1_score: 0.8871 - val_loss: 0.3790 - val_accuracy: 0.8436 - val_f1_score: 0.8366\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2641 - accuracy: 0.8987 - f1_score: 0.8984 - val_loss: 0.3794 - val_accuracy: 0.8436 - val_f1_score: 0.8341\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2674 - accuracy: 0.8972 - f1_score: 0.8965 - val_loss: 0.3939 - val_accuracy: 0.8526 - val_f1_score: 0.8569\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2453 - accuracy: 0.9099 - f1_score: 0.9096 - val_loss: 0.3812 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2424 - accuracy: 0.9063 - f1_score: 0.9056 - val_loss: 0.3777 - val_accuracy: 0.8490 - val_f1_score: 0.8510\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3438 - accuracy: 0.8596 - f1_score: 0.8806\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6249 - accuracy: 0.6564 - f1_score: 0.6803 - val_loss: 0.5140 - val_accuracy: 0.7640 - val_f1_score: 0.7590\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4825 - accuracy: 0.7715 - f1_score: 0.7564 - val_loss: 0.4254 - val_accuracy: 0.8128 - val_f1_score: 0.8127\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4136 - accuracy: 0.8125 - f1_score: 0.8054 - val_loss: 0.3966 - val_accuracy: 0.8363 - val_f1_score: 0.8436\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3683 - accuracy: 0.8466 - f1_score: 0.8437 - val_loss: 0.3671 - val_accuracy: 0.8418 - val_f1_score: 0.8309\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3688 - accuracy: 0.8487 - f1_score: 0.8455 - val_loss: 0.3438 - val_accuracy: 0.8608 - val_f1_score: 0.8635\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3389 - accuracy: 0.8571 - f1_score: 0.8551 - val_loss: 0.3347 - val_accuracy: 0.8707 - val_f1_score: 0.8729\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3158 - accuracy: 0.8737 - f1_score: 0.8726 - val_loss: 0.3409 - val_accuracy: 0.8626 - val_f1_score: 0.8536\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3031 - accuracy: 0.8813 - f1_score: 0.8798 - val_loss: 0.3311 - val_accuracy: 0.8716 - val_f1_score: 0.8743\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2955 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.3502 - val_accuracy: 0.8562 - val_f1_score: 0.8421\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2876 - accuracy: 0.8864 - f1_score: 0.8852 - val_loss: 0.3266 - val_accuracy: 0.8662 - val_f1_score: 0.8664\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2746 - accuracy: 0.8975 - f1_score: 0.8963 - val_loss: 0.3265 - val_accuracy: 0.8734 - val_f1_score: 0.8761\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2764 - accuracy: 0.8870 - f1_score: 0.8855 - val_loss: 0.3579 - val_accuracy: 0.8571 - val_f1_score: 0.8643\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2627 - accuracy: 0.8966 - f1_score: 0.8958 - val_loss: 0.3457 - val_accuracy: 0.8698 - val_f1_score: 0.8739\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2572 - accuracy: 0.8957 - f1_score: 0.8950 - val_loss: 0.3255 - val_accuracy: 0.8698 - val_f1_score: 0.8639\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2614 - accuracy: 0.8990 - f1_score: 0.8983 - val_loss: 0.3554 - val_accuracy: 0.8680 - val_f1_score: 0.8717\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2355 - accuracy: 0.9135 - f1_score: 0.9133 - val_loss: 0.3631 - val_accuracy: 0.8608 - val_f1_score: 0.8677\n","Epoch 17/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2307 - accuracy: 0.9093 - f1_score: 0.9083 - val_loss: 0.3704 - val_accuracy: 0.8590 - val_f1_score: 0.8657\n","Epoch 18/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2115 - accuracy: 0.9195 - f1_score: 0.9188 - val_loss: 0.3445 - val_accuracy: 0.8770 - val_f1_score: 0.8741\n","Epoch 19/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2101 - accuracy: 0.9216 - f1_score: 0.9211 - val_loss: 0.3498 - val_accuracy: 0.8680 - val_f1_score: 0.8670\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3789 - accuracy: 0.8460 - f1_score: 0.8662\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6215 - accuracy: 0.6643 - f1_score: 0.6639 - val_loss: 0.5465 - val_accuracy: 0.7179 - val_f1_score: 0.6609\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4820 - accuracy: 0.7782 - f1_score: 0.7706 - val_loss: 0.4381 - val_accuracy: 0.7993 - val_f1_score: 0.7840\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4218 - accuracy: 0.8068 - f1_score: 0.7998 - val_loss: 0.3838 - val_accuracy: 0.8427 - val_f1_score: 0.8383\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3739 - accuracy: 0.8376 - f1_score: 0.8341 - val_loss: 0.3982 - val_accuracy: 0.8237 - val_f1_score: 0.8365\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3498 - accuracy: 0.8520 - f1_score: 0.8491 - val_loss: 0.3500 - val_accuracy: 0.8590 - val_f1_score: 0.8488\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3217 - accuracy: 0.8680 - f1_score: 0.8666 - val_loss: 0.3332 - val_accuracy: 0.8743 - val_f1_score: 0.8685\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3099 - accuracy: 0.8740 - f1_score: 0.8731 - val_loss: 0.3518 - val_accuracy: 0.8590 - val_f1_score: 0.8488\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3019 - accuracy: 0.8810 - f1_score: 0.8797 - val_loss: 0.3344 - val_accuracy: 0.8698 - val_f1_score: 0.8712\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2951 - accuracy: 0.8797 - f1_score: 0.8788 - val_loss: 0.3233 - val_accuracy: 0.8734 - val_f1_score: 0.8723\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2754 - accuracy: 0.8888 - f1_score: 0.8874 - val_loss: 0.3597 - val_accuracy: 0.8544 - val_f1_score: 0.8644\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2796 - accuracy: 0.8852 - f1_score: 0.8852 - val_loss: 0.3254 - val_accuracy: 0.8725 - val_f1_score: 0.8661\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2605 - accuracy: 0.8960 - f1_score: 0.8954 - val_loss: 0.3798 - val_accuracy: 0.8454 - val_f1_score: 0.8278\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2680 - accuracy: 0.8927 - f1_score: 0.8916 - val_loss: 0.4667 - val_accuracy: 0.8083 - val_f1_score: 0.7730\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2369 - accuracy: 0.9054 - f1_score: 0.9048 - val_loss: 0.3162 - val_accuracy: 0.8770 - val_f1_score: 0.8738\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2287 - accuracy: 0.9108 - f1_score: 0.9104 - val_loss: 0.3833 - val_accuracy: 0.8599 - val_f1_score: 0.8485\n","Epoch 16/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2317 - accuracy: 0.9063 - f1_score: 0.9053 - val_loss: 0.3289 - val_accuracy: 0.8662 - val_f1_score: 0.8642\n","Epoch 17/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2202 - accuracy: 0.9117 - f1_score: 0.9111 - val_loss: 0.3376 - val_accuracy: 0.8725 - val_f1_score: 0.8738\n","Epoch 18/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2145 - accuracy: 0.9126 - f1_score: 0.9117 - val_loss: 0.3284 - val_accuracy: 0.8779 - val_f1_score: 0.8767\n","Epoch 19/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2043 - accuracy: 0.9277 - f1_score: 0.9271 - val_loss: 0.3551 - val_accuracy: 0.8707 - val_f1_score: 0.8642\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.8818 - f1_score: 0.9024\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6102 - accuracy: 0.6754 - f1_score: 0.6697 - val_loss: 0.5015 - val_accuracy: 0.7423 - val_f1_score: 0.6919\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4552 - accuracy: 0.7911 - f1_score: 0.7844 - val_loss: 0.3566 - val_accuracy: 0.8553 - val_f1_score: 0.8537\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3866 - accuracy: 0.8288 - f1_score: 0.8274 - val_loss: 0.3962 - val_accuracy: 0.8183 - val_f1_score: 0.7878\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3437 - accuracy: 0.8602 - f1_score: 0.8576 - val_loss: 0.3154 - val_accuracy: 0.8743 - val_f1_score: 0.8731\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3216 - accuracy: 0.8731 - f1_score: 0.8715 - val_loss: 0.3217 - val_accuracy: 0.8680 - val_f1_score: 0.8719\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3184 - accuracy: 0.8692 - f1_score: 0.8684 - val_loss: 0.3504 - val_accuracy: 0.8499 - val_f1_score: 0.8333\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3150 - accuracy: 0.8731 - f1_score: 0.8716 - val_loss: 0.3146 - val_accuracy: 0.8716 - val_f1_score: 0.8714\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2890 - accuracy: 0.8876 - f1_score: 0.8860 - val_loss: 0.3188 - val_accuracy: 0.8689 - val_f1_score: 0.8700\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2928 - accuracy: 0.8843 - f1_score: 0.8824 - val_loss: 0.3118 - val_accuracy: 0.8734 - val_f1_score: 0.8682\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2669 - accuracy: 0.8981 - f1_score: 0.8966 - val_loss: 0.3113 - val_accuracy: 0.8698 - val_f1_score: 0.8700\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2656 - accuracy: 0.8975 - f1_score: 0.8961 - val_loss: 0.4123 - val_accuracy: 0.8101 - val_f1_score: 0.7742\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2633 - accuracy: 0.8906 - f1_score: 0.8887 - val_loss: 0.3550 - val_accuracy: 0.8580 - val_f1_score: 0.8655\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2535 - accuracy: 0.9005 - f1_score: 0.8993 - val_loss: 0.3169 - val_accuracy: 0.8662 - val_f1_score: 0.8601\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2404 - accuracy: 0.9078 - f1_score: 0.9067 - val_loss: 0.3937 - val_accuracy: 0.8336 - val_f1_score: 0.8083\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2392 - accuracy: 0.9024 - f1_score: 0.9015 - val_loss: 0.3733 - val_accuracy: 0.8662 - val_f1_score: 0.8699\n","47/47 [==============================] - 0s 6ms/step - loss: 0.2929 - accuracy: 0.8805 - f1_score: 0.9032\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXNFlC6V3hvH","executionInfo":{"status":"ok","timestamp":1689802423802,"user_tz":-330,"elapsed":31,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"30b66cc7-5a92-4521-dc40-564259963322"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8413234353065491, 0.8561782836914062, 0.8582038879394531, 0.8609048128128052, 0.8595543503761292, 0.8582038879394531, 0.8575286865234375, 0.852126955986023, 0.8534773588180542, 0.8582038879394531, 0.8683322072029114, 0.8629304766654968, 0.8669817447662354, 0.8622552156448364, 0.8629304766654968, 0.8602295517921448, 0.852126955986023, 0.8622552156448364, 0.8555030226707458, 0.8602295517921448, 0.8649561405181885, 0.8595543503761292, 0.846049964427948, 0.8818365931510925, 0.8804861307144165]\n","0.8600945258140564\n","0.008551592186290702\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7OrSOPoU3hxg","executionInfo":{"status":"ok","timestamp":1689802423802,"user_tz":-330,"elapsed":15,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"e5250bce-12ce-463c-f5f0-a935ffc95351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.35806578397750854, 0.34878355264663696, 0.3290470838546753, 0.3308935761451721, 0.3272150754928589, 0.3385242521762848, 0.34944960474967957, 0.34622544050216675, 0.35514214634895325, 0.3312378227710724, 0.3242238461971283, 0.31488925218582153, 0.32482218742370605, 0.3326953053474426, 0.33018776774406433, 0.3564598560333252, 0.3429882824420929, 0.3391534388065338, 0.342156320810318, 0.34601229429244995, 0.33247774839401245, 0.3438188135623932, 0.37892282009124756, 0.3178999722003937, 0.2929273247718811]\n","0.3373687827587128\n","0.01661841280038253\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1-tl3A23hze","executionInfo":{"status":"ok","timestamp":1689802423802,"user_tz":-330,"elapsed":13,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"a5b4b7e0-55e9-4e43-a4f8-0e2ca471e085"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8636099696159363, 0.8777968287467957, 0.8808171153068542, 0.8833521008491516, 0.882219672203064, 0.8821548223495483, 0.8824512362480164, 0.878670334815979, 0.8782949447631836, 0.8859934210777283, 0.8935007452964783, 0.8913857340812683, 0.8913401961326599, 0.8866665363311768, 0.8900919556617737, 0.8840335011482239, 0.8779943585395813, 0.885905921459198, 0.8800447583198547, 0.8828521966934204, 0.8881431221961975, 0.8805969953536987, 0.8661971092224121, 0.9023981690406799, 0.9032257199287415]\n","0.8839894986152649\n","0.008699578502942782\n"]}]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing  -\n","1. Accuracy - 0.859/0.008\n","2. Loss - 0.338/0.018\n","3. F1 score - 0.883/0.008\n","\n","LSTM CNN LSTM nothing  -\n","1. Accuracy - 0.860/0.008\n","2. Loss - 0.337/0.016\n","3. F1 score - 0.883/0.008"],"metadata":{"id":"cLmJqPMPOH56"}},{"cell_type":"code","source":[],"metadata":{"id":"3fTB4Gno3lFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zIbPl_2aPXwZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m5Xto6ERPXzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sTfeax7SPX1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g2oVtIl9PX4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"B4FCUXALPX7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uYnnhAS2PX9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i93K5eo1PYAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EuO1PSJ6PYCY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Fast things to try next for both architectures that we are considering now:\n","\n"," - add dropout layer between two dense layer\n"," - also try with Dense layer 256 instead of immediately going to dense 128"],"metadata":{"id":"Faf6CbDFPYL8"}},{"cell_type":"code","source":["# to register f1 score separately in Keras (working)\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.metrics import Metric\n","\n","class F1Score(Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","        false_positives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * y_pred, 0, 1)))\n","        false_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * (1 - y_pred), 0, 1)))\n","\n","        self.true_positives.assign_add(true_positives)\n","        self.false_positives.assign_add(false_positives)\n","        self.false_negatives.assign_add(false_negatives)\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n","        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","        return f1"],"metadata":{"id":"5ehikk4IWrIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"4vjrw3TqQBly","executionInfo":{"status":"ok","timestamp":1689807028883,"user_tz":-330,"elapsed":282,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"fe51c727-b53c-4bbe-8f1d-ca2503c433be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-163ae6ec-714d-4840-82d3-cffbda6603b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-163ae6ec-714d-4840-82d3-cffbda6603b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-4db33bf5-164b-47cd-b99d-c664f2c104ee\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4db33bf5-164b-47cd-b99d-c664f2c104ee')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-4db33bf5-164b-47cd-b99d-c664f2c104ee button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-163ae6ec-714d-4840-82d3-cffbda6603b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-163ae6ec-714d-4840-82d3-cffbda6603b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"fKRsnsB4RFB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NuXSkO5JRmqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM CNN nothing nothing"],"metadata":{"id":"fNDGAjDERmw5"}},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(256, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRKWosynPYRY","executionInfo":{"status":"ok","timestamp":1689806293778,"user_tz":-330,"elapsed":600477,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"47e3a79c-c6d9-4d43-9011-16b5230daf21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 7s 15ms/step - loss: 0.6475 - accuracy: 0.6239 - f1_score: 0.6325 - val_loss: 0.5814 - val_accuracy: 0.6917 - val_f1_score: 0.7487\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5348 - accuracy: 0.7357 - f1_score: 0.7194 - val_loss: 0.4164 - val_accuracy: 0.8183 - val_f1_score: 0.8106\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4562 - accuracy: 0.7929 - f1_score: 0.7797 - val_loss: 0.3704 - val_accuracy: 0.8354 - val_f1_score: 0.8360\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4038 - accuracy: 0.8168 - f1_score: 0.8092 - val_loss: 0.3329 - val_accuracy: 0.8553 - val_f1_score: 0.8516\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3780 - accuracy: 0.8357 - f1_score: 0.8298 - val_loss: 0.3573 - val_accuracy: 0.8363 - val_f1_score: 0.8192\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3484 - accuracy: 0.8532 - f1_score: 0.8491 - val_loss: 0.3158 - val_accuracy: 0.8671 - val_f1_score: 0.8700\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3316 - accuracy: 0.8629 - f1_score: 0.8594 - val_loss: 0.3078 - val_accuracy: 0.8761 - val_f1_score: 0.8735\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3180 - accuracy: 0.8692 - f1_score: 0.8665 - val_loss: 0.3056 - val_accuracy: 0.8644 - val_f1_score: 0.8571\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2908 - accuracy: 0.8837 - f1_score: 0.8808 - val_loss: 0.3109 - val_accuracy: 0.8689 - val_f1_score: 0.8659\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3175 - accuracy: 0.8713 - f1_score: 0.8683 - val_loss: 0.3218 - val_accuracy: 0.8617 - val_f1_score: 0.8579\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2916 - accuracy: 0.8855 - f1_score: 0.8827 - val_loss: 0.3153 - val_accuracy: 0.8626 - val_f1_score: 0.8538\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2620 - accuracy: 0.8984 - f1_score: 0.8961 - val_loss: 0.3564 - val_accuracy: 0.8309 - val_f1_score: 0.8098\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2640 - accuracy: 0.8906 - f1_score: 0.8882 - val_loss: 0.3124 - val_accuracy: 0.8770 - val_f1_score: 0.8764\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3613 - accuracy: 0.8488 - f1_score: 0.8684\n","Epoch 1/20\n","104/104 [==============================] - 9s 23ms/step - loss: 0.6210 - accuracy: 0.6564 - f1_score: 0.6637 - val_loss: 0.5548 - val_accuracy: 0.7315 - val_f1_score: 0.7322\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4854 - accuracy: 0.7758 - f1_score: 0.7615 - val_loss: 0.4765 - val_accuracy: 0.7731 - val_f1_score: 0.7298\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4299 - accuracy: 0.8032 - f1_score: 0.7938 - val_loss: 0.5139 - val_accuracy: 0.7450 - val_f1_score: 0.7851\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3887 - accuracy: 0.8249 - f1_score: 0.8201 - val_loss: 0.4518 - val_accuracy: 0.7857 - val_f1_score: 0.8132\n","Epoch 5/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3447 - accuracy: 0.8502 - f1_score: 0.8481 - val_loss: 0.4126 - val_accuracy: 0.8146 - val_f1_score: 0.8313\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3193 - accuracy: 0.8683 - f1_score: 0.8665 - val_loss: 0.3612 - val_accuracy: 0.8517 - val_f1_score: 0.8414\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3196 - accuracy: 0.8614 - f1_score: 0.8616 - val_loss: 0.3446 - val_accuracy: 0.8481 - val_f1_score: 0.8519\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2997 - accuracy: 0.8737 - f1_score: 0.8736 - val_loss: 0.3611 - val_accuracy: 0.8463 - val_f1_score: 0.8509\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2857 - accuracy: 0.8837 - f1_score: 0.8826 - val_loss: 0.3672 - val_accuracy: 0.8580 - val_f1_score: 0.8619\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2691 - accuracy: 0.8918 - f1_score: 0.8919 - val_loss: 0.3660 - val_accuracy: 0.8689 - val_f1_score: 0.8685\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2673 - accuracy: 0.8870 - f1_score: 0.8865 - val_loss: 0.4323 - val_accuracy: 0.8183 - val_f1_score: 0.8365\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2617 - accuracy: 0.8954 - f1_score: 0.8955 - val_loss: 0.3688 - val_accuracy: 0.8535 - val_f1_score: 0.8430\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3241 - accuracy: 0.8670 - f1_score: 0.8917\n","Epoch 1/20\n","104/104 [==============================] - 10s 29ms/step - loss: 0.6310 - accuracy: 0.6420 - f1_score: 0.6327 - val_loss: 0.5763 - val_accuracy: 0.6835 - val_f1_score: 0.7204\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.5119 - accuracy: 0.7604 - f1_score: 0.7477 - val_loss: 0.4645 - val_accuracy: 0.7794 - val_f1_score: 0.7631\n","Epoch 3/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.4150 - accuracy: 0.8125 - f1_score: 0.8055 - val_loss: 0.4313 - val_accuracy: 0.7884 - val_f1_score: 0.7516\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3589 - accuracy: 0.8421 - f1_score: 0.8373 - val_loss: 0.3866 - val_accuracy: 0.8309 - val_f1_score: 0.8186\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3446 - accuracy: 0.8623 - f1_score: 0.8597 - val_loss: 0.3939 - val_accuracy: 0.8201 - val_f1_score: 0.8043\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3083 - accuracy: 0.8807 - f1_score: 0.8791 - val_loss: 0.3963 - val_accuracy: 0.8318 - val_f1_score: 0.8218\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2994 - accuracy: 0.8746 - f1_score: 0.8732 - val_loss: 0.4142 - val_accuracy: 0.8246 - val_f1_score: 0.8353\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2936 - accuracy: 0.8803 - f1_score: 0.8791 - val_loss: 0.4014 - val_accuracy: 0.8345 - val_f1_score: 0.8249\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2726 - accuracy: 0.8891 - f1_score: 0.8883 - val_loss: 0.4195 - val_accuracy: 0.8237 - val_f1_score: 0.8044\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3889 - accuracy: 0.8305 - f1_score: 0.8510\n","Epoch 1/20\n","104/104 [==============================] - 13s 36ms/step - loss: 0.6293 - accuracy: 0.6471 - f1_score: 0.6207 - val_loss: 0.4997 - val_accuracy: 0.7712 - val_f1_score: 0.7790\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4943 - accuracy: 0.7661 - f1_score: 0.7563 - val_loss: 0.4170 - val_accuracy: 0.8210 - val_f1_score: 0.8275\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4119 - accuracy: 0.8153 - f1_score: 0.8102 - val_loss: 0.4196 - val_accuracy: 0.8165 - val_f1_score: 0.8340\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3574 - accuracy: 0.8520 - f1_score: 0.8506 - val_loss: 0.3724 - val_accuracy: 0.8463 - val_f1_score: 0.8399\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3439 - accuracy: 0.8641 - f1_score: 0.8610 - val_loss: 0.4002 - val_accuracy: 0.8391 - val_f1_score: 0.8519\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3194 - accuracy: 0.8725 - f1_score: 0.8707 - val_loss: 0.3591 - val_accuracy: 0.8562 - val_f1_score: 0.8619\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2926 - accuracy: 0.8800 - f1_score: 0.8783 - val_loss: 0.3901 - val_accuracy: 0.8391 - val_f1_score: 0.8529\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2861 - accuracy: 0.8855 - f1_score: 0.8844 - val_loss: 0.4470 - val_accuracy: 0.8463 - val_f1_score: 0.8552\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2859 - accuracy: 0.8855 - f1_score: 0.8836 - val_loss: 0.4471 - val_accuracy: 0.8327 - val_f1_score: 0.8475\n","Epoch 10/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2728 - accuracy: 0.8927 - f1_score: 0.8915 - val_loss: 0.3829 - val_accuracy: 0.8562 - val_f1_score: 0.8594\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2513 - accuracy: 0.9027 - f1_score: 0.9016 - val_loss: 0.4472 - val_accuracy: 0.8354 - val_f1_score: 0.8486\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3249 - accuracy: 0.8589 - f1_score: 0.8843\n","Epoch 1/20\n","104/104 [==============================] - 8s 24ms/step - loss: 0.6294 - accuracy: 0.6477 - f1_score: 0.6327 - val_loss: 0.4974 - val_accuracy: 0.7685 - val_f1_score: 0.7647\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4765 - accuracy: 0.7791 - f1_score: 0.7727 - val_loss: 0.4005 - val_accuracy: 0.8228 - val_f1_score: 0.8067\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4133 - accuracy: 0.8116 - f1_score: 0.8092 - val_loss: 0.3941 - val_accuracy: 0.8119 - val_f1_score: 0.8295\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3703 - accuracy: 0.8430 - f1_score: 0.8398 - val_loss: 0.5171 - val_accuracy: 0.7631 - val_f1_score: 0.8015\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3513 - accuracy: 0.8562 - f1_score: 0.8528 - val_loss: 0.3343 - val_accuracy: 0.8608 - val_f1_score: 0.8582\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3169 - accuracy: 0.8707 - f1_score: 0.8690 - val_loss: 0.3523 - val_accuracy: 0.8490 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3048 - accuracy: 0.8770 - f1_score: 0.8743 - val_loss: 0.3427 - val_accuracy: 0.8644 - val_f1_score: 0.8693\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3203 - accuracy: 0.8626 - f1_score: 0.8608 - val_loss: 0.3362 - val_accuracy: 0.8635 - val_f1_score: 0.8646\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2884 - accuracy: 0.8888 - f1_score: 0.8872 - val_loss: 0.3659 - val_accuracy: 0.8626 - val_f1_score: 0.8555\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2804 - accuracy: 0.8864 - f1_score: 0.8851 - val_loss: 0.3865 - val_accuracy: 0.8499 - val_f1_score: 0.8617\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3694 - accuracy: 0.8373 - f1_score: 0.8576\n","Epoch 1/20\n","104/104 [==============================] - 12s 30ms/step - loss: 0.6297 - accuracy: 0.6438 - f1_score: 0.6127 - val_loss: 0.5449 - val_accuracy: 0.7188 - val_f1_score: 0.7580\n","Epoch 2/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.4762 - accuracy: 0.7764 - f1_score: 0.7687 - val_loss: 0.3878 - val_accuracy: 0.8318 - val_f1_score: 0.8318\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4058 - accuracy: 0.8192 - f1_score: 0.8161 - val_loss: 0.3791 - val_accuracy: 0.8318 - val_f1_score: 0.8465\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3615 - accuracy: 0.8457 - f1_score: 0.8462 - val_loss: 0.3491 - val_accuracy: 0.8553 - val_f1_score: 0.8625\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3409 - accuracy: 0.8571 - f1_score: 0.8570 - val_loss: 0.3169 - val_accuracy: 0.8689 - val_f1_score: 0.8725\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3169 - accuracy: 0.8743 - f1_score: 0.8737 - val_loss: 0.3021 - val_accuracy: 0.8852 - val_f1_score: 0.8785\n","Epoch 7/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3160 - accuracy: 0.8680 - f1_score: 0.8685 - val_loss: 0.3282 - val_accuracy: 0.8517 - val_f1_score: 0.8636\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3067 - accuracy: 0.8707 - f1_score: 0.8705 - val_loss: 0.3681 - val_accuracy: 0.8255 - val_f1_score: 0.8445\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2741 - accuracy: 0.8942 - f1_score: 0.8946 - val_loss: 0.2847 - val_accuracy: 0.8852 - val_f1_score: 0.8849\n","Epoch 10/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2698 - accuracy: 0.8948 - f1_score: 0.8949 - val_loss: 0.3227 - val_accuracy: 0.8626 - val_f1_score: 0.8714\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2661 - accuracy: 0.8918 - f1_score: 0.8920 - val_loss: 0.3004 - val_accuracy: 0.8788 - val_f1_score: 0.8733\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2453 - accuracy: 0.9051 - f1_score: 0.9052 - val_loss: 0.2870 - val_accuracy: 0.8888 - val_f1_score: 0.8875\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2445 - accuracy: 0.9011 - f1_score: 0.9011 - val_loss: 0.2989 - val_accuracy: 0.8770 - val_f1_score: 0.8729\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2450 - accuracy: 0.9017 - f1_score: 0.9014 - val_loss: 0.3111 - val_accuracy: 0.8689 - val_f1_score: 0.8742\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3361 - accuracy: 0.8494 - f1_score: 0.8771\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6351 - accuracy: 0.6338 - f1_score: 0.6018 - val_loss: 0.5520 - val_accuracy: 0.7152 - val_f1_score: 0.6897\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4991 - accuracy: 0.7670 - f1_score: 0.7571 - val_loss: 0.4313 - val_accuracy: 0.8065 - val_f1_score: 0.8076\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4144 - accuracy: 0.8162 - f1_score: 0.8117 - val_loss: 0.3653 - val_accuracy: 0.8363 - val_f1_score: 0.8288\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3790 - accuracy: 0.8273 - f1_score: 0.8262 - val_loss: 0.3735 - val_accuracy: 0.8309 - val_f1_score: 0.8456\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3369 - accuracy: 0.8535 - f1_score: 0.8525 - val_loss: 0.3387 - val_accuracy: 0.8517 - val_f1_score: 0.8610\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3196 - accuracy: 0.8653 - f1_score: 0.8654 - val_loss: 0.3933 - val_accuracy: 0.8327 - val_f1_score: 0.8502\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2979 - accuracy: 0.8822 - f1_score: 0.8816 - val_loss: 0.3302 - val_accuracy: 0.8680 - val_f1_score: 0.8739\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2888 - accuracy: 0.8770 - f1_score: 0.8754 - val_loss: 0.3392 - val_accuracy: 0.8671 - val_f1_score: 0.8721\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2753 - accuracy: 0.8846 - f1_score: 0.8832 - val_loss: 0.3546 - val_accuracy: 0.8671 - val_f1_score: 0.8751\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2561 - accuracy: 0.8942 - f1_score: 0.8939 - val_loss: 0.3376 - val_accuracy: 0.8662 - val_f1_score: 0.8724\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2482 - accuracy: 0.9027 - f1_score: 0.9020 - val_loss: 0.3505 - val_accuracy: 0.8653 - val_f1_score: 0.8580\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2439 - accuracy: 0.9005 - f1_score: 0.8994 - val_loss: 0.3505 - val_accuracy: 0.8698 - val_f1_score: 0.8769\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3424 - accuracy: 0.8542 - f1_score: 0.8850\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6305 - accuracy: 0.6414 - f1_score: 0.6325 - val_loss: 0.5289 - val_accuracy: 0.7360 - val_f1_score: 0.6952\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4708 - accuracy: 0.7830 - f1_score: 0.7730 - val_loss: 0.4279 - val_accuracy: 0.8002 - val_f1_score: 0.7810\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4034 - accuracy: 0.8186 - f1_score: 0.8129 - val_loss: 0.3819 - val_accuracy: 0.8391 - val_f1_score: 0.8430\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3468 - accuracy: 0.8487 - f1_score: 0.8460 - val_loss: 0.3636 - val_accuracy: 0.8508 - val_f1_score: 0.8564\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3333 - accuracy: 0.8565 - f1_score: 0.8547 - val_loss: 0.4542 - val_accuracy: 0.7857 - val_f1_score: 0.8164\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3030 - accuracy: 0.8737 - f1_score: 0.8732 - val_loss: 0.3288 - val_accuracy: 0.8662 - val_f1_score: 0.8659\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3111 - accuracy: 0.8674 - f1_score: 0.8649 - val_loss: 0.3233 - val_accuracy: 0.8743 - val_f1_score: 0.8764\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2741 - accuracy: 0.8939 - f1_score: 0.8942 - val_loss: 0.3810 - val_accuracy: 0.8363 - val_f1_score: 0.8500\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2710 - accuracy: 0.8900 - f1_score: 0.8896 - val_loss: 0.3270 - val_accuracy: 0.8698 - val_f1_score: 0.8721\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2608 - accuracy: 0.8924 - f1_score: 0.8917 - val_loss: 0.3646 - val_accuracy: 0.8427 - val_f1_score: 0.8523\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2581 - accuracy: 0.8987 - f1_score: 0.8984 - val_loss: 0.3559 - val_accuracy: 0.8508 - val_f1_score: 0.8569\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2522 - accuracy: 0.8960 - f1_score: 0.8957 - val_loss: 0.3435 - val_accuracy: 0.8653 - val_f1_score: 0.8654\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3456 - accuracy: 0.8508 - f1_score: 0.8773\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6314 - accuracy: 0.6468 - f1_score: 0.6398 - val_loss: 0.5340 - val_accuracy: 0.7378 - val_f1_score: 0.7406\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4986 - accuracy: 0.7667 - f1_score: 0.7527 - val_loss: 0.4227 - val_accuracy: 0.8137 - val_f1_score: 0.8038\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4113 - accuracy: 0.8131 - f1_score: 0.8043 - val_loss: 0.3621 - val_accuracy: 0.8363 - val_f1_score: 0.8234\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3790 - accuracy: 0.8403 - f1_score: 0.8381 - val_loss: 0.3302 - val_accuracy: 0.8590 - val_f1_score: 0.8600\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3416 - accuracy: 0.8571 - f1_score: 0.8550 - val_loss: 0.3187 - val_accuracy: 0.8662 - val_f1_score: 0.8571\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3051 - accuracy: 0.8770 - f1_score: 0.8761 - val_loss: 0.3460 - val_accuracy: 0.8508 - val_f1_score: 0.8622\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2957 - accuracy: 0.8764 - f1_score: 0.8760 - val_loss: 0.3152 - val_accuracy: 0.8716 - val_f1_score: 0.8734\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2944 - accuracy: 0.8734 - f1_score: 0.8709 - val_loss: 0.3030 - val_accuracy: 0.8770 - val_f1_score: 0.8759\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2776 - accuracy: 0.8825 - f1_score: 0.8818 - val_loss: 0.3309 - val_accuracy: 0.8662 - val_f1_score: 0.8574\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2640 - accuracy: 0.8957 - f1_score: 0.8953 - val_loss: 0.4338 - val_accuracy: 0.8210 - val_f1_score: 0.7903\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2729 - accuracy: 0.8858 - f1_score: 0.8841 - val_loss: 0.3471 - val_accuracy: 0.8481 - val_f1_score: 0.8327\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2482 - accuracy: 0.9017 - f1_score: 0.9005 - val_loss: 0.3642 - val_accuracy: 0.8363 - val_f1_score: 0.8162\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2469 - accuracy: 0.8990 - f1_score: 0.8976 - val_loss: 0.3677 - val_accuracy: 0.8689 - val_f1_score: 0.8738\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3378 - accuracy: 0.8623 - f1_score: 0.8872\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6475 - accuracy: 0.6218 - f1_score: 0.6168 - val_loss: 0.5700 - val_accuracy: 0.6980 - val_f1_score: 0.7391\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.5136 - accuracy: 0.7577 - f1_score: 0.7456 - val_loss: 0.4446 - val_accuracy: 0.7866 - val_f1_score: 0.7631\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4138 - accuracy: 0.8149 - f1_score: 0.8092 - val_loss: 0.3962 - val_accuracy: 0.8255 - val_f1_score: 0.8076\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3713 - accuracy: 0.8451 - f1_score: 0.8415 - val_loss: 0.3662 - val_accuracy: 0.8454 - val_f1_score: 0.8507\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3379 - accuracy: 0.8599 - f1_score: 0.8581 - val_loss: 0.4369 - val_accuracy: 0.8011 - val_f1_score: 0.7629\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3156 - accuracy: 0.8704 - f1_score: 0.8688 - val_loss: 0.3622 - val_accuracy: 0.8382 - val_f1_score: 0.8479\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2989 - accuracy: 0.8773 - f1_score: 0.8776 - val_loss: 0.3276 - val_accuracy: 0.8553 - val_f1_score: 0.8464\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2894 - accuracy: 0.8773 - f1_score: 0.8753 - val_loss: 0.3132 - val_accuracy: 0.8689 - val_f1_score: 0.8641\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2705 - accuracy: 0.8915 - f1_score: 0.8906 - val_loss: 0.4447 - val_accuracy: 0.8029 - val_f1_score: 0.8264\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2815 - accuracy: 0.8897 - f1_score: 0.8896 - val_loss: 0.3442 - val_accuracy: 0.8571 - val_f1_score: 0.8451\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2544 - accuracy: 0.8936 - f1_score: 0.8931 - val_loss: 0.3288 - val_accuracy: 0.8617 - val_f1_score: 0.8513\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2521 - accuracy: 0.9020 - f1_score: 0.9011 - val_loss: 0.3791 - val_accuracy: 0.8562 - val_f1_score: 0.8424\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2292 - accuracy: 0.9099 - f1_score: 0.9090 - val_loss: 0.3543 - val_accuracy: 0.8599 - val_f1_score: 0.8511\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.8562 - f1_score: 0.8790\n","Epoch 1/20\n","104/104 [==============================] - 6s 16ms/step - loss: 0.6310 - accuracy: 0.6426 - f1_score: 0.6244 - val_loss: 0.5555 - val_accuracy: 0.7251 - val_f1_score: 0.6553\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4910 - accuracy: 0.7679 - f1_score: 0.7586 - val_loss: 0.4867 - val_accuracy: 0.7740 - val_f1_score: 0.7253\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3964 - accuracy: 0.8198 - f1_score: 0.8150 - val_loss: 0.3776 - val_accuracy: 0.8436 - val_f1_score: 0.8351\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3533 - accuracy: 0.8454 - f1_score: 0.8450 - val_loss: 0.3738 - val_accuracy: 0.8427 - val_f1_score: 0.8324\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3259 - accuracy: 0.8550 - f1_score: 0.8554 - val_loss: 0.3490 - val_accuracy: 0.8526 - val_f1_score: 0.8481\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3093 - accuracy: 0.8701 - f1_score: 0.8696 - val_loss: 0.3792 - val_accuracy: 0.8571 - val_f1_score: 0.8478\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3053 - accuracy: 0.8755 - f1_score: 0.8745 - val_loss: 0.3476 - val_accuracy: 0.8580 - val_f1_score: 0.8523\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2831 - accuracy: 0.8858 - f1_score: 0.8853 - val_loss: 0.3678 - val_accuracy: 0.8508 - val_f1_score: 0.8348\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2800 - accuracy: 0.8846 - f1_score: 0.8842 - val_loss: 0.3399 - val_accuracy: 0.8617 - val_f1_score: 0.8582\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2647 - accuracy: 0.8900 - f1_score: 0.8891 - val_loss: 0.5356 - val_accuracy: 0.7939 - val_f1_score: 0.7478\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2667 - accuracy: 0.8909 - f1_score: 0.8903 - val_loss: 0.3962 - val_accuracy: 0.8400 - val_f1_score: 0.8478\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2282 - accuracy: 0.9120 - f1_score: 0.9119 - val_loss: 0.3873 - val_accuracy: 0.8635 - val_f1_score: 0.8549\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2356 - accuracy: 0.9090 - f1_score: 0.9085 - val_loss: 0.3601 - val_accuracy: 0.8698 - val_f1_score: 0.8647\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2162 - accuracy: 0.9114 - f1_score: 0.9105 - val_loss: 0.4119 - val_accuracy: 0.8580 - val_f1_score: 0.8592\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3164 - accuracy: 0.8602 - f1_score: 0.8876\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6465 - accuracy: 0.6157 - f1_score: 0.6004 - val_loss: 0.5635 - val_accuracy: 0.7125 - val_f1_score: 0.6259\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4952 - accuracy: 0.7658 - f1_score: 0.7565 - val_loss: 0.3824 - val_accuracy: 0.8345 - val_f1_score: 0.8259\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4114 - accuracy: 0.8137 - f1_score: 0.8100 - val_loss: 0.3578 - val_accuracy: 0.8508 - val_f1_score: 0.8415\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3576 - accuracy: 0.8481 - f1_score: 0.8440 - val_loss: 0.3269 - val_accuracy: 0.8689 - val_f1_score: 0.8649\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3600 - accuracy: 0.8403 - f1_score: 0.8370 - val_loss: 0.3604 - val_accuracy: 0.8336 - val_f1_score: 0.8099\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3297 - accuracy: 0.8623 - f1_score: 0.8591 - val_loss: 0.3430 - val_accuracy: 0.8526 - val_f1_score: 0.8594\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3046 - accuracy: 0.8746 - f1_score: 0.8719 - val_loss: 0.4501 - val_accuracy: 0.8047 - val_f1_score: 0.7662\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2989 - accuracy: 0.8737 - f1_score: 0.8710 - val_loss: 0.3176 - val_accuracy: 0.8680 - val_f1_score: 0.8620\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2880 - accuracy: 0.8855 - f1_score: 0.8832 - val_loss: 0.3233 - val_accuracy: 0.8734 - val_f1_score: 0.8664\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2776 - accuracy: 0.8855 - f1_score: 0.8839 - val_loss: 0.3272 - val_accuracy: 0.8680 - val_f1_score: 0.8646\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2587 - accuracy: 0.8957 - f1_score: 0.8941 - val_loss: 0.3434 - val_accuracy: 0.8599 - val_f1_score: 0.8488\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2523 - accuracy: 0.8978 - f1_score: 0.8956 - val_loss: 0.3637 - val_accuracy: 0.8617 - val_f1_score: 0.8504\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2425 - accuracy: 0.9020 - f1_score: 0.9006 - val_loss: 0.3416 - val_accuracy: 0.8788 - val_f1_score: 0.8736\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8596 - f1_score: 0.8882\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6487 - accuracy: 0.6251 - f1_score: 0.5966 - val_loss: 0.5824 - val_accuracy: 0.6835 - val_f1_score: 0.5813\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5039 - accuracy: 0.7655 - f1_score: 0.7572 - val_loss: 0.4385 - val_accuracy: 0.7920 - val_f1_score: 0.7754\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4262 - accuracy: 0.8119 - f1_score: 0.8015 - val_loss: 0.4504 - val_accuracy: 0.8092 - val_f1_score: 0.8275\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3549 - accuracy: 0.8505 - f1_score: 0.8470 - val_loss: 0.3745 - val_accuracy: 0.8445 - val_f1_score: 0.8504\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3239 - accuracy: 0.8602 - f1_score: 0.8569 - val_loss: 0.3627 - val_accuracy: 0.8662 - val_f1_score: 0.8671\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3190 - accuracy: 0.8644 - f1_score: 0.8623 - val_loss: 0.3762 - val_accuracy: 0.8517 - val_f1_score: 0.8470\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2958 - accuracy: 0.8782 - f1_score: 0.8755 - val_loss: 0.3756 - val_accuracy: 0.8626 - val_f1_score: 0.8652\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2777 - accuracy: 0.8864 - f1_score: 0.8842 - val_loss: 0.3594 - val_accuracy: 0.8617 - val_f1_score: 0.8558\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2761 - accuracy: 0.8852 - f1_score: 0.8826 - val_loss: 0.3567 - val_accuracy: 0.8617 - val_f1_score: 0.8600\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2578 - accuracy: 0.8942 - f1_score: 0.8924 - val_loss: 0.3975 - val_accuracy: 0.8544 - val_f1_score: 0.8471\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2580 - accuracy: 0.8948 - f1_score: 0.8931 - val_loss: 0.4065 - val_accuracy: 0.8273 - val_f1_score: 0.8428\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2557 - accuracy: 0.8942 - f1_score: 0.8935 - val_loss: 0.3847 - val_accuracy: 0.8653 - val_f1_score: 0.8690\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2305 - accuracy: 0.9027 - f1_score: 0.9011 - val_loss: 0.3729 - val_accuracy: 0.8590 - val_f1_score: 0.8624\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2287 - accuracy: 0.9033 - f1_score: 0.9020 - val_loss: 0.4158 - val_accuracy: 0.8626 - val_f1_score: 0.8655\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3247 - accuracy: 0.8575 - f1_score: 0.8835\n","Epoch 1/20\n","104/104 [==============================] - 6s 15ms/step - loss: 0.6299 - accuracy: 0.6404 - f1_score: 0.6319 - val_loss: 0.5343 - val_accuracy: 0.7414 - val_f1_score: 0.6898\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4956 - accuracy: 0.7706 - f1_score: 0.7571 - val_loss: 0.4335 - val_accuracy: 0.7839 - val_f1_score: 0.7487\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4024 - accuracy: 0.8201 - f1_score: 0.8131 - val_loss: 0.4024 - val_accuracy: 0.8183 - val_f1_score: 0.8324\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3579 - accuracy: 0.8457 - f1_score: 0.8401 - val_loss: 0.3558 - val_accuracy: 0.8345 - val_f1_score: 0.8443\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3499 - accuracy: 0.8496 - f1_score: 0.8473 - val_loss: 0.3298 - val_accuracy: 0.8644 - val_f1_score: 0.8629\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3114 - accuracy: 0.8641 - f1_score: 0.8613 - val_loss: 0.3471 - val_accuracy: 0.8571 - val_f1_score: 0.8647\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3005 - accuracy: 0.8731 - f1_score: 0.8714 - val_loss: 0.3436 - val_accuracy: 0.8481 - val_f1_score: 0.8571\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2998 - accuracy: 0.8800 - f1_score: 0.8781 - val_loss: 0.3117 - val_accuracy: 0.8797 - val_f1_score: 0.8758\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2735 - accuracy: 0.8924 - f1_score: 0.8903 - val_loss: 0.4928 - val_accuracy: 0.7821 - val_f1_score: 0.8148\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2751 - accuracy: 0.8867 - f1_score: 0.8857 - val_loss: 0.3335 - val_accuracy: 0.8707 - val_f1_score: 0.8753\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2602 - accuracy: 0.8936 - f1_score: 0.8921 - val_loss: 0.4531 - val_accuracy: 0.8029 - val_f1_score: 0.8275\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2592 - accuracy: 0.8960 - f1_score: 0.8948 - val_loss: 0.3271 - val_accuracy: 0.8725 - val_f1_score: 0.8747\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2684 - accuracy: 0.8900 - f1_score: 0.8883 - val_loss: 0.3348 - val_accuracy: 0.8608 - val_f1_score: 0.8679\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.8542 - f1_score: 0.8759\n","Epoch 1/20\n","104/104 [==============================] - 6s 20ms/step - loss: 0.6291 - accuracy: 0.6459 - f1_score: 0.6343 - val_loss: 0.5285 - val_accuracy: 0.7414 - val_f1_score: 0.7174\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4982 - accuracy: 0.7586 - f1_score: 0.7490 - val_loss: 0.4643 - val_accuracy: 0.7740 - val_f1_score: 0.7984\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4208 - accuracy: 0.8113 - f1_score: 0.8079 - val_loss: 0.3838 - val_accuracy: 0.8309 - val_f1_score: 0.8292\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3595 - accuracy: 0.8463 - f1_score: 0.8463 - val_loss: 0.3694 - val_accuracy: 0.8309 - val_f1_score: 0.8430\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3429 - accuracy: 0.8535 - f1_score: 0.8540 - val_loss: 0.3803 - val_accuracy: 0.8418 - val_f1_score: 0.8536\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3182 - accuracy: 0.8614 - f1_score: 0.8602 - val_loss: 0.3314 - val_accuracy: 0.8553 - val_f1_score: 0.8510\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3009 - accuracy: 0.8752 - f1_score: 0.8740 - val_loss: 0.3254 - val_accuracy: 0.8590 - val_f1_score: 0.8545\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2857 - accuracy: 0.8864 - f1_score: 0.8861 - val_loss: 0.4142 - val_accuracy: 0.8318 - val_f1_score: 0.8470\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2877 - accuracy: 0.8825 - f1_score: 0.8813 - val_loss: 0.3912 - val_accuracy: 0.8156 - val_f1_score: 0.8373\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2906 - accuracy: 0.8740 - f1_score: 0.8733 - val_loss: 0.3401 - val_accuracy: 0.8544 - val_f1_score: 0.8423\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2719 - accuracy: 0.8867 - f1_score: 0.8859 - val_loss: 0.3346 - val_accuracy: 0.8562 - val_f1_score: 0.8606\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2450 - accuracy: 0.9011 - f1_score: 0.9007 - val_loss: 0.3291 - val_accuracy: 0.8571 - val_f1_score: 0.8661\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3262 - accuracy: 0.8656 - f1_score: 0.8901\n","Epoch 1/20\n","104/104 [==============================] - 6s 17ms/step - loss: 0.6229 - accuracy: 0.6456 - f1_score: 0.6648 - val_loss: 0.5689 - val_accuracy: 0.7034 - val_f1_score: 0.6067\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4823 - accuracy: 0.7764 - f1_score: 0.7635 - val_loss: 0.4392 - val_accuracy: 0.7993 - val_f1_score: 0.8053\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3997 - accuracy: 0.8146 - f1_score: 0.8087 - val_loss: 0.3933 - val_accuracy: 0.8273 - val_f1_score: 0.8133\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3510 - accuracy: 0.8487 - f1_score: 0.8446 - val_loss: 0.3659 - val_accuracy: 0.8354 - val_f1_score: 0.8439\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3178 - accuracy: 0.8692 - f1_score: 0.8665 - val_loss: 0.3569 - val_accuracy: 0.8418 - val_f1_score: 0.8531\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3244 - accuracy: 0.8683 - f1_score: 0.8682 - val_loss: 0.3649 - val_accuracy: 0.8418 - val_f1_score: 0.8490\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2948 - accuracy: 0.8755 - f1_score: 0.8736 - val_loss: 0.3311 - val_accuracy: 0.8626 - val_f1_score: 0.8595\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2860 - accuracy: 0.8819 - f1_score: 0.8799 - val_loss: 0.3279 - val_accuracy: 0.8608 - val_f1_score: 0.8595\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2761 - accuracy: 0.8837 - f1_score: 0.8814 - val_loss: 0.3353 - val_accuracy: 0.8635 - val_f1_score: 0.8697\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2838 - accuracy: 0.8846 - f1_score: 0.8835 - val_loss: 0.4579 - val_accuracy: 0.7812 - val_f1_score: 0.8130\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2648 - accuracy: 0.8927 - f1_score: 0.8918 - val_loss: 0.3351 - val_accuracy: 0.8698 - val_f1_score: 0.8693\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2650 - accuracy: 0.8927 - f1_score: 0.8925 - val_loss: 0.3301 - val_accuracy: 0.8662 - val_f1_score: 0.8676\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2489 - accuracy: 0.8969 - f1_score: 0.8960 - val_loss: 0.3309 - val_accuracy: 0.8743 - val_f1_score: 0.8712\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3508 - accuracy: 0.8515 - f1_score: 0.8757\n","Epoch 1/20\n","104/104 [==============================] - 7s 15ms/step - loss: 0.6338 - accuracy: 0.6359 - f1_score: 0.6177 - val_loss: 0.5531 - val_accuracy: 0.7242 - val_f1_score: 0.6600\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4630 - accuracy: 0.7833 - f1_score: 0.7786 - val_loss: 0.4346 - val_accuracy: 0.7911 - val_f1_score: 0.7631\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3906 - accuracy: 0.8231 - f1_score: 0.8207 - val_loss: 0.4443 - val_accuracy: 0.7957 - val_f1_score: 0.7570\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3595 - accuracy: 0.8448 - f1_score: 0.8441 - val_loss: 0.3657 - val_accuracy: 0.8490 - val_f1_score: 0.8371\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3294 - accuracy: 0.8574 - f1_score: 0.8577 - val_loss: 0.3315 - val_accuracy: 0.8562 - val_f1_score: 0.8602\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3273 - accuracy: 0.8638 - f1_score: 0.8623 - val_loss: 0.3522 - val_accuracy: 0.8490 - val_f1_score: 0.8335\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3053 - accuracy: 0.8728 - f1_score: 0.8689 - val_loss: 0.3173 - val_accuracy: 0.8716 - val_f1_score: 0.8678\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3013 - accuracy: 0.8692 - f1_score: 0.8677 - val_loss: 0.3345 - val_accuracy: 0.8626 - val_f1_score: 0.8636\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2849 - accuracy: 0.8849 - f1_score: 0.8832 - val_loss: 0.3459 - val_accuracy: 0.8644 - val_f1_score: 0.8549\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2715 - accuracy: 0.8879 - f1_score: 0.8862 - val_loss: 0.3626 - val_accuracy: 0.8499 - val_f1_score: 0.8327\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2653 - accuracy: 0.8978 - f1_score: 0.8965 - val_loss: 0.3502 - val_accuracy: 0.8662 - val_f1_score: 0.8627\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2546 - accuracy: 0.9008 - f1_score: 0.8987 - val_loss: 0.3518 - val_accuracy: 0.8671 - val_f1_score: 0.8691\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.8508 - f1_score: 0.8738\n","Epoch 1/20\n","104/104 [==============================] - 6s 22ms/step - loss: 0.6304 - accuracy: 0.6474 - f1_score: 0.6501 - val_loss: 0.5133 - val_accuracy: 0.7667 - val_f1_score: 0.7367\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4953 - accuracy: 0.7619 - f1_score: 0.7534 - val_loss: 0.4098 - val_accuracy: 0.8137 - val_f1_score: 0.8218\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4264 - accuracy: 0.8038 - f1_score: 0.8005 - val_loss: 0.3583 - val_accuracy: 0.8463 - val_f1_score: 0.8365\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3517 - accuracy: 0.8538 - f1_score: 0.8537 - val_loss: 0.3379 - val_accuracy: 0.8635 - val_f1_score: 0.8574\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3494 - accuracy: 0.8593 - f1_score: 0.8579 - val_loss: 0.3434 - val_accuracy: 0.8463 - val_f1_score: 0.8569\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3235 - accuracy: 0.8596 - f1_score: 0.8580 - val_loss: 0.3140 - val_accuracy: 0.8725 - val_f1_score: 0.8683\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3250 - accuracy: 0.8635 - f1_score: 0.8627 - val_loss: 0.3031 - val_accuracy: 0.8770 - val_f1_score: 0.8727\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2866 - accuracy: 0.8870 - f1_score: 0.8863 - val_loss: 0.3406 - val_accuracy: 0.8544 - val_f1_score: 0.8417\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2785 - accuracy: 0.8966 - f1_score: 0.8951 - val_loss: 0.2946 - val_accuracy: 0.8779 - val_f1_score: 0.8751\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2690 - accuracy: 0.8951 - f1_score: 0.8944 - val_loss: 0.4195 - val_accuracy: 0.8074 - val_f1_score: 0.7702\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2633 - accuracy: 0.8987 - f1_score: 0.8977 - val_loss: 0.3365 - val_accuracy: 0.8526 - val_f1_score: 0.8416\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2484 - accuracy: 0.9017 - f1_score: 0.9012 - val_loss: 0.3792 - val_accuracy: 0.8291 - val_f1_score: 0.8025\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2488 - accuracy: 0.9017 - f1_score: 0.9005 - val_loss: 0.3267 - val_accuracy: 0.8617 - val_f1_score: 0.8516\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2365 - accuracy: 0.9117 - f1_score: 0.9112 - val_loss: 0.3467 - val_accuracy: 0.8707 - val_f1_score: 0.8747\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3226 - accuracy: 0.8636 - f1_score: 0.8893\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6257 - accuracy: 0.6486 - f1_score: 0.6204 - val_loss: 0.5898 - val_accuracy: 0.6962 - val_f1_score: 0.7439\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5012 - accuracy: 0.7622 - f1_score: 0.7518 - val_loss: 0.4624 - val_accuracy: 0.7794 - val_f1_score: 0.7732\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4195 - accuracy: 0.8180 - f1_score: 0.8144 - val_loss: 0.4098 - val_accuracy: 0.8165 - val_f1_score: 0.8054\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3702 - accuracy: 0.8360 - f1_score: 0.8339 - val_loss: 0.3741 - val_accuracy: 0.8373 - val_f1_score: 0.8330\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3270 - accuracy: 0.8571 - f1_score: 0.8559 - val_loss: 0.3633 - val_accuracy: 0.8499 - val_f1_score: 0.8410\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3078 - accuracy: 0.8722 - f1_score: 0.8717 - val_loss: 0.3755 - val_accuracy: 0.8517 - val_f1_score: 0.8517\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2892 - accuracy: 0.8846 - f1_score: 0.8850 - val_loss: 0.3708 - val_accuracy: 0.8562 - val_f1_score: 0.8604\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2883 - accuracy: 0.8834 - f1_score: 0.8824 - val_loss: 0.3569 - val_accuracy: 0.8535 - val_f1_score: 0.8535\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2762 - accuracy: 0.8915 - f1_score: 0.8910 - val_loss: 0.3724 - val_accuracy: 0.8599 - val_f1_score: 0.8644\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2571 - accuracy: 0.9005 - f1_score: 0.8998 - val_loss: 0.3705 - val_accuracy: 0.8436 - val_f1_score: 0.8360\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2570 - accuracy: 0.8984 - f1_score: 0.8975 - val_loss: 0.3491 - val_accuracy: 0.8599 - val_f1_score: 0.8550\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2397 - accuracy: 0.9084 - f1_score: 0.9075 - val_loss: 0.3718 - val_accuracy: 0.8571 - val_f1_score: 0.8607\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2280 - accuracy: 0.9159 - f1_score: 0.9153 - val_loss: 0.3835 - val_accuracy: 0.8562 - val_f1_score: 0.8496\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2322 - accuracy: 0.9072 - f1_score: 0.9066 - val_loss: 0.3808 - val_accuracy: 0.8472 - val_f1_score: 0.8325\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2180 - accuracy: 0.9159 - f1_score: 0.9145 - val_loss: 0.4276 - val_accuracy: 0.8472 - val_f1_score: 0.8312\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2135 - accuracy: 0.9144 - f1_score: 0.9135 - val_loss: 0.4268 - val_accuracy: 0.8590 - val_f1_score: 0.8579\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3518 - accuracy: 0.8521 - f1_score: 0.8755\n","Epoch 1/20\n","104/104 [==============================] - 7s 15ms/step - loss: 0.6268 - accuracy: 0.6498 - f1_score: 0.6396 - val_loss: 0.5014 - val_accuracy: 0.7712 - val_f1_score: 0.7743\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5069 - accuracy: 0.7631 - f1_score: 0.7497 - val_loss: 0.4036 - val_accuracy: 0.8273 - val_f1_score: 0.8223\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4356 - accuracy: 0.8044 - f1_score: 0.7951 - val_loss: 0.4340 - val_accuracy: 0.8074 - val_f1_score: 0.8311\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3824 - accuracy: 0.8363 - f1_score: 0.8324 - val_loss: 0.3277 - val_accuracy: 0.8626 - val_f1_score: 0.8694\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3486 - accuracy: 0.8520 - f1_score: 0.8502 - val_loss: 0.2906 - val_accuracy: 0.8788 - val_f1_score: 0.8762\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3189 - accuracy: 0.8656 - f1_score: 0.8634 - val_loss: 0.3194 - val_accuracy: 0.8698 - val_f1_score: 0.8780\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3167 - accuracy: 0.8668 - f1_score: 0.8654 - val_loss: 0.3006 - val_accuracy: 0.8743 - val_f1_score: 0.8649\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3030 - accuracy: 0.8710 - f1_score: 0.8692 - val_loss: 0.3737 - val_accuracy: 0.8436 - val_f1_score: 0.8588\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2981 - accuracy: 0.8776 - f1_score: 0.8760 - val_loss: 0.3556 - val_accuracy: 0.8345 - val_f1_score: 0.8518\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2702 - accuracy: 0.8858 - f1_score: 0.8850 - val_loss: 0.2951 - val_accuracy: 0.8888 - val_f1_score: 0.8881\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3534 - accuracy: 0.8413 - f1_score: 0.8666\n","Epoch 1/20\n","104/104 [==============================] - 6s 16ms/step - loss: 0.6300 - accuracy: 0.6423 - f1_score: 0.6084 - val_loss: 0.5341 - val_accuracy: 0.7369 - val_f1_score: 0.7445\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5029 - accuracy: 0.7670 - f1_score: 0.7576 - val_loss: 0.4285 - val_accuracy: 0.8074 - val_f1_score: 0.8037\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4127 - accuracy: 0.8186 - f1_score: 0.8149 - val_loss: 0.3851 - val_accuracy: 0.8282 - val_f1_score: 0.8254\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3622 - accuracy: 0.8454 - f1_score: 0.8412 - val_loss: 0.3646 - val_accuracy: 0.8418 - val_f1_score: 0.8466\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3456 - accuracy: 0.8538 - f1_score: 0.8512 - val_loss: 0.3417 - val_accuracy: 0.8562 - val_f1_score: 0.8582\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3215 - accuracy: 0.8653 - f1_score: 0.8638 - val_loss: 0.4051 - val_accuracy: 0.8237 - val_f1_score: 0.7992\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3042 - accuracy: 0.8779 - f1_score: 0.8754 - val_loss: 0.3474 - val_accuracy: 0.8580 - val_f1_score: 0.8604\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2952 - accuracy: 0.8858 - f1_score: 0.8842 - val_loss: 0.4285 - val_accuracy: 0.8201 - val_f1_score: 0.8370\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3074 - accuracy: 0.8719 - f1_score: 0.8694 - val_loss: 0.3459 - val_accuracy: 0.8544 - val_f1_score: 0.8468\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2613 - accuracy: 0.8987 - f1_score: 0.8973 - val_loss: 0.3329 - val_accuracy: 0.8716 - val_f1_score: 0.8716\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2567 - accuracy: 0.8993 - f1_score: 0.8979 - val_loss: 0.3594 - val_accuracy: 0.8635 - val_f1_score: 0.8598\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2489 - accuracy: 0.9057 - f1_score: 0.9046 - val_loss: 0.3407 - val_accuracy: 0.8662 - val_f1_score: 0.8622\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2546 - accuracy: 0.9005 - f1_score: 0.8998 - val_loss: 0.3743 - val_accuracy: 0.8680 - val_f1_score: 0.8687\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2245 - accuracy: 0.9138 - f1_score: 0.9129 - val_loss: 0.3878 - val_accuracy: 0.8653 - val_f1_score: 0.8639\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2278 - accuracy: 0.9072 - f1_score: 0.9056 - val_loss: 0.4243 - val_accuracy: 0.8635 - val_f1_score: 0.8653\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.8650 - f1_score: 0.8878\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6213 - accuracy: 0.6480 - f1_score: 0.6343 - val_loss: 0.5287 - val_accuracy: 0.7342 - val_f1_score: 0.7517\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4906 - accuracy: 0.7776 - f1_score: 0.7711 - val_loss: 0.4323 - val_accuracy: 0.8119 - val_f1_score: 0.8085\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4035 - accuracy: 0.8204 - f1_score: 0.8171 - val_loss: 0.4105 - val_accuracy: 0.8318 - val_f1_score: 0.8306\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3583 - accuracy: 0.8499 - f1_score: 0.8484 - val_loss: 0.3790 - val_accuracy: 0.8526 - val_f1_score: 0.8556\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3262 - accuracy: 0.8647 - f1_score: 0.8626 - val_loss: 0.4942 - val_accuracy: 0.7685 - val_f1_score: 0.8009\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3147 - accuracy: 0.8716 - f1_score: 0.8719 - val_loss: 0.3533 - val_accuracy: 0.8608 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2908 - accuracy: 0.8843 - f1_score: 0.8834 - val_loss: 0.4322 - val_accuracy: 0.8174 - val_f1_score: 0.8368\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2845 - accuracy: 0.8834 - f1_score: 0.8830 - val_loss: 0.3638 - val_accuracy: 0.8553 - val_f1_score: 0.8577\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2653 - accuracy: 0.8957 - f1_score: 0.8949 - val_loss: 0.4117 - val_accuracy: 0.8354 - val_f1_score: 0.8481\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2562 - accuracy: 0.9033 - f1_score: 0.9020 - val_loss: 0.5726 - val_accuracy: 0.7712 - val_f1_score: 0.8067\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2589 - accuracy: 0.9002 - f1_score: 0.8995 - val_loss: 0.4262 - val_accuracy: 0.8282 - val_f1_score: 0.8422\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3606 - accuracy: 0.8515 - f1_score: 0.8716\n","Epoch 1/20\n","104/104 [==============================] - 8s 16ms/step - loss: 0.6174 - accuracy: 0.6513 - f1_score: 0.6437 - val_loss: 0.4893 - val_accuracy: 0.7911 - val_f1_score: 0.7883\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4749 - accuracy: 0.7785 - f1_score: 0.7688 - val_loss: 0.4079 - val_accuracy: 0.8264 - val_f1_score: 0.8328\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3880 - accuracy: 0.8282 - f1_score: 0.8245 - val_loss: 0.3566 - val_accuracy: 0.8599 - val_f1_score: 0.8592\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3488 - accuracy: 0.8535 - f1_score: 0.8516 - val_loss: 0.3680 - val_accuracy: 0.8526 - val_f1_score: 0.8603\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3552 - accuracy: 0.8496 - f1_score: 0.8460 - val_loss: 0.5008 - val_accuracy: 0.7495 - val_f1_score: 0.7934\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3255 - accuracy: 0.8656 - f1_score: 0.8643 - val_loss: 0.5145 - val_accuracy: 0.7333 - val_f1_score: 0.7848\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3114 - accuracy: 0.8767 - f1_score: 0.8763 - val_loss: 0.3305 - val_accuracy: 0.8752 - val_f1_score: 0.8741\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2858 - accuracy: 0.8840 - f1_score: 0.8835 - val_loss: 0.3630 - val_accuracy: 0.8626 - val_f1_score: 0.8714\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2849 - accuracy: 0.8888 - f1_score: 0.8877 - val_loss: 0.3244 - val_accuracy: 0.8770 - val_f1_score: 0.8803\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2730 - accuracy: 0.8882 - f1_score: 0.8868 - val_loss: 0.3390 - val_accuracy: 0.8716 - val_f1_score: 0.8746\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2678 - accuracy: 0.8957 - f1_score: 0.8946 - val_loss: 0.3460 - val_accuracy: 0.8662 - val_f1_score: 0.8697\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2464 - accuracy: 0.9063 - f1_score: 0.9061 - val_loss: 0.3504 - val_accuracy: 0.8743 - val_f1_score: 0.8695\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2411 - accuracy: 0.9054 - f1_score: 0.9048 - val_loss: 0.3704 - val_accuracy: 0.8743 - val_f1_score: 0.8726\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2469 - accuracy: 0.9017 - f1_score: 0.9004 - val_loss: 0.3577 - val_accuracy: 0.8779 - val_f1_score: 0.8756\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3251 - accuracy: 0.8629 - f1_score: 0.8867\n","Epoch 1/20\n","104/104 [==============================] - 5s 15ms/step - loss: 0.6210 - accuracy: 0.6564 - f1_score: 0.6351 - val_loss: 0.5019 - val_accuracy: 0.7613 - val_f1_score: 0.7371\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4623 - accuracy: 0.7881 - f1_score: 0.7786 - val_loss: 0.4167 - val_accuracy: 0.8128 - val_f1_score: 0.8067\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4022 - accuracy: 0.8243 - f1_score: 0.8196 - val_loss: 0.4097 - val_accuracy: 0.8128 - val_f1_score: 0.7911\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3583 - accuracy: 0.8487 - f1_score: 0.8450 - val_loss: 0.3454 - val_accuracy: 0.8689 - val_f1_score: 0.8681\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3248 - accuracy: 0.8653 - f1_score: 0.8630 - val_loss: 0.3668 - val_accuracy: 0.8463 - val_f1_score: 0.8320\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3350 - accuracy: 0.8611 - f1_score: 0.8575 - val_loss: 0.3371 - val_accuracy: 0.8553 - val_f1_score: 0.8464\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2964 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.3906 - val_accuracy: 0.8363 - val_f1_score: 0.8177\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2935 - accuracy: 0.8797 - f1_score: 0.8781 - val_loss: 0.3621 - val_accuracy: 0.8644 - val_f1_score: 0.8707\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2910 - accuracy: 0.8779 - f1_score: 0.8749 - val_loss: 0.3991 - val_accuracy: 0.8336 - val_f1_score: 0.8499\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2790 - accuracy: 0.8927 - f1_score: 0.8926 - val_loss: 0.3305 - val_accuracy: 0.8635 - val_f1_score: 0.8561\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2569 - accuracy: 0.8984 - f1_score: 0.8963 - val_loss: 0.3252 - val_accuracy: 0.8752 - val_f1_score: 0.8710\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2405 - accuracy: 0.9030 - f1_score: 0.9016 - val_loss: 0.3542 - val_accuracy: 0.8662 - val_f1_score: 0.8729\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2389 - accuracy: 0.9054 - f1_score: 0.9034 - val_loss: 0.3385 - val_accuracy: 0.8698 - val_f1_score: 0.8644\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2300 - accuracy: 0.9102 - f1_score: 0.9090 - val_loss: 0.3793 - val_accuracy: 0.8734 - val_f1_score: 0.8664\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2281 - accuracy: 0.9093 - f1_score: 0.9085 - val_loss: 0.3706 - val_accuracy: 0.8653 - val_f1_score: 0.8563\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1979 - accuracy: 0.9256 - f1_score: 0.9247 - val_loss: 0.3720 - val_accuracy: 0.8626 - val_f1_score: 0.8550\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3252 - accuracy: 0.8636 - f1_score: 0.8855\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6400 - accuracy: 0.6287 - f1_score: 0.6198 - val_loss: 0.5553 - val_accuracy: 0.7423 - val_f1_score: 0.6945\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5086 - accuracy: 0.7601 - f1_score: 0.7505 - val_loss: 0.4099 - val_accuracy: 0.8165 - val_f1_score: 0.8215\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4221 - accuracy: 0.8149 - f1_score: 0.8059 - val_loss: 0.3680 - val_accuracy: 0.8409 - val_f1_score: 0.8370\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3742 - accuracy: 0.8478 - f1_score: 0.8446 - val_loss: 0.3502 - val_accuracy: 0.8571 - val_f1_score: 0.8599\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3342 - accuracy: 0.8650 - f1_score: 0.8628 - val_loss: 0.3243 - val_accuracy: 0.8752 - val_f1_score: 0.8725\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3071 - accuracy: 0.8767 - f1_score: 0.8750 - val_loss: 0.3228 - val_accuracy: 0.8626 - val_f1_score: 0.8582\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3122 - accuracy: 0.8710 - f1_score: 0.8689 - val_loss: 0.3185 - val_accuracy: 0.8599 - val_f1_score: 0.8520\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2947 - accuracy: 0.8770 - f1_score: 0.8758 - val_loss: 0.3325 - val_accuracy: 0.8671 - val_f1_score: 0.8591\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2605 - accuracy: 0.8945 - f1_score: 0.8937 - val_loss: 0.3237 - val_accuracy: 0.8635 - val_f1_score: 0.8563\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2582 - accuracy: 0.8927 - f1_score: 0.8917 - val_loss: 0.3319 - val_accuracy: 0.8580 - val_f1_score: 0.8483\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2438 - accuracy: 0.9030 - f1_score: 0.9012 - val_loss: 0.3316 - val_accuracy: 0.8653 - val_f1_score: 0.8580\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2497 - accuracy: 0.9060 - f1_score: 0.9046 - val_loss: 0.3825 - val_accuracy: 0.8318 - val_f1_score: 0.8058\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3669 - accuracy: 0.8440 - f1_score: 0.8645\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yI3GqPzmRM_e","executionInfo":{"status":"ok","timestamp":1689806293779,"user_tz":-330,"elapsed":33,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"29c35c6c-cf0a-4ee2-84b5-c336b271237a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8487508296966553, 0.8669817447662354, 0.8305199146270752, 0.8588791489601135, 0.8372721076011658, 0.8494260907173157, 0.8541526198387146, 0.8507764935493469, 0.8622552156448364, 0.8561782836914062, 0.8602295517921448, 0.8595543503761292, 0.8575286865234375, 0.8541526198387146, 0.8656313419342041, 0.8514516949653625, 0.8507764935493469, 0.8636056780815125, 0.852126955986023, 0.8413234353065491, 0.8649561405181885, 0.8514516949653625, 0.8629304766654968, 0.8636056780815125, 0.8440243005752563]\n","0.8543416619300842\n","0.009035881933187433\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aqd38xsiRNB9","executionInfo":{"status":"ok","timestamp":1689806293779,"user_tz":-330,"elapsed":18,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7ed70148-f3b5-48ed-8aec-755647d73ef3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.36129820346832275, 0.3240622878074646, 0.3888685405254364, 0.32488468289375305, 0.36939942836761475, 0.33609312772750854, 0.3423801064491272, 0.3455919325351715, 0.33784565329551697, 0.3341531455516815, 0.316448837518692, 0.3199617862701416, 0.3246786892414093, 0.32981300354003906, 0.32619139552116394, 0.35084080696105957, 0.36778759956359863, 0.3226262331008911, 0.35182422399520874, 0.35341188311576843, 0.33184006810188293, 0.36055684089660645, 0.32507044076919556, 0.3252338767051697, 0.36691808700561523]\n","0.34151123523712157\n","0.018840131043923902\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DMPs_LYMRNEj","executionInfo":{"status":"ok","timestamp":1689806293779,"user_tz":-330,"elapsed":15,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d20539b5-1ada-4495-b93c-3ef4409bf5bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8683901429176331, 0.8916987180709839, 0.8510385155677795, 0.8843386173248291, 0.8576490879058838, 0.8771349191665649, 0.8849839568138123, 0.8772902488708496, 0.8871680498123169, 0.8790459632873535, 0.887561023235321, 0.8881719708442688, 0.883489727973938, 0.8758620023727417, 0.8901159167289734, 0.8757061958312988, 0.873786449432373, 0.8892543315887451, 0.8754973411560059, 0.8665530681610107, 0.8877664804458618, 0.8716452121734619, 0.8866554498672485, 0.8854874968528748, 0.864516019821167]\n","0.8784322762489318\n","0.010399400685910089\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_kRqUwQyQQXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GAJ9Y83dRROI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM CNN LSTM nothing"],"metadata":{"id":"V2S3CSpvRi3p"}},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = GlobalMaxPooling1D()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        merged_output = Dense(256, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0rH12II4RRTC","executionInfo":{"status":"ok","timestamp":1689808042852,"user_tz":-330,"elapsed":895846,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"b2e01fd6-5c83-4cd0-8bea-f3c209a8ef4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 16s 53ms/step - loss: 0.6360 - accuracy: 0.6353 - f1_score: 0.6020 - val_loss: 0.5428 - val_accuracy: 0.7269 - val_f1_score: 0.7688\n","Epoch 2/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.5161 - accuracy: 0.7508 - f1_score: 0.7273 - val_loss: 0.4215 - val_accuracy: 0.8119 - val_f1_score: 0.8045\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4389 - accuracy: 0.7984 - f1_score: 0.7854 - val_loss: 0.4105 - val_accuracy: 0.8264 - val_f1_score: 0.8431\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3931 - accuracy: 0.8294 - f1_score: 0.8233 - val_loss: 0.3803 - val_accuracy: 0.8291 - val_f1_score: 0.8475\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3721 - accuracy: 0.8363 - f1_score: 0.8351 - val_loss: 0.3315 - val_accuracy: 0.8535 - val_f1_score: 0.8503\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3372 - accuracy: 0.8662 - f1_score: 0.8638 - val_loss: 0.3081 - val_accuracy: 0.8680 - val_f1_score: 0.8641\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3354 - accuracy: 0.8626 - f1_score: 0.8606 - val_loss: 0.3217 - val_accuracy: 0.8454 - val_f1_score: 0.8315\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2983 - accuracy: 0.8752 - f1_score: 0.8733 - val_loss: 0.3126 - val_accuracy: 0.8590 - val_f1_score: 0.8500\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2926 - accuracy: 0.8797 - f1_score: 0.8769 - val_loss: 0.3061 - val_accuracy: 0.8671 - val_f1_score: 0.8622\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2971 - accuracy: 0.8861 - f1_score: 0.8846 - val_loss: 0.3188 - val_accuracy: 0.8743 - val_f1_score: 0.8780\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2690 - accuracy: 0.8966 - f1_score: 0.8938 - val_loss: 0.3526 - val_accuracy: 0.8580 - val_f1_score: 0.8673\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2659 - accuracy: 0.8921 - f1_score: 0.8898 - val_loss: 0.3258 - val_accuracy: 0.8635 - val_f1_score: 0.8544\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2539 - accuracy: 0.9017 - f1_score: 0.9002 - val_loss: 0.3595 - val_accuracy: 0.8553 - val_f1_score: 0.8422\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2322 - accuracy: 0.9093 - f1_score: 0.9075 - val_loss: 0.3429 - val_accuracy: 0.8734 - val_f1_score: 0.8746\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3711 - accuracy: 0.8433 - f1_score: 0.8638\n","Epoch 1/20\n","104/104 [==============================] - 9s 23ms/step - loss: 0.6369 - accuracy: 0.6242 - f1_score: 0.6075 - val_loss: 0.5551 - val_accuracy: 0.7288 - val_f1_score: 0.7278\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.5105 - accuracy: 0.7514 - f1_score: 0.7341 - val_loss: 0.4717 - val_accuracy: 0.7712 - val_f1_score: 0.7373\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4209 - accuracy: 0.8098 - f1_score: 0.7996 - val_loss: 0.4150 - val_accuracy: 0.8210 - val_f1_score: 0.8278\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3716 - accuracy: 0.8382 - f1_score: 0.8347 - val_loss: 0.3808 - val_accuracy: 0.8273 - val_f1_score: 0.8320\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3429 - accuracy: 0.8505 - f1_score: 0.8487 - val_loss: 0.4018 - val_accuracy: 0.8463 - val_f1_score: 0.8327\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3308 - accuracy: 0.8656 - f1_score: 0.8651 - val_loss: 0.3551 - val_accuracy: 0.8535 - val_f1_score: 0.8533\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3083 - accuracy: 0.8689 - f1_score: 0.8688 - val_loss: 0.3623 - val_accuracy: 0.8590 - val_f1_score: 0.8632\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3043 - accuracy: 0.8707 - f1_score: 0.8707 - val_loss: 0.3644 - val_accuracy: 0.8508 - val_f1_score: 0.8482\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2844 - accuracy: 0.8800 - f1_score: 0.8799 - val_loss: 0.3833 - val_accuracy: 0.8445 - val_f1_score: 0.8330\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2802 - accuracy: 0.8813 - f1_score: 0.8800 - val_loss: 0.3575 - val_accuracy: 0.8653 - val_f1_score: 0.8642\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2586 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.4680 - val_accuracy: 0.8282 - val_f1_score: 0.8419\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3237 - accuracy: 0.8616 - f1_score: 0.8846\n","Epoch 1/20\n","104/104 [==============================] - 9s 27ms/step - loss: 0.6136 - accuracy: 0.6661 - f1_score: 0.6544 - val_loss: 0.5306 - val_accuracy: 0.7414 - val_f1_score: 0.6983\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4868 - accuracy: 0.7767 - f1_score: 0.7705 - val_loss: 0.4489 - val_accuracy: 0.7830 - val_f1_score: 0.7865\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4051 - accuracy: 0.8255 - f1_score: 0.8191 - val_loss: 0.4439 - val_accuracy: 0.7966 - val_f1_score: 0.8133\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3689 - accuracy: 0.8454 - f1_score: 0.8433 - val_loss: 0.4656 - val_accuracy: 0.7857 - val_f1_score: 0.7421\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3269 - accuracy: 0.8686 - f1_score: 0.8663 - val_loss: 0.3941 - val_accuracy: 0.8255 - val_f1_score: 0.8080\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3129 - accuracy: 0.8686 - f1_score: 0.8663 - val_loss: 0.4242 - val_accuracy: 0.7920 - val_f1_score: 0.8124\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3022 - accuracy: 0.8734 - f1_score: 0.8708 - val_loss: 0.4008 - val_accuracy: 0.8345 - val_f1_score: 0.8288\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2775 - accuracy: 0.8846 - f1_score: 0.8824 - val_loss: 0.4088 - val_accuracy: 0.8336 - val_f1_score: 0.8241\n","Epoch 9/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2687 - accuracy: 0.8936 - f1_score: 0.8914 - val_loss: 0.4614 - val_accuracy: 0.8264 - val_f1_score: 0.8110\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2917 - accuracy: 0.8761 - f1_score: 0.8736 - val_loss: 0.4695 - val_accuracy: 0.8237 - val_f1_score: 0.8040\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3843 - accuracy: 0.8285 - f1_score: 0.8481\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.6343 - accuracy: 0.6414 - f1_score: 0.6332 - val_loss: 0.5164 - val_accuracy: 0.7649 - val_f1_score: 0.7785\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4878 - accuracy: 0.7731 - f1_score: 0.7616 - val_loss: 0.3991 - val_accuracy: 0.8255 - val_f1_score: 0.8188\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3951 - accuracy: 0.8186 - f1_score: 0.8145 - val_loss: 0.3542 - val_accuracy: 0.8499 - val_f1_score: 0.8502\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3512 - accuracy: 0.8502 - f1_score: 0.8464 - val_loss: 0.4209 - val_accuracy: 0.8246 - val_f1_score: 0.8410\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3288 - accuracy: 0.8647 - f1_score: 0.8636 - val_loss: 0.4063 - val_accuracy: 0.8345 - val_f1_score: 0.8481\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3159 - accuracy: 0.8695 - f1_score: 0.8670 - val_loss: 0.4360 - val_accuracy: 0.8128 - val_f1_score: 0.8343\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2957 - accuracy: 0.8800 - f1_score: 0.8782 - val_loss: 0.3889 - val_accuracy: 0.8318 - val_f1_score: 0.8470\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2906 - accuracy: 0.8861 - f1_score: 0.8848 - val_loss: 0.3468 - val_accuracy: 0.8590 - val_f1_score: 0.8622\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2872 - accuracy: 0.8909 - f1_score: 0.8890 - val_loss: 0.3886 - val_accuracy: 0.8617 - val_f1_score: 0.8539\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2675 - accuracy: 0.8987 - f1_score: 0.8984 - val_loss: 0.3653 - val_accuracy: 0.8689 - val_f1_score: 0.8688\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2660 - accuracy: 0.8933 - f1_score: 0.8918 - val_loss: 0.5629 - val_accuracy: 0.8092 - val_f1_score: 0.8319\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2478 - accuracy: 0.9051 - f1_score: 0.9041 - val_loss: 0.4024 - val_accuracy: 0.8580 - val_f1_score: 0.8624\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2425 - accuracy: 0.9066 - f1_score: 0.9051 - val_loss: 0.4487 - val_accuracy: 0.8309 - val_f1_score: 0.8463\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3527 - accuracy: 0.8528 - f1_score: 0.8743\n","Epoch 1/20\n","104/104 [==============================] - 8s 23ms/step - loss: 0.6464 - accuracy: 0.6172 - f1_score: 0.6207 - val_loss: 0.5580 - val_accuracy: 0.7233 - val_f1_score: 0.7282\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.5249 - accuracy: 0.7438 - f1_score: 0.7272 - val_loss: 0.4759 - val_accuracy: 0.7857 - val_f1_score: 0.7990\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4471 - accuracy: 0.7972 - f1_score: 0.7867 - val_loss: 0.3819 - val_accuracy: 0.8363 - val_f1_score: 0.8380\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3834 - accuracy: 0.8363 - f1_score: 0.8321 - val_loss: 0.4036 - val_accuracy: 0.8219 - val_f1_score: 0.8384\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3550 - accuracy: 0.8502 - f1_score: 0.8478 - val_loss: 0.3529 - val_accuracy: 0.8526 - val_f1_score: 0.8452\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3543 - accuracy: 0.8454 - f1_score: 0.8405 - val_loss: 0.3819 - val_accuracy: 0.8391 - val_f1_score: 0.8494\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3176 - accuracy: 0.8623 - f1_score: 0.8595 - val_loss: 0.3650 - val_accuracy: 0.8409 - val_f1_score: 0.8511\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2937 - accuracy: 0.8819 - f1_score: 0.8800 - val_loss: 0.3847 - val_accuracy: 0.8282 - val_f1_score: 0.8119\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2934 - accuracy: 0.8731 - f1_score: 0.8711 - val_loss: 0.3262 - val_accuracy: 0.8716 - val_f1_score: 0.8721\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3029 - accuracy: 0.8707 - f1_score: 0.8690 - val_loss: 0.3457 - val_accuracy: 0.8644 - val_f1_score: 0.8601\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2634 - accuracy: 0.8948 - f1_score: 0.8935 - val_loss: 0.3625 - val_accuracy: 0.8689 - val_f1_score: 0.8738\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2583 - accuracy: 0.8957 - f1_score: 0.8933 - val_loss: 0.3850 - val_accuracy: 0.8626 - val_f1_score: 0.8696\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2531 - accuracy: 0.8978 - f1_score: 0.8964 - val_loss: 0.4016 - val_accuracy: 0.8608 - val_f1_score: 0.8672\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2394 - accuracy: 0.9093 - f1_score: 0.9074 - val_loss: 0.3758 - val_accuracy: 0.8599 - val_f1_score: 0.8665\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3410 - accuracy: 0.8616 - f1_score: 0.8841\n","Epoch 1/20\n","104/104 [==============================] - 8s 23ms/step - loss: 0.6508 - accuracy: 0.6190 - f1_score: 0.5698 - val_loss: 0.5227 - val_accuracy: 0.7423 - val_f1_score: 0.7283\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4880 - accuracy: 0.7734 - f1_score: 0.7690 - val_loss: 0.4510 - val_accuracy: 0.7929 - val_f1_score: 0.8187\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3975 - accuracy: 0.8276 - f1_score: 0.8280 - val_loss: 0.3628 - val_accuracy: 0.8535 - val_f1_score: 0.8412\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3541 - accuracy: 0.8559 - f1_score: 0.8548 - val_loss: 0.3753 - val_accuracy: 0.8156 - val_f1_score: 0.8363\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3283 - accuracy: 0.8614 - f1_score: 0.8634 - val_loss: 0.3036 - val_accuracy: 0.8825 - val_f1_score: 0.8839\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3414 - accuracy: 0.8469 - f1_score: 0.8476 - val_loss: 0.3062 - val_accuracy: 0.8707 - val_f1_score: 0.8768\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3069 - accuracy: 0.8731 - f1_score: 0.8732 - val_loss: 0.3084 - val_accuracy: 0.8716 - val_f1_score: 0.8780\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3008 - accuracy: 0.8794 - f1_score: 0.8792 - val_loss: 0.2900 - val_accuracy: 0.8834 - val_f1_score: 0.8847\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2973 - accuracy: 0.8716 - f1_score: 0.8708 - val_loss: 0.3091 - val_accuracy: 0.8716 - val_f1_score: 0.8784\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2835 - accuracy: 0.8785 - f1_score: 0.8786 - val_loss: 0.3122 - val_accuracy: 0.8689 - val_f1_score: 0.8776\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2612 - accuracy: 0.8945 - f1_score: 0.8950 - val_loss: 0.2890 - val_accuracy: 0.8743 - val_f1_score: 0.8786\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2737 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.3211 - val_accuracy: 0.8680 - val_f1_score: 0.8765\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2320 - accuracy: 0.9108 - f1_score: 0.9106 - val_loss: 0.3027 - val_accuracy: 0.8779 - val_f1_score: 0.8698\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2219 - accuracy: 0.9102 - f1_score: 0.9093 - val_loss: 0.2950 - val_accuracy: 0.8734 - val_f1_score: 0.8739\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2179 - accuracy: 0.9144 - f1_score: 0.9140 - val_loss: 0.3381 - val_accuracy: 0.8689 - val_f1_score: 0.8599\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2250 - accuracy: 0.9075 - f1_score: 0.9078 - val_loss: 0.3349 - val_accuracy: 0.8436 - val_f1_score: 0.8557\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3516 - accuracy: 0.8589 - f1_score: 0.8877\n","Epoch 1/20\n","104/104 [==============================] - 10s 33ms/step - loss: 0.6218 - accuracy: 0.6627 - f1_score: 0.6539 - val_loss: 0.5085 - val_accuracy: 0.7586 - val_f1_score: 0.7267\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4809 - accuracy: 0.7830 - f1_score: 0.7714 - val_loss: 0.4123 - val_accuracy: 0.8128 - val_f1_score: 0.8153\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3994 - accuracy: 0.8204 - f1_score: 0.8158 - val_loss: 0.3781 - val_accuracy: 0.8246 - val_f1_score: 0.8375\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3426 - accuracy: 0.8556 - f1_score: 0.8541 - val_loss: 0.3228 - val_accuracy: 0.8662 - val_f1_score: 0.8659\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3158 - accuracy: 0.8710 - f1_score: 0.8685 - val_loss: 0.3695 - val_accuracy: 0.8409 - val_f1_score: 0.8550\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2959 - accuracy: 0.8770 - f1_score: 0.8766 - val_loss: 0.3008 - val_accuracy: 0.8743 - val_f1_score: 0.8721\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3030 - accuracy: 0.8746 - f1_score: 0.8739 - val_loss: 0.3094 - val_accuracy: 0.8662 - val_f1_score: 0.8625\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2770 - accuracy: 0.8828 - f1_score: 0.8817 - val_loss: 0.3532 - val_accuracy: 0.8608 - val_f1_score: 0.8706\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2810 - accuracy: 0.8840 - f1_score: 0.8832 - val_loss: 0.3073 - val_accuracy: 0.8662 - val_f1_score: 0.8574\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2706 - accuracy: 0.8930 - f1_score: 0.8913 - val_loss: 0.3379 - val_accuracy: 0.8617 - val_f1_score: 0.8715\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2457 - accuracy: 0.9057 - f1_score: 0.9050 - val_loss: 0.3209 - val_accuracy: 0.8716 - val_f1_score: 0.8741\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3448 - accuracy: 0.8508 - f1_score: 0.8744\n","Epoch 1/20\n","104/104 [==============================] - 9s 37ms/step - loss: 0.6282 - accuracy: 0.6531 - f1_score: 0.6473 - val_loss: 0.5448 - val_accuracy: 0.7360 - val_f1_score: 0.6887\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4920 - accuracy: 0.7788 - f1_score: 0.7695 - val_loss: 0.4982 - val_accuracy: 0.7667 - val_f1_score: 0.7949\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4065 - accuracy: 0.8225 - f1_score: 0.8187 - val_loss: 0.4037 - val_accuracy: 0.8318 - val_f1_score: 0.8418\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3560 - accuracy: 0.8502 - f1_score: 0.8480 - val_loss: 0.3517 - val_accuracy: 0.8571 - val_f1_score: 0.8564\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3362 - accuracy: 0.8568 - f1_score: 0.8546 - val_loss: 0.3726 - val_accuracy: 0.8363 - val_f1_score: 0.8177\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3245 - accuracy: 0.8629 - f1_score: 0.8602 - val_loss: 0.3647 - val_accuracy: 0.8363 - val_f1_score: 0.8498\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2855 - accuracy: 0.8822 - f1_score: 0.8818 - val_loss: 0.3402 - val_accuracy: 0.8725 - val_f1_score: 0.8760\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2776 - accuracy: 0.8882 - f1_score: 0.8871 - val_loss: 0.3387 - val_accuracy: 0.8761 - val_f1_score: 0.8758\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2653 - accuracy: 0.8963 - f1_score: 0.8956 - val_loss: 0.3357 - val_accuracy: 0.8752 - val_f1_score: 0.8766\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2576 - accuracy: 0.8960 - f1_score: 0.8950 - val_loss: 0.3575 - val_accuracy: 0.8571 - val_f1_score: 0.8640\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2357 - accuracy: 0.9066 - f1_score: 0.9059 - val_loss: 0.3366 - val_accuracy: 0.8662 - val_f1_score: 0.8659\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2385 - accuracy: 0.9042 - f1_score: 0.9044 - val_loss: 0.3508 - val_accuracy: 0.8716 - val_f1_score: 0.8723\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2282 - accuracy: 0.9105 - f1_score: 0.9099 - val_loss: 0.3435 - val_accuracy: 0.8797 - val_f1_score: 0.8796\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2009 - accuracy: 0.9216 - f1_score: 0.9212 - val_loss: 0.4587 - val_accuracy: 0.8291 - val_f1_score: 0.8434\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3567 - accuracy: 0.8575 - f1_score: 0.8827\n","Epoch 1/20\n","104/104 [==============================] - 9s 35ms/step - loss: 0.6346 - accuracy: 0.6407 - f1_score: 0.6280 - val_loss: 0.5120 - val_accuracy: 0.7622 - val_f1_score: 0.7373\n","Epoch 2/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.5030 - accuracy: 0.7634 - f1_score: 0.7424 - val_loss: 0.4355 - val_accuracy: 0.8047 - val_f1_score: 0.7939\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4312 - accuracy: 0.8029 - f1_score: 0.7924 - val_loss: 0.3909 - val_accuracy: 0.8273 - val_f1_score: 0.8172\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3892 - accuracy: 0.8327 - f1_score: 0.8272 - val_loss: 0.4028 - val_accuracy: 0.8219 - val_f1_score: 0.8400\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3458 - accuracy: 0.8617 - f1_score: 0.8604 - val_loss: 0.3229 - val_accuracy: 0.8698 - val_f1_score: 0.8712\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3173 - accuracy: 0.8656 - f1_score: 0.8637 - val_loss: 0.3245 - val_accuracy: 0.8770 - val_f1_score: 0.8729\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3098 - accuracy: 0.8704 - f1_score: 0.8690 - val_loss: 0.3565 - val_accuracy: 0.8454 - val_f1_score: 0.8257\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2933 - accuracy: 0.8797 - f1_score: 0.8787 - val_loss: 0.3107 - val_accuracy: 0.8825 - val_f1_score: 0.8810\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2828 - accuracy: 0.8876 - f1_score: 0.8868 - val_loss: 0.3053 - val_accuracy: 0.8734 - val_f1_score: 0.8770\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2661 - accuracy: 0.8894 - f1_score: 0.8889 - val_loss: 0.3011 - val_accuracy: 0.8834 - val_f1_score: 0.8800\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2430 - accuracy: 0.9005 - f1_score: 0.8993 - val_loss: 0.3083 - val_accuracy: 0.8897 - val_f1_score: 0.8887\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2422 - accuracy: 0.8993 - f1_score: 0.8979 - val_loss: 0.3146 - val_accuracy: 0.8816 - val_f1_score: 0.8825\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2321 - accuracy: 0.9036 - f1_score: 0.9024 - val_loss: 0.3800 - val_accuracy: 0.8354 - val_f1_score: 0.8491\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2295 - accuracy: 0.9102 - f1_score: 0.9095 - val_loss: 0.4117 - val_accuracy: 0.8363 - val_f1_score: 0.8473\n","Epoch 15/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2059 - accuracy: 0.9204 - f1_score: 0.9199 - val_loss: 0.4146 - val_accuracy: 0.8698 - val_f1_score: 0.8629\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3477 - accuracy: 0.8670 - f1_score: 0.8909\n","Epoch 1/20\n","104/104 [==============================] - 10s 24ms/step - loss: 0.6352 - accuracy: 0.6335 - f1_score: 0.6302 - val_loss: 0.5115 - val_accuracy: 0.7649 - val_f1_score: 0.7601\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4826 - accuracy: 0.7743 - f1_score: 0.7656 - val_loss: 0.4982 - val_accuracy: 0.7640 - val_f1_score: 0.7097\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4069 - accuracy: 0.8237 - f1_score: 0.8187 - val_loss: 0.3738 - val_accuracy: 0.8255 - val_f1_score: 0.8181\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3585 - accuracy: 0.8493 - f1_score: 0.8487 - val_loss: 0.4588 - val_accuracy: 0.8101 - val_f1_score: 0.7766\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3453 - accuracy: 0.8532 - f1_score: 0.8519 - val_loss: 0.3376 - val_accuracy: 0.8499 - val_f1_score: 0.8544\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3010 - accuracy: 0.8822 - f1_score: 0.8814 - val_loss: 0.3381 - val_accuracy: 0.8517 - val_f1_score: 0.8581\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2896 - accuracy: 0.8879 - f1_score: 0.8881 - val_loss: 0.3648 - val_accuracy: 0.8544 - val_f1_score: 0.8392\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2953 - accuracy: 0.8803 - f1_score: 0.8794 - val_loss: 0.3603 - val_accuracy: 0.8228 - val_f1_score: 0.8388\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2746 - accuracy: 0.8918 - f1_score: 0.8916 - val_loss: 0.3475 - val_accuracy: 0.8617 - val_f1_score: 0.8654\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2550 - accuracy: 0.9011 - f1_score: 0.9014 - val_loss: 0.3304 - val_accuracy: 0.8562 - val_f1_score: 0.8437\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2577 - accuracy: 0.8957 - f1_score: 0.8955 - val_loss: 0.3248 - val_accuracy: 0.8635 - val_f1_score: 0.8585\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2400 - accuracy: 0.9063 - f1_score: 0.9062 - val_loss: 0.3223 - val_accuracy: 0.8689 - val_f1_score: 0.8676\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2303 - accuracy: 0.9060 - f1_score: 0.9049 - val_loss: 0.3535 - val_accuracy: 0.8544 - val_f1_score: 0.8586\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2135 - accuracy: 0.9162 - f1_score: 0.9163 - val_loss: 0.3363 - val_accuracy: 0.8689 - val_f1_score: 0.8654\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2286 - accuracy: 0.9129 - f1_score: 0.9121 - val_loss: 0.3841 - val_accuracy: 0.8327 - val_f1_score: 0.8482\n","Epoch 16/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2344 - accuracy: 0.9063 - f1_score: 0.9067 - val_loss: 0.3351 - val_accuracy: 0.8626 - val_f1_score: 0.8510\n","Epoch 17/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.1978 - accuracy: 0.9195 - f1_score: 0.9188 - val_loss: 0.3362 - val_accuracy: 0.8698 - val_f1_score: 0.8672\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3631 - accuracy: 0.8683 - f1_score: 0.8929\n","Epoch 1/20\n","104/104 [==============================] - 11s 37ms/step - loss: 0.6434 - accuracy: 0.6287 - f1_score: 0.6074 - val_loss: 0.5681 - val_accuracy: 0.7152 - val_f1_score: 0.6316\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4967 - accuracy: 0.7655 - f1_score: 0.7500 - val_loss: 0.4718 - val_accuracy: 0.7821 - val_f1_score: 0.7411\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3914 - accuracy: 0.8198 - f1_score: 0.8138 - val_loss: 0.4415 - val_accuracy: 0.8011 - val_f1_score: 0.8232\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3455 - accuracy: 0.8526 - f1_score: 0.8508 - val_loss: 0.3694 - val_accuracy: 0.8517 - val_f1_score: 0.8411\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3129 - accuracy: 0.8761 - f1_score: 0.8748 - val_loss: 0.3700 - val_accuracy: 0.8418 - val_f1_score: 0.8518\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3002 - accuracy: 0.8776 - f1_score: 0.8764 - val_loss: 0.3476 - val_accuracy: 0.8608 - val_f1_score: 0.8508\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2890 - accuracy: 0.8800 - f1_score: 0.8789 - val_loss: 0.3625 - val_accuracy: 0.8689 - val_f1_score: 0.8707\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2786 - accuracy: 0.8864 - f1_score: 0.8860 - val_loss: 0.3420 - val_accuracy: 0.8680 - val_f1_score: 0.8628\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2757 - accuracy: 0.8897 - f1_score: 0.8886 - val_loss: 0.4122 - val_accuracy: 0.8092 - val_f1_score: 0.8308\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2593 - accuracy: 0.8924 - f1_score: 0.8920 - val_loss: 0.3999 - val_accuracy: 0.8445 - val_f1_score: 0.8535\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2523 - accuracy: 0.8966 - f1_score: 0.8960 - val_loss: 0.3647 - val_accuracy: 0.8662 - val_f1_score: 0.8671\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2476 - accuracy: 0.9017 - f1_score: 0.9008 - val_loss: 0.3551 - val_accuracy: 0.8752 - val_f1_score: 0.8741\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2289 - accuracy: 0.9105 - f1_score: 0.9100 - val_loss: 0.3810 - val_accuracy: 0.8608 - val_f1_score: 0.8635\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3267 - accuracy: 0.8602 - f1_score: 0.8866\n","Epoch 1/20\n","104/104 [==============================] - 10s 24ms/step - loss: 0.6351 - accuracy: 0.6377 - f1_score: 0.6351 - val_loss: 0.6131 - val_accuracy: 0.6808 - val_f1_score: 0.5480\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.5060 - accuracy: 0.7520 - f1_score: 0.7401 - val_loss: 0.4213 - val_accuracy: 0.8083 - val_f1_score: 0.8157\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4067 - accuracy: 0.8171 - f1_score: 0.8149 - val_loss: 0.4247 - val_accuracy: 0.8056 - val_f1_score: 0.7710\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3665 - accuracy: 0.8369 - f1_score: 0.8365 - val_loss: 0.3418 - val_accuracy: 0.8580 - val_f1_score: 0.8597\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3488 - accuracy: 0.8511 - f1_score: 0.8472 - val_loss: 0.3535 - val_accuracy: 0.8490 - val_f1_score: 0.8562\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3162 - accuracy: 0.8773 - f1_score: 0.8756 - val_loss: 0.3318 - val_accuracy: 0.8680 - val_f1_score: 0.8678\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3068 - accuracy: 0.8749 - f1_score: 0.8745 - val_loss: 0.3526 - val_accuracy: 0.8580 - val_f1_score: 0.8441\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3133 - accuracy: 0.8710 - f1_score: 0.8689 - val_loss: 0.3193 - val_accuracy: 0.8662 - val_f1_score: 0.8674\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2855 - accuracy: 0.8816 - f1_score: 0.8801 - val_loss: 0.3492 - val_accuracy: 0.8698 - val_f1_score: 0.8602\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2822 - accuracy: 0.8858 - f1_score: 0.8842 - val_loss: 0.3116 - val_accuracy: 0.8734 - val_f1_score: 0.8716\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2567 - accuracy: 0.8981 - f1_score: 0.8966 - val_loss: 0.3332 - val_accuracy: 0.8743 - val_f1_score: 0.8639\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2666 - accuracy: 0.8939 - f1_score: 0.8933 - val_loss: 0.3267 - val_accuracy: 0.8689 - val_f1_score: 0.8588\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2435 - accuracy: 0.9008 - f1_score: 0.8997 - val_loss: 0.3342 - val_accuracy: 0.8779 - val_f1_score: 0.8758\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2419 - accuracy: 0.9005 - f1_score: 0.9002 - val_loss: 0.3629 - val_accuracy: 0.8580 - val_f1_score: 0.8432\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2342 - accuracy: 0.9057 - f1_score: 0.9044 - val_loss: 0.3354 - val_accuracy: 0.8698 - val_f1_score: 0.8644\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3272 - accuracy: 0.8656 - f1_score: 0.8956\n","Epoch 1/20\n","104/104 [==============================] - 9s 27ms/step - loss: 0.6456 - accuracy: 0.6172 - f1_score: 0.6016 - val_loss: 0.5535 - val_accuracy: 0.7233 - val_f1_score: 0.7339\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4945 - accuracy: 0.7613 - f1_score: 0.7445 - val_loss: 0.4148 - val_accuracy: 0.8074 - val_f1_score: 0.8004\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3979 - accuracy: 0.8267 - f1_score: 0.8198 - val_loss: 0.3868 - val_accuracy: 0.8409 - val_f1_score: 0.8488\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3642 - accuracy: 0.8345 - f1_score: 0.8323 - val_loss: 0.3614 - val_accuracy: 0.8535 - val_f1_score: 0.8576\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3149 - accuracy: 0.8650 - f1_score: 0.8617 - val_loss: 0.3837 - val_accuracy: 0.8264 - val_f1_score: 0.8413\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3029 - accuracy: 0.8734 - f1_score: 0.8718 - val_loss: 0.3720 - val_accuracy: 0.8599 - val_f1_score: 0.8617\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2903 - accuracy: 0.8770 - f1_score: 0.8758 - val_loss: 0.3631 - val_accuracy: 0.8635 - val_f1_score: 0.8638\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2862 - accuracy: 0.8831 - f1_score: 0.8822 - val_loss: 0.4237 - val_accuracy: 0.8382 - val_f1_score: 0.8505\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2696 - accuracy: 0.8891 - f1_score: 0.8875 - val_loss: 0.4227 - val_accuracy: 0.8409 - val_f1_score: 0.8485\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8616 - f1_score: 0.8903\n","Epoch 1/20\n","104/104 [==============================] - 11s 26ms/step - loss: 0.6217 - accuracy: 0.6462 - f1_score: 0.6442 - val_loss: 0.4947 - val_accuracy: 0.7731 - val_f1_score: 0.7570\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4795 - accuracy: 0.7797 - f1_score: 0.7653 - val_loss: 0.4244 - val_accuracy: 0.8210 - val_f1_score: 0.8293\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4201 - accuracy: 0.8089 - f1_score: 0.7991 - val_loss: 0.4283 - val_accuracy: 0.8038 - val_f1_score: 0.8287\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3549 - accuracy: 0.8526 - f1_score: 0.8500 - val_loss: 0.3369 - val_accuracy: 0.8635 - val_f1_score: 0.8629\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3326 - accuracy: 0.8614 - f1_score: 0.8593 - val_loss: 0.3271 - val_accuracy: 0.8707 - val_f1_score: 0.8687\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3296 - accuracy: 0.8626 - f1_score: 0.8586 - val_loss: 0.4490 - val_accuracy: 0.7785 - val_f1_score: 0.8117\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3120 - accuracy: 0.8725 - f1_score: 0.8704 - val_loss: 0.3374 - val_accuracy: 0.8517 - val_f1_score: 0.8598\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2932 - accuracy: 0.8822 - f1_score: 0.8800 - val_loss: 0.3712 - val_accuracy: 0.8436 - val_f1_score: 0.8555\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2855 - accuracy: 0.8849 - f1_score: 0.8833 - val_loss: 0.3594 - val_accuracy: 0.8400 - val_f1_score: 0.8519\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2718 - accuracy: 0.8915 - f1_score: 0.8900 - val_loss: 0.4104 - val_accuracy: 0.8237 - val_f1_score: 0.8426\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3527 - accuracy: 0.8474 - f1_score: 0.8713\n","Epoch 1/20\n","104/104 [==============================] - 9s 35ms/step - loss: 0.6264 - accuracy: 0.6579 - f1_score: 0.6326 - val_loss: 0.5613 - val_accuracy: 0.7251 - val_f1_score: 0.7587\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4975 - accuracy: 0.7640 - f1_score: 0.7488 - val_loss: 0.4318 - val_accuracy: 0.8020 - val_f1_score: 0.8004\n","Epoch 3/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4308 - accuracy: 0.8047 - f1_score: 0.8007 - val_loss: 0.4284 - val_accuracy: 0.7884 - val_f1_score: 0.8119\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3757 - accuracy: 0.8336 - f1_score: 0.8306 - val_loss: 0.3762 - val_accuracy: 0.8282 - val_f1_score: 0.8362\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3247 - accuracy: 0.8686 - f1_score: 0.8679 - val_loss: 0.3698 - val_accuracy: 0.8535 - val_f1_score: 0.8454\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3475 - accuracy: 0.8505 - f1_score: 0.8500 - val_loss: 0.3482 - val_accuracy: 0.8526 - val_f1_score: 0.8422\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3110 - accuracy: 0.8674 - f1_score: 0.8667 - val_loss: 0.3345 - val_accuracy: 0.8526 - val_f1_score: 0.8569\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2985 - accuracy: 0.8761 - f1_score: 0.8748 - val_loss: 0.3239 - val_accuracy: 0.8617 - val_f1_score: 0.8657\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2816 - accuracy: 0.8858 - f1_score: 0.8858 - val_loss: 0.4019 - val_accuracy: 0.8517 - val_f1_score: 0.8360\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2841 - accuracy: 0.8840 - f1_score: 0.8839 - val_loss: 0.4627 - val_accuracy: 0.8038 - val_f1_score: 0.8301\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2674 - accuracy: 0.8909 - f1_score: 0.8908 - val_loss: 0.3683 - val_accuracy: 0.8463 - val_f1_score: 0.8307\n","Epoch 12/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2585 - accuracy: 0.8972 - f1_score: 0.8965 - val_loss: 0.3279 - val_accuracy: 0.8617 - val_f1_score: 0.8566\n","Epoch 13/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2369 - accuracy: 0.9048 - f1_score: 0.9045 - val_loss: 0.3452 - val_accuracy: 0.8517 - val_f1_score: 0.8549\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8690 - f1_score: 0.8956\n","Epoch 1/20\n","104/104 [==============================] - 9s 24ms/step - loss: 0.6197 - accuracy: 0.6573 - f1_score: 0.6373 - val_loss: 0.5001 - val_accuracy: 0.7649 - val_f1_score: 0.7561\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4722 - accuracy: 0.7884 - f1_score: 0.7756 - val_loss: 0.4212 - val_accuracy: 0.8174 - val_f1_score: 0.8200\n","Epoch 3/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3946 - accuracy: 0.8297 - f1_score: 0.8249 - val_loss: 0.4109 - val_accuracy: 0.8065 - val_f1_score: 0.8260\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3611 - accuracy: 0.8478 - f1_score: 0.8463 - val_loss: 0.3792 - val_accuracy: 0.8354 - val_f1_score: 0.8476\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3361 - accuracy: 0.8511 - f1_score: 0.8506 - val_loss: 0.3409 - val_accuracy: 0.8535 - val_f1_score: 0.8489\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3012 - accuracy: 0.8734 - f1_score: 0.8720 - val_loss: 0.3757 - val_accuracy: 0.8436 - val_f1_score: 0.8545\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3060 - accuracy: 0.8749 - f1_score: 0.8737 - val_loss: 0.3970 - val_accuracy: 0.8165 - val_f1_score: 0.8382\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2736 - accuracy: 0.8951 - f1_score: 0.8938 - val_loss: 0.3442 - val_accuracy: 0.8635 - val_f1_score: 0.8636\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2677 - accuracy: 0.8936 - f1_score: 0.8921 - val_loss: 0.3318 - val_accuracy: 0.8635 - val_f1_score: 0.8672\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2583 - accuracy: 0.8900 - f1_score: 0.8898 - val_loss: 0.3352 - val_accuracy: 0.8671 - val_f1_score: 0.8691\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2527 - accuracy: 0.9024 - f1_score: 0.9018 - val_loss: 0.4727 - val_accuracy: 0.7920 - val_f1_score: 0.8212\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2360 - accuracy: 0.9057 - f1_score: 0.9052 - val_loss: 0.3461 - val_accuracy: 0.8743 - val_f1_score: 0.8762\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2266 - accuracy: 0.9108 - f1_score: 0.9105 - val_loss: 0.4121 - val_accuracy: 0.8454 - val_f1_score: 0.8576\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2186 - accuracy: 0.9150 - f1_score: 0.9144 - val_loss: 0.4127 - val_accuracy: 0.8653 - val_f1_score: 0.8606\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3397 - accuracy: 0.8643 - f1_score: 0.8900\n","Epoch 1/20\n","104/104 [==============================] - 9s 26ms/step - loss: 0.6176 - accuracy: 0.6531 - f1_score: 0.6594 - val_loss: 0.5143 - val_accuracy: 0.7577 - val_f1_score: 0.7519\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4548 - accuracy: 0.7878 - f1_score: 0.7775 - val_loss: 0.4343 - val_accuracy: 0.8101 - val_f1_score: 0.8250\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3897 - accuracy: 0.8336 - f1_score: 0.8300 - val_loss: 0.3489 - val_accuracy: 0.8499 - val_f1_score: 0.8483\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3276 - accuracy: 0.8614 - f1_score: 0.8592 - val_loss: 0.3240 - val_accuracy: 0.8671 - val_f1_score: 0.8640\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3259 - accuracy: 0.8626 - f1_score: 0.8611 - val_loss: 0.3454 - val_accuracy: 0.8463 - val_f1_score: 0.8303\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3196 - accuracy: 0.8668 - f1_score: 0.8647 - val_loss: 0.3611 - val_accuracy: 0.8409 - val_f1_score: 0.8543\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3129 - accuracy: 0.8692 - f1_score: 0.8690 - val_loss: 0.3111 - val_accuracy: 0.8752 - val_f1_score: 0.8739\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2802 - accuracy: 0.8828 - f1_score: 0.8813 - val_loss: 0.3289 - val_accuracy: 0.8617 - val_f1_score: 0.8654\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2703 - accuracy: 0.8912 - f1_score: 0.8898 - val_loss: 0.3286 - val_accuracy: 0.8779 - val_f1_score: 0.8765\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2555 - accuracy: 0.8954 - f1_score: 0.8947 - val_loss: 0.3310 - val_accuracy: 0.8644 - val_f1_score: 0.8665\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2400 - accuracy: 0.9024 - f1_score: 0.9004 - val_loss: 0.3299 - val_accuracy: 0.8716 - val_f1_score: 0.8714\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2454 - accuracy: 0.9020 - f1_score: 0.9005 - val_loss: 0.3433 - val_accuracy: 0.8680 - val_f1_score: 0.8610\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3652 - accuracy: 0.8548 - f1_score: 0.8787\n","Epoch 1/20\n","104/104 [==============================] - 10s 37ms/step - loss: 0.6273 - accuracy: 0.6432 - f1_score: 0.6251 - val_loss: 0.4928 - val_accuracy: 0.7722 - val_f1_score: 0.7510\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4835 - accuracy: 0.7770 - f1_score: 0.7716 - val_loss: 0.5295 - val_accuracy: 0.7514 - val_f1_score: 0.6843\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3941 - accuracy: 0.8312 - f1_score: 0.8293 - val_loss: 0.3687 - val_accuracy: 0.8409 - val_f1_score: 0.8233\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3397 - accuracy: 0.8596 - f1_score: 0.8581 - val_loss: 0.3163 - val_accuracy: 0.8725 - val_f1_score: 0.8708\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3275 - accuracy: 0.8617 - f1_score: 0.8619 - val_loss: 0.3403 - val_accuracy: 0.8626 - val_f1_score: 0.8527\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3160 - accuracy: 0.8689 - f1_score: 0.8675 - val_loss: 0.3213 - val_accuracy: 0.8689 - val_f1_score: 0.8599\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2980 - accuracy: 0.8822 - f1_score: 0.8815 - val_loss: 0.3159 - val_accuracy: 0.8707 - val_f1_score: 0.8621\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2792 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3074 - val_accuracy: 0.8689 - val_f1_score: 0.8685\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3038 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.3249 - val_accuracy: 0.8635 - val_f1_score: 0.8521\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2663 - accuracy: 0.8927 - f1_score: 0.8922 - val_loss: 0.3239 - val_accuracy: 0.8544 - val_f1_score: 0.8414\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2680 - accuracy: 0.8990 - f1_score: 0.8978 - val_loss: 0.3081 - val_accuracy: 0.8698 - val_f1_score: 0.8667\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2475 - accuracy: 0.9017 - f1_score: 0.9004 - val_loss: 0.3432 - val_accuracy: 0.8553 - val_f1_score: 0.8431\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2402 - accuracy: 0.9045 - f1_score: 0.9038 - val_loss: 0.3255 - val_accuracy: 0.8788 - val_f1_score: 0.8741\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3424 - accuracy: 0.8569 - f1_score: 0.8859\n","Epoch 1/20\n","104/104 [==============================] - 10s 44ms/step - loss: 0.6187 - accuracy: 0.6624 - f1_score: 0.6612 - val_loss: 0.5175 - val_accuracy: 0.7532 - val_f1_score: 0.7422\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4860 - accuracy: 0.7725 - f1_score: 0.7598 - val_loss: 0.5106 - val_accuracy: 0.7631 - val_f1_score: 0.7146\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3971 - accuracy: 0.8204 - f1_score: 0.8170 - val_loss: 0.4075 - val_accuracy: 0.8228 - val_f1_score: 0.8215\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3441 - accuracy: 0.8517 - f1_score: 0.8509 - val_loss: 0.3817 - val_accuracy: 0.8427 - val_f1_score: 0.8401\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3267 - accuracy: 0.8617 - f1_score: 0.8604 - val_loss: 0.3975 - val_accuracy: 0.8436 - val_f1_score: 0.8292\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3106 - accuracy: 0.8713 - f1_score: 0.8704 - val_loss: 0.3881 - val_accuracy: 0.8282 - val_f1_score: 0.8417\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2954 - accuracy: 0.8822 - f1_score: 0.8805 - val_loss: 0.3884 - val_accuracy: 0.8454 - val_f1_score: 0.8547\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2845 - accuracy: 0.8858 - f1_score: 0.8848 - val_loss: 0.3420 - val_accuracy: 0.8599 - val_f1_score: 0.8600\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2676 - accuracy: 0.8939 - f1_score: 0.8929 - val_loss: 0.3639 - val_accuracy: 0.8553 - val_f1_score: 0.8592\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2590 - accuracy: 0.8966 - f1_score: 0.8946 - val_loss: 0.4010 - val_accuracy: 0.8463 - val_f1_score: 0.8346\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2641 - accuracy: 0.8951 - f1_score: 0.8936 - val_loss: 0.3610 - val_accuracy: 0.8626 - val_f1_score: 0.8640\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2631 - accuracy: 0.8936 - f1_score: 0.8923 - val_loss: 0.3718 - val_accuracy: 0.8544 - val_f1_score: 0.8579\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2256 - accuracy: 0.9171 - f1_score: 0.9158 - val_loss: 0.3971 - val_accuracy: 0.8626 - val_f1_score: 0.8643\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3340 - accuracy: 0.8582 - f1_score: 0.8857\n","Epoch 1/20\n","104/104 [==============================] - 10s 24ms/step - loss: 0.6421 - accuracy: 0.6269 - f1_score: 0.6307 - val_loss: 0.5019 - val_accuracy: 0.7604 - val_f1_score: 0.7640\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.5068 - accuracy: 0.7685 - f1_score: 0.7567 - val_loss: 0.3996 - val_accuracy: 0.8074 - val_f1_score: 0.7820\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4216 - accuracy: 0.8038 - f1_score: 0.7941 - val_loss: 0.3377 - val_accuracy: 0.8644 - val_f1_score: 0.8641\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3593 - accuracy: 0.8463 - f1_score: 0.8441 - val_loss: 0.3288 - val_accuracy: 0.8617 - val_f1_score: 0.8478\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3549 - accuracy: 0.8490 - f1_score: 0.8461 - val_loss: 0.2939 - val_accuracy: 0.8852 - val_f1_score: 0.8810\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3279 - accuracy: 0.8647 - f1_score: 0.8626 - val_loss: 0.2822 - val_accuracy: 0.8888 - val_f1_score: 0.8885\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3110 - accuracy: 0.8725 - f1_score: 0.8700 - val_loss: 0.2878 - val_accuracy: 0.8816 - val_f1_score: 0.8779\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3048 - accuracy: 0.8776 - f1_score: 0.8758 - val_loss: 0.3060 - val_accuracy: 0.8779 - val_f1_score: 0.8835\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2943 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.3152 - val_accuracy: 0.8788 - val_f1_score: 0.8861\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2697 - accuracy: 0.8930 - f1_score: 0.8918 - val_loss: 0.3027 - val_accuracy: 0.8870 - val_f1_score: 0.8918\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2764 - accuracy: 0.8861 - f1_score: 0.8851 - val_loss: 0.2950 - val_accuracy: 0.8915 - val_f1_score: 0.8917\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8548 - f1_score: 0.8800\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6260 - accuracy: 0.6456 - f1_score: 0.6648 - val_loss: 0.4999 - val_accuracy: 0.7559 - val_f1_score: 0.7389\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4904 - accuracy: 0.7697 - f1_score: 0.7535 - val_loss: 0.4199 - val_accuracy: 0.8083 - val_f1_score: 0.7863\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4050 - accuracy: 0.8228 - f1_score: 0.8183 - val_loss: 0.4245 - val_accuracy: 0.8101 - val_f1_score: 0.8287\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3562 - accuracy: 0.8508 - f1_score: 0.8479 - val_loss: 0.3449 - val_accuracy: 0.8517 - val_f1_score: 0.8498\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3422 - accuracy: 0.8602 - f1_score: 0.8574 - val_loss: 0.3516 - val_accuracy: 0.8553 - val_f1_score: 0.8594\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3079 - accuracy: 0.8764 - f1_score: 0.8754 - val_loss: 0.3340 - val_accuracy: 0.8590 - val_f1_score: 0.8600\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2951 - accuracy: 0.8807 - f1_score: 0.8788 - val_loss: 0.3777 - val_accuracy: 0.8418 - val_f1_score: 0.8252\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3004 - accuracy: 0.8825 - f1_score: 0.8820 - val_loss: 0.3559 - val_accuracy: 0.8499 - val_f1_score: 0.8366\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2721 - accuracy: 0.8942 - f1_score: 0.8926 - val_loss: 0.3746 - val_accuracy: 0.8499 - val_f1_score: 0.8579\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2752 - accuracy: 0.8957 - f1_score: 0.8946 - val_loss: 0.3312 - val_accuracy: 0.8716 - val_f1_score: 0.8678\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2589 - accuracy: 0.8933 - f1_score: 0.8912 - val_loss: 0.3700 - val_accuracy: 0.8454 - val_f1_score: 0.8315\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2540 - accuracy: 0.8990 - f1_score: 0.8978 - val_loss: 0.3692 - val_accuracy: 0.8599 - val_f1_score: 0.8508\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2471 - accuracy: 0.9030 - f1_score: 0.9021 - val_loss: 0.3553 - val_accuracy: 0.8635 - val_f1_score: 0.8684\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2345 - accuracy: 0.9084 - f1_score: 0.9073 - val_loss: 0.3540 - val_accuracy: 0.8770 - val_f1_score: 0.8792\n","Epoch 15/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2138 - accuracy: 0.9201 - f1_score: 0.9193 - val_loss: 0.3880 - val_accuracy: 0.8725 - val_f1_score: 0.8735\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3435 - accuracy: 0.8629 - f1_score: 0.8841\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6260 - accuracy: 0.6371 - f1_score: 0.6282 - val_loss: 0.5858 - val_accuracy: 0.6591 - val_f1_score: 0.7313\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4778 - accuracy: 0.7842 - f1_score: 0.7798 - val_loss: 0.4152 - val_accuracy: 0.8174 - val_f1_score: 0.8209\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3954 - accuracy: 0.8327 - f1_score: 0.8301 - val_loss: 0.3892 - val_accuracy: 0.8309 - val_f1_score: 0.8227\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3491 - accuracy: 0.8550 - f1_score: 0.8538 - val_loss: 0.4028 - val_accuracy: 0.8336 - val_f1_score: 0.8435\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3216 - accuracy: 0.8698 - f1_score: 0.8677 - val_loss: 0.4401 - val_accuracy: 0.8119 - val_f1_score: 0.8301\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3010 - accuracy: 0.8825 - f1_score: 0.8819 - val_loss: 0.3582 - val_accuracy: 0.8490 - val_f1_score: 0.8432\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3002 - accuracy: 0.8864 - f1_score: 0.8852 - val_loss: 0.3786 - val_accuracy: 0.8571 - val_f1_score: 0.8548\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2869 - accuracy: 0.8876 - f1_score: 0.8876 - val_loss: 0.3639 - val_accuracy: 0.8626 - val_f1_score: 0.8585\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2729 - accuracy: 0.8909 - f1_score: 0.8889 - val_loss: 0.4000 - val_accuracy: 0.8345 - val_f1_score: 0.8461\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2649 - accuracy: 0.8933 - f1_score: 0.8930 - val_loss: 0.3763 - val_accuracy: 0.8626 - val_f1_score: 0.8631\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2441 - accuracy: 0.9072 - f1_score: 0.9071 - val_loss: 0.3803 - val_accuracy: 0.8544 - val_f1_score: 0.8566\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3757 - accuracy: 0.8292 - f1_score: 0.8491\n","Epoch 1/20\n","104/104 [==============================] - 8s 24ms/step - loss: 0.6359 - accuracy: 0.6206 - f1_score: 0.6364 - val_loss: 0.5109 - val_accuracy: 0.7658 - val_f1_score: 0.7473\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4796 - accuracy: 0.7797 - f1_score: 0.7652 - val_loss: 0.5128 - val_accuracy: 0.7586 - val_f1_score: 0.7932\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4200 - accuracy: 0.8186 - f1_score: 0.8130 - val_loss: 0.4624 - val_accuracy: 0.7667 - val_f1_score: 0.8006\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3669 - accuracy: 0.8403 - f1_score: 0.8379 - val_loss: 0.3746 - val_accuracy: 0.8391 - val_f1_score: 0.8509\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3475 - accuracy: 0.8550 - f1_score: 0.8533 - val_loss: 0.4343 - val_accuracy: 0.8146 - val_f1_score: 0.8356\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3173 - accuracy: 0.8743 - f1_score: 0.8726 - val_loss: 0.3865 - val_accuracy: 0.8445 - val_f1_score: 0.8569\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3053 - accuracy: 0.8816 - f1_score: 0.8799 - val_loss: 0.3237 - val_accuracy: 0.8734 - val_f1_score: 0.8684\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2890 - accuracy: 0.8819 - f1_score: 0.8803 - val_loss: 0.4063 - val_accuracy: 0.8463 - val_f1_score: 0.8579\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2864 - accuracy: 0.8864 - f1_score: 0.8854 - val_loss: 0.3251 - val_accuracy: 0.8698 - val_f1_score: 0.8649\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2679 - accuracy: 0.8915 - f1_score: 0.8902 - val_loss: 0.3212 - val_accuracy: 0.8734 - val_f1_score: 0.8746\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2557 - accuracy: 0.9024 - f1_score: 0.9014 - val_loss: 0.3460 - val_accuracy: 0.8599 - val_f1_score: 0.8646\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2561 - accuracy: 0.8969 - f1_score: 0.8957 - val_loss: 0.3431 - val_accuracy: 0.8734 - val_f1_score: 0.8713\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2535 - accuracy: 0.8897 - f1_score: 0.8892 - val_loss: 0.3417 - val_accuracy: 0.8680 - val_f1_score: 0.8699\n","Epoch 14/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2442 - accuracy: 0.8984 - f1_score: 0.8968 - val_loss: 0.3532 - val_accuracy: 0.8734 - val_f1_score: 0.8746\n","Epoch 15/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2095 - accuracy: 0.9234 - f1_score: 0.9225 - val_loss: 0.4226 - val_accuracy: 0.8535 - val_f1_score: 0.8603\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3464 - accuracy: 0.8596 - f1_score: 0.8814\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6153 - accuracy: 0.6634 - f1_score: 0.6647 - val_loss: 0.5058 - val_accuracy: 0.7604 - val_f1_score: 0.7459\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4886 - accuracy: 0.7719 - f1_score: 0.7626 - val_loss: 0.4335 - val_accuracy: 0.8038 - val_f1_score: 0.7823\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4033 - accuracy: 0.8210 - f1_score: 0.8159 - val_loss: 0.5924 - val_accuracy: 0.7405 - val_f1_score: 0.7879\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3597 - accuracy: 0.8490 - f1_score: 0.8484 - val_loss: 0.3451 - val_accuracy: 0.8662 - val_f1_score: 0.8601\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3283 - accuracy: 0.8689 - f1_score: 0.8677 - val_loss: 0.3285 - val_accuracy: 0.8680 - val_f1_score: 0.8708\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3174 - accuracy: 0.8665 - f1_score: 0.8665 - val_loss: 0.3404 - val_accuracy: 0.8590 - val_f1_score: 0.8488\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3105 - accuracy: 0.8734 - f1_score: 0.8724 - val_loss: 0.4267 - val_accuracy: 0.8192 - val_f1_score: 0.7895\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2959 - accuracy: 0.8807 - f1_score: 0.8799 - val_loss: 0.3259 - val_accuracy: 0.8788 - val_f1_score: 0.8752\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2828 - accuracy: 0.8855 - f1_score: 0.8844 - val_loss: 0.3358 - val_accuracy: 0.8680 - val_f1_score: 0.8719\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2660 - accuracy: 0.8972 - f1_score: 0.8967 - val_loss: 0.3119 - val_accuracy: 0.8725 - val_f1_score: 0.8731\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2560 - accuracy: 0.8969 - f1_score: 0.8966 - val_loss: 0.3328 - val_accuracy: 0.8807 - val_f1_score: 0.8755\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2544 - accuracy: 0.8954 - f1_score: 0.8953 - val_loss: 0.3652 - val_accuracy: 0.8553 - val_f1_score: 0.8662\n","Epoch 13/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2555 - accuracy: 0.8978 - f1_score: 0.8968 - val_loss: 0.3271 - val_accuracy: 0.8680 - val_f1_score: 0.8717\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2288 - accuracy: 0.9102 - f1_score: 0.9103 - val_loss: 0.3618 - val_accuracy: 0.8725 - val_f1_score: 0.8635\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2265 - accuracy: 0.9099 - f1_score: 0.9096 - val_loss: 0.3376 - val_accuracy: 0.8707 - val_f1_score: 0.8758\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3039 - accuracy: 0.8771 - f1_score: 0.8998\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6208 - accuracy: 0.6567 - f1_score: 0.6446 - val_loss: 0.4911 - val_accuracy: 0.7694 - val_f1_score: 0.7390\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4861 - accuracy: 0.7658 - f1_score: 0.7485 - val_loss: 0.4829 - val_accuracy: 0.7884 - val_f1_score: 0.8143\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3987 - accuracy: 0.8276 - f1_score: 0.8211 - val_loss: 0.3460 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3541 - accuracy: 0.8535 - f1_score: 0.8506 - val_loss: 0.3406 - val_accuracy: 0.8562 - val_f1_score: 0.8635\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3251 - accuracy: 0.8653 - f1_score: 0.8634 - val_loss: 0.3855 - val_accuracy: 0.8038 - val_f1_score: 0.7654\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3134 - accuracy: 0.8776 - f1_score: 0.8758 - val_loss: 0.3618 - val_accuracy: 0.8499 - val_f1_score: 0.8316\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2857 - accuracy: 0.8945 - f1_score: 0.8928 - val_loss: 0.4137 - val_accuracy: 0.8183 - val_f1_score: 0.7869\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2782 - accuracy: 0.8900 - f1_score: 0.8879 - val_loss: 0.3139 - val_accuracy: 0.8689 - val_f1_score: 0.8602\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2659 - accuracy: 0.8975 - f1_score: 0.8959 - val_loss: 0.3019 - val_accuracy: 0.8843 - val_f1_score: 0.8826\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2727 - accuracy: 0.8912 - f1_score: 0.8894 - val_loss: 0.3814 - val_accuracy: 0.8382 - val_f1_score: 0.8153\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2427 - accuracy: 0.9066 - f1_score: 0.9057 - val_loss: 0.3316 - val_accuracy: 0.8635 - val_f1_score: 0.8538\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2427 - accuracy: 0.9048 - f1_score: 0.9038 - val_loss: 0.3259 - val_accuracy: 0.8698 - val_f1_score: 0.8686\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2219 - accuracy: 0.9183 - f1_score: 0.9173 - val_loss: 0.3430 - val_accuracy: 0.8580 - val_f1_score: 0.8617\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2460 - accuracy: 0.9036 - f1_score: 0.9030 - val_loss: 0.4566 - val_accuracy: 0.8119 - val_f1_score: 0.7778\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3144 - accuracy: 0.8758 - f1_score: 0.8971\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2lKD0JkaRRWD","executionInfo":{"status":"ok","timestamp":1689808042852,"user_tz":-330,"elapsed":26,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"46ed7591-bcba-4355-9da8-65fc3007f523"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8433490991592407, 0.8615800142288208, 0.8284942507743835, 0.8528021574020386, 0.8615800142288208, 0.8588791489601135, 0.8507764935493469, 0.8575286865234375, 0.8669817447662354, 0.8683322072029114, 0.8602295517921448, 0.8656313419342041, 0.8615800142288208, 0.847400426864624, 0.869007408618927, 0.8642808794975281, 0.8548278212547302, 0.8568534851074219, 0.8582038879394531, 0.8548278212547302, 0.8629304766654968, 0.8291694521903992, 0.8595543503761292, 0.8771100640296936, 0.8757596015930176]\n","0.8579068160057068\n","0.01146693641452229\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-vNUJgMRSN1","executionInfo":{"status":"ok","timestamp":1689808042853,"user_tz":-330,"elapsed":15,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"4388d954-aa91-4e04-b830-2072db644a82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.37106794118881226, 0.3237060308456421, 0.3843397796154022, 0.35266369581222534, 0.34101602435112, 0.3515925109386444, 0.34480032324790955, 0.35673704743385315, 0.3476754426956177, 0.3630906045436859, 0.32673540711402893, 0.3271839916706085, 0.3311788737773895, 0.35267189145088196, 0.3200409412384033, 0.339666485786438, 0.3651861548423767, 0.3423856198787689, 0.33400771021842957, 0.3336097300052643, 0.3434813320636749, 0.3757292926311493, 0.3464460074901581, 0.3039354681968689, 0.3143533170223236]\n","0.34373206496238706\n","0.01899690837838869\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LK3BWTJhRSQZ","executionInfo":{"status":"ok","timestamp":1689808043335,"user_tz":-330,"elapsed":494,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"c61404cc-3bd1-45d4-ec77-6dbd87b5e231"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8638496398925781, 0.8846369981765747, 0.8480859994888306, 0.8742791414260864, 0.8841152191162109, 0.8876947164535522, 0.8743603825569153, 0.8827126026153564, 0.8908586502075195, 0.8929159641265869, 0.8865753412246704, 0.8956475853919983, 0.890315592288971, 0.8712984323501587, 0.8955865502357483, 0.8899835348129272, 0.8787365555763245, 0.885898768901825, 0.8857453465461731, 0.8799551725387573, 0.8840661644935608, 0.8491353392601013, 0.8814138174057007, 0.8997796177864075, 0.8970916867256165]\n","0.882189552783966\n","0.012884871059322765\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uZjryIdqRrLp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing  -\n","1. Accuracy - 0.854/0.009\n","2. Loss - 0.341/0.018\n","3. F1 score - 0.878/0.010\n","\n","LSTM CNN LSTM nothing -\n","1. Accuracy - 0.857/0.011\n","2. Loss - 0.343/0.018\n","3. F1 score - 0.882/0.012"],"metadata":{"id":"k0vQYiDdUU6L"}},{"cell_type":"code","source":[],"metadata":{"id":"oVawltCNaxpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T-pFC076RL__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"726C7BWoRMC3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LPWj9exNRMHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZEERpgvJRMKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gbcw-KwZRMM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BXTVoco9RMOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Understand MaxP and switch from GlobalMaxP to MaxP for CNN for emotion probs."],"metadata":{"id":"Zg1r9eZtRMYQ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Concatenate, GlobalMaxPooling1D, Reshape, Dropout, MaxPooling1D, MaxPooling2D, MaxPooling3D, RepeatVector, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from imblearn.under_sampling import RandomUnderSampler"],"metadata":{"id":"8IeYoexESA9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaling = MinMaxScaler(feature_range=(0,1))  # (0,1) is default"],"metadata":{"id":"igrLSp4iSW3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to register f1 score separately in Keras (working)\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.metrics import Metric\n","\n","class F1Score(Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","        false_positives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * y_pred, 0, 1)))\n","        false_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * (1 - y_pred), 0, 1)))\n","\n","        self.true_positives.assign_add(true_positives)\n","        self.false_positives.assign_add(false_positives)\n","        self.false_negatives.assign_add(false_negatives)\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n","        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","        return f1"],"metadata":{"id":"YkfTWxo0RNBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"Ls5DgmLdRck_","executionInfo":{"status":"ok","timestamp":1689839663777,"user_tz":-330,"elapsed":14,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7657999b-ab7f-44a7-abe9-0ad1b82b2675"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-2faa8475-3920-41fa-9fe0-4cfa7d9fa80b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2faa8475-3920-41fa-9fe0-4cfa7d9fa80b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-b16e7499-341a-42a6-83ea-4c38335ef836\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b16e7499-341a-42a6-83ea-4c38335ef836')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-b16e7499-341a-42a6-83ea-4c38335ef836 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2faa8475-3920-41fa-9fe0-4cfa7d9fa80b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2faa8475-3920-41fa-9fe0-4cfa7d9fa80b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"-eIfTjHYRcnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cuhDl1O3Rcp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM CNN nothing nothing"],"metadata":{"id":"sxj2PHAiRlQ6"}},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=3, strides=1)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYiQMS9cRcsX","executionInfo":{"status":"ok","timestamp":1689842326146,"user_tz":-330,"elapsed":1086032,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"29a41c2f-1268-42c6-f2bc-6964256726cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 21s 76ms/step - loss: 0.6479 - accuracy: 0.6248 - f1_score: 0.6034 - val_loss: 0.5411 - val_accuracy: 0.7378 - val_f1_score: 0.6915\n","Epoch 2/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.5121 - accuracy: 0.7565 - f1_score: 0.7465 - val_loss: 0.3844 - val_accuracy: 0.8336 - val_f1_score: 0.8342\n","Epoch 3/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.4414 - accuracy: 0.8014 - f1_score: 0.7977 - val_loss: 0.3518 - val_accuracy: 0.8436 - val_f1_score: 0.8382\n","Epoch 4/20\n","104/104 [==============================] - 3s 33ms/step - loss: 0.3873 - accuracy: 0.8369 - f1_score: 0.8342 - val_loss: 0.3419 - val_accuracy: 0.8463 - val_f1_score: 0.8384\n","Epoch 5/20\n","104/104 [==============================] - 4s 42ms/step - loss: 0.3464 - accuracy: 0.8553 - f1_score: 0.8538 - val_loss: 0.3234 - val_accuracy: 0.8590 - val_f1_score: 0.8534\n","Epoch 6/20\n","104/104 [==============================] - 3s 31ms/step - loss: 0.3601 - accuracy: 0.8475 - f1_score: 0.8444 - val_loss: 0.3052 - val_accuracy: 0.8725 - val_f1_score: 0.8656\n","Epoch 7/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3482 - accuracy: 0.8505 - f1_score: 0.8475 - val_loss: 0.3085 - val_accuracy: 0.8743 - val_f1_score: 0.8809\n","Epoch 8/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3210 - accuracy: 0.8713 - f1_score: 0.8690 - val_loss: 0.2924 - val_accuracy: 0.8788 - val_f1_score: 0.8799\n","Epoch 9/20\n","104/104 [==============================] - 3s 33ms/step - loss: 0.3045 - accuracy: 0.8779 - f1_score: 0.8766 - val_loss: 0.3119 - val_accuracy: 0.8689 - val_f1_score: 0.8751\n","Epoch 10/20\n","104/104 [==============================] - 3s 31ms/step - loss: 0.2968 - accuracy: 0.8788 - f1_score: 0.8768 - val_loss: 0.2898 - val_accuracy: 0.8707 - val_f1_score: 0.8687\n","Epoch 11/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2918 - accuracy: 0.8831 - f1_score: 0.8812 - val_loss: 0.2889 - val_accuracy: 0.8807 - val_f1_score: 0.8785\n","Epoch 12/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2833 - accuracy: 0.8852 - f1_score: 0.8836 - val_loss: 0.2904 - val_accuracy: 0.8825 - val_f1_score: 0.8807\n","Epoch 13/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2822 - accuracy: 0.8840 - f1_score: 0.8814 - val_loss: 0.3021 - val_accuracy: 0.8671 - val_f1_score: 0.8593\n","Epoch 14/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2671 - accuracy: 0.8966 - f1_score: 0.8947 - val_loss: 0.3110 - val_accuracy: 0.8725 - val_f1_score: 0.8661\n","Epoch 15/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2766 - accuracy: 0.8942 - f1_score: 0.8921 - val_loss: 0.3355 - val_accuracy: 0.8608 - val_f1_score: 0.8686\n","Epoch 16/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2625 - accuracy: 0.8942 - f1_score: 0.8930 - val_loss: 0.3269 - val_accuracy: 0.8635 - val_f1_score: 0.8681\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3463 - accuracy: 0.8542 - f1_score: 0.8764\n","Epoch 1/20\n","104/104 [==============================] - 6s 23ms/step - loss: 0.6346 - accuracy: 0.6570 - f1_score: 0.6583 - val_loss: 0.5600 - val_accuracy: 0.7269 - val_f1_score: 0.6760\n","Epoch 2/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.4857 - accuracy: 0.7652 - f1_score: 0.7545 - val_loss: 0.4998 - val_accuracy: 0.7658 - val_f1_score: 0.7920\n","Epoch 3/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4054 - accuracy: 0.8156 - f1_score: 0.8112 - val_loss: 0.4023 - val_accuracy: 0.8282 - val_f1_score: 0.8354\n","Epoch 4/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3552 - accuracy: 0.8460 - f1_score: 0.8434 - val_loss: 0.3618 - val_accuracy: 0.8526 - val_f1_score: 0.8481\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3336 - accuracy: 0.8538 - f1_score: 0.8526 - val_loss: 0.3436 - val_accuracy: 0.8553 - val_f1_score: 0.8548\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3085 - accuracy: 0.8740 - f1_score: 0.8737 - val_loss: 0.3394 - val_accuracy: 0.8590 - val_f1_score: 0.8624\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3037 - accuracy: 0.8755 - f1_score: 0.8750 - val_loss: 0.3391 - val_accuracy: 0.8707 - val_f1_score: 0.8634\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2882 - accuracy: 0.8831 - f1_score: 0.8824 - val_loss: 0.3335 - val_accuracy: 0.8608 - val_f1_score: 0.8550\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2943 - accuracy: 0.8782 - f1_score: 0.8774 - val_loss: 0.3274 - val_accuracy: 0.8617 - val_f1_score: 0.8625\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2756 - accuracy: 0.8855 - f1_score: 0.8844 - val_loss: 0.3580 - val_accuracy: 0.8617 - val_f1_score: 0.8522\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2825 - accuracy: 0.8819 - f1_score: 0.8809 - val_loss: 0.3587 - val_accuracy: 0.8553 - val_f1_score: 0.8628\n","Epoch 12/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2662 - accuracy: 0.8897 - f1_score: 0.8892 - val_loss: 0.3914 - val_accuracy: 0.8409 - val_f1_score: 0.8219\n","Epoch 13/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2904 - accuracy: 0.8731 - f1_score: 0.8716 - val_loss: 0.3472 - val_accuracy: 0.8571 - val_f1_score: 0.8607\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2480 - accuracy: 0.9005 - f1_score: 0.9002 - val_loss: 0.3339 - val_accuracy: 0.8653 - val_f1_score: 0.8624\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8683 - f1_score: 0.8923\n","Epoch 1/20\n","104/104 [==============================] - 7s 23ms/step - loss: 0.6340 - accuracy: 0.6386 - f1_score: 0.6355 - val_loss: 0.5508 - val_accuracy: 0.7206 - val_f1_score: 0.6798\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4985 - accuracy: 0.7676 - f1_score: 0.7519 - val_loss: 0.5029 - val_accuracy: 0.7550 - val_f1_score: 0.7806\n","Epoch 3/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.4089 - accuracy: 0.8168 - f1_score: 0.8111 - val_loss: 0.4016 - val_accuracy: 0.8092 - val_f1_score: 0.7929\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3668 - accuracy: 0.8421 - f1_score: 0.8371 - val_loss: 0.3917 - val_accuracy: 0.8300 - val_f1_score: 0.8345\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3308 - accuracy: 0.8635 - f1_score: 0.8610 - val_loss: 0.3789 - val_accuracy: 0.8382 - val_f1_score: 0.8287\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3191 - accuracy: 0.8662 - f1_score: 0.8639 - val_loss: 0.3957 - val_accuracy: 0.8291 - val_f1_score: 0.8391\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3141 - accuracy: 0.8719 - f1_score: 0.8700 - val_loss: 0.3879 - val_accuracy: 0.8309 - val_f1_score: 0.8158\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3013 - accuracy: 0.8803 - f1_score: 0.8788 - val_loss: 0.4693 - val_accuracy: 0.7929 - val_f1_score: 0.7503\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2842 - accuracy: 0.8864 - f1_score: 0.8845 - val_loss: 0.3798 - val_accuracy: 0.8427 - val_f1_score: 0.8368\n","Epoch 10/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2797 - accuracy: 0.8867 - f1_score: 0.8852 - val_loss: 0.3791 - val_accuracy: 0.8382 - val_f1_score: 0.8380\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3693 - accuracy: 0.8427 - f1_score: 0.8640\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6242 - accuracy: 0.6606 - f1_score: 0.6375 - val_loss: 0.5106 - val_accuracy: 0.7523 - val_f1_score: 0.7158\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4941 - accuracy: 0.7646 - f1_score: 0.7531 - val_loss: 0.4165 - val_accuracy: 0.8020 - val_f1_score: 0.7817\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4002 - accuracy: 0.8273 - f1_score: 0.8242 - val_loss: 0.3632 - val_accuracy: 0.8463 - val_f1_score: 0.8446\n","Epoch 4/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3532 - accuracy: 0.8496 - f1_score: 0.8470 - val_loss: 0.3703 - val_accuracy: 0.8553 - val_f1_score: 0.8621\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3325 - accuracy: 0.8632 - f1_score: 0.8605 - val_loss: 0.4099 - val_accuracy: 0.8273 - val_f1_score: 0.8425\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3229 - accuracy: 0.8683 - f1_score: 0.8659 - val_loss: 0.3940 - val_accuracy: 0.8174 - val_f1_score: 0.8363\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3143 - accuracy: 0.8707 - f1_score: 0.8688 - val_loss: 0.4048 - val_accuracy: 0.8336 - val_f1_score: 0.8141\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2991 - accuracy: 0.8797 - f1_score: 0.8777 - val_loss: 0.3320 - val_accuracy: 0.8644 - val_f1_score: 0.8673\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3077 - accuracy: 0.8788 - f1_score: 0.8774 - val_loss: 0.3409 - val_accuracy: 0.8544 - val_f1_score: 0.8459\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2952 - accuracy: 0.8800 - f1_score: 0.8780 - val_loss: 0.3613 - val_accuracy: 0.8535 - val_f1_score: 0.8615\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2822 - accuracy: 0.8870 - f1_score: 0.8849 - val_loss: 0.4023 - val_accuracy: 0.8264 - val_f1_score: 0.8436\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2707 - accuracy: 0.8933 - f1_score: 0.8916 - val_loss: 0.3390 - val_accuracy: 0.8590 - val_f1_score: 0.8602\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2596 - accuracy: 0.9005 - f1_score: 0.8993 - val_loss: 0.3372 - val_accuracy: 0.8635 - val_f1_score: 0.8624\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8589 - f1_score: 0.8824\n","Epoch 1/20\n","104/104 [==============================] - 6s 23ms/step - loss: 0.6431 - accuracy: 0.6233 - f1_score: 0.5973 - val_loss: 0.5710 - val_accuracy: 0.7071 - val_f1_score: 0.6455\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.5123 - accuracy: 0.7529 - f1_score: 0.7418 - val_loss: 0.4178 - val_accuracy: 0.8156 - val_f1_score: 0.8201\n","Epoch 3/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3960 - accuracy: 0.8204 - f1_score: 0.8160 - val_loss: 0.3956 - val_accuracy: 0.8219 - val_f1_score: 0.8379\n","Epoch 4/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3593 - accuracy: 0.8445 - f1_score: 0.8434 - val_loss: 0.3517 - val_accuracy: 0.8526 - val_f1_score: 0.8566\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3552 - accuracy: 0.8520 - f1_score: 0.8494 - val_loss: 0.3346 - val_accuracy: 0.8590 - val_f1_score: 0.8617\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3401 - accuracy: 0.8541 - f1_score: 0.8530 - val_loss: 0.3323 - val_accuracy: 0.8698 - val_f1_score: 0.8705\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3211 - accuracy: 0.8680 - f1_score: 0.8669 - val_loss: 0.3683 - val_accuracy: 0.8472 - val_f1_score: 0.8559\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3083 - accuracy: 0.8743 - f1_score: 0.8724 - val_loss: 0.3289 - val_accuracy: 0.8734 - val_f1_score: 0.8694\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3062 - accuracy: 0.8770 - f1_score: 0.8749 - val_loss: 0.3426 - val_accuracy: 0.8590 - val_f1_score: 0.8632\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2893 - accuracy: 0.8891 - f1_score: 0.8878 - val_loss: 0.3269 - val_accuracy: 0.8662 - val_f1_score: 0.8657\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2856 - accuracy: 0.8843 - f1_score: 0.8825 - val_loss: 0.3562 - val_accuracy: 0.8571 - val_f1_score: 0.8457\n","Epoch 12/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2712 - accuracy: 0.8897 - f1_score: 0.8875 - val_loss: 0.3352 - val_accuracy: 0.8644 - val_f1_score: 0.8582\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2724 - accuracy: 0.8915 - f1_score: 0.8903 - val_loss: 0.3692 - val_accuracy: 0.8499 - val_f1_score: 0.8382\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2623 - accuracy: 0.8957 - f1_score: 0.8948 - val_loss: 0.3316 - val_accuracy: 0.8779 - val_f1_score: 0.8753\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2536 - accuracy: 0.8984 - f1_score: 0.8973 - val_loss: 0.3343 - val_accuracy: 0.8653 - val_f1_score: 0.8664\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3504 - accuracy: 0.8467 - f1_score: 0.8687\n","Epoch 1/20\n","104/104 [==============================] - 7s 31ms/step - loss: 0.6342 - accuracy: 0.6365 - f1_score: 0.6498 - val_loss: 0.5412 - val_accuracy: 0.7333 - val_f1_score: 0.7188\n","Epoch 2/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.4940 - accuracy: 0.7667 - f1_score: 0.7549 - val_loss: 0.4382 - val_accuracy: 0.7939 - val_f1_score: 0.8109\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3933 - accuracy: 0.8264 - f1_score: 0.8219 - val_loss: 0.4444 - val_accuracy: 0.7939 - val_f1_score: 0.8205\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3632 - accuracy: 0.8394 - f1_score: 0.8375 - val_loss: 0.3222 - val_accuracy: 0.8680 - val_f1_score: 0.8668\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3335 - accuracy: 0.8556 - f1_score: 0.8552 - val_loss: 0.3044 - val_accuracy: 0.8861 - val_f1_score: 0.8833\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3305 - accuracy: 0.8626 - f1_score: 0.8611 - val_loss: 0.3031 - val_accuracy: 0.8843 - val_f1_score: 0.8801\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3031 - accuracy: 0.8755 - f1_score: 0.8751 - val_loss: 0.2955 - val_accuracy: 0.8816 - val_f1_score: 0.8850\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3027 - accuracy: 0.8749 - f1_score: 0.8744 - val_loss: 0.2908 - val_accuracy: 0.8797 - val_f1_score: 0.8809\n","Epoch 9/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2883 - accuracy: 0.8794 - f1_score: 0.8789 - val_loss: 0.2852 - val_accuracy: 0.8816 - val_f1_score: 0.8817\n","Epoch 10/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2837 - accuracy: 0.8858 - f1_score: 0.8850 - val_loss: 0.3311 - val_accuracy: 0.8499 - val_f1_score: 0.8310\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2684 - accuracy: 0.8933 - f1_score: 0.8925 - val_loss: 0.2916 - val_accuracy: 0.8707 - val_f1_score: 0.8751\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2688 - accuracy: 0.8900 - f1_score: 0.8894 - val_loss: 0.2742 - val_accuracy: 0.8888 - val_f1_score: 0.8852\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2631 - accuracy: 0.8942 - f1_score: 0.8934 - val_loss: 0.2751 - val_accuracy: 0.8870 - val_f1_score: 0.8839\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2835 - accuracy: 0.8776 - f1_score: 0.8762 - val_loss: 0.2844 - val_accuracy: 0.8852 - val_f1_score: 0.8796\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2406 - accuracy: 0.9017 - f1_score: 0.9013 - val_loss: 0.2886 - val_accuracy: 0.8797 - val_f1_score: 0.8796\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2283 - accuracy: 0.9126 - f1_score: 0.9120 - val_loss: 0.3061 - val_accuracy: 0.8734 - val_f1_score: 0.8627\n","Epoch 17/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2261 - accuracy: 0.9117 - f1_score: 0.9113 - val_loss: 0.2853 - val_accuracy: 0.8897 - val_f1_score: 0.8836\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3393 - accuracy: 0.8542 - f1_score: 0.8776\n","Epoch 1/20\n","104/104 [==============================] - 8s 29ms/step - loss: 0.6230 - accuracy: 0.6525 - f1_score: 0.6447 - val_loss: 0.5487 - val_accuracy: 0.7206 - val_f1_score: 0.6785\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4808 - accuracy: 0.7797 - f1_score: 0.7692 - val_loss: 0.4092 - val_accuracy: 0.8119 - val_f1_score: 0.8116\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4011 - accuracy: 0.8219 - f1_score: 0.8198 - val_loss: 0.3465 - val_accuracy: 0.8508 - val_f1_score: 0.8454\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3657 - accuracy: 0.8448 - f1_score: 0.8422 - val_loss: 0.3279 - val_accuracy: 0.8626 - val_f1_score: 0.8566\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3243 - accuracy: 0.8695 - f1_score: 0.8685 - val_loss: 0.3310 - val_accuracy: 0.8617 - val_f1_score: 0.8702\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3276 - accuracy: 0.8716 - f1_score: 0.8708 - val_loss: 0.3241 - val_accuracy: 0.8617 - val_f1_score: 0.8696\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3037 - accuracy: 0.8746 - f1_score: 0.8741 - val_loss: 0.2996 - val_accuracy: 0.8788 - val_f1_score: 0.8777\n","Epoch 8/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3079 - accuracy: 0.8722 - f1_score: 0.8710 - val_loss: 0.3016 - val_accuracy: 0.8834 - val_f1_score: 0.8822\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2942 - accuracy: 0.8810 - f1_score: 0.8794 - val_loss: 0.3096 - val_accuracy: 0.8707 - val_f1_score: 0.8745\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2752 - accuracy: 0.8867 - f1_score: 0.8859 - val_loss: 0.2978 - val_accuracy: 0.8788 - val_f1_score: 0.8808\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2603 - accuracy: 0.8963 - f1_score: 0.8960 - val_loss: 0.3094 - val_accuracy: 0.8725 - val_f1_score: 0.8781\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2531 - accuracy: 0.8996 - f1_score: 0.8989 - val_loss: 0.2992 - val_accuracy: 0.8788 - val_f1_score: 0.8814\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2614 - accuracy: 0.8960 - f1_score: 0.8948 - val_loss: 0.3165 - val_accuracy: 0.8698 - val_f1_score: 0.8767\n","Epoch 14/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2368 - accuracy: 0.9102 - f1_score: 0.9096 - val_loss: 0.3547 - val_accuracy: 0.8644 - val_f1_score: 0.8733\n","Epoch 15/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2465 - accuracy: 0.8999 - f1_score: 0.8992 - val_loss: 0.3021 - val_accuracy: 0.8797 - val_f1_score: 0.8818\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8589 - f1_score: 0.8843\n","Epoch 1/20\n","104/104 [==============================] - 6s 24ms/step - loss: 0.6138 - accuracy: 0.6540 - f1_score: 0.6321 - val_loss: 0.5358 - val_accuracy: 0.7405 - val_f1_score: 0.7435\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4825 - accuracy: 0.7767 - f1_score: 0.7662 - val_loss: 0.4454 - val_accuracy: 0.7893 - val_f1_score: 0.7766\n","Epoch 3/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3888 - accuracy: 0.8279 - f1_score: 0.8245 - val_loss: 0.3832 - val_accuracy: 0.8400 - val_f1_score: 0.8427\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3502 - accuracy: 0.8529 - f1_score: 0.8509 - val_loss: 0.3568 - val_accuracy: 0.8490 - val_f1_score: 0.8536\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3260 - accuracy: 0.8611 - f1_score: 0.8596 - val_loss: 0.3978 - val_accuracy: 0.8309 - val_f1_score: 0.8453\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3061 - accuracy: 0.8761 - f1_score: 0.8761 - val_loss: 0.3514 - val_accuracy: 0.8535 - val_f1_score: 0.8594\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2957 - accuracy: 0.8788 - f1_score: 0.8783 - val_loss: 0.3380 - val_accuracy: 0.8635 - val_f1_score: 0.8684\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2902 - accuracy: 0.8819 - f1_score: 0.8810 - val_loss: 0.3234 - val_accuracy: 0.8725 - val_f1_score: 0.8703\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2760 - accuracy: 0.8870 - f1_score: 0.8866 - val_loss: 0.3229 - val_accuracy: 0.8698 - val_f1_score: 0.8710\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2646 - accuracy: 0.8912 - f1_score: 0.8914 - val_loss: 0.3351 - val_accuracy: 0.8571 - val_f1_score: 0.8492\n","Epoch 11/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2948 - accuracy: 0.8734 - f1_score: 0.8720 - val_loss: 0.3159 - val_accuracy: 0.8779 - val_f1_score: 0.8753\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2599 - accuracy: 0.8945 - f1_score: 0.8938 - val_loss: 0.3323 - val_accuracy: 0.8662 - val_f1_score: 0.8713\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2611 - accuracy: 0.8957 - f1_score: 0.8946 - val_loss: 0.3500 - val_accuracy: 0.8617 - val_f1_score: 0.8678\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2456 - accuracy: 0.9030 - f1_score: 0.9021 - val_loss: 0.3371 - val_accuracy: 0.8707 - val_f1_score: 0.8747\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2345 - accuracy: 0.9081 - f1_score: 0.9072 - val_loss: 0.3272 - val_accuracy: 0.8788 - val_f1_score: 0.8782\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2287 - accuracy: 0.9075 - f1_score: 0.9063 - val_loss: 0.3708 - val_accuracy: 0.8617 - val_f1_score: 0.8654\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3462 - accuracy: 0.8555 - f1_score: 0.8780\n","Epoch 1/20\n","104/104 [==============================] - 6s 28ms/step - loss: 0.6214 - accuracy: 0.6447 - f1_score: 0.6123 - val_loss: 0.6203 - val_accuracy: 0.6429 - val_f1_score: 0.7181\n","Epoch 2/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.4667 - accuracy: 0.7848 - f1_score: 0.7789 - val_loss: 0.4157 - val_accuracy: 0.8137 - val_f1_score: 0.8272\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3816 - accuracy: 0.8306 - f1_score: 0.8278 - val_loss: 0.3404 - val_accuracy: 0.8526 - val_f1_score: 0.8517\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3502 - accuracy: 0.8451 - f1_score: 0.8431 - val_loss: 0.3989 - val_accuracy: 0.8255 - val_f1_score: 0.8427\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3468 - accuracy: 0.8553 - f1_score: 0.8552 - val_loss: 0.3150 - val_accuracy: 0.8671 - val_f1_score: 0.8615\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3183 - accuracy: 0.8656 - f1_score: 0.8649 - val_loss: 0.3467 - val_accuracy: 0.8490 - val_f1_score: 0.8322\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3064 - accuracy: 0.8719 - f1_score: 0.8710 - val_loss: 0.2987 - val_accuracy: 0.8788 - val_f1_score: 0.8797\n","Epoch 8/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3100 - accuracy: 0.8713 - f1_score: 0.8712 - val_loss: 0.3105 - val_accuracy: 0.8752 - val_f1_score: 0.8678\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2964 - accuracy: 0.8701 - f1_score: 0.8682 - val_loss: 0.3136 - val_accuracy: 0.8680 - val_f1_score: 0.8735\n","Epoch 10/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2764 - accuracy: 0.8882 - f1_score: 0.8881 - val_loss: 0.3373 - val_accuracy: 0.8517 - val_f1_score: 0.8357\n","Epoch 11/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2775 - accuracy: 0.8870 - f1_score: 0.8865 - val_loss: 0.3067 - val_accuracy: 0.8852 - val_f1_score: 0.8798\n","Epoch 12/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2614 - accuracy: 0.8918 - f1_score: 0.8916 - val_loss: 0.3180 - val_accuracy: 0.8635 - val_f1_score: 0.8713\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3284 - accuracy: 0.8582 - f1_score: 0.8844\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6307 - accuracy: 0.6453 - f1_score: 0.6291 - val_loss: 0.5131 - val_accuracy: 0.7559 - val_f1_score: 0.7332\n","Epoch 2/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.4791 - accuracy: 0.7676 - f1_score: 0.7596 - val_loss: 0.4251 - val_accuracy: 0.8083 - val_f1_score: 0.8160\n","Epoch 3/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3930 - accuracy: 0.8249 - f1_score: 0.8216 - val_loss: 0.4092 - val_accuracy: 0.8264 - val_f1_score: 0.8378\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3691 - accuracy: 0.8348 - f1_score: 0.8304 - val_loss: 0.4030 - val_accuracy: 0.8210 - val_f1_score: 0.8361\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3533 - accuracy: 0.8475 - f1_score: 0.8442 - val_loss: 0.3420 - val_accuracy: 0.8490 - val_f1_score: 0.8486\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3337 - accuracy: 0.8550 - f1_score: 0.8548 - val_loss: 0.3436 - val_accuracy: 0.8608 - val_f1_score: 0.8511\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3299 - accuracy: 0.8559 - f1_score: 0.8548 - val_loss: 0.3265 - val_accuracy: 0.8599 - val_f1_score: 0.8571\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2995 - accuracy: 0.8794 - f1_score: 0.8783 - val_loss: 0.3259 - val_accuracy: 0.8626 - val_f1_score: 0.8650\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2895 - accuracy: 0.8843 - f1_score: 0.8834 - val_loss: 0.3240 - val_accuracy: 0.8716 - val_f1_score: 0.8640\n","Epoch 10/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2874 - accuracy: 0.8813 - f1_score: 0.8805 - val_loss: 0.3169 - val_accuracy: 0.8680 - val_f1_score: 0.8623\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2821 - accuracy: 0.8891 - f1_score: 0.8884 - val_loss: 0.3093 - val_accuracy: 0.8698 - val_f1_score: 0.8644\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2722 - accuracy: 0.8933 - f1_score: 0.8925 - val_loss: 0.3153 - val_accuracy: 0.8725 - val_f1_score: 0.8705\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2866 - accuracy: 0.8849 - f1_score: 0.8854 - val_loss: 0.3307 - val_accuracy: 0.8571 - val_f1_score: 0.8439\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2593 - accuracy: 0.8966 - f1_score: 0.8954 - val_loss: 0.3167 - val_accuracy: 0.8644 - val_f1_score: 0.8585\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2633 - accuracy: 0.8933 - f1_score: 0.8927 - val_loss: 0.3551 - val_accuracy: 0.8508 - val_f1_score: 0.8338\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2565 - accuracy: 0.8969 - f1_score: 0.8968 - val_loss: 0.3295 - val_accuracy: 0.8599 - val_f1_score: 0.8470\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3632 - accuracy: 0.8488 - f1_score: 0.8704\n","Epoch 1/20\n","104/104 [==============================] - 6s 24ms/step - loss: 0.6263 - accuracy: 0.6353 - f1_score: 0.6197 - val_loss: 0.5282 - val_accuracy: 0.7450 - val_f1_score: 0.7163\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.5058 - accuracy: 0.7574 - f1_score: 0.7386 - val_loss: 0.4414 - val_accuracy: 0.7911 - val_f1_score: 0.7755\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4196 - accuracy: 0.8053 - f1_score: 0.7976 - val_loss: 0.3856 - val_accuracy: 0.8345 - val_f1_score: 0.8288\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3637 - accuracy: 0.8406 - f1_score: 0.8365 - val_loss: 0.3642 - val_accuracy: 0.8409 - val_f1_score: 0.8355\n","Epoch 5/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3368 - accuracy: 0.8596 - f1_score: 0.8561 - val_loss: 0.3964 - val_accuracy: 0.8336 - val_f1_score: 0.8435\n","Epoch 6/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3197 - accuracy: 0.8638 - f1_score: 0.8617 - val_loss: 0.3448 - val_accuracy: 0.8553 - val_f1_score: 0.8482\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3049 - accuracy: 0.8749 - f1_score: 0.8734 - val_loss: 0.4002 - val_accuracy: 0.8309 - val_f1_score: 0.8448\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2890 - accuracy: 0.8770 - f1_score: 0.8761 - val_loss: 0.4101 - val_accuracy: 0.8345 - val_f1_score: 0.8111\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2772 - accuracy: 0.8849 - f1_score: 0.8835 - val_loss: 0.3381 - val_accuracy: 0.8698 - val_f1_score: 0.8681\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2743 - accuracy: 0.8876 - f1_score: 0.8855 - val_loss: 0.3590 - val_accuracy: 0.8580 - val_f1_score: 0.8594\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2606 - accuracy: 0.8954 - f1_score: 0.8943 - val_loss: 0.3437 - val_accuracy: 0.8590 - val_f1_score: 0.8547\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2505 - accuracy: 0.8993 - f1_score: 0.8978 - val_loss: 0.4015 - val_accuracy: 0.8282 - val_f1_score: 0.8000\n","Epoch 13/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2415 - accuracy: 0.9030 - f1_score: 0.9010 - val_loss: 0.3779 - val_accuracy: 0.8553 - val_f1_score: 0.8540\n","Epoch 14/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2553 - accuracy: 0.8960 - f1_score: 0.8949 - val_loss: 0.3479 - val_accuracy: 0.8725 - val_f1_score: 0.8683\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.8683 - f1_score: 0.8950\n","Epoch 1/20\n","104/104 [==============================] - 6s 23ms/step - loss: 0.6397 - accuracy: 0.6450 - f1_score: 0.6081 - val_loss: 0.5440 - val_accuracy: 0.7396 - val_f1_score: 0.6897\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4985 - accuracy: 0.7616 - f1_score: 0.7506 - val_loss: 0.4150 - val_accuracy: 0.8047 - val_f1_score: 0.7907\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4278 - accuracy: 0.8026 - f1_score: 0.7961 - val_loss: 0.4518 - val_accuracy: 0.7975 - val_f1_score: 0.8214\n","Epoch 4/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3756 - accuracy: 0.8363 - f1_score: 0.8327 - val_loss: 0.3591 - val_accuracy: 0.8445 - val_f1_score: 0.8504\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3502 - accuracy: 0.8556 - f1_score: 0.8531 - val_loss: 0.3626 - val_accuracy: 0.8418 - val_f1_score: 0.8495\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3255 - accuracy: 0.8659 - f1_score: 0.8640 - val_loss: 0.3214 - val_accuracy: 0.8752 - val_f1_score: 0.8678\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3079 - accuracy: 0.8722 - f1_score: 0.8699 - val_loss: 0.3065 - val_accuracy: 0.8788 - val_f1_score: 0.8748\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3027 - accuracy: 0.8731 - f1_score: 0.8697 - val_loss: 0.3313 - val_accuracy: 0.8617 - val_f1_score: 0.8499\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2987 - accuracy: 0.8755 - f1_score: 0.8734 - val_loss: 0.3041 - val_accuracy: 0.8779 - val_f1_score: 0.8744\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2933 - accuracy: 0.8843 - f1_score: 0.8821 - val_loss: 0.3140 - val_accuracy: 0.8770 - val_f1_score: 0.8743\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2805 - accuracy: 0.8846 - f1_score: 0.8820 - val_loss: 0.3253 - val_accuracy: 0.8562 - val_f1_score: 0.8418\n","Epoch 12/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2756 - accuracy: 0.8864 - f1_score: 0.8847 - val_loss: 0.3535 - val_accuracy: 0.8544 - val_f1_score: 0.8375\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2728 - accuracy: 0.8936 - f1_score: 0.8921 - val_loss: 0.3100 - val_accuracy: 0.8852 - val_f1_score: 0.8851\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2764 - accuracy: 0.8819 - f1_score: 0.8805 - val_loss: 0.3129 - val_accuracy: 0.8698 - val_f1_score: 0.8613\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8623 - f1_score: 0.8903\n","Epoch 1/20\n","104/104 [==============================] - 8s 33ms/step - loss: 0.6334 - accuracy: 0.6423 - f1_score: 0.6331 - val_loss: 0.5736 - val_accuracy: 0.7061 - val_f1_score: 0.7334\n","Epoch 2/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.5229 - accuracy: 0.7471 - f1_score: 0.7296 - val_loss: 0.4735 - val_accuracy: 0.7803 - val_f1_score: 0.7781\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4422 - accuracy: 0.7869 - f1_score: 0.7730 - val_loss: 0.4496 - val_accuracy: 0.8083 - val_f1_score: 0.8191\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3997 - accuracy: 0.8273 - f1_score: 0.8207 - val_loss: 0.4625 - val_accuracy: 0.7893 - val_f1_score: 0.8167\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3634 - accuracy: 0.8333 - f1_score: 0.8293 - val_loss: 0.4083 - val_accuracy: 0.8300 - val_f1_score: 0.8418\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3363 - accuracy: 0.8544 - f1_score: 0.8514 - val_loss: 0.3870 - val_accuracy: 0.8418 - val_f1_score: 0.8506\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3188 - accuracy: 0.8614 - f1_score: 0.8578 - val_loss: 0.5971 - val_accuracy: 0.7396 - val_f1_score: 0.7889\n","Epoch 8/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3138 - accuracy: 0.8698 - f1_score: 0.8674 - val_loss: 0.3802 - val_accuracy: 0.8553 - val_f1_score: 0.8625\n","Epoch 9/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2922 - accuracy: 0.8788 - f1_score: 0.8764 - val_loss: 0.3766 - val_accuracy: 0.8626 - val_f1_score: 0.8674\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2746 - accuracy: 0.8825 - f1_score: 0.8806 - val_loss: 0.4479 - val_accuracy: 0.8255 - val_f1_score: 0.8442\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2742 - accuracy: 0.8834 - f1_score: 0.8820 - val_loss: 0.3880 - val_accuracy: 0.8635 - val_f1_score: 0.8655\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2553 - accuracy: 0.8897 - f1_score: 0.8877 - val_loss: 0.4072 - val_accuracy: 0.8481 - val_f1_score: 0.8583\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2602 - accuracy: 0.8948 - f1_score: 0.8943 - val_loss: 0.4734 - val_accuracy: 0.8391 - val_f1_score: 0.8519\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2404 - accuracy: 0.8990 - f1_score: 0.8970 - val_loss: 0.5497 - val_accuracy: 0.7975 - val_f1_score: 0.8250\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3289 - accuracy: 0.8663 - f1_score: 0.8931\n","Epoch 1/20\n","104/104 [==============================] - 7s 24ms/step - loss: 0.6355 - accuracy: 0.6471 - f1_score: 0.6330 - val_loss: 0.5264 - val_accuracy: 0.7414 - val_f1_score: 0.7123\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4812 - accuracy: 0.7676 - f1_score: 0.7602 - val_loss: 0.3941 - val_accuracy: 0.8309 - val_f1_score: 0.8286\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3892 - accuracy: 0.8261 - f1_score: 0.8233 - val_loss: 0.4630 - val_accuracy: 0.7803 - val_f1_score: 0.8103\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3547 - accuracy: 0.8436 - f1_score: 0.8417 - val_loss: 0.5113 - val_accuracy: 0.7622 - val_f1_score: 0.8024\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3407 - accuracy: 0.8544 - f1_score: 0.8521 - val_loss: 0.3366 - val_accuracy: 0.8662 - val_f1_score: 0.8593\n","Epoch 6/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3256 - accuracy: 0.8674 - f1_score: 0.8644 - val_loss: 0.3158 - val_accuracy: 0.8743 - val_f1_score: 0.8721\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3234 - accuracy: 0.8653 - f1_score: 0.8633 - val_loss: 0.4022 - val_accuracy: 0.8137 - val_f1_score: 0.8352\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2942 - accuracy: 0.8764 - f1_score: 0.8748 - val_loss: 0.4578 - val_accuracy: 0.7920 - val_f1_score: 0.8206\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2820 - accuracy: 0.8831 - f1_score: 0.8808 - val_loss: 0.4398 - val_accuracy: 0.7993 - val_f1_score: 0.8246\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2768 - accuracy: 0.8846 - f1_score: 0.8829 - val_loss: 0.3195 - val_accuracy: 0.8834 - val_f1_score: 0.8855\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2704 - accuracy: 0.8867 - f1_score: 0.8849 - val_loss: 0.4092 - val_accuracy: 0.8255 - val_f1_score: 0.8435\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3386 - accuracy: 0.8569 - f1_score: 0.8800\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6427 - accuracy: 0.6263 - f1_score: 0.5872 - val_loss: 0.5542 - val_accuracy: 0.7188 - val_f1_score: 0.6797\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4955 - accuracy: 0.7646 - f1_score: 0.7478 - val_loss: 0.4539 - val_accuracy: 0.7884 - val_f1_score: 0.7979\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4177 - accuracy: 0.8101 - f1_score: 0.8041 - val_loss: 0.3866 - val_accuracy: 0.8255 - val_f1_score: 0.8311\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3769 - accuracy: 0.8354 - f1_score: 0.8333 - val_loss: 0.3737 - val_accuracy: 0.8309 - val_f1_score: 0.8367\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3491 - accuracy: 0.8493 - f1_score: 0.8463 - val_loss: 0.3567 - val_accuracy: 0.8490 - val_f1_score: 0.8539\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3230 - accuracy: 0.8611 - f1_score: 0.8592 - val_loss: 0.4080 - val_accuracy: 0.8137 - val_f1_score: 0.8331\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3229 - accuracy: 0.8656 - f1_score: 0.8635 - val_loss: 0.4182 - val_accuracy: 0.8146 - val_f1_score: 0.8351\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3063 - accuracy: 0.8755 - f1_score: 0.8746 - val_loss: 0.3322 - val_accuracy: 0.8562 - val_f1_score: 0.8513\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2912 - accuracy: 0.8831 - f1_score: 0.8813 - val_loss: 0.3222 - val_accuracy: 0.8707 - val_f1_score: 0.8715\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2850 - accuracy: 0.8849 - f1_score: 0.8837 - val_loss: 0.3352 - val_accuracy: 0.8599 - val_f1_score: 0.8569\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2828 - accuracy: 0.8834 - f1_score: 0.8827 - val_loss: 0.3289 - val_accuracy: 0.8535 - val_f1_score: 0.8599\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2750 - accuracy: 0.8891 - f1_score: 0.8877 - val_loss: 0.3675 - val_accuracy: 0.8481 - val_f1_score: 0.8591\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2783 - accuracy: 0.8861 - f1_score: 0.8854 - val_loss: 0.3589 - val_accuracy: 0.8562 - val_f1_score: 0.8430\n","Epoch 14/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2612 - accuracy: 0.8969 - f1_score: 0.8956 - val_loss: 0.3104 - val_accuracy: 0.8662 - val_f1_score: 0.8645\n","Epoch 15/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2543 - accuracy: 0.8966 - f1_score: 0.8953 - val_loss: 0.3360 - val_accuracy: 0.8599 - val_f1_score: 0.8508\n","Epoch 16/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2483 - accuracy: 0.9020 - f1_score: 0.9014 - val_loss: 0.3134 - val_accuracy: 0.8752 - val_f1_score: 0.8783\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2342 - accuracy: 0.8984 - f1_score: 0.8985 - val_loss: 0.3397 - val_accuracy: 0.8617 - val_f1_score: 0.8550\n","Epoch 18/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2236 - accuracy: 0.9120 - f1_score: 0.9117 - val_loss: 0.3685 - val_accuracy: 0.8544 - val_f1_score: 0.8623\n","Epoch 19/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2137 - accuracy: 0.9123 - f1_score: 0.9118 - val_loss: 0.3317 - val_accuracy: 0.8761 - val_f1_score: 0.8753\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3305 - accuracy: 0.8636 - f1_score: 0.8882\n","Epoch 1/20\n","104/104 [==============================] - 7s 24ms/step - loss: 0.6202 - accuracy: 0.6570 - f1_score: 0.6539 - val_loss: 0.5147 - val_accuracy: 0.7468 - val_f1_score: 0.7233\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4659 - accuracy: 0.7752 - f1_score: 0.7620 - val_loss: 0.4330 - val_accuracy: 0.8110 - val_f1_score: 0.8187\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3859 - accuracy: 0.8282 - f1_score: 0.8227 - val_loss: 0.3711 - val_accuracy: 0.8354 - val_f1_score: 0.8363\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3514 - accuracy: 0.8475 - f1_score: 0.8442 - val_loss: 0.5088 - val_accuracy: 0.7613 - val_f1_score: 0.8018\n","Epoch 5/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3399 - accuracy: 0.8571 - f1_score: 0.8555 - val_loss: 0.3302 - val_accuracy: 0.8535 - val_f1_score: 0.8543\n","Epoch 6/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3056 - accuracy: 0.8719 - f1_score: 0.8701 - val_loss: 0.3949 - val_accuracy: 0.8282 - val_f1_score: 0.8455\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2887 - accuracy: 0.8782 - f1_score: 0.8768 - val_loss: 0.3459 - val_accuracy: 0.8517 - val_f1_score: 0.8615\n","Epoch 8/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2835 - accuracy: 0.8819 - f1_score: 0.8804 - val_loss: 0.3335 - val_accuracy: 0.8562 - val_f1_score: 0.8623\n","Epoch 9/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2736 - accuracy: 0.8897 - f1_score: 0.8894 - val_loss: 0.3215 - val_accuracy: 0.8689 - val_f1_score: 0.8636\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2702 - accuracy: 0.8906 - f1_score: 0.8895 - val_loss: 0.3152 - val_accuracy: 0.8680 - val_f1_score: 0.8668\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2667 - accuracy: 0.8945 - f1_score: 0.8932 - val_loss: 0.3312 - val_accuracy: 0.8626 - val_f1_score: 0.8655\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2537 - accuracy: 0.8969 - f1_score: 0.8952 - val_loss: 0.3206 - val_accuracy: 0.8653 - val_f1_score: 0.8676\n","Epoch 13/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2476 - accuracy: 0.8972 - f1_score: 0.8965 - val_loss: 0.3228 - val_accuracy: 0.8653 - val_f1_score: 0.8687\n","Epoch 14/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2365 - accuracy: 0.9078 - f1_score: 0.9067 - val_loss: 0.3544 - val_accuracy: 0.8535 - val_f1_score: 0.8608\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2381 - accuracy: 0.9030 - f1_score: 0.9022 - val_loss: 0.3290 - val_accuracy: 0.8770 - val_f1_score: 0.8687\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3448 - accuracy: 0.8636 - f1_score: 0.8863\n","Epoch 1/20\n","104/104 [==============================] - 6s 26ms/step - loss: 0.6191 - accuracy: 0.6582 - f1_score: 0.6570 - val_loss: 0.5399 - val_accuracy: 0.7351 - val_f1_score: 0.7067\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4823 - accuracy: 0.7755 - f1_score: 0.7654 - val_loss: 0.4883 - val_accuracy: 0.7712 - val_f1_score: 0.7879\n","Epoch 3/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.4247 - accuracy: 0.8065 - f1_score: 0.7990 - val_loss: 0.4138 - val_accuracy: 0.8110 - val_f1_score: 0.8181\n","Epoch 4/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3798 - accuracy: 0.8388 - f1_score: 0.8349 - val_loss: 0.3800 - val_accuracy: 0.8237 - val_f1_score: 0.8090\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3474 - accuracy: 0.8454 - f1_score: 0.8437 - val_loss: 0.3480 - val_accuracy: 0.8409 - val_f1_score: 0.8470\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3293 - accuracy: 0.8550 - f1_score: 0.8527 - val_loss: 0.3802 - val_accuracy: 0.8264 - val_f1_score: 0.8421\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3102 - accuracy: 0.8701 - f1_score: 0.8688 - val_loss: 0.3359 - val_accuracy: 0.8590 - val_f1_score: 0.8491\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3072 - accuracy: 0.8683 - f1_score: 0.8672 - val_loss: 0.3319 - val_accuracy: 0.8662 - val_f1_score: 0.8702\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2768 - accuracy: 0.8897 - f1_score: 0.8883 - val_loss: 0.3475 - val_accuracy: 0.8544 - val_f1_score: 0.8625\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2692 - accuracy: 0.8915 - f1_score: 0.8908 - val_loss: 0.3780 - val_accuracy: 0.8273 - val_f1_score: 0.8008\n","Epoch 11/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2785 - accuracy: 0.8803 - f1_score: 0.8783 - val_loss: 0.3105 - val_accuracy: 0.8734 - val_f1_score: 0.8704\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2548 - accuracy: 0.8996 - f1_score: 0.8981 - val_loss: 0.3238 - val_accuracy: 0.8635 - val_f1_score: 0.8555\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2511 - accuracy: 0.8996 - f1_score: 0.8979 - val_loss: 0.3264 - val_accuracy: 0.8716 - val_f1_score: 0.8714\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2488 - accuracy: 0.8972 - f1_score: 0.8956 - val_loss: 0.3329 - val_accuracy: 0.8770 - val_f1_score: 0.8805\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2279 - accuracy: 0.9147 - f1_score: 0.9133 - val_loss: 0.3280 - val_accuracy: 0.8698 - val_f1_score: 0.8698\n","Epoch 16/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2375 - accuracy: 0.9039 - f1_score: 0.9022 - val_loss: 0.3377 - val_accuracy: 0.8707 - val_f1_score: 0.8696\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3345 - accuracy: 0.8575 - f1_score: 0.8803\n","Epoch 1/20\n","104/104 [==============================] - 6s 24ms/step - loss: 0.6354 - accuracy: 0.6359 - f1_score: 0.6101 - val_loss: 0.5376 - val_accuracy: 0.7459 - val_f1_score: 0.7480\n","Epoch 2/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.5067 - accuracy: 0.7577 - f1_score: 0.7462 - val_loss: 0.5148 - val_accuracy: 0.7604 - val_f1_score: 0.7052\n","Epoch 3/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.4315 - accuracy: 0.7990 - f1_score: 0.7908 - val_loss: 0.3995 - val_accuracy: 0.8174 - val_f1_score: 0.7900\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3700 - accuracy: 0.8394 - f1_score: 0.8354 - val_loss: 0.3366 - val_accuracy: 0.8481 - val_f1_score: 0.8511\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3347 - accuracy: 0.8568 - f1_score: 0.8550 - val_loss: 0.3199 - val_accuracy: 0.8617 - val_f1_score: 0.8620\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3114 - accuracy: 0.8767 - f1_score: 0.8755 - val_loss: 0.3205 - val_accuracy: 0.8698 - val_f1_score: 0.8634\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3262 - accuracy: 0.8659 - f1_score: 0.8640 - val_loss: 0.4531 - val_accuracy: 0.8038 - val_f1_score: 0.7634\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3078 - accuracy: 0.8752 - f1_score: 0.8726 - val_loss: 0.3415 - val_accuracy: 0.8535 - val_f1_score: 0.8402\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2932 - accuracy: 0.8852 - f1_score: 0.8834 - val_loss: 0.3067 - val_accuracy: 0.8698 - val_f1_score: 0.8721\n","Epoch 10/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2914 - accuracy: 0.8800 - f1_score: 0.8784 - val_loss: 0.3021 - val_accuracy: 0.8788 - val_f1_score: 0.8759\n","Epoch 11/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2781 - accuracy: 0.8873 - f1_score: 0.8858 - val_loss: 0.3297 - val_accuracy: 0.8626 - val_f1_score: 0.8510\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2652 - accuracy: 0.8978 - f1_score: 0.8968 - val_loss: 0.3511 - val_accuracy: 0.8517 - val_f1_score: 0.8360\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2700 - accuracy: 0.8912 - f1_score: 0.8898 - val_loss: 0.2970 - val_accuracy: 0.8779 - val_f1_score: 0.8780\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2531 - accuracy: 0.9014 - f1_score: 0.9005 - val_loss: 0.4088 - val_accuracy: 0.8119 - val_f1_score: 0.7768\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2410 - accuracy: 0.9063 - f1_score: 0.9052 - val_loss: 0.3140 - val_accuracy: 0.8671 - val_f1_score: 0.8609\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2480 - accuracy: 0.9036 - f1_score: 0.9030 - val_loss: 0.2967 - val_accuracy: 0.8752 - val_f1_score: 0.8748\n","Epoch 17/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2351 - accuracy: 0.9108 - f1_score: 0.9099 - val_loss: 0.3708 - val_accuracy: 0.8481 - val_f1_score: 0.8296\n","Epoch 18/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2181 - accuracy: 0.9180 - f1_score: 0.9171 - val_loss: 0.3547 - val_accuracy: 0.8517 - val_f1_score: 0.8370\n","Epoch 19/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2075 - accuracy: 0.9265 - f1_score: 0.9262 - val_loss: 0.3518 - val_accuracy: 0.8490 - val_f1_score: 0.8383\n","Epoch 20/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2035 - accuracy: 0.9195 - f1_score: 0.9184 - val_loss: 0.3575 - val_accuracy: 0.8743 - val_f1_score: 0.8767\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4249 - accuracy: 0.8602 - f1_score: 0.8892\n","Epoch 1/20\n","104/104 [==============================] - 6s 23ms/step - loss: 0.6348 - accuracy: 0.6404 - f1_score: 0.6238 - val_loss: 0.5523 - val_accuracy: 0.7315 - val_f1_score: 0.7397\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4741 - accuracy: 0.7812 - f1_score: 0.7707 - val_loss: 0.4292 - val_accuracy: 0.7902 - val_f1_score: 0.7824\n","Epoch 3/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3855 - accuracy: 0.8348 - f1_score: 0.8312 - val_loss: 0.3947 - val_accuracy: 0.8246 - val_f1_score: 0.8307\n","Epoch 4/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3557 - accuracy: 0.8439 - f1_score: 0.8421 - val_loss: 0.3684 - val_accuracy: 0.8436 - val_f1_score: 0.8360\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3247 - accuracy: 0.8626 - f1_score: 0.8613 - val_loss: 0.3599 - val_accuracy: 0.8526 - val_f1_score: 0.8481\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3076 - accuracy: 0.8731 - f1_score: 0.8722 - val_loss: 0.3609 - val_accuracy: 0.8490 - val_f1_score: 0.8516\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2971 - accuracy: 0.8773 - f1_score: 0.8772 - val_loss: 0.4258 - val_accuracy: 0.8345 - val_f1_score: 0.8168\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2946 - accuracy: 0.8794 - f1_score: 0.8782 - val_loss: 0.3414 - val_accuracy: 0.8599 - val_f1_score: 0.8610\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2857 - accuracy: 0.8846 - f1_score: 0.8839 - val_loss: 0.3371 - val_accuracy: 0.8590 - val_f1_score: 0.8523\n","Epoch 10/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2830 - accuracy: 0.8870 - f1_score: 0.8868 - val_loss: 0.3464 - val_accuracy: 0.8599 - val_f1_score: 0.8577\n","Epoch 11/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2626 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3729 - val_accuracy: 0.8590 - val_f1_score: 0.8477\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2656 - accuracy: 0.8957 - f1_score: 0.8944 - val_loss: 0.3553 - val_accuracy: 0.8562 - val_f1_score: 0.8564\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2481 - accuracy: 0.9045 - f1_score: 0.9034 - val_loss: 0.4399 - val_accuracy: 0.8327 - val_f1_score: 0.8099\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2456 - accuracy: 0.9030 - f1_score: 0.9019 - val_loss: 0.3529 - val_accuracy: 0.8590 - val_f1_score: 0.8641\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3521 - accuracy: 0.8454 - f1_score: 0.8692\n","Epoch 1/20\n","104/104 [==============================] - 9s 39ms/step - loss: 0.6363 - accuracy: 0.6380 - f1_score: 0.6097 - val_loss: 0.5235 - val_accuracy: 0.7523 - val_f1_score: 0.6976\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4736 - accuracy: 0.7836 - f1_score: 0.7765 - val_loss: 0.3607 - val_accuracy: 0.8454 - val_f1_score: 0.8540\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3922 - accuracy: 0.8264 - f1_score: 0.8245 - val_loss: 0.3172 - val_accuracy: 0.8689 - val_f1_score: 0.8720\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3545 - accuracy: 0.8532 - f1_score: 0.8527 - val_loss: 0.3616 - val_accuracy: 0.8436 - val_f1_score: 0.8592\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3469 - accuracy: 0.8532 - f1_score: 0.8522 - val_loss: 0.3124 - val_accuracy: 0.8680 - val_f1_score: 0.8761\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3330 - accuracy: 0.8605 - f1_score: 0.8604 - val_loss: 0.2884 - val_accuracy: 0.8843 - val_f1_score: 0.8873\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3148 - accuracy: 0.8743 - f1_score: 0.8746 - val_loss: 0.2708 - val_accuracy: 0.8888 - val_f1_score: 0.8895\n","Epoch 8/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3110 - accuracy: 0.8719 - f1_score: 0.8716 - val_loss: 0.2694 - val_accuracy: 0.8924 - val_f1_score: 0.8933\n","Epoch 9/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2962 - accuracy: 0.8776 - f1_score: 0.8770 - val_loss: 0.3656 - val_accuracy: 0.8463 - val_f1_score: 0.8616\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2861 - accuracy: 0.8834 - f1_score: 0.8823 - val_loss: 0.3609 - val_accuracy: 0.8472 - val_f1_score: 0.8625\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3087 - accuracy: 0.8680 - f1_score: 0.8666 - val_loss: 0.2656 - val_accuracy: 0.8933 - val_f1_score: 0.8929\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2848 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.2793 - val_accuracy: 0.8978 - val_f1_score: 0.8912\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2665 - accuracy: 0.8945 - f1_score: 0.8934 - val_loss: 0.3630 - val_accuracy: 0.8445 - val_f1_score: 0.8602\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2675 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.2694 - val_accuracy: 0.9005 - val_f1_score: 0.9025\n","Epoch 15/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2461 - accuracy: 0.9087 - f1_score: 0.9078 - val_loss: 0.2682 - val_accuracy: 0.9033 - val_f1_score: 0.9032\n","Epoch 16/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2528 - accuracy: 0.8969 - f1_score: 0.8959 - val_loss: 0.2982 - val_accuracy: 0.8834 - val_f1_score: 0.8896\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3331 - accuracy: 0.8656 - f1_score: 0.8890\n","Epoch 1/20\n","104/104 [==============================] - 6s 25ms/step - loss: 0.6307 - accuracy: 0.6483 - f1_score: 0.6399 - val_loss: 0.5499 - val_accuracy: 0.7306 - val_f1_score: 0.6768\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4718 - accuracy: 0.7827 - f1_score: 0.7766 - val_loss: 0.4260 - val_accuracy: 0.7993 - val_f1_score: 0.7784\n","Epoch 3/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3915 - accuracy: 0.8336 - f1_score: 0.8315 - val_loss: 0.3711 - val_accuracy: 0.8391 - val_f1_score: 0.8416\n","Epoch 4/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3527 - accuracy: 0.8577 - f1_score: 0.8559 - val_loss: 0.3817 - val_accuracy: 0.8264 - val_f1_score: 0.8099\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3339 - accuracy: 0.8614 - f1_score: 0.8605 - val_loss: 0.3456 - val_accuracy: 0.8553 - val_f1_score: 0.8579\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3153 - accuracy: 0.8722 - f1_score: 0.8710 - val_loss: 0.3557 - val_accuracy: 0.8553 - val_f1_score: 0.8594\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3037 - accuracy: 0.8773 - f1_score: 0.8756 - val_loss: 0.3367 - val_accuracy: 0.8580 - val_f1_score: 0.8561\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2871 - accuracy: 0.8858 - f1_score: 0.8844 - val_loss: 0.3528 - val_accuracy: 0.8481 - val_f1_score: 0.8359\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2817 - accuracy: 0.8870 - f1_score: 0.8851 - val_loss: 0.3228 - val_accuracy: 0.8671 - val_f1_score: 0.8665\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2885 - accuracy: 0.8849 - f1_score: 0.8833 - val_loss: 0.3126 - val_accuracy: 0.8743 - val_f1_score: 0.8740\n","Epoch 11/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2666 - accuracy: 0.8915 - f1_score: 0.8906 - val_loss: 0.3237 - val_accuracy: 0.8671 - val_f1_score: 0.8689\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2699 - accuracy: 0.8936 - f1_score: 0.8926 - val_loss: 0.3583 - val_accuracy: 0.8499 - val_f1_score: 0.8369\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2635 - accuracy: 0.8954 - f1_score: 0.8943 - val_loss: 0.3281 - val_accuracy: 0.8644 - val_f1_score: 0.8580\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2513 - accuracy: 0.9036 - f1_score: 0.9024 - val_loss: 0.3502 - val_accuracy: 0.8653 - val_f1_score: 0.8678\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2540 - accuracy: 0.9005 - f1_score: 0.8990 - val_loss: 0.3334 - val_accuracy: 0.8644 - val_f1_score: 0.8698\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8663 - f1_score: 0.8896\n","Epoch 1/20\n","104/104 [==============================] - 7s 33ms/step - loss: 0.6379 - accuracy: 0.6335 - f1_score: 0.6103 - val_loss: 0.5579 - val_accuracy: 0.7233 - val_f1_score: 0.6779\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.5054 - accuracy: 0.7583 - f1_score: 0.7500 - val_loss: 0.4808 - val_accuracy: 0.7821 - val_f1_score: 0.8023\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3982 - accuracy: 0.8165 - f1_score: 0.8112 - val_loss: 0.4295 - val_accuracy: 0.8228 - val_f1_score: 0.8364\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3635 - accuracy: 0.8439 - f1_score: 0.8404 - val_loss: 0.3777 - val_accuracy: 0.8427 - val_f1_score: 0.8404\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3253 - accuracy: 0.8680 - f1_score: 0.8670 - val_loss: 0.3674 - val_accuracy: 0.8535 - val_f1_score: 0.8551\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3118 - accuracy: 0.8722 - f1_score: 0.8702 - val_loss: 0.4150 - val_accuracy: 0.8327 - val_f1_score: 0.8460\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2953 - accuracy: 0.8776 - f1_score: 0.8758 - val_loss: 0.3659 - val_accuracy: 0.8517 - val_f1_score: 0.8444\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2908 - accuracy: 0.8840 - f1_score: 0.8827 - val_loss: 0.3683 - val_accuracy: 0.8499 - val_f1_score: 0.8404\n","Epoch 9/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2868 - accuracy: 0.8858 - f1_score: 0.8839 - val_loss: 0.3732 - val_accuracy: 0.8445 - val_f1_score: 0.8520\n","Epoch 10/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2774 - accuracy: 0.8864 - f1_score: 0.8857 - val_loss: 0.3728 - val_accuracy: 0.8562 - val_f1_score: 0.8507\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2748 - accuracy: 0.8933 - f1_score: 0.8920 - val_loss: 0.4162 - val_accuracy: 0.8363 - val_f1_score: 0.8475\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2563 - accuracy: 0.8987 - f1_score: 0.8982 - val_loss: 0.3645 - val_accuracy: 0.8490 - val_f1_score: 0.8549\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2552 - accuracy: 0.8993 - f1_score: 0.8981 - val_loss: 0.3605 - val_accuracy: 0.8635 - val_f1_score: 0.8636\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2408 - accuracy: 0.9132 - f1_score: 0.9121 - val_loss: 0.3909 - val_accuracy: 0.8490 - val_f1_score: 0.8355\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2380 - accuracy: 0.9099 - f1_score: 0.9094 - val_loss: 0.3472 - val_accuracy: 0.8590 - val_f1_score: 0.8553\n","Epoch 16/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2373 - accuracy: 0.9102 - f1_score: 0.9089 - val_loss: 0.3735 - val_accuracy: 0.8580 - val_f1_score: 0.8506\n","Epoch 17/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2410 - accuracy: 0.9063 - f1_score: 0.9052 - val_loss: 0.4750 - val_accuracy: 0.8011 - val_f1_score: 0.8259\n","Epoch 18/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2262 - accuracy: 0.9081 - f1_score: 0.9069 - val_loss: 0.4095 - val_accuracy: 0.8363 - val_f1_score: 0.8457\n","Epoch 19/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2137 - accuracy: 0.9210 - f1_score: 0.9201 - val_loss: 0.4032 - val_accuracy: 0.8490 - val_f1_score: 0.8557\n","Epoch 20/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2184 - accuracy: 0.9132 - f1_score: 0.9124 - val_loss: 0.3979 - val_accuracy: 0.8490 - val_f1_score: 0.8549\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3522 - accuracy: 0.8521 - f1_score: 0.8732\n","Epoch 1/20\n","104/104 [==============================] - 9s 31ms/step - loss: 0.6331 - accuracy: 0.6398 - f1_score: 0.6344 - val_loss: 0.5350 - val_accuracy: 0.7432 - val_f1_score: 0.7609\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4940 - accuracy: 0.7634 - f1_score: 0.7529 - val_loss: 0.4672 - val_accuracy: 0.7685 - val_f1_score: 0.7187\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4071 - accuracy: 0.8204 - f1_score: 0.8142 - val_loss: 0.3714 - val_accuracy: 0.8436 - val_f1_score: 0.8457\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3655 - accuracy: 0.8415 - f1_score: 0.8380 - val_loss: 0.3491 - val_accuracy: 0.8571 - val_f1_score: 0.8577\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3308 - accuracy: 0.8647 - f1_score: 0.8626 - val_loss: 0.3944 - val_accuracy: 0.8219 - val_f1_score: 0.8402\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3199 - accuracy: 0.8710 - f1_score: 0.8695 - val_loss: 0.3448 - val_accuracy: 0.8635 - val_f1_score: 0.8688\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3121 - accuracy: 0.8737 - f1_score: 0.8717 - val_loss: 0.4302 - val_accuracy: 0.7993 - val_f1_score: 0.8263\n","Epoch 8/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3251 - accuracy: 0.8680 - f1_score: 0.8670 - val_loss: 0.3282 - val_accuracy: 0.8779 - val_f1_score: 0.8798\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2906 - accuracy: 0.8831 - f1_score: 0.8821 - val_loss: 0.3038 - val_accuracy: 0.8779 - val_f1_score: 0.8744\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2914 - accuracy: 0.8837 - f1_score: 0.8830 - val_loss: 0.3267 - val_accuracy: 0.8662 - val_f1_score: 0.8695\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2743 - accuracy: 0.8912 - f1_score: 0.8901 - val_loss: 0.3085 - val_accuracy: 0.8752 - val_f1_score: 0.8698\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2706 - accuracy: 0.8969 - f1_score: 0.8956 - val_loss: 0.3033 - val_accuracy: 0.8825 - val_f1_score: 0.8792\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2594 - accuracy: 0.9027 - f1_score: 0.9018 - val_loss: 0.3004 - val_accuracy: 0.8843 - val_f1_score: 0.8806\n","Epoch 14/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2466 - accuracy: 0.9057 - f1_score: 0.9045 - val_loss: 0.3177 - val_accuracy: 0.8807 - val_f1_score: 0.8752\n","Epoch 15/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2457 - accuracy: 0.9048 - f1_score: 0.9041 - val_loss: 0.3076 - val_accuracy: 0.8852 - val_f1_score: 0.8812\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2371 - accuracy: 0.9120 - f1_score: 0.9109 - val_loss: 0.3361 - val_accuracy: 0.8734 - val_f1_score: 0.8641\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2365 - accuracy: 0.9087 - f1_score: 0.9075 - val_loss: 0.3300 - val_accuracy: 0.8689 - val_f1_score: 0.8681\n","Epoch 18/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2303 - accuracy: 0.9138 - f1_score: 0.9126 - val_loss: 0.3424 - val_accuracy: 0.8734 - val_f1_score: 0.8776\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3562 - accuracy: 0.8555 - f1_score: 0.8762\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6247 - accuracy: 0.6561 - f1_score: 0.6413 - val_loss: 0.5595 - val_accuracy: 0.7061 - val_f1_score: 0.7347\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4926 - accuracy: 0.7740 - f1_score: 0.7674 - val_loss: 0.4296 - val_accuracy: 0.8020 - val_f1_score: 0.7859\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4222 - accuracy: 0.8140 - f1_score: 0.8101 - val_loss: 0.3915 - val_accuracy: 0.8363 - val_f1_score: 0.8316\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3569 - accuracy: 0.8499 - f1_score: 0.8480 - val_loss: 0.3589 - val_accuracy: 0.8544 - val_f1_score: 0.8513\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3377 - accuracy: 0.8571 - f1_score: 0.8544 - val_loss: 0.3758 - val_accuracy: 0.8409 - val_f1_score: 0.8511\n","Epoch 6/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3310 - accuracy: 0.8596 - f1_score: 0.8579 - val_loss: 0.3299 - val_accuracy: 0.8752 - val_f1_score: 0.8741\n","Epoch 7/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3074 - accuracy: 0.8713 - f1_score: 0.8695 - val_loss: 0.3301 - val_accuracy: 0.8716 - val_f1_score: 0.8748\n","Epoch 8/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2912 - accuracy: 0.8819 - f1_score: 0.8814 - val_loss: 0.3237 - val_accuracy: 0.8716 - val_f1_score: 0.8655\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2942 - accuracy: 0.8810 - f1_score: 0.8799 - val_loss: 0.3560 - val_accuracy: 0.8463 - val_f1_score: 0.8303\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2841 - accuracy: 0.8794 - f1_score: 0.8783 - val_loss: 0.3229 - val_accuracy: 0.8707 - val_f1_score: 0.8642\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2728 - accuracy: 0.8864 - f1_score: 0.8851 - val_loss: 0.3672 - val_accuracy: 0.8526 - val_f1_score: 0.8378\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2622 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3807 - val_accuracy: 0.8309 - val_f1_score: 0.8086\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2746 - accuracy: 0.8927 - f1_score: 0.8915 - val_loss: 0.3283 - val_accuracy: 0.8716 - val_f1_score: 0.8741\n","Epoch 14/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2485 - accuracy: 0.9014 - f1_score: 0.9003 - val_loss: 0.3114 - val_accuracy: 0.8843 - val_f1_score: 0.8836\n","Epoch 15/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2377 - accuracy: 0.9075 - f1_score: 0.9060 - val_loss: 0.3657 - val_accuracy: 0.8626 - val_f1_score: 0.8507\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2281 - accuracy: 0.9132 - f1_score: 0.9120 - val_loss: 0.3408 - val_accuracy: 0.8571 - val_f1_score: 0.8484\n","Epoch 17/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2242 - accuracy: 0.9153 - f1_score: 0.9149 - val_loss: 0.3235 - val_accuracy: 0.8770 - val_f1_score: 0.8702\n","Epoch 18/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2084 - accuracy: 0.9247 - f1_score: 0.9235 - val_loss: 0.3538 - val_accuracy: 0.8608 - val_f1_score: 0.8675\n","Epoch 19/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2005 - accuracy: 0.9253 - f1_score: 0.9247 - val_loss: 0.3367 - val_accuracy: 0.8807 - val_f1_score: 0.8787\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3020 - accuracy: 0.8805 - f1_score: 0.9020\n","Epoch 1/20\n","104/104 [==============================] - 7s 24ms/step - loss: 0.6337 - accuracy: 0.6398 - f1_score: 0.6301 - val_loss: 0.5206 - val_accuracy: 0.7459 - val_f1_score: 0.7164\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4909 - accuracy: 0.7646 - f1_score: 0.7498 - val_loss: 0.4224 - val_accuracy: 0.8038 - val_f1_score: 0.7737\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4206 - accuracy: 0.8092 - f1_score: 0.8020 - val_loss: 0.6437 - val_accuracy: 0.6872 - val_f1_score: 0.7570\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3850 - accuracy: 0.8351 - f1_score: 0.8306 - val_loss: 0.3491 - val_accuracy: 0.8562 - val_f1_score: 0.8478\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3463 - accuracy: 0.8541 - f1_score: 0.8509 - val_loss: 0.3302 - val_accuracy: 0.8662 - val_f1_score: 0.8645\n","Epoch 6/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3211 - accuracy: 0.8707 - f1_score: 0.8684 - val_loss: 0.3479 - val_accuracy: 0.8508 - val_f1_score: 0.8591\n","Epoch 7/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3056 - accuracy: 0.8740 - f1_score: 0.8725 - val_loss: 0.3203 - val_accuracy: 0.8680 - val_f1_score: 0.8685\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2911 - accuracy: 0.8879 - f1_score: 0.8869 - val_loss: 0.3207 - val_accuracy: 0.8752 - val_f1_score: 0.8703\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2919 - accuracy: 0.8852 - f1_score: 0.8832 - val_loss: 0.3309 - val_accuracy: 0.8689 - val_f1_score: 0.8734\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2739 - accuracy: 0.8915 - f1_score: 0.8897 - val_loss: 0.3489 - val_accuracy: 0.8499 - val_f1_score: 0.8327\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2751 - accuracy: 0.8861 - f1_score: 0.8850 - val_loss: 0.3450 - val_accuracy: 0.8608 - val_f1_score: 0.8475\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2725 - accuracy: 0.8906 - f1_score: 0.8890 - val_loss: 0.3553 - val_accuracy: 0.8409 - val_f1_score: 0.8211\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3226 - accuracy: 0.8704 - f1_score: 0.8926\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7zJu8yRRcu7","executionInfo":{"status":"ok","timestamp":1689842326147,"user_tz":-330,"elapsed":112,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3821b60f-5126-4ccd-d994-30f4df1718c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8541526198387146, 0.8683322072029114, 0.8426738977432251, 0.8588791489601135, 0.8467251658439636, 0.8541526198387146, 0.8588791489601135, 0.8555030226707458, 0.8582038879394531, 0.8487508296966553, 0.8683322072029114, 0.8622552156448364, 0.8663065433502197, 0.8568534851074219, 0.8636056780815125, 0.8636056780815125, 0.8575286865234375, 0.8602295517921448, 0.8453747630119324, 0.8656313419342041, 0.8663065433502197, 0.852126955986023, 0.8555030226707458, 0.8804861307144165, 0.870357871055603]\n","0.8592302489280701\n","0.008491925911853238\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coM1YaEFRcxS","executionInfo":{"status":"ok","timestamp":1689842326148,"user_tz":-330,"elapsed":26,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7ac28d29-bd6d-4213-9e41-cbf54b561bf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.34625929594039917, 0.31525301933288574, 0.3693404197692871, 0.3296143710613251, 0.3503953814506531, 0.3393493592739105, 0.3187730014324188, 0.346189022064209, 0.3283575475215912, 0.36322206258773804, 0.32381510734558105, 0.31947341561317444, 0.328868567943573, 0.33859357237815857, 0.33053725957870483, 0.3447935879230499, 0.33449286222457886, 0.42486515641212463, 0.35206693410873413, 0.333064466714859, 0.31981003284454346, 0.3521612584590912, 0.35621368885040283, 0.30198416113853455, 0.322603702545166]\n","0.3396038901805878\n","0.023496948158229766\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tneAGFA2Rc0v","executionInfo":{"status":"ok","timestamp":1689842326149,"user_tz":-330,"elapsed":20,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"59a41cfc-b8a4-47e5-c3a7-0f069b81416d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8764301538467407, 0.8923246264457703, 0.863981306552887, 0.8823859691619873, 0.8687101602554321, 0.8775509595870972, 0.8843386173248291, 0.877993106842041, 0.884361207485199, 0.8703703284263611, 0.8949918150901794, 0.8903225064277649, 0.8930884599685669, 0.8799546957015991, 0.8881505727767944, 0.8862612247467041, 0.8803175687789917, 0.8892455697059631, 0.8692175149917603, 0.8890127539634705, 0.889631986618042, 0.8731904625892639, 0.8761574029922485, 0.9020475745201111, 0.8926173448562622]\n","0.8829061555862426\n","0.009326045572945344\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ije5eM6UR1DU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM CNN LSTM nothing"],"metadata":{"id":"2z_vLrvGR1SH"}},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=3, strides=1)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8wkt0yvRc45","executionInfo":{"status":"ok","timestamp":1689843672210,"user_tz":-330,"elapsed":1104046,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"3ecb5570-2aea-405d-923e-efdf91d00c26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 11s 44ms/step - loss: 0.6402 - accuracy: 0.6254 - f1_score: 0.6117 - val_loss: 0.5130 - val_accuracy: 0.7658 - val_f1_score: 0.7726\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5158 - accuracy: 0.7523 - f1_score: 0.7409 - val_loss: 0.4032 - val_accuracy: 0.8219 - val_f1_score: 0.8181\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4275 - accuracy: 0.8029 - f1_score: 0.7956 - val_loss: 0.3585 - val_accuracy: 0.8427 - val_f1_score: 0.8311\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3830 - accuracy: 0.8345 - f1_score: 0.8319 - val_loss: 0.3603 - val_accuracy: 0.8409 - val_f1_score: 0.8236\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3577 - accuracy: 0.8499 - f1_score: 0.8473 - val_loss: 0.3116 - val_accuracy: 0.8689 - val_f1_score: 0.8669\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3357 - accuracy: 0.8593 - f1_score: 0.8583 - val_loss: 0.3118 - val_accuracy: 0.8590 - val_f1_score: 0.8558\n","Epoch 7/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3259 - accuracy: 0.8677 - f1_score: 0.8650 - val_loss: 0.3060 - val_accuracy: 0.8680 - val_f1_score: 0.8617\n","Epoch 8/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3226 - accuracy: 0.8653 - f1_score: 0.8629 - val_loss: 0.3052 - val_accuracy: 0.8725 - val_f1_score: 0.8751\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3074 - accuracy: 0.8749 - f1_score: 0.8730 - val_loss: 0.2978 - val_accuracy: 0.8689 - val_f1_score: 0.8664\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3002 - accuracy: 0.8800 - f1_score: 0.8794 - val_loss: 0.3138 - val_accuracy: 0.8635 - val_f1_score: 0.8544\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2924 - accuracy: 0.8807 - f1_score: 0.8789 - val_loss: 0.2940 - val_accuracy: 0.8716 - val_f1_score: 0.8683\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2766 - accuracy: 0.8930 - f1_score: 0.8911 - val_loss: 0.3062 - val_accuracy: 0.8707 - val_f1_score: 0.8639\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2758 - accuracy: 0.8915 - f1_score: 0.8897 - val_loss: 0.3350 - val_accuracy: 0.8707 - val_f1_score: 0.8764\n","Epoch 14/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2651 - accuracy: 0.8975 - f1_score: 0.8963 - val_loss: 0.3097 - val_accuracy: 0.8797 - val_f1_score: 0.8816\n","Epoch 15/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2573 - accuracy: 0.9039 - f1_score: 0.9025 - val_loss: 0.3168 - val_accuracy: 0.8644 - val_f1_score: 0.8611\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2536 - accuracy: 0.9039 - f1_score: 0.9025 - val_loss: 0.3059 - val_accuracy: 0.8716 - val_f1_score: 0.8688\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.8521 - f1_score: 0.8726\n","Epoch 1/20\n","104/104 [==============================] - 9s 37ms/step - loss: 0.6177 - accuracy: 0.6525 - f1_score: 0.6265 - val_loss: 0.5599 - val_accuracy: 0.7125 - val_f1_score: 0.7273\n","Epoch 2/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.4727 - accuracy: 0.7752 - f1_score: 0.7670 - val_loss: 0.4864 - val_accuracy: 0.7731 - val_f1_score: 0.7997\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3878 - accuracy: 0.8315 - f1_score: 0.8283 - val_loss: 0.4288 - val_accuracy: 0.8065 - val_f1_score: 0.8257\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3431 - accuracy: 0.8559 - f1_score: 0.8543 - val_loss: 0.3494 - val_accuracy: 0.8599 - val_f1_score: 0.8563\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3350 - accuracy: 0.8559 - f1_score: 0.8543 - val_loss: 0.3405 - val_accuracy: 0.8499 - val_f1_score: 0.8523\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3164 - accuracy: 0.8716 - f1_score: 0.8702 - val_loss: 0.3384 - val_accuracy: 0.8680 - val_f1_score: 0.8628\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3121 - accuracy: 0.8674 - f1_score: 0.8667 - val_loss: 0.3367 - val_accuracy: 0.8662 - val_f1_score: 0.8635\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2943 - accuracy: 0.8734 - f1_score: 0.8736 - val_loss: 0.3889 - val_accuracy: 0.8517 - val_f1_score: 0.8360\n","Epoch 9/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2933 - accuracy: 0.8849 - f1_score: 0.8846 - val_loss: 0.3337 - val_accuracy: 0.8626 - val_f1_score: 0.8618\n","Epoch 10/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3042 - accuracy: 0.8707 - f1_score: 0.8700 - val_loss: 0.3504 - val_accuracy: 0.8508 - val_f1_score: 0.8576\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2872 - accuracy: 0.8831 - f1_score: 0.8820 - val_loss: 0.3384 - val_accuracy: 0.8653 - val_f1_score: 0.8637\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2744 - accuracy: 0.8885 - f1_score: 0.8883 - val_loss: 0.3569 - val_accuracy: 0.8553 - val_f1_score: 0.8462\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2649 - accuracy: 0.8888 - f1_score: 0.8880 - val_loss: 0.3439 - val_accuracy: 0.8580 - val_f1_score: 0.8597\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2505 - accuracy: 0.8999 - f1_score: 0.8995 - val_loss: 0.3511 - val_accuracy: 0.8644 - val_f1_score: 0.8598\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3221 - accuracy: 0.8636 - f1_score: 0.8879\n","Epoch 1/20\n","104/104 [==============================] - 11s 32ms/step - loss: 0.6254 - accuracy: 0.6480 - f1_score: 0.6542 - val_loss: 0.5424 - val_accuracy: 0.7233 - val_f1_score: 0.6915\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4825 - accuracy: 0.7731 - f1_score: 0.7639 - val_loss: 0.4419 - val_accuracy: 0.7866 - val_f1_score: 0.7626\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4137 - accuracy: 0.8140 - f1_score: 0.8096 - val_loss: 0.3910 - val_accuracy: 0.8183 - val_f1_score: 0.8168\n","Epoch 4/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3702 - accuracy: 0.8421 - f1_score: 0.8395 - val_loss: 0.3819 - val_accuracy: 0.8264 - val_f1_score: 0.8157\n","Epoch 5/20\n","104/104 [==============================] - 4s 36ms/step - loss: 0.3322 - accuracy: 0.8629 - f1_score: 0.8600 - val_loss: 0.3857 - val_accuracy: 0.8273 - val_f1_score: 0.8281\n","Epoch 6/20\n","104/104 [==============================] - 4s 34ms/step - loss: 0.3108 - accuracy: 0.8746 - f1_score: 0.8725 - val_loss: 0.4228 - val_accuracy: 0.8065 - val_f1_score: 0.7789\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3022 - accuracy: 0.8734 - f1_score: 0.8718 - val_loss: 0.3941 - val_accuracy: 0.8273 - val_f1_score: 0.8111\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2915 - accuracy: 0.8807 - f1_score: 0.8785 - val_loss: 0.3790 - val_accuracy: 0.8327 - val_f1_score: 0.8219\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2812 - accuracy: 0.8897 - f1_score: 0.8888 - val_loss: 0.4258 - val_accuracy: 0.8255 - val_f1_score: 0.8139\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2749 - accuracy: 0.8879 - f1_score: 0.8868 - val_loss: 0.4253 - val_accuracy: 0.8146 - val_f1_score: 0.7960\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2643 - accuracy: 0.8954 - f1_score: 0.8943 - val_loss: 0.4578 - val_accuracy: 0.8029 - val_f1_score: 0.7691\n","Epoch 12/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2683 - accuracy: 0.8894 - f1_score: 0.8881 - val_loss: 0.4082 - val_accuracy: 0.8373 - val_f1_score: 0.8308\n","Epoch 13/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2465 - accuracy: 0.9024 - f1_score: 0.9012 - val_loss: 0.4333 - val_accuracy: 0.8201 - val_f1_score: 0.8036\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3551 - accuracy: 0.8427 - f1_score: 0.8641\n","Epoch 1/20\n","104/104 [==============================] - 9s 35ms/step - loss: 0.6320 - accuracy: 0.6456 - f1_score: 0.6330 - val_loss: 0.5258 - val_accuracy: 0.7532 - val_f1_score: 0.7294\n","Epoch 2/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.5247 - accuracy: 0.7526 - f1_score: 0.7354 - val_loss: 0.4566 - val_accuracy: 0.7884 - val_f1_score: 0.7737\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4578 - accuracy: 0.7866 - f1_score: 0.7744 - val_loss: 0.4259 - val_accuracy: 0.8011 - val_f1_score: 0.7737\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3927 - accuracy: 0.8198 - f1_score: 0.8119 - val_loss: 0.3762 - val_accuracy: 0.8391 - val_f1_score: 0.8441\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3653 - accuracy: 0.8433 - f1_score: 0.8392 - val_loss: 0.3577 - val_accuracy: 0.8481 - val_f1_score: 0.8516\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3452 - accuracy: 0.8577 - f1_score: 0.8552 - val_loss: 0.3651 - val_accuracy: 0.8427 - val_f1_score: 0.8513\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3165 - accuracy: 0.8725 - f1_score: 0.8702 - val_loss: 0.3409 - val_accuracy: 0.8608 - val_f1_score: 0.8585\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3438 - accuracy: 0.8496 - f1_score: 0.8479 - val_loss: 0.3347 - val_accuracy: 0.8635 - val_f1_score: 0.8638\n","Epoch 9/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2967 - accuracy: 0.8803 - f1_score: 0.8784 - val_loss: 0.3383 - val_accuracy: 0.8580 - val_f1_score: 0.8631\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2872 - accuracy: 0.8873 - f1_score: 0.8860 - val_loss: 0.3273 - val_accuracy: 0.8671 - val_f1_score: 0.8633\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2787 - accuracy: 0.8888 - f1_score: 0.8871 - val_loss: 0.3333 - val_accuracy: 0.8635 - val_f1_score: 0.8643\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2815 - accuracy: 0.8897 - f1_score: 0.8877 - val_loss: 0.3383 - val_accuracy: 0.8653 - val_f1_score: 0.8685\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2681 - accuracy: 0.8942 - f1_score: 0.8930 - val_loss: 0.3751 - val_accuracy: 0.8508 - val_f1_score: 0.8612\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2482 - accuracy: 0.8996 - f1_score: 0.8984 - val_loss: 0.4004 - val_accuracy: 0.8571 - val_f1_score: 0.8647\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2539 - accuracy: 0.8981 - f1_score: 0.8964 - val_loss: 0.3574 - val_accuracy: 0.8635 - val_f1_score: 0.8684\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3546 - accuracy: 0.8420 - f1_score: 0.8624\n","Epoch 1/20\n","104/104 [==============================] - 11s 33ms/step - loss: 0.6287 - accuracy: 0.6407 - f1_score: 0.6581 - val_loss: 0.5609 - val_accuracy: 0.7297 - val_f1_score: 0.7551\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4996 - accuracy: 0.7655 - f1_score: 0.7529 - val_loss: 0.4766 - val_accuracy: 0.7875 - val_f1_score: 0.8078\n","Epoch 3/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.4190 - accuracy: 0.8110 - f1_score: 0.8010 - val_loss: 0.3690 - val_accuracy: 0.8391 - val_f1_score: 0.8391\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3598 - accuracy: 0.8454 - f1_score: 0.8414 - val_loss: 0.3491 - val_accuracy: 0.8608 - val_f1_score: 0.8542\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3366 - accuracy: 0.8593 - f1_score: 0.8569 - val_loss: 0.3432 - val_accuracy: 0.8590 - val_f1_score: 0.8506\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3212 - accuracy: 0.8689 - f1_score: 0.8662 - val_loss: 0.3265 - val_accuracy: 0.8734 - val_f1_score: 0.8687\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3179 - accuracy: 0.8647 - f1_score: 0.8614 - val_loss: 0.3350 - val_accuracy: 0.8644 - val_f1_score: 0.8670\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3033 - accuracy: 0.8770 - f1_score: 0.8755 - val_loss: 0.3308 - val_accuracy: 0.8608 - val_f1_score: 0.8640\n","Epoch 9/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2874 - accuracy: 0.8858 - f1_score: 0.8841 - val_loss: 0.3385 - val_accuracy: 0.8671 - val_f1_score: 0.8615\n","Epoch 10/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2948 - accuracy: 0.8776 - f1_score: 0.8747 - val_loss: 0.3647 - val_accuracy: 0.8490 - val_f1_score: 0.8355\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2917 - accuracy: 0.8846 - f1_score: 0.8829 - val_loss: 0.3235 - val_accuracy: 0.8743 - val_f1_score: 0.8724\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2652 - accuracy: 0.8921 - f1_score: 0.8898 - val_loss: 0.3290 - val_accuracy: 0.8743 - val_f1_score: 0.8721\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2735 - accuracy: 0.8927 - f1_score: 0.8911 - val_loss: 0.3336 - val_accuracy: 0.8725 - val_f1_score: 0.8674\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2542 - accuracy: 0.8993 - f1_score: 0.8977 - val_loss: 0.3713 - val_accuracy: 0.8517 - val_f1_score: 0.8389\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2635 - accuracy: 0.8942 - f1_score: 0.8928 - val_loss: 0.3263 - val_accuracy: 0.8662 - val_f1_score: 0.8664\n","Epoch 16/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2442 - accuracy: 0.9030 - f1_score: 0.9009 - val_loss: 0.3683 - val_accuracy: 0.8544 - val_f1_score: 0.8613\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.8542 - f1_score: 0.8757\n","Epoch 1/20\n","104/104 [==============================] - 11s 39ms/step - loss: 0.6257 - accuracy: 0.6501 - f1_score: 0.6453 - val_loss: 0.5128 - val_accuracy: 0.7532 - val_f1_score: 0.7284\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4691 - accuracy: 0.7836 - f1_score: 0.7765 - val_loss: 0.4366 - val_accuracy: 0.8002 - val_f1_score: 0.7676\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3915 - accuracy: 0.8312 - f1_score: 0.8286 - val_loss: 0.3869 - val_accuracy: 0.8246 - val_f1_score: 0.8412\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3786 - accuracy: 0.8339 - f1_score: 0.8334 - val_loss: 0.3197 - val_accuracy: 0.8761 - val_f1_score: 0.8760\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3413 - accuracy: 0.8529 - f1_score: 0.8523 - val_loss: 0.3460 - val_accuracy: 0.8454 - val_f1_score: 0.8567\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3162 - accuracy: 0.8662 - f1_score: 0.8649 - val_loss: 0.3101 - val_accuracy: 0.8716 - val_f1_score: 0.8772\n","Epoch 7/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3201 - accuracy: 0.8695 - f1_score: 0.8688 - val_loss: 0.2946 - val_accuracy: 0.8825 - val_f1_score: 0.8829\n","Epoch 8/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2981 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.2945 - val_accuracy: 0.8834 - val_f1_score: 0.8811\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2890 - accuracy: 0.8825 - f1_score: 0.8817 - val_loss: 0.2860 - val_accuracy: 0.8797 - val_f1_score: 0.8801\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2749 - accuracy: 0.8900 - f1_score: 0.8900 - val_loss: 0.2953 - val_accuracy: 0.8788 - val_f1_score: 0.8719\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2785 - accuracy: 0.8828 - f1_score: 0.8824 - val_loss: 0.2813 - val_accuracy: 0.8825 - val_f1_score: 0.8818\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2729 - accuracy: 0.8867 - f1_score: 0.8864 - val_loss: 0.2942 - val_accuracy: 0.8734 - val_f1_score: 0.8757\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2635 - accuracy: 0.8969 - f1_score: 0.8967 - val_loss: 0.2785 - val_accuracy: 0.8843 - val_f1_score: 0.8806\n","Epoch 14/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2474 - accuracy: 0.9048 - f1_score: 0.9044 - val_loss: 0.3019 - val_accuracy: 0.8662 - val_f1_score: 0.8645\n","Epoch 15/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2493 - accuracy: 0.8969 - f1_score: 0.8963 - val_loss: 0.2855 - val_accuracy: 0.8807 - val_f1_score: 0.8798\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2506 - accuracy: 0.8957 - f1_score: 0.8953 - val_loss: 0.2951 - val_accuracy: 0.8734 - val_f1_score: 0.8649\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2233 - accuracy: 0.9123 - f1_score: 0.9120 - val_loss: 0.3207 - val_accuracy: 0.8608 - val_f1_score: 0.8693\n","Epoch 18/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2137 - accuracy: 0.9198 - f1_score: 0.9193 - val_loss: 0.3250 - val_accuracy: 0.8680 - val_f1_score: 0.8746\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3478 - accuracy: 0.8542 - f1_score: 0.8776\n","Epoch 1/20\n","104/104 [==============================] - 11s 29ms/step - loss: 0.6071 - accuracy: 0.6703 - f1_score: 0.6750 - val_loss: 0.5351 - val_accuracy: 0.7414 - val_f1_score: 0.7593\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4623 - accuracy: 0.7797 - f1_score: 0.7701 - val_loss: 0.4226 - val_accuracy: 0.8056 - val_f1_score: 0.8173\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3963 - accuracy: 0.8249 - f1_score: 0.8218 - val_loss: 0.3635 - val_accuracy: 0.8418 - val_f1_score: 0.8286\n","Epoch 4/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3619 - accuracy: 0.8442 - f1_score: 0.8404 - val_loss: 0.3486 - val_accuracy: 0.8544 - val_f1_score: 0.8630\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3268 - accuracy: 0.8635 - f1_score: 0.8617 - val_loss: 0.3209 - val_accuracy: 0.8590 - val_f1_score: 0.8526\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3262 - accuracy: 0.8617 - f1_score: 0.8598 - val_loss: 0.3581 - val_accuracy: 0.8472 - val_f1_score: 0.8595\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2983 - accuracy: 0.8779 - f1_score: 0.8766 - val_loss: 0.3030 - val_accuracy: 0.8779 - val_f1_score: 0.8780\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2886 - accuracy: 0.8819 - f1_score: 0.8801 - val_loss: 0.4820 - val_accuracy: 0.8020 - val_f1_score: 0.8306\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2868 - accuracy: 0.8858 - f1_score: 0.8848 - val_loss: 0.3004 - val_accuracy: 0.8770 - val_f1_score: 0.8796\n","Epoch 10/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2731 - accuracy: 0.8894 - f1_score: 0.8882 - val_loss: 0.3122 - val_accuracy: 0.8734 - val_f1_score: 0.8789\n","Epoch 11/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2636 - accuracy: 0.8894 - f1_score: 0.8883 - val_loss: 0.2990 - val_accuracy: 0.8770 - val_f1_score: 0.8724\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2646 - accuracy: 0.8948 - f1_score: 0.8940 - val_loss: 0.3088 - val_accuracy: 0.8698 - val_f1_score: 0.8613\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2597 - accuracy: 0.9017 - f1_score: 0.9003 - val_loss: 0.3035 - val_accuracy: 0.8716 - val_f1_score: 0.8683\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2418 - accuracy: 0.9033 - f1_score: 0.9019 - val_loss: 0.3255 - val_accuracy: 0.8725 - val_f1_score: 0.8794\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2255 - accuracy: 0.9072 - f1_score: 0.9056 - val_loss: 0.3136 - val_accuracy: 0.8797 - val_f1_score: 0.8803\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2366 - accuracy: 0.9048 - f1_score: 0.9042 - val_loss: 0.3456 - val_accuracy: 0.8680 - val_f1_score: 0.8765\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3516 - accuracy: 0.8481 - f1_score: 0.8691\n","Epoch 1/20\n","104/104 [==============================] - 11s 37ms/step - loss: 0.6281 - accuracy: 0.6426 - f1_score: 0.6342 - val_loss: 0.5402 - val_accuracy: 0.7288 - val_f1_score: 0.7115\n","Epoch 2/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4819 - accuracy: 0.7694 - f1_score: 0.7543 - val_loss: 0.4866 - val_accuracy: 0.7622 - val_f1_score: 0.7832\n","Epoch 3/20\n","104/104 [==============================] - 4s 36ms/step - loss: 0.3951 - accuracy: 0.8201 - f1_score: 0.8160 - val_loss: 0.4060 - val_accuracy: 0.8146 - val_f1_score: 0.8000\n","Epoch 4/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3640 - accuracy: 0.8354 - f1_score: 0.8311 - val_loss: 0.4460 - val_accuracy: 0.7939 - val_f1_score: 0.8176\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3258 - accuracy: 0.8650 - f1_score: 0.8637 - val_loss: 0.3508 - val_accuracy: 0.8508 - val_f1_score: 0.8533\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3328 - accuracy: 0.8532 - f1_score: 0.8509 - val_loss: 0.3660 - val_accuracy: 0.8373 - val_f1_score: 0.8459\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2995 - accuracy: 0.8749 - f1_score: 0.8744 - val_loss: 0.3328 - val_accuracy: 0.8580 - val_f1_score: 0.8523\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2947 - accuracy: 0.8752 - f1_score: 0.8739 - val_loss: 0.3327 - val_accuracy: 0.8680 - val_f1_score: 0.8706\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2770 - accuracy: 0.8885 - f1_score: 0.8877 - val_loss: 0.3751 - val_accuracy: 0.8327 - val_f1_score: 0.8470\n","Epoch 10/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2652 - accuracy: 0.8945 - f1_score: 0.8940 - val_loss: 0.4149 - val_accuracy: 0.8092 - val_f1_score: 0.8305\n","Epoch 11/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2499 - accuracy: 0.8966 - f1_score: 0.8958 - val_loss: 0.3318 - val_accuracy: 0.8653 - val_f1_score: 0.8687\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2483 - accuracy: 0.9036 - f1_score: 0.9024 - val_loss: 0.3557 - val_accuracy: 0.8734 - val_f1_score: 0.8765\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2453 - accuracy: 0.9045 - f1_score: 0.9042 - val_loss: 0.3517 - val_accuracy: 0.8689 - val_f1_score: 0.8745\n","Epoch 14/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2409 - accuracy: 0.9036 - f1_score: 0.9029 - val_loss: 0.3354 - val_accuracy: 0.8698 - val_f1_score: 0.8714\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2255 - accuracy: 0.9147 - f1_score: 0.9140 - val_loss: 0.3446 - val_accuracy: 0.8752 - val_f1_score: 0.8772\n","Epoch 16/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2341 - accuracy: 0.9045 - f1_score: 0.9038 - val_loss: 0.3466 - val_accuracy: 0.8752 - val_f1_score: 0.8766\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3350 - accuracy: 0.8562 - f1_score: 0.8834\n","Epoch 1/20\n","104/104 [==============================] - 11s 47ms/step - loss: 0.6134 - accuracy: 0.6531 - f1_score: 0.6532 - val_loss: 0.5136 - val_accuracy: 0.7505 - val_f1_score: 0.7299\n","Epoch 2/20\n","104/104 [==============================] - 4s 34ms/step - loss: 0.4606 - accuracy: 0.7827 - f1_score: 0.7759 - val_loss: 0.3791 - val_accuracy: 0.8255 - val_f1_score: 0.8244\n","Epoch 3/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3961 - accuracy: 0.8186 - f1_score: 0.8139 - val_loss: 0.3409 - val_accuracy: 0.8508 - val_f1_score: 0.8454\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3632 - accuracy: 0.8421 - f1_score: 0.8408 - val_loss: 0.3693 - val_accuracy: 0.8300 - val_f1_score: 0.8456\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3373 - accuracy: 0.8562 - f1_score: 0.8553 - val_loss: 0.3089 - val_accuracy: 0.8752 - val_f1_score: 0.8750\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3084 - accuracy: 0.8740 - f1_score: 0.8728 - val_loss: 0.3171 - val_accuracy: 0.8635 - val_f1_score: 0.8702\n","Epoch 7/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2955 - accuracy: 0.8794 - f1_score: 0.8791 - val_loss: 0.2985 - val_accuracy: 0.8816 - val_f1_score: 0.8817\n","Epoch 8/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3009 - accuracy: 0.8716 - f1_score: 0.8718 - val_loss: 0.3073 - val_accuracy: 0.8779 - val_f1_score: 0.8800\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2944 - accuracy: 0.8822 - f1_score: 0.8818 - val_loss: 0.3340 - val_accuracy: 0.8526 - val_f1_score: 0.8362\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2849 - accuracy: 0.8828 - f1_score: 0.8818 - val_loss: 0.3303 - val_accuracy: 0.8698 - val_f1_score: 0.8610\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2689 - accuracy: 0.8936 - f1_score: 0.8933 - val_loss: 0.3207 - val_accuracy: 0.8653 - val_f1_score: 0.8558\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2602 - accuracy: 0.8939 - f1_score: 0.8937 - val_loss: 0.3260 - val_accuracy: 0.8725 - val_f1_score: 0.8756\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3328 - accuracy: 0.8582 - f1_score: 0.8847\n","Epoch 1/20\n","104/104 [==============================] - 10s 32ms/step - loss: 0.6260 - accuracy: 0.6477 - f1_score: 0.6346 - val_loss: 0.5271 - val_accuracy: 0.7559 - val_f1_score: 0.7648\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4664 - accuracy: 0.7842 - f1_score: 0.7720 - val_loss: 0.4079 - val_accuracy: 0.8192 - val_f1_score: 0.8195\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3903 - accuracy: 0.8249 - f1_score: 0.8197 - val_loss: 0.4053 - val_accuracy: 0.8092 - val_f1_score: 0.8249\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3493 - accuracy: 0.8505 - f1_score: 0.8476 - val_loss: 0.3433 - val_accuracy: 0.8481 - val_f1_score: 0.8444\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3353 - accuracy: 0.8596 - f1_score: 0.8570 - val_loss: 0.3304 - val_accuracy: 0.8608 - val_f1_score: 0.8561\n","Epoch 6/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3246 - accuracy: 0.8683 - f1_score: 0.8676 - val_loss: 0.3283 - val_accuracy: 0.8599 - val_f1_score: 0.8522\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3042 - accuracy: 0.8788 - f1_score: 0.8773 - val_loss: 0.4354 - val_accuracy: 0.8083 - val_f1_score: 0.7706\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2954 - accuracy: 0.8773 - f1_score: 0.8763 - val_loss: 0.3531 - val_accuracy: 0.8553 - val_f1_score: 0.8400\n","Epoch 9/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2801 - accuracy: 0.8885 - f1_score: 0.8881 - val_loss: 0.3301 - val_accuracy: 0.8580 - val_f1_score: 0.8459\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2804 - accuracy: 0.8852 - f1_score: 0.8839 - val_loss: 0.3393 - val_accuracy: 0.8590 - val_f1_score: 0.8440\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2628 - accuracy: 0.8978 - f1_score: 0.8971 - val_loss: 0.4544 - val_accuracy: 0.8047 - val_f1_score: 0.7637\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3608 - accuracy: 0.8406 - f1_score: 0.8634\n","Epoch 1/20\n","104/104 [==============================] - 11s 30ms/step - loss: 0.6187 - accuracy: 0.6528 - f1_score: 0.6301 - val_loss: 0.5174 - val_accuracy: 0.7631 - val_f1_score: 0.7635\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4743 - accuracy: 0.7767 - f1_score: 0.7698 - val_loss: 0.4074 - val_accuracy: 0.8101 - val_f1_score: 0.8118\n","Epoch 3/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3844 - accuracy: 0.8303 - f1_score: 0.8265 - val_loss: 0.3705 - val_accuracy: 0.8472 - val_f1_score: 0.8431\n","Epoch 4/20\n","104/104 [==============================] - 3s 31ms/step - loss: 0.3429 - accuracy: 0.8532 - f1_score: 0.8508 - val_loss: 0.3531 - val_accuracy: 0.8508 - val_f1_score: 0.8433\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3284 - accuracy: 0.8662 - f1_score: 0.8641 - val_loss: 0.3572 - val_accuracy: 0.8508 - val_f1_score: 0.8390\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3210 - accuracy: 0.8602 - f1_score: 0.8583 - val_loss: 0.3841 - val_accuracy: 0.8427 - val_f1_score: 0.8508\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2926 - accuracy: 0.8803 - f1_score: 0.8785 - val_loss: 0.3395 - val_accuracy: 0.8590 - val_f1_score: 0.8612\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3120 - accuracy: 0.8680 - f1_score: 0.8669 - val_loss: 0.4447 - val_accuracy: 0.8192 - val_f1_score: 0.8379\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2765 - accuracy: 0.8864 - f1_score: 0.8850 - val_loss: 0.3529 - val_accuracy: 0.8653 - val_f1_score: 0.8596\n","Epoch 10/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2632 - accuracy: 0.8960 - f1_score: 0.8952 - val_loss: 0.3621 - val_accuracy: 0.8553 - val_f1_score: 0.8628\n","Epoch 11/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2599 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.3257 - val_accuracy: 0.8716 - val_f1_score: 0.8650\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2540 - accuracy: 0.9033 - f1_score: 0.9022 - val_loss: 0.3546 - val_accuracy: 0.8653 - val_f1_score: 0.8543\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2379 - accuracy: 0.9078 - f1_score: 0.9070 - val_loss: 0.3552 - val_accuracy: 0.8752 - val_f1_score: 0.8713\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2323 - accuracy: 0.9081 - f1_score: 0.9076 - val_loss: 0.3514 - val_accuracy: 0.8698 - val_f1_score: 0.8723\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2304 - accuracy: 0.9069 - f1_score: 0.9059 - val_loss: 0.3574 - val_accuracy: 0.8644 - val_f1_score: 0.8547\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2147 - accuracy: 0.9087 - f1_score: 0.9075 - val_loss: 0.3983 - val_accuracy: 0.8635 - val_f1_score: 0.8670\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3051 - accuracy: 0.8778 - f1_score: 0.9008\n","Epoch 1/20\n","104/104 [==============================] - 10s 44ms/step - loss: 0.6176 - accuracy: 0.6615 - f1_score: 0.6643 - val_loss: 0.5140 - val_accuracy: 0.7423 - val_f1_score: 0.6990\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4740 - accuracy: 0.7767 - f1_score: 0.7665 - val_loss: 0.3940 - val_accuracy: 0.8228 - val_f1_score: 0.8144\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4012 - accuracy: 0.8228 - f1_score: 0.8189 - val_loss: 0.3610 - val_accuracy: 0.8418 - val_f1_score: 0.8413\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3555 - accuracy: 0.8424 - f1_score: 0.8403 - val_loss: 0.3394 - val_accuracy: 0.8562 - val_f1_score: 0.8532\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3311 - accuracy: 0.8596 - f1_score: 0.8577 - val_loss: 0.3352 - val_accuracy: 0.8590 - val_f1_score: 0.8655\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3269 - accuracy: 0.8644 - f1_score: 0.8625 - val_loss: 0.3179 - val_accuracy: 0.8725 - val_f1_score: 0.8724\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3049 - accuracy: 0.8773 - f1_score: 0.8753 - val_loss: 0.3056 - val_accuracy: 0.8734 - val_f1_score: 0.8677\n","Epoch 8/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3087 - accuracy: 0.8713 - f1_score: 0.8697 - val_loss: 0.3079 - val_accuracy: 0.8779 - val_f1_score: 0.8780\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2884 - accuracy: 0.8828 - f1_score: 0.8811 - val_loss: 0.3100 - val_accuracy: 0.8752 - val_f1_score: 0.8688\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2764 - accuracy: 0.8825 - f1_score: 0.8810 - val_loss: 0.3562 - val_accuracy: 0.8481 - val_f1_score: 0.8300\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2691 - accuracy: 0.8912 - f1_score: 0.8892 - val_loss: 0.3178 - val_accuracy: 0.8807 - val_f1_score: 0.8791\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2557 - accuracy: 0.8966 - f1_score: 0.8954 - val_loss: 0.3075 - val_accuracy: 0.8843 - val_f1_score: 0.8795\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3100 - accuracy: 0.8643 - f1_score: 0.8911\n","Epoch 1/20\n","104/104 [==============================] - 11s 31ms/step - loss: 0.6401 - accuracy: 0.6254 - f1_score: 0.5799 - val_loss: 0.5382 - val_accuracy: 0.7369 - val_f1_score: 0.7177\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4814 - accuracy: 0.7682 - f1_score: 0.7584 - val_loss: 0.4088 - val_accuracy: 0.8101 - val_f1_score: 0.8084\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3890 - accuracy: 0.8255 - f1_score: 0.8212 - val_loss: 0.3967 - val_accuracy: 0.8255 - val_f1_score: 0.8146\n","Epoch 4/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3548 - accuracy: 0.8487 - f1_score: 0.8470 - val_loss: 0.3713 - val_accuracy: 0.8472 - val_f1_score: 0.8459\n","Epoch 5/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3418 - accuracy: 0.8632 - f1_score: 0.8616 - val_loss: 0.3817 - val_accuracy: 0.8544 - val_f1_score: 0.8589\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3182 - accuracy: 0.8632 - f1_score: 0.8611 - val_loss: 0.4471 - val_accuracy: 0.8183 - val_f1_score: 0.8365\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2983 - accuracy: 0.8749 - f1_score: 0.8730 - val_loss: 0.4118 - val_accuracy: 0.8363 - val_f1_score: 0.8480\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2988 - accuracy: 0.8683 - f1_score: 0.8669 - val_loss: 0.3816 - val_accuracy: 0.8445 - val_f1_score: 0.8545\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3043 - accuracy: 0.8719 - f1_score: 0.8712 - val_loss: 0.3939 - val_accuracy: 0.8544 - val_f1_score: 0.8613\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3461 - accuracy: 0.8548 - f1_score: 0.8804\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6389 - accuracy: 0.6227 - f1_score: 0.6375 - val_loss: 0.5240 - val_accuracy: 0.7595 - val_f1_score: 0.7491\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4824 - accuracy: 0.7722 - f1_score: 0.7547 - val_loss: 0.4125 - val_accuracy: 0.8074 - val_f1_score: 0.7930\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3990 - accuracy: 0.8249 - f1_score: 0.8169 - val_loss: 0.3798 - val_accuracy: 0.8327 - val_f1_score: 0.8409\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3611 - accuracy: 0.8427 - f1_score: 0.8389 - val_loss: 0.3423 - val_accuracy: 0.8608 - val_f1_score: 0.8542\n","Epoch 5/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3282 - accuracy: 0.8590 - f1_score: 0.8555 - val_loss: 0.3386 - val_accuracy: 0.8571 - val_f1_score: 0.8599\n","Epoch 6/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3195 - accuracy: 0.8644 - f1_score: 0.8615 - val_loss: 0.3576 - val_accuracy: 0.8418 - val_f1_score: 0.8533\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3051 - accuracy: 0.8734 - f1_score: 0.8709 - val_loss: 0.3219 - val_accuracy: 0.8734 - val_f1_score: 0.8748\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2915 - accuracy: 0.8822 - f1_score: 0.8800 - val_loss: 0.3179 - val_accuracy: 0.8743 - val_f1_score: 0.8760\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3011 - accuracy: 0.8743 - f1_score: 0.8724 - val_loss: 0.3134 - val_accuracy: 0.8752 - val_f1_score: 0.8741\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2795 - accuracy: 0.8822 - f1_score: 0.8794 - val_loss: 0.3492 - val_accuracy: 0.8508 - val_f1_score: 0.8605\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2752 - accuracy: 0.8885 - f1_score: 0.8863 - val_loss: 0.3210 - val_accuracy: 0.8761 - val_f1_score: 0.8780\n","Epoch 12/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2644 - accuracy: 0.8891 - f1_score: 0.8867 - val_loss: 0.3551 - val_accuracy: 0.8553 - val_f1_score: 0.8655\n","Epoch 13/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2589 - accuracy: 0.8960 - f1_score: 0.8939 - val_loss: 0.3840 - val_accuracy: 0.8553 - val_f1_score: 0.8662\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2476 - accuracy: 0.9008 - f1_score: 0.8994 - val_loss: 0.4153 - val_accuracy: 0.8336 - val_f1_score: 0.8489\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3161 - accuracy: 0.8643 - f1_score: 0.8866\n","Epoch 1/20\n","104/104 [==============================] - 12s 43ms/step - loss: 0.6276 - accuracy: 0.6459 - f1_score: 0.6464 - val_loss: 0.5734 - val_accuracy: 0.7043 - val_f1_score: 0.7451\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4964 - accuracy: 0.7580 - f1_score: 0.7498 - val_loss: 0.4322 - val_accuracy: 0.7993 - val_f1_score: 0.8035\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4006 - accuracy: 0.8258 - f1_score: 0.8218 - val_loss: 0.3866 - val_accuracy: 0.8345 - val_f1_score: 0.8235\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3678 - accuracy: 0.8475 - f1_score: 0.8445 - val_loss: 0.3586 - val_accuracy: 0.8445 - val_f1_score: 0.8504\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3451 - accuracy: 0.8605 - f1_score: 0.8599 - val_loss: 0.3307 - val_accuracy: 0.8544 - val_f1_score: 0.8551\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3248 - accuracy: 0.8650 - f1_score: 0.8641 - val_loss: 0.3608 - val_accuracy: 0.8427 - val_f1_score: 0.8530\n","Epoch 7/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3105 - accuracy: 0.8692 - f1_score: 0.8669 - val_loss: 0.4212 - val_accuracy: 0.8029 - val_f1_score: 0.8256\n","Epoch 8/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3024 - accuracy: 0.8746 - f1_score: 0.8732 - val_loss: 0.3165 - val_accuracy: 0.8761 - val_f1_score: 0.8782\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2802 - accuracy: 0.8873 - f1_score: 0.8869 - val_loss: 0.3332 - val_accuracy: 0.8553 - val_f1_score: 0.8635\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2779 - accuracy: 0.8803 - f1_score: 0.8796 - val_loss: 0.3113 - val_accuracy: 0.8716 - val_f1_score: 0.8746\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2746 - accuracy: 0.8879 - f1_score: 0.8870 - val_loss: 0.3156 - val_accuracy: 0.8671 - val_f1_score: 0.8725\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2614 - accuracy: 0.8972 - f1_score: 0.8955 - val_loss: 0.3266 - val_accuracy: 0.8508 - val_f1_score: 0.8591\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2674 - accuracy: 0.8909 - f1_score: 0.8910 - val_loss: 0.3176 - val_accuracy: 0.8707 - val_f1_score: 0.8684\n","Epoch 14/20\n","104/104 [==============================] - 4s 37ms/step - loss: 0.2452 - accuracy: 0.9036 - f1_score: 0.9030 - val_loss: 0.3074 - val_accuracy: 0.8807 - val_f1_score: 0.8796\n","Epoch 15/20\n","104/104 [==============================] - 4s 35ms/step - loss: 0.2363 - accuracy: 0.9084 - f1_score: 0.9079 - val_loss: 0.3755 - val_accuracy: 0.8373 - val_f1_score: 0.8490\n","Epoch 16/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2482 - accuracy: 0.9020 - f1_score: 0.9017 - val_loss: 0.3260 - val_accuracy: 0.8698 - val_f1_score: 0.8626\n","Epoch 17/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2268 - accuracy: 0.9075 - f1_score: 0.9065 - val_loss: 0.3174 - val_accuracy: 0.8734 - val_f1_score: 0.8759\n","Epoch 18/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2181 - accuracy: 0.9090 - f1_score: 0.9084 - val_loss: 0.3423 - val_accuracy: 0.8653 - val_f1_score: 0.8566\n","Epoch 19/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2028 - accuracy: 0.9201 - f1_score: 0.9193 - val_loss: 0.3980 - val_accuracy: 0.8300 - val_f1_score: 0.8481\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3446 - accuracy: 0.8616 - f1_score: 0.8859\n","Epoch 1/20\n","104/104 [==============================] - 9s 32ms/step - loss: 0.6103 - accuracy: 0.6646 - f1_score: 0.6436 - val_loss: 0.4937 - val_accuracy: 0.7712 - val_f1_score: 0.7478\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4446 - accuracy: 0.7966 - f1_score: 0.7907 - val_loss: 0.3995 - val_accuracy: 0.8210 - val_f1_score: 0.8107\n","Epoch 3/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3574 - accuracy: 0.8397 - f1_score: 0.8374 - val_loss: 0.3847 - val_accuracy: 0.8300 - val_f1_score: 0.8444\n","Epoch 4/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3246 - accuracy: 0.8677 - f1_score: 0.8669 - val_loss: 0.3513 - val_accuracy: 0.8544 - val_f1_score: 0.8432\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3377 - accuracy: 0.8541 - f1_score: 0.8533 - val_loss: 0.3394 - val_accuracy: 0.8599 - val_f1_score: 0.8525\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3078 - accuracy: 0.8716 - f1_score: 0.8704 - val_loss: 0.3281 - val_accuracy: 0.8698 - val_f1_score: 0.8672\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2926 - accuracy: 0.8825 - f1_score: 0.8814 - val_loss: 0.3760 - val_accuracy: 0.8336 - val_f1_score: 0.8484\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2788 - accuracy: 0.8846 - f1_score: 0.8832 - val_loss: 0.3463 - val_accuracy: 0.8508 - val_f1_score: 0.8605\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2744 - accuracy: 0.8858 - f1_score: 0.8848 - val_loss: 0.3357 - val_accuracy: 0.8499 - val_f1_score: 0.8591\n","Epoch 10/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2670 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3482 - val_accuracy: 0.8544 - val_f1_score: 0.8613\n","Epoch 11/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2631 - accuracy: 0.8900 - f1_score: 0.8885 - val_loss: 0.3161 - val_accuracy: 0.8725 - val_f1_score: 0.8696\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2480 - accuracy: 0.9002 - f1_score: 0.8990 - val_loss: 0.3590 - val_accuracy: 0.8445 - val_f1_score: 0.8532\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2610 - accuracy: 0.8918 - f1_score: 0.8913 - val_loss: 0.3271 - val_accuracy: 0.8599 - val_f1_score: 0.8627\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2328 - accuracy: 0.9096 - f1_score: 0.9085 - val_loss: 0.3176 - val_accuracy: 0.8761 - val_f1_score: 0.8711\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2313 - accuracy: 0.9039 - f1_score: 0.9027 - val_loss: 0.3298 - val_accuracy: 0.8716 - val_f1_score: 0.8741\n","Epoch 16/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2216 - accuracy: 0.9126 - f1_score: 0.9119 - val_loss: 0.3342 - val_accuracy: 0.8698 - val_f1_score: 0.8623\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3621 - accuracy: 0.8548 - f1_score: 0.8774\n","Epoch 1/20\n","104/104 [==============================] - 10s 29ms/step - loss: 0.6319 - accuracy: 0.6423 - f1_score: 0.6250 - val_loss: 0.5523 - val_accuracy: 0.7098 - val_f1_score: 0.6515\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4810 - accuracy: 0.7782 - f1_score: 0.7663 - val_loss: 0.4625 - val_accuracy: 0.7875 - val_f1_score: 0.8014\n","Epoch 3/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3990 - accuracy: 0.8198 - f1_score: 0.8141 - val_loss: 0.3686 - val_accuracy: 0.8363 - val_f1_score: 0.8362\n","Epoch 4/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3618 - accuracy: 0.8451 - f1_score: 0.8424 - val_loss: 0.3502 - val_accuracy: 0.8454 - val_f1_score: 0.8504\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3334 - accuracy: 0.8611 - f1_score: 0.8589 - val_loss: 0.3330 - val_accuracy: 0.8544 - val_f1_score: 0.8589\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3095 - accuracy: 0.8737 - f1_score: 0.8725 - val_loss: 0.3249 - val_accuracy: 0.8635 - val_f1_score: 0.8670\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3081 - accuracy: 0.8725 - f1_score: 0.8718 - val_loss: 0.3428 - val_accuracy: 0.8526 - val_f1_score: 0.8391\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2820 - accuracy: 0.8912 - f1_score: 0.8897 - val_loss: 0.3282 - val_accuracy: 0.8635 - val_f1_score: 0.8667\n","Epoch 9/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2820 - accuracy: 0.8834 - f1_score: 0.8820 - val_loss: 0.3169 - val_accuracy: 0.8671 - val_f1_score: 0.8612\n","Epoch 10/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2784 - accuracy: 0.8849 - f1_score: 0.8831 - val_loss: 0.3360 - val_accuracy: 0.8680 - val_f1_score: 0.8593\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2761 - accuracy: 0.8846 - f1_score: 0.8828 - val_loss: 0.3576 - val_accuracy: 0.8382 - val_f1_score: 0.8168\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2630 - accuracy: 0.8948 - f1_score: 0.8931 - val_loss: 0.3195 - val_accuracy: 0.8752 - val_f1_score: 0.8732\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2518 - accuracy: 0.8966 - f1_score: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.8653 - val_f1_score: 0.8687\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2441 - accuracy: 0.9017 - f1_score: 0.9005 - val_loss: 0.3729 - val_accuracy: 0.8427 - val_f1_score: 0.8217\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3556 - accuracy: 0.8460 - f1_score: 0.8688\n","Epoch 1/20\n","104/104 [==============================] - 10s 46ms/step - loss: 0.6390 - accuracy: 0.6314 - f1_score: 0.5778 - val_loss: 0.6025 - val_accuracy: 0.6844 - val_f1_score: 0.5840\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5275 - accuracy: 0.7414 - f1_score: 0.7216 - val_loss: 0.4430 - val_accuracy: 0.7966 - val_f1_score: 0.7739\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4321 - accuracy: 0.8071 - f1_score: 0.7992 - val_loss: 0.3735 - val_accuracy: 0.8345 - val_f1_score: 0.8344\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3723 - accuracy: 0.8360 - f1_score: 0.8328 - val_loss: 0.3312 - val_accuracy: 0.8617 - val_f1_score: 0.8569\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3353 - accuracy: 0.8638 - f1_score: 0.8617 - val_loss: 0.3558 - val_accuracy: 0.8472 - val_f1_score: 0.8312\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3223 - accuracy: 0.8674 - f1_score: 0.8664 - val_loss: 0.3520 - val_accuracy: 0.8427 - val_f1_score: 0.8235\n","Epoch 7/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3175 - accuracy: 0.8716 - f1_score: 0.8708 - val_loss: 0.3137 - val_accuracy: 0.8580 - val_f1_score: 0.8517\n","Epoch 8/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2914 - accuracy: 0.8864 - f1_score: 0.8859 - val_loss: 0.3256 - val_accuracy: 0.8644 - val_f1_score: 0.8544\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2931 - accuracy: 0.8840 - f1_score: 0.8824 - val_loss: 0.3449 - val_accuracy: 0.8508 - val_f1_score: 0.8342\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2773 - accuracy: 0.8915 - f1_score: 0.8902 - val_loss: 0.2994 - val_accuracy: 0.8716 - val_f1_score: 0.8642\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2756 - accuracy: 0.8921 - f1_score: 0.8911 - val_loss: 0.2945 - val_accuracy: 0.8779 - val_f1_score: 0.8742\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2607 - accuracy: 0.8987 - f1_score: 0.8977 - val_loss: 0.3077 - val_accuracy: 0.8779 - val_f1_score: 0.8783\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2561 - accuracy: 0.8999 - f1_score: 0.8989 - val_loss: 0.2896 - val_accuracy: 0.8861 - val_f1_score: 0.8846\n","Epoch 14/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2435 - accuracy: 0.9057 - f1_score: 0.9044 - val_loss: 0.3048 - val_accuracy: 0.8761 - val_f1_score: 0.8706\n","Epoch 15/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2571 - accuracy: 0.9002 - f1_score: 0.8986 - val_loss: 0.3172 - val_accuracy: 0.8689 - val_f1_score: 0.8612\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2364 - accuracy: 0.9102 - f1_score: 0.9089 - val_loss: 0.3255 - val_accuracy: 0.8644 - val_f1_score: 0.8541\n","Epoch 17/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2286 - accuracy: 0.9138 - f1_score: 0.9125 - val_loss: 0.3419 - val_accuracy: 0.8617 - val_f1_score: 0.8525\n","Epoch 18/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2213 - accuracy: 0.9174 - f1_score: 0.9163 - val_loss: 0.3260 - val_accuracy: 0.8852 - val_f1_score: 0.8861\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3370 - accuracy: 0.8656 - f1_score: 0.8919\n","Epoch 1/20\n","104/104 [==============================] - 11s 31ms/step - loss: 0.6129 - accuracy: 0.6652 - f1_score: 0.6559 - val_loss: 0.5128 - val_accuracy: 0.7631 - val_f1_score: 0.7605\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4716 - accuracy: 0.7890 - f1_score: 0.7808 - val_loss: 0.4381 - val_accuracy: 0.7929 - val_f1_score: 0.7901\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3926 - accuracy: 0.8270 - f1_score: 0.8236 - val_loss: 0.4118 - val_accuracy: 0.8291 - val_f1_score: 0.8167\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3587 - accuracy: 0.8421 - f1_score: 0.8412 - val_loss: 0.3815 - val_accuracy: 0.8327 - val_f1_score: 0.8320\n","Epoch 5/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3323 - accuracy: 0.8568 - f1_score: 0.8565 - val_loss: 0.4636 - val_accuracy: 0.8020 - val_f1_score: 0.7697\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3072 - accuracy: 0.8728 - f1_score: 0.8716 - val_loss: 0.3631 - val_accuracy: 0.8499 - val_f1_score: 0.8528\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3069 - accuracy: 0.8728 - f1_score: 0.8724 - val_loss: 0.3515 - val_accuracy: 0.8535 - val_f1_score: 0.8475\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2889 - accuracy: 0.8834 - f1_score: 0.8828 - val_loss: 0.3570 - val_accuracy: 0.8635 - val_f1_score: 0.8670\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2922 - accuracy: 0.8776 - f1_score: 0.8770 - val_loss: 0.4324 - val_accuracy: 0.8165 - val_f1_score: 0.8362\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2839 - accuracy: 0.8840 - f1_score: 0.8830 - val_loss: 0.3479 - val_accuracy: 0.8662 - val_f1_score: 0.8693\n","Epoch 11/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2759 - accuracy: 0.8864 - f1_score: 0.8855 - val_loss: 0.3611 - val_accuracy: 0.8617 - val_f1_score: 0.8533\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2735 - accuracy: 0.8876 - f1_score: 0.8853 - val_loss: 0.3381 - val_accuracy: 0.8707 - val_f1_score: 0.8657\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2542 - accuracy: 0.9014 - f1_score: 0.9009 - val_loss: 0.3373 - val_accuracy: 0.8671 - val_f1_score: 0.8643\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2513 - accuracy: 0.8948 - f1_score: 0.8938 - val_loss: 0.3470 - val_accuracy: 0.8571 - val_f1_score: 0.8469\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2407 - accuracy: 0.9060 - f1_score: 0.9049 - val_loss: 0.3410 - val_accuracy: 0.8644 - val_f1_score: 0.8614\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2427 - accuracy: 0.9027 - f1_score: 0.9019 - val_loss: 0.3426 - val_accuracy: 0.8698 - val_f1_score: 0.8662\n","Epoch 17/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2350 - accuracy: 0.9066 - f1_score: 0.9057 - val_loss: 0.3655 - val_accuracy: 0.8626 - val_f1_score: 0.8645\n","Epoch 18/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2288 - accuracy: 0.9099 - f1_score: 0.9092 - val_loss: 0.3798 - val_accuracy: 0.8662 - val_f1_score: 0.8601\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3437 - accuracy: 0.8515 - f1_score: 0.8783\n","Epoch 1/20\n","104/104 [==============================] - 9s 33ms/step - loss: 0.6355 - accuracy: 0.6281 - f1_score: 0.6115 - val_loss: 0.5034 - val_accuracy: 0.7767 - val_f1_score: 0.7641\n","Epoch 2/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4885 - accuracy: 0.7661 - f1_score: 0.7581 - val_loss: 0.4155 - val_accuracy: 0.8165 - val_f1_score: 0.8372\n","Epoch 3/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.4048 - accuracy: 0.8222 - f1_score: 0.8197 - val_loss: 0.3283 - val_accuracy: 0.8644 - val_f1_score: 0.8720\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3674 - accuracy: 0.8421 - f1_score: 0.8411 - val_loss: 0.3522 - val_accuracy: 0.8454 - val_f1_score: 0.8606\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3442 - accuracy: 0.8520 - f1_score: 0.8519 - val_loss: 0.2787 - val_accuracy: 0.8924 - val_f1_score: 0.8905\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3167 - accuracy: 0.8716 - f1_score: 0.8709 - val_loss: 0.2744 - val_accuracy: 0.8969 - val_f1_score: 0.8931\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3110 - accuracy: 0.8659 - f1_score: 0.8639 - val_loss: 0.2671 - val_accuracy: 0.8933 - val_f1_score: 0.8937\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2967 - accuracy: 0.8773 - f1_score: 0.8763 - val_loss: 0.2718 - val_accuracy: 0.8870 - val_f1_score: 0.8910\n","Epoch 9/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2897 - accuracy: 0.8761 - f1_score: 0.8746 - val_loss: 0.2593 - val_accuracy: 0.8942 - val_f1_score: 0.8951\n","Epoch 10/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2846 - accuracy: 0.8837 - f1_score: 0.8817 - val_loss: 0.3362 - val_accuracy: 0.8580 - val_f1_score: 0.8704\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2844 - accuracy: 0.8822 - f1_score: 0.8808 - val_loss: 0.3049 - val_accuracy: 0.8834 - val_f1_score: 0.8898\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2657 - accuracy: 0.8906 - f1_score: 0.8900 - val_loss: 0.2795 - val_accuracy: 0.8906 - val_f1_score: 0.8936\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2601 - accuracy: 0.8897 - f1_score: 0.8875 - val_loss: 0.3276 - val_accuracy: 0.8671 - val_f1_score: 0.8770\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2513 - accuracy: 0.8981 - f1_score: 0.8975 - val_loss: 0.3071 - val_accuracy: 0.8834 - val_f1_score: 0.8887\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3276 - accuracy: 0.8602 - f1_score: 0.8854\n","Epoch 1/20\n","104/104 [==============================] - 12s 38ms/step - loss: 0.6292 - accuracy: 0.6447 - f1_score: 0.6344 - val_loss: 0.5292 - val_accuracy: 0.7468 - val_f1_score: 0.7407\n","Epoch 2/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.4680 - accuracy: 0.7860 - f1_score: 0.7799 - val_loss: 0.4158 - val_accuracy: 0.8128 - val_f1_score: 0.8198\n","Epoch 3/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3817 - accuracy: 0.8342 - f1_score: 0.8325 - val_loss: 0.3762 - val_accuracy: 0.8472 - val_f1_score: 0.8521\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3597 - accuracy: 0.8496 - f1_score: 0.8474 - val_loss: 0.3665 - val_accuracy: 0.8544 - val_f1_score: 0.8594\n","Epoch 5/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3340 - accuracy: 0.8677 - f1_score: 0.8669 - val_loss: 0.3908 - val_accuracy: 0.8273 - val_f1_score: 0.8065\n","Epoch 6/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3181 - accuracy: 0.8695 - f1_score: 0.8674 - val_loss: 0.3389 - val_accuracy: 0.8553 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3050 - accuracy: 0.8773 - f1_score: 0.8759 - val_loss: 0.3302 - val_accuracy: 0.8653 - val_f1_score: 0.8652\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3027 - accuracy: 0.8794 - f1_score: 0.8773 - val_loss: 0.3480 - val_accuracy: 0.8526 - val_f1_score: 0.8581\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2829 - accuracy: 0.8915 - f1_score: 0.8906 - val_loss: 0.3634 - val_accuracy: 0.8508 - val_f1_score: 0.8406\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2788 - accuracy: 0.8912 - f1_score: 0.8898 - val_loss: 0.3497 - val_accuracy: 0.8562 - val_f1_score: 0.8484\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2749 - accuracy: 0.8936 - f1_score: 0.8925 - val_loss: 0.3379 - val_accuracy: 0.8644 - val_f1_score: 0.8590\n","Epoch 12/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2671 - accuracy: 0.8963 - f1_score: 0.8945 - val_loss: 0.3436 - val_accuracy: 0.8580 - val_f1_score: 0.8512\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3224 - accuracy: 0.8710 - f1_score: 0.8942\n","Epoch 1/20\n","104/104 [==============================] - 9s 30ms/step - loss: 0.6257 - accuracy: 0.6516 - f1_score: 0.6574 - val_loss: 0.5584 - val_accuracy: 0.7179 - val_f1_score: 0.6422\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4931 - accuracy: 0.7688 - f1_score: 0.7572 - val_loss: 0.4518 - val_accuracy: 0.7911 - val_f1_score: 0.8037\n","Epoch 3/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.4128 - accuracy: 0.8128 - f1_score: 0.8086 - val_loss: 0.5163 - val_accuracy: 0.7486 - val_f1_score: 0.7891\n","Epoch 4/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3738 - accuracy: 0.8360 - f1_score: 0.8334 - val_loss: 0.3876 - val_accuracy: 0.8481 - val_f1_score: 0.8511\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3397 - accuracy: 0.8593 - f1_score: 0.8566 - val_loss: 0.4394 - val_accuracy: 0.8020 - val_f1_score: 0.8218\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3185 - accuracy: 0.8668 - f1_score: 0.8659 - val_loss: 0.3808 - val_accuracy: 0.8472 - val_f1_score: 0.8508\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3060 - accuracy: 0.8752 - f1_score: 0.8734 - val_loss: 0.4192 - val_accuracy: 0.8192 - val_f1_score: 0.8344\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2915 - accuracy: 0.8813 - f1_score: 0.8796 - val_loss: 0.3780 - val_accuracy: 0.8490 - val_f1_score: 0.8518\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2796 - accuracy: 0.8903 - f1_score: 0.8896 - val_loss: 0.3616 - val_accuracy: 0.8499 - val_f1_score: 0.8502\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2774 - accuracy: 0.8900 - f1_score: 0.8881 - val_loss: 0.4010 - val_accuracy: 0.8445 - val_f1_score: 0.8515\n","Epoch 11/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2655 - accuracy: 0.8972 - f1_score: 0.8963 - val_loss: 0.3634 - val_accuracy: 0.8553 - val_f1_score: 0.8540\n","Epoch 12/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2591 - accuracy: 0.8984 - f1_score: 0.8973 - val_loss: 0.3711 - val_accuracy: 0.8553 - val_f1_score: 0.8535\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2544 - accuracy: 0.9045 - f1_score: 0.9034 - val_loss: 0.3810 - val_accuracy: 0.8599 - val_f1_score: 0.8595\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2530 - accuracy: 0.9033 - f1_score: 0.9021 - val_loss: 0.4476 - val_accuracy: 0.8192 - val_f1_score: 0.8358\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3365 - accuracy: 0.8656 - f1_score: 0.8871\n","Epoch 1/20\n","104/104 [==============================] - 10s 49ms/step - loss: 0.6190 - accuracy: 0.6573 - f1_score: 0.6593 - val_loss: 0.5075 - val_accuracy: 0.7803 - val_f1_score: 0.7756\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4656 - accuracy: 0.7848 - f1_score: 0.7767 - val_loss: 0.4300 - val_accuracy: 0.8192 - val_f1_score: 0.8322\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4077 - accuracy: 0.8240 - f1_score: 0.8204 - val_loss: 0.3807 - val_accuracy: 0.8436 - val_f1_score: 0.8470\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3667 - accuracy: 0.8385 - f1_score: 0.8365 - val_loss: 0.5316 - val_accuracy: 0.7559 - val_f1_score: 0.7942\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3433 - accuracy: 0.8529 - f1_score: 0.8510 - val_loss: 0.3380 - val_accuracy: 0.8680 - val_f1_score: 0.8692\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3291 - accuracy: 0.8677 - f1_score: 0.8664 - val_loss: 0.3244 - val_accuracy: 0.8743 - val_f1_score: 0.8767\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3110 - accuracy: 0.8692 - f1_score: 0.8675 - val_loss: 0.3187 - val_accuracy: 0.8797 - val_f1_score: 0.8779\n","Epoch 8/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3044 - accuracy: 0.8761 - f1_score: 0.8747 - val_loss: 0.3091 - val_accuracy: 0.8879 - val_f1_score: 0.8858\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2868 - accuracy: 0.8864 - f1_score: 0.8854 - val_loss: 0.3332 - val_accuracy: 0.8626 - val_f1_score: 0.8660\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2801 - accuracy: 0.8882 - f1_score: 0.8877 - val_loss: 0.3482 - val_accuracy: 0.8553 - val_f1_score: 0.8447\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2818 - accuracy: 0.8897 - f1_score: 0.8877 - val_loss: 0.3274 - val_accuracy: 0.8635 - val_f1_score: 0.8693\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2860 - accuracy: 0.8807 - f1_score: 0.8790 - val_loss: 0.3447 - val_accuracy: 0.8590 - val_f1_score: 0.8671\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2708 - accuracy: 0.8927 - f1_score: 0.8910 - val_loss: 0.4039 - val_accuracy: 0.8165 - val_f1_score: 0.8369\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3571 - accuracy: 0.8433 - f1_score: 0.8646\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6230 - accuracy: 0.6389 - f1_score: 0.6180 - val_loss: 0.5302 - val_accuracy: 0.7387 - val_f1_score: 0.7024\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4511 - accuracy: 0.7857 - f1_score: 0.7784 - val_loss: 0.4195 - val_accuracy: 0.8065 - val_f1_score: 0.7869\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3932 - accuracy: 0.8267 - f1_score: 0.8240 - val_loss: 0.3822 - val_accuracy: 0.8391 - val_f1_score: 0.8258\n","Epoch 4/20\n","104/104 [==============================] - 3s 33ms/step - loss: 0.3691 - accuracy: 0.8439 - f1_score: 0.8408 - val_loss: 0.3576 - val_accuracy: 0.8571 - val_f1_score: 0.8509\n","Epoch 5/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3353 - accuracy: 0.8623 - f1_score: 0.8613 - val_loss: 0.3483 - val_accuracy: 0.8617 - val_f1_score: 0.8561\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3245 - accuracy: 0.8656 - f1_score: 0.8648 - val_loss: 0.3360 - val_accuracy: 0.8707 - val_f1_score: 0.8682\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3135 - accuracy: 0.8710 - f1_score: 0.8701 - val_loss: 0.3269 - val_accuracy: 0.8716 - val_f1_score: 0.8707\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3115 - accuracy: 0.8719 - f1_score: 0.8709 - val_loss: 0.3309 - val_accuracy: 0.8662 - val_f1_score: 0.8585\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2947 - accuracy: 0.8761 - f1_score: 0.8748 - val_loss: 0.3154 - val_accuracy: 0.8743 - val_f1_score: 0.8751\n","Epoch 10/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2826 - accuracy: 0.8858 - f1_score: 0.8843 - val_loss: 0.3144 - val_accuracy: 0.8761 - val_f1_score: 0.8742\n","Epoch 11/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2850 - accuracy: 0.8837 - f1_score: 0.8833 - val_loss: 0.3289 - val_accuracy: 0.8698 - val_f1_score: 0.8618\n","Epoch 12/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2617 - accuracy: 0.8972 - f1_score: 0.8968 - val_loss: 0.3197 - val_accuracy: 0.8752 - val_f1_score: 0.8686\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2559 - accuracy: 0.9014 - f1_score: 0.9004 - val_loss: 0.3184 - val_accuracy: 0.8743 - val_f1_score: 0.8697\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2504 - accuracy: 0.9011 - f1_score: 0.9001 - val_loss: 0.3306 - val_accuracy: 0.8788 - val_f1_score: 0.8748\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2597 - accuracy: 0.8918 - f1_score: 0.8907 - val_loss: 0.4297 - val_accuracy: 0.7984 - val_f1_score: 0.7563\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3047 - accuracy: 0.8710 - f1_score: 0.8946\n","Epoch 1/20\n","104/104 [==============================] - 8s 31ms/step - loss: 0.6188 - accuracy: 0.6576 - f1_score: 0.6599 - val_loss: 0.4863 - val_accuracy: 0.7803 - val_f1_score: 0.7760\n","Epoch 2/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4548 - accuracy: 0.7842 - f1_score: 0.7744 - val_loss: 0.3550 - val_accuracy: 0.8481 - val_f1_score: 0.8511\n","Epoch 3/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3878 - accuracy: 0.8297 - f1_score: 0.8265 - val_loss: 0.3524 - val_accuracy: 0.8535 - val_f1_score: 0.8396\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3591 - accuracy: 0.8460 - f1_score: 0.8430 - val_loss: 0.3189 - val_accuracy: 0.8689 - val_f1_score: 0.8693\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3269 - accuracy: 0.8608 - f1_score: 0.8585 - val_loss: 0.3207 - val_accuracy: 0.8743 - val_f1_score: 0.8773\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3123 - accuracy: 0.8725 - f1_score: 0.8704 - val_loss: 0.3228 - val_accuracy: 0.8608 - val_f1_score: 0.8668\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3042 - accuracy: 0.8761 - f1_score: 0.8749 - val_loss: 0.3069 - val_accuracy: 0.8788 - val_f1_score: 0.8780\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2911 - accuracy: 0.8834 - f1_score: 0.8818 - val_loss: 0.3218 - val_accuracy: 0.8689 - val_f1_score: 0.8588\n","Epoch 9/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2772 - accuracy: 0.8876 - f1_score: 0.8852 - val_loss: 0.3019 - val_accuracy: 0.8788 - val_f1_score: 0.8768\n","Epoch 10/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2761 - accuracy: 0.8903 - f1_score: 0.8890 - val_loss: 0.3328 - val_accuracy: 0.8608 - val_f1_score: 0.8478\n","Epoch 11/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2689 - accuracy: 0.8921 - f1_score: 0.8905 - val_loss: 0.3093 - val_accuracy: 0.8761 - val_f1_score: 0.8701\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2590 - accuracy: 0.8999 - f1_score: 0.8986 - val_loss: 0.3856 - val_accuracy: 0.8146 - val_f1_score: 0.7826\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2529 - accuracy: 0.8951 - f1_score: 0.8938 - val_loss: 0.3162 - val_accuracy: 0.8671 - val_f1_score: 0.8588\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2456 - accuracy: 0.9057 - f1_score: 0.9044 - val_loss: 0.3051 - val_accuracy: 0.8779 - val_f1_score: 0.8735\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3102 - accuracy: 0.8744 - f1_score: 0.8968\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDacXbeRR3Yz","executionInfo":{"status":"ok","timestamp":1689843672211,"user_tz":-330,"elapsed":82,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"188b9765-0fb6-4d11-cebf-64f645d564f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.852126955986023, 0.8636056780815125, 0.8426738977432251, 0.8419986367225647, 0.8541526198387146, 0.8541526198387146, 0.8480756282806396, 0.8561782836914062, 0.8582038879394531, 0.8406482338905334, 0.8777852654457092, 0.8642808794975281, 0.8548278212547302, 0.8642808794975281, 0.8615800142288208, 0.8548278212547302, 0.846049964427948, 0.8656313419342041, 0.8514516949653625, 0.8602295517921448, 0.8710330724716187, 0.8656313419342041, 0.8433490991592407, 0.8710330724716187, 0.8744091987609863]\n","0.8575286984443664\n","0.01023307878833385\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHKzR45cR3ba","executionInfo":{"status":"ok","timestamp":1689843672212,"user_tz":-330,"elapsed":26,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"80aaef66-58fc-447f-e074-5c72475676af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.36524301767349243, 0.32209575176239014, 0.3550552725791931, 0.35455214977264404, 0.34090086817741394, 0.34783366322517395, 0.35164687037467957, 0.3349646329879761, 0.3327803909778595, 0.36083048582077026, 0.3050822913646698, 0.3099886476993561, 0.34607604146003723, 0.31613069772720337, 0.3445826470851898, 0.36211997270584106, 0.35555005073547363, 0.336999773979187, 0.34365466237068176, 0.3276359438896179, 0.3223623037338257, 0.33653125166893005, 0.3571097254753113, 0.3046647608280182, 0.31017765402793884]\n","0.337782781124115\n","0.018319651779441563\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_Rp8J6TR3eZ","executionInfo":{"status":"ok","timestamp":1689843672212,"user_tz":-330,"elapsed":18,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"513b5421-c555-479e-f205-6d5e050925a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8726003170013428, 0.8879022598266602, 0.8641398549079895, 0.8623529076576233, 0.8757191896438599, 0.8775509595870972, 0.8691098690032959, 0.8834154009819031, 0.884742021560669, 0.8634258508682251, 0.90082186460495, 0.8910568356513977, 0.8803560733795166, 0.8866328001022339, 0.8859209418296814, 0.8773530721664429, 0.8688146471977234, 0.8919064998626709, 0.8783184885978699, 0.8854453563690186, 0.8941828012466431, 0.8871241211891174, 0.8646440505981445, 0.8946496844291687, 0.8967812657356262]\n","0.8809986853599548\n","0.011013231671913076\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_T-MMxhdR7iq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing  -\n","1. Accuracy - 0.859/0.008\n","2. Loss - 0.339/0.023\n","3. F1 score - 0.882/0.009\n","\n","LSTM CNN LSTM nothing -\n","1. Accuracy - 0.857/0.010\n","2. Loss - 0.337/0.018\n","3. F1 score - 0.880/0.011"],"metadata":{"id":"0i6qfq4fiOGH"}},{"cell_type":"code","source":[],"metadata":{"id":"FH-N37AwiOhQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qfywv5iZkJ3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"20zIKI5HkRWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JCUL3YFnkRcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z1dKdoLIkRgN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.1 Understand MaxP and switch from GlobalMaxP to MaxP for CNN for emotion probs (WITH THAT 1 DROPOUT LAYER WITH DENSE 64 AND 32)"],"metadata":{"id":"DeEUbWUGkjku"}},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=3, strides=1)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWtBTiAekRid","executionInfo":{"status":"ok","timestamp":1689845592307,"user_tz":-330,"elapsed":1139899,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d05f5989-b9fa-4ca1-f9ce-ce3de2b27f34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 14s 62ms/step - loss: 0.6523 - accuracy: 0.6157 - f1_score: 0.6061 - val_loss: 0.5466 - val_accuracy: 0.7532 - val_f1_score: 0.7673\n","Epoch 2/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.5278 - accuracy: 0.7429 - f1_score: 0.7272 - val_loss: 0.4312 - val_accuracy: 0.8146 - val_f1_score: 0.8243\n","Epoch 3/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.4431 - accuracy: 0.7875 - f1_score: 0.7792 - val_loss: 0.4190 - val_accuracy: 0.8119 - val_f1_score: 0.7801\n","Epoch 4/20\n","104/104 [==============================] - 3s 32ms/step - loss: 0.3925 - accuracy: 0.8276 - f1_score: 0.8242 - val_loss: 0.3246 - val_accuracy: 0.8599 - val_f1_score: 0.8587\n","Epoch 5/20\n","104/104 [==============================] - 4s 40ms/step - loss: 0.3648 - accuracy: 0.8490 - f1_score: 0.8475 - val_loss: 0.3480 - val_accuracy: 0.8590 - val_f1_score: 0.8682\n","Epoch 6/20\n","104/104 [==============================] - 4s 34ms/step - loss: 0.3406 - accuracy: 0.8583 - f1_score: 0.8562 - val_loss: 0.3154 - val_accuracy: 0.8734 - val_f1_score: 0.8791\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3350 - accuracy: 0.8632 - f1_score: 0.8611 - val_loss: 0.3144 - val_accuracy: 0.8716 - val_f1_score: 0.8772\n","Epoch 8/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3288 - accuracy: 0.8677 - f1_score: 0.8648 - val_loss: 0.2976 - val_accuracy: 0.8770 - val_f1_score: 0.8777\n","Epoch 9/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3121 - accuracy: 0.8668 - f1_score: 0.8650 - val_loss: 0.3207 - val_accuracy: 0.8526 - val_f1_score: 0.8413\n","Epoch 10/20\n","104/104 [==============================] - 4s 37ms/step - loss: 0.3082 - accuracy: 0.8719 - f1_score: 0.8693 - val_loss: 0.3066 - val_accuracy: 0.8626 - val_f1_score: 0.8555\n","Epoch 11/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.3005 - accuracy: 0.8828 - f1_score: 0.8810 - val_loss: 0.3248 - val_accuracy: 0.8535 - val_f1_score: 0.8393\n","Epoch 12/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2956 - accuracy: 0.8822 - f1_score: 0.8794 - val_loss: 0.2888 - val_accuracy: 0.8797 - val_f1_score: 0.8796\n","Epoch 13/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3069 - accuracy: 0.8755 - f1_score: 0.8737 - val_loss: 0.3151 - val_accuracy: 0.8788 - val_f1_score: 0.8839\n","Epoch 14/20\n","104/104 [==============================] - 3s 31ms/step - loss: 0.2861 - accuracy: 0.8909 - f1_score: 0.8891 - val_loss: 0.3110 - val_accuracy: 0.8779 - val_f1_score: 0.8806\n","Epoch 15/20\n","104/104 [==============================] - 4s 39ms/step - loss: 0.2792 - accuracy: 0.8855 - f1_score: 0.8836 - val_loss: 0.3113 - val_accuracy: 0.8825 - val_f1_score: 0.8862\n","Epoch 16/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2637 - accuracy: 0.8972 - f1_score: 0.8959 - val_loss: 0.3069 - val_accuracy: 0.8707 - val_f1_score: 0.8645\n","Epoch 17/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2437 - accuracy: 0.9069 - f1_score: 0.9053 - val_loss: 0.3090 - val_accuracy: 0.8689 - val_f1_score: 0.8612\n","47/47 [==============================] - 1s 9ms/step - loss: 0.3519 - accuracy: 0.8515 - f1_score: 0.8737\n","Epoch 1/20\n","104/104 [==============================] - 14s 42ms/step - loss: 0.6369 - accuracy: 0.6410 - f1_score: 0.6191 - val_loss: 0.5794 - val_accuracy: 0.7107 - val_f1_score: 0.6380\n","Epoch 2/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.4937 - accuracy: 0.7703 - f1_score: 0.7599 - val_loss: 0.4654 - val_accuracy: 0.7893 - val_f1_score: 0.8050\n","Epoch 3/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.4123 - accuracy: 0.8131 - f1_score: 0.8077 - val_loss: 0.3860 - val_accuracy: 0.8400 - val_f1_score: 0.8407\n","Epoch 4/20\n","104/104 [==============================] - 3s 32ms/step - loss: 0.3640 - accuracy: 0.8439 - f1_score: 0.8415 - val_loss: 0.3590 - val_accuracy: 0.8499 - val_f1_score: 0.8485\n","Epoch 5/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3405 - accuracy: 0.8580 - f1_score: 0.8567 - val_loss: 0.3480 - val_accuracy: 0.8481 - val_f1_score: 0.8505\n","Epoch 6/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3267 - accuracy: 0.8692 - f1_score: 0.8687 - val_loss: 0.3388 - val_accuracy: 0.8544 - val_f1_score: 0.8564\n","Epoch 7/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3232 - accuracy: 0.8635 - f1_score: 0.8619 - val_loss: 0.3295 - val_accuracy: 0.8599 - val_f1_score: 0.8590\n","Epoch 8/20\n","104/104 [==============================] - 3s 31ms/step - loss: 0.2996 - accuracy: 0.8828 - f1_score: 0.8819 - val_loss: 0.3252 - val_accuracy: 0.8698 - val_f1_score: 0.8681\n","Epoch 9/20\n","104/104 [==============================] - 3s 33ms/step - loss: 0.2937 - accuracy: 0.8843 - f1_score: 0.8831 - val_loss: 0.3296 - val_accuracy: 0.8662 - val_f1_score: 0.8674\n","Epoch 10/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2918 - accuracy: 0.8797 - f1_score: 0.8781 - val_loss: 0.3296 - val_accuracy: 0.8644 - val_f1_score: 0.8656\n","Epoch 11/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2885 - accuracy: 0.8843 - f1_score: 0.8829 - val_loss: 0.3555 - val_accuracy: 0.8418 - val_f1_score: 0.8508\n","Epoch 12/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2758 - accuracy: 0.8882 - f1_score: 0.8876 - val_loss: 0.3501 - val_accuracy: 0.8590 - val_f1_score: 0.8497\n","Epoch 13/20\n","104/104 [==============================] - 3s 32ms/step - loss: 0.2709 - accuracy: 0.8909 - f1_score: 0.8894 - val_loss: 0.3336 - val_accuracy: 0.8662 - val_f1_score: 0.8622\n","47/47 [==============================] - 1s 11ms/step - loss: 0.3270 - accuracy: 0.8596 - f1_score: 0.8829\n","Epoch 1/20\n","104/104 [==============================] - 7s 27ms/step - loss: 0.6249 - accuracy: 0.6561 - f1_score: 0.6529 - val_loss: 0.5557 - val_accuracy: 0.7315 - val_f1_score: 0.6850\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5139 - accuracy: 0.7562 - f1_score: 0.7471 - val_loss: 0.4621 - val_accuracy: 0.7758 - val_f1_score: 0.7449\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4298 - accuracy: 0.8038 - f1_score: 0.7978 - val_loss: 0.4046 - val_accuracy: 0.8074 - val_f1_score: 0.7973\n","Epoch 4/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3682 - accuracy: 0.8409 - f1_score: 0.8382 - val_loss: 0.3900 - val_accuracy: 0.8192 - val_f1_score: 0.8069\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3369 - accuracy: 0.8644 - f1_score: 0.8623 - val_loss: 0.3672 - val_accuracy: 0.8409 - val_f1_score: 0.8385\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3190 - accuracy: 0.8716 - f1_score: 0.8707 - val_loss: 0.3712 - val_accuracy: 0.8373 - val_f1_score: 0.8299\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2978 - accuracy: 0.8813 - f1_score: 0.8799 - val_loss: 0.3717 - val_accuracy: 0.8327 - val_f1_score: 0.8209\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2960 - accuracy: 0.8831 - f1_score: 0.8819 - val_loss: 0.3816 - val_accuracy: 0.8409 - val_f1_score: 0.8379\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2867 - accuracy: 0.8846 - f1_score: 0.8840 - val_loss: 0.4087 - val_accuracy: 0.8246 - val_f1_score: 0.8048\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2911 - accuracy: 0.8834 - f1_score: 0.8817 - val_loss: 0.3683 - val_accuracy: 0.8472 - val_f1_score: 0.8442\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3449 - accuracy: 0.8515 - f1_score: 0.8744\n","Epoch 1/20\n","104/104 [==============================] - 8s 23ms/step - loss: 0.6427 - accuracy: 0.6314 - f1_score: 0.6313 - val_loss: 0.5604 - val_accuracy: 0.7152 - val_f1_score: 0.6416\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5000 - accuracy: 0.7616 - f1_score: 0.7490 - val_loss: 0.4134 - val_accuracy: 0.8210 - val_f1_score: 0.8167\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4147 - accuracy: 0.8243 - f1_score: 0.8172 - val_loss: 0.4595 - val_accuracy: 0.8047 - val_f1_score: 0.8278\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3724 - accuracy: 0.8427 - f1_score: 0.8398 - val_loss: 0.3551 - val_accuracy: 0.8608 - val_f1_score: 0.8600\n","Epoch 5/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3462 - accuracy: 0.8532 - f1_score: 0.8511 - val_loss: 0.3465 - val_accuracy: 0.8635 - val_f1_score: 0.8624\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3360 - accuracy: 0.8647 - f1_score: 0.8622 - val_loss: 0.3508 - val_accuracy: 0.8635 - val_f1_score: 0.8569\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3140 - accuracy: 0.8749 - f1_score: 0.8728 - val_loss: 0.3624 - val_accuracy: 0.8499 - val_f1_score: 0.8569\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3068 - accuracy: 0.8779 - f1_score: 0.8753 - val_loss: 0.3393 - val_accuracy: 0.8707 - val_f1_score: 0.8704\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3085 - accuracy: 0.8725 - f1_score: 0.8700 - val_loss: 0.3503 - val_accuracy: 0.8626 - val_f1_score: 0.8652\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2949 - accuracy: 0.8813 - f1_score: 0.8797 - val_loss: 0.3446 - val_accuracy: 0.8580 - val_f1_score: 0.8509\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2803 - accuracy: 0.8873 - f1_score: 0.8851 - val_loss: 0.3365 - val_accuracy: 0.8617 - val_f1_score: 0.8571\n","Epoch 12/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2828 - accuracy: 0.8924 - f1_score: 0.8903 - val_loss: 0.3358 - val_accuracy: 0.8662 - val_f1_score: 0.8627\n","Epoch 13/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2865 - accuracy: 0.8837 - f1_score: 0.8814 - val_loss: 0.4634 - val_accuracy: 0.8201 - val_f1_score: 0.8386\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2779 - accuracy: 0.8930 - f1_score: 0.8918 - val_loss: 0.3533 - val_accuracy: 0.8617 - val_f1_score: 0.8640\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2782 - accuracy: 0.8957 - f1_score: 0.8944 - val_loss: 0.3397 - val_accuracy: 0.8671 - val_f1_score: 0.8630\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2517 - accuracy: 0.8984 - f1_score: 0.8967 - val_loss: 0.3474 - val_accuracy: 0.8680 - val_f1_score: 0.8678\n","Epoch 17/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2451 - accuracy: 0.9096 - f1_score: 0.9083 - val_loss: 0.3855 - val_accuracy: 0.8553 - val_f1_score: 0.8587\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3750 - accuracy: 0.8386 - f1_score: 0.8590\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6497 - accuracy: 0.6221 - f1_score: 0.5861 - val_loss: 0.5703 - val_accuracy: 0.7061 - val_f1_score: 0.6377\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.5100 - accuracy: 0.7505 - f1_score: 0.7393 - val_loss: 0.4559 - val_accuracy: 0.7740 - val_f1_score: 0.7941\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4432 - accuracy: 0.7963 - f1_score: 0.7898 - val_loss: 0.3887 - val_accuracy: 0.8400 - val_f1_score: 0.8287\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3999 - accuracy: 0.8252 - f1_score: 0.8211 - val_loss: 0.3835 - val_accuracy: 0.8463 - val_f1_score: 0.8317\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3521 - accuracy: 0.8496 - f1_score: 0.8474 - val_loss: 0.3385 - val_accuracy: 0.8635 - val_f1_score: 0.8618\n","Epoch 6/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3480 - accuracy: 0.8523 - f1_score: 0.8497 - val_loss: 0.3331 - val_accuracy: 0.8653 - val_f1_score: 0.8627\n","Epoch 7/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3268 - accuracy: 0.8623 - f1_score: 0.8617 - val_loss: 0.3352 - val_accuracy: 0.8653 - val_f1_score: 0.8664\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3144 - accuracy: 0.8686 - f1_score: 0.8665 - val_loss: 0.3250 - val_accuracy: 0.8680 - val_f1_score: 0.8668\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3132 - accuracy: 0.8671 - f1_score: 0.8653 - val_loss: 0.3623 - val_accuracy: 0.8354 - val_f1_score: 0.8483\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2951 - accuracy: 0.8816 - f1_score: 0.8801 - val_loss: 0.3380 - val_accuracy: 0.8653 - val_f1_score: 0.8629\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2943 - accuracy: 0.8761 - f1_score: 0.8747 - val_loss: 0.3287 - val_accuracy: 0.8734 - val_f1_score: 0.8713\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2860 - accuracy: 0.8849 - f1_score: 0.8832 - val_loss: 0.3281 - val_accuracy: 0.8680 - val_f1_score: 0.8710\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2827 - accuracy: 0.8846 - f1_score: 0.8826 - val_loss: 0.3454 - val_accuracy: 0.8544 - val_f1_score: 0.8618\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3673 - accuracy: 0.8427 - f1_score: 0.8643\n","Epoch 1/20\n","104/104 [==============================] - 6s 23ms/step - loss: 0.6243 - accuracy: 0.6410 - f1_score: 0.6052 - val_loss: 0.5065 - val_accuracy: 0.7613 - val_f1_score: 0.7481\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4680 - accuracy: 0.7785 - f1_score: 0.7689 - val_loss: 0.3989 - val_accuracy: 0.8146 - val_f1_score: 0.8252\n","Epoch 3/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3898 - accuracy: 0.8195 - f1_score: 0.8169 - val_loss: 0.3441 - val_accuracy: 0.8571 - val_f1_score: 0.8550\n","Epoch 4/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3483 - accuracy: 0.8505 - f1_score: 0.8484 - val_loss: 0.3207 - val_accuracy: 0.8680 - val_f1_score: 0.8653\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3374 - accuracy: 0.8571 - f1_score: 0.8558 - val_loss: 0.3101 - val_accuracy: 0.8752 - val_f1_score: 0.8796\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3302 - accuracy: 0.8626 - f1_score: 0.8625 - val_loss: 0.3067 - val_accuracy: 0.8816 - val_f1_score: 0.8836\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3057 - accuracy: 0.8734 - f1_score: 0.8736 - val_loss: 0.2899 - val_accuracy: 0.8870 - val_f1_score: 0.8856\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2922 - accuracy: 0.8764 - f1_score: 0.8757 - val_loss: 0.2847 - val_accuracy: 0.8834 - val_f1_score: 0.8847\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2963 - accuracy: 0.8797 - f1_score: 0.8786 - val_loss: 0.2937 - val_accuracy: 0.8834 - val_f1_score: 0.8877\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2871 - accuracy: 0.8831 - f1_score: 0.8829 - val_loss: 0.3261 - val_accuracy: 0.8571 - val_f1_score: 0.8677\n","Epoch 11/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2798 - accuracy: 0.8882 - f1_score: 0.8875 - val_loss: 0.2809 - val_accuracy: 0.8852 - val_f1_score: 0.8842\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2730 - accuracy: 0.8873 - f1_score: 0.8867 - val_loss: 0.3495 - val_accuracy: 0.8382 - val_f1_score: 0.8141\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2952 - accuracy: 0.8803 - f1_score: 0.8792 - val_loss: 0.2979 - val_accuracy: 0.8825 - val_f1_score: 0.8735\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2574 - accuracy: 0.8945 - f1_score: 0.8943 - val_loss: 0.3058 - val_accuracy: 0.8743 - val_f1_score: 0.8622\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2588 - accuracy: 0.8975 - f1_score: 0.8974 - val_loss: 0.3036 - val_accuracy: 0.8734 - val_f1_score: 0.8619\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2409 - accuracy: 0.9027 - f1_score: 0.9018 - val_loss: 0.3222 - val_accuracy: 0.8671 - val_f1_score: 0.8534\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3306 - accuracy: 0.8548 - f1_score: 0.8808\n","Epoch 1/20\n","104/104 [==============================] - 7s 33ms/step - loss: 0.6287 - accuracy: 0.6392 - f1_score: 0.6349 - val_loss: 0.5687 - val_accuracy: 0.7125 - val_f1_score: 0.7268\n","Epoch 2/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.4943 - accuracy: 0.7625 - f1_score: 0.7506 - val_loss: 0.4211 - val_accuracy: 0.8065 - val_f1_score: 0.7977\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3955 - accuracy: 0.8219 - f1_score: 0.8175 - val_loss: 0.3844 - val_accuracy: 0.8309 - val_f1_score: 0.8453\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3606 - accuracy: 0.8502 - f1_score: 0.8484 - val_loss: 0.4076 - val_accuracy: 0.8156 - val_f1_score: 0.8384\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3228 - accuracy: 0.8680 - f1_score: 0.8665 - val_loss: 0.3468 - val_accuracy: 0.8562 - val_f1_score: 0.8672\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3200 - accuracy: 0.8644 - f1_score: 0.8631 - val_loss: 0.3281 - val_accuracy: 0.8635 - val_f1_score: 0.8530\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3116 - accuracy: 0.8740 - f1_score: 0.8726 - val_loss: 0.3164 - val_accuracy: 0.8725 - val_f1_score: 0.8786\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2968 - accuracy: 0.8770 - f1_score: 0.8764 - val_loss: 0.3323 - val_accuracy: 0.8517 - val_f1_score: 0.8626\n","Epoch 9/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2862 - accuracy: 0.8867 - f1_score: 0.8862 - val_loss: 0.2968 - val_accuracy: 0.8807 - val_f1_score: 0.8842\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2878 - accuracy: 0.8822 - f1_score: 0.8810 - val_loss: 0.3082 - val_accuracy: 0.8761 - val_f1_score: 0.8808\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2647 - accuracy: 0.8930 - f1_score: 0.8921 - val_loss: 0.2951 - val_accuracy: 0.8797 - val_f1_score: 0.8799\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2661 - accuracy: 0.8954 - f1_score: 0.8940 - val_loss: 0.3565 - val_accuracy: 0.8535 - val_f1_score: 0.8663\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2573 - accuracy: 0.9017 - f1_score: 0.9010 - val_loss: 0.3298 - val_accuracy: 0.8617 - val_f1_score: 0.8698\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2471 - accuracy: 0.8990 - f1_score: 0.8985 - val_loss: 0.3255 - val_accuracy: 0.8680 - val_f1_score: 0.8744\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2409 - accuracy: 0.9045 - f1_score: 0.9038 - val_loss: 0.3066 - val_accuracy: 0.8843 - val_f1_score: 0.8867\n","Epoch 16/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2356 - accuracy: 0.9051 - f1_score: 0.9043 - val_loss: 0.3179 - val_accuracy: 0.8834 - val_f1_score: 0.8835\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3390 - accuracy: 0.8602 - f1_score: 0.8834\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6288 - accuracy: 0.6465 - f1_score: 0.6340 - val_loss: 0.5479 - val_accuracy: 0.7260 - val_f1_score: 0.6967\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4762 - accuracy: 0.7764 - f1_score: 0.7650 - val_loss: 0.4312 - val_accuracy: 0.8083 - val_f1_score: 0.7989\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4018 - accuracy: 0.8183 - f1_score: 0.8117 - val_loss: 0.4481 - val_accuracy: 0.7984 - val_f1_score: 0.8197\n","Epoch 4/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3559 - accuracy: 0.8481 - f1_score: 0.8453 - val_loss: 0.3638 - val_accuracy: 0.8508 - val_f1_score: 0.8523\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3298 - accuracy: 0.8565 - f1_score: 0.8548 - val_loss: 0.3706 - val_accuracy: 0.8391 - val_f1_score: 0.8504\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3363 - accuracy: 0.8599 - f1_score: 0.8593 - val_loss: 0.3353 - val_accuracy: 0.8599 - val_f1_score: 0.8605\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2990 - accuracy: 0.8794 - f1_score: 0.8789 - val_loss: 0.3307 - val_accuracy: 0.8716 - val_f1_score: 0.8725\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2857 - accuracy: 0.8837 - f1_score: 0.8832 - val_loss: 0.3374 - val_accuracy: 0.8580 - val_f1_score: 0.8634\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3013 - accuracy: 0.8698 - f1_score: 0.8688 - val_loss: 0.3589 - val_accuracy: 0.8481 - val_f1_score: 0.8574\n","Epoch 10/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2729 - accuracy: 0.8909 - f1_score: 0.8905 - val_loss: 0.3315 - val_accuracy: 0.8671 - val_f1_score: 0.8617\n","Epoch 11/20\n","104/104 [==============================] - 3s 34ms/step - loss: 0.2636 - accuracy: 0.8945 - f1_score: 0.8941 - val_loss: 0.3973 - val_accuracy: 0.8282 - val_f1_score: 0.8443\n","Epoch 12/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2571 - accuracy: 0.8996 - f1_score: 0.8991 - val_loss: 0.3232 - val_accuracy: 0.8797 - val_f1_score: 0.8816\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2541 - accuracy: 0.9002 - f1_score: 0.8998 - val_loss: 0.4471 - val_accuracy: 0.7830 - val_f1_score: 0.8131\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2388 - accuracy: 0.9051 - f1_score: 0.9043 - val_loss: 0.3446 - val_accuracy: 0.8680 - val_f1_score: 0.8726\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2347 - accuracy: 0.9090 - f1_score: 0.9084 - val_loss: 0.3205 - val_accuracy: 0.8734 - val_f1_score: 0.8696\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2241 - accuracy: 0.9117 - f1_score: 0.9111 - val_loss: 0.3378 - val_accuracy: 0.8644 - val_f1_score: 0.8673\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2232 - accuracy: 0.9150 - f1_score: 0.9144 - val_loss: 0.3453 - val_accuracy: 0.8644 - val_f1_score: 0.8574\n","Epoch 18/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2059 - accuracy: 0.9234 - f1_score: 0.9229 - val_loss: 0.3699 - val_accuracy: 0.8680 - val_f1_score: 0.8724\n","Epoch 19/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2005 - accuracy: 0.9237 - f1_score: 0.9234 - val_loss: 0.3393 - val_accuracy: 0.8662 - val_f1_score: 0.8617\n","Epoch 20/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2127 - accuracy: 0.9189 - f1_score: 0.9186 - val_loss: 0.3466 - val_accuracy: 0.8743 - val_f1_score: 0.8692\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3801 - accuracy: 0.8454 - f1_score: 0.8688\n","Epoch 1/20\n","104/104 [==============================] - 6s 24ms/step - loss: 0.6432 - accuracy: 0.6266 - f1_score: 0.6165 - val_loss: 0.5488 - val_accuracy: 0.7414 - val_f1_score: 0.7093\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4973 - accuracy: 0.7625 - f1_score: 0.7547 - val_loss: 0.4368 - val_accuracy: 0.7993 - val_f1_score: 0.8103\n","Epoch 3/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3978 - accuracy: 0.8219 - f1_score: 0.8185 - val_loss: 0.3449 - val_accuracy: 0.8472 - val_f1_score: 0.8495\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3537 - accuracy: 0.8532 - f1_score: 0.8518 - val_loss: 0.3212 - val_accuracy: 0.8671 - val_f1_score: 0.8709\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3277 - accuracy: 0.8692 - f1_score: 0.8686 - val_loss: 0.3098 - val_accuracy: 0.8689 - val_f1_score: 0.8727\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3318 - accuracy: 0.8617 - f1_score: 0.8620 - val_loss: 0.3020 - val_accuracy: 0.8797 - val_f1_score: 0.8814\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3126 - accuracy: 0.8758 - f1_score: 0.8758 - val_loss: 0.3043 - val_accuracy: 0.8770 - val_f1_score: 0.8794\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3088 - accuracy: 0.8770 - f1_score: 0.8762 - val_loss: 0.3007 - val_accuracy: 0.8834 - val_f1_score: 0.8811\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2845 - accuracy: 0.8819 - f1_score: 0.8813 - val_loss: 0.3000 - val_accuracy: 0.8852 - val_f1_score: 0.8827\n","Epoch 10/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2772 - accuracy: 0.8894 - f1_score: 0.8890 - val_loss: 0.3024 - val_accuracy: 0.8807 - val_f1_score: 0.8776\n","Epoch 11/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2816 - accuracy: 0.8846 - f1_score: 0.8839 - val_loss: 0.3042 - val_accuracy: 0.8770 - val_f1_score: 0.8786\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2752 - accuracy: 0.8912 - f1_score: 0.8910 - val_loss: 0.3067 - val_accuracy: 0.8870 - val_f1_score: 0.8844\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2537 - accuracy: 0.9042 - f1_score: 0.9030 - val_loss: 0.3262 - val_accuracy: 0.8725 - val_f1_score: 0.8766\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2576 - accuracy: 0.8975 - f1_score: 0.8966 - val_loss: 0.3217 - val_accuracy: 0.8671 - val_f1_score: 0.8585\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3409 - accuracy: 0.8629 - f1_score: 0.8856\n","Epoch 1/20\n","104/104 [==============================] - 7s 36ms/step - loss: 0.6654 - accuracy: 0.5973 - f1_score: 0.5121 - val_loss: 0.5864 - val_accuracy: 0.6989 - val_f1_score: 0.7325\n","Epoch 2/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.5160 - accuracy: 0.7523 - f1_score: 0.7434 - val_loss: 0.4163 - val_accuracy: 0.8056 - val_f1_score: 0.8033\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4066 - accuracy: 0.8192 - f1_score: 0.8162 - val_loss: 0.4897 - val_accuracy: 0.7875 - val_f1_score: 0.7426\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3729 - accuracy: 0.8403 - f1_score: 0.8382 - val_loss: 0.3894 - val_accuracy: 0.8363 - val_f1_score: 0.8159\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3366 - accuracy: 0.8580 - f1_score: 0.8571 - val_loss: 0.3357 - val_accuracy: 0.8517 - val_f1_score: 0.8514\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3202 - accuracy: 0.8695 - f1_score: 0.8683 - val_loss: 0.3454 - val_accuracy: 0.8626 - val_f1_score: 0.8544\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3185 - accuracy: 0.8734 - f1_score: 0.8732 - val_loss: 0.3187 - val_accuracy: 0.8635 - val_f1_score: 0.8648\n","Epoch 8/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2971 - accuracy: 0.8813 - f1_score: 0.8813 - val_loss: 0.4648 - val_accuracy: 0.7875 - val_f1_score: 0.7380\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2958 - accuracy: 0.8797 - f1_score: 0.8791 - val_loss: 0.3124 - val_accuracy: 0.8626 - val_f1_score: 0.8631\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2814 - accuracy: 0.8900 - f1_score: 0.8891 - val_loss: 0.3052 - val_accuracy: 0.8689 - val_f1_score: 0.8659\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2849 - accuracy: 0.8870 - f1_score: 0.8865 - val_loss: 0.3340 - val_accuracy: 0.8626 - val_f1_score: 0.8671\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2774 - accuracy: 0.8867 - f1_score: 0.8861 - val_loss: 0.3068 - val_accuracy: 0.8680 - val_f1_score: 0.8648\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2762 - accuracy: 0.8969 - f1_score: 0.8966 - val_loss: 0.3054 - val_accuracy: 0.8707 - val_f1_score: 0.8667\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2613 - accuracy: 0.9017 - f1_score: 0.9013 - val_loss: 0.3329 - val_accuracy: 0.8671 - val_f1_score: 0.8563\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2490 - accuracy: 0.9075 - f1_score: 0.9069 - val_loss: 0.3271 - val_accuracy: 0.8662 - val_f1_score: 0.8549\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3281 - accuracy: 0.8616 - f1_score: 0.8858\n","Epoch 1/20\n","104/104 [==============================] - 9s 26ms/step - loss: 0.6473 - accuracy: 0.6154 - f1_score: 0.6369 - val_loss: 0.5351 - val_accuracy: 0.7550 - val_f1_score: 0.7335\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5120 - accuracy: 0.7489 - f1_score: 0.7266 - val_loss: 0.4494 - val_accuracy: 0.7821 - val_f1_score: 0.7543\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4444 - accuracy: 0.7815 - f1_score: 0.7628 - val_loss: 0.3928 - val_accuracy: 0.8291 - val_f1_score: 0.8184\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3895 - accuracy: 0.8222 - f1_score: 0.8148 - val_loss: 0.3587 - val_accuracy: 0.8472 - val_f1_score: 0.8431\n","Epoch 5/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3433 - accuracy: 0.8427 - f1_score: 0.8389 - val_loss: 0.3444 - val_accuracy: 0.8481 - val_f1_score: 0.8484\n","Epoch 6/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3221 - accuracy: 0.8605 - f1_score: 0.8579 - val_loss: 0.3702 - val_accuracy: 0.8508 - val_f1_score: 0.8348\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3038 - accuracy: 0.8704 - f1_score: 0.8683 - val_loss: 0.3479 - val_accuracy: 0.8490 - val_f1_score: 0.8559\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3031 - accuracy: 0.8725 - f1_score: 0.8715 - val_loss: 0.3351 - val_accuracy: 0.8562 - val_f1_score: 0.8589\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2776 - accuracy: 0.8933 - f1_score: 0.8922 - val_loss: 0.3404 - val_accuracy: 0.8644 - val_f1_score: 0.8532\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2786 - accuracy: 0.8879 - f1_score: 0.8864 - val_loss: 0.3341 - val_accuracy: 0.8653 - val_f1_score: 0.8611\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2620 - accuracy: 0.8927 - f1_score: 0.8915 - val_loss: 0.3450 - val_accuracy: 0.8716 - val_f1_score: 0.8695\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2679 - accuracy: 0.8840 - f1_score: 0.8815 - val_loss: 0.3231 - val_accuracy: 0.8716 - val_f1_score: 0.8683\n","Epoch 13/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2572 - accuracy: 0.8990 - f1_score: 0.8977 - val_loss: 0.3538 - val_accuracy: 0.8653 - val_f1_score: 0.8678\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2495 - accuracy: 0.8945 - f1_score: 0.8940 - val_loss: 0.3926 - val_accuracy: 0.8571 - val_f1_score: 0.8436\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2276 - accuracy: 0.9087 - f1_score: 0.9071 - val_loss: 0.3835 - val_accuracy: 0.8662 - val_f1_score: 0.8588\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2312 - accuracy: 0.9081 - f1_score: 0.9068 - val_loss: 0.3465 - val_accuracy: 0.8671 - val_f1_score: 0.8622\n","Epoch 17/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2097 - accuracy: 0.9147 - f1_score: 0.9134 - val_loss: 0.3816 - val_accuracy: 0.8617 - val_f1_score: 0.8582\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3091 - accuracy: 0.8683 - f1_score: 0.8940\n","Epoch 1/20\n","104/104 [==============================] - 7s 27ms/step - loss: 0.6503 - accuracy: 0.6172 - f1_score: 0.6029 - val_loss: 0.5599 - val_accuracy: 0.7360 - val_f1_score: 0.6840\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.5107 - accuracy: 0.7483 - f1_score: 0.7348 - val_loss: 0.4220 - val_accuracy: 0.7948 - val_f1_score: 0.7848\n","Epoch 3/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.4194 - accuracy: 0.8122 - f1_score: 0.8039 - val_loss: 0.3742 - val_accuracy: 0.8345 - val_f1_score: 0.8344\n","Epoch 4/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3630 - accuracy: 0.8448 - f1_score: 0.8399 - val_loss: 0.3513 - val_accuracy: 0.8526 - val_f1_score: 0.8566\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3435 - accuracy: 0.8562 - f1_score: 0.8535 - val_loss: 0.3300 - val_accuracy: 0.8644 - val_f1_score: 0.8658\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3400 - accuracy: 0.8568 - f1_score: 0.8538 - val_loss: 0.3392 - val_accuracy: 0.8544 - val_f1_score: 0.8417\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3210 - accuracy: 0.8611 - f1_score: 0.8580 - val_loss: 0.3257 - val_accuracy: 0.8716 - val_f1_score: 0.8734\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3115 - accuracy: 0.8683 - f1_score: 0.8657 - val_loss: 0.3350 - val_accuracy: 0.8544 - val_f1_score: 0.8404\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2940 - accuracy: 0.8770 - f1_score: 0.8742 - val_loss: 0.3374 - val_accuracy: 0.8599 - val_f1_score: 0.8464\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2876 - accuracy: 0.8834 - f1_score: 0.8807 - val_loss: 0.3157 - val_accuracy: 0.8761 - val_f1_score: 0.8740\n","Epoch 11/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2773 - accuracy: 0.8864 - f1_score: 0.8839 - val_loss: 0.3187 - val_accuracy: 0.8707 - val_f1_score: 0.8711\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2689 - accuracy: 0.8885 - f1_score: 0.8862 - val_loss: 0.3220 - val_accuracy: 0.8788 - val_f1_score: 0.8714\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2677 - accuracy: 0.8882 - f1_score: 0.8860 - val_loss: 0.3212 - val_accuracy: 0.8644 - val_f1_score: 0.8560\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2628 - accuracy: 0.8918 - f1_score: 0.8895 - val_loss: 0.3406 - val_accuracy: 0.8852 - val_f1_score: 0.8842\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2582 - accuracy: 0.8933 - f1_score: 0.8907 - val_loss: 0.3247 - val_accuracy: 0.8816 - val_f1_score: 0.8772\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.8575 - f1_score: 0.8882\n","Epoch 1/20\n","104/104 [==============================] - 7s 29ms/step - loss: 0.6307 - accuracy: 0.6432 - f1_score: 0.6279 - val_loss: 0.5404 - val_accuracy: 0.7288 - val_f1_score: 0.6875\n","Epoch 2/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.4915 - accuracy: 0.7622 - f1_score: 0.7472 - val_loss: 0.4313 - val_accuracy: 0.8029 - val_f1_score: 0.8047\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4051 - accuracy: 0.8270 - f1_score: 0.8208 - val_loss: 0.4359 - val_accuracy: 0.8237 - val_f1_score: 0.8392\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3615 - accuracy: 0.8412 - f1_score: 0.8371 - val_loss: 0.3690 - val_accuracy: 0.8373 - val_f1_score: 0.8279\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3394 - accuracy: 0.8571 - f1_score: 0.8542 - val_loss: 0.3563 - val_accuracy: 0.8517 - val_f1_score: 0.8506\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3213 - accuracy: 0.8620 - f1_score: 0.8590 - val_loss: 0.4031 - val_accuracy: 0.8445 - val_f1_score: 0.8542\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3106 - accuracy: 0.8725 - f1_score: 0.8702 - val_loss: 0.4721 - val_accuracy: 0.7920 - val_f1_score: 0.8189\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2916 - accuracy: 0.8834 - f1_score: 0.8818 - val_loss: 0.3633 - val_accuracy: 0.8599 - val_f1_score: 0.8639\n","Epoch 9/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2815 - accuracy: 0.8849 - f1_score: 0.8833 - val_loss: 0.4025 - val_accuracy: 0.8535 - val_f1_score: 0.8596\n","Epoch 10/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2636 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3817 - val_accuracy: 0.8599 - val_f1_score: 0.8649\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.8575 - f1_score: 0.8828\n","Epoch 1/20\n","104/104 [==============================] - 9s 36ms/step - loss: 0.6515 - accuracy: 0.6248 - f1_score: 0.5577 - val_loss: 0.5389 - val_accuracy: 0.7351 - val_f1_score: 0.7191\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5046 - accuracy: 0.7604 - f1_score: 0.7474 - val_loss: 0.4465 - val_accuracy: 0.7911 - val_f1_score: 0.8089\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4160 - accuracy: 0.8089 - f1_score: 0.8008 - val_loss: 0.3687 - val_accuracy: 0.8400 - val_f1_score: 0.8410\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3837 - accuracy: 0.8270 - f1_score: 0.8224 - val_loss: 0.4081 - val_accuracy: 0.8128 - val_f1_score: 0.8316\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3483 - accuracy: 0.8487 - f1_score: 0.8433 - val_loss: 0.4435 - val_accuracy: 0.8047 - val_f1_score: 0.8280\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3272 - accuracy: 0.8641 - f1_score: 0.8611 - val_loss: 0.3426 - val_accuracy: 0.8544 - val_f1_score: 0.8620\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3239 - accuracy: 0.8686 - f1_score: 0.8659 - val_loss: 0.3911 - val_accuracy: 0.8183 - val_f1_score: 0.8370\n","Epoch 8/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2979 - accuracy: 0.8791 - f1_score: 0.8767 - val_loss: 0.3352 - val_accuracy: 0.8571 - val_f1_score: 0.8643\n","Epoch 9/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2912 - accuracy: 0.8800 - f1_score: 0.8782 - val_loss: 0.3087 - val_accuracy: 0.8834 - val_f1_score: 0.8809\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2901 - accuracy: 0.8831 - f1_score: 0.8804 - val_loss: 0.3175 - val_accuracy: 0.8761 - val_f1_score: 0.8769\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2816 - accuracy: 0.8822 - f1_score: 0.8805 - val_loss: 0.3744 - val_accuracy: 0.8391 - val_f1_score: 0.8507\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2719 - accuracy: 0.8903 - f1_score: 0.8889 - val_loss: 0.3067 - val_accuracy: 0.8852 - val_f1_score: 0.8842\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2730 - accuracy: 0.8840 - f1_score: 0.8816 - val_loss: 0.3128 - val_accuracy: 0.8788 - val_f1_score: 0.8768\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2582 - accuracy: 0.8951 - f1_score: 0.8930 - val_loss: 0.3284 - val_accuracy: 0.8698 - val_f1_score: 0.8728\n","Epoch 15/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2548 - accuracy: 0.8990 - f1_score: 0.8976 - val_loss: 0.3869 - val_accuracy: 0.8354 - val_f1_score: 0.8513\n","Epoch 16/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2402 - accuracy: 0.9045 - f1_score: 0.9033 - val_loss: 0.3225 - val_accuracy: 0.8770 - val_f1_score: 0.8768\n","Epoch 17/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2387 - accuracy: 0.9078 - f1_score: 0.9060 - val_loss: 0.3729 - val_accuracy: 0.8562 - val_f1_score: 0.8651\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8710 - f1_score: 0.8934\n","Epoch 1/20\n","104/104 [==============================] - 8s 32ms/step - loss: 0.6395 - accuracy: 0.6347 - f1_score: 0.6050 - val_loss: 0.5699 - val_accuracy: 0.7016 - val_f1_score: 0.6605\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.5017 - accuracy: 0.7610 - f1_score: 0.7507 - val_loss: 0.4625 - val_accuracy: 0.7821 - val_f1_score: 0.7482\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4186 - accuracy: 0.8113 - f1_score: 0.8047 - val_loss: 0.3929 - val_accuracy: 0.8336 - val_f1_score: 0.8238\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3772 - accuracy: 0.8385 - f1_score: 0.8343 - val_loss: 0.3578 - val_accuracy: 0.8445 - val_f1_score: 0.8436\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3351 - accuracy: 0.8596 - f1_score: 0.8565 - val_loss: 0.3834 - val_accuracy: 0.8427 - val_f1_score: 0.8267\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3218 - accuracy: 0.8680 - f1_score: 0.8662 - val_loss: 0.3361 - val_accuracy: 0.8599 - val_f1_score: 0.8610\n","Epoch 7/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3166 - accuracy: 0.8698 - f1_score: 0.8682 - val_loss: 0.3306 - val_accuracy: 0.8671 - val_f1_score: 0.8672\n","Epoch 8/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3034 - accuracy: 0.8767 - f1_score: 0.8746 - val_loss: 0.3453 - val_accuracy: 0.8599 - val_f1_score: 0.8491\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2884 - accuracy: 0.8813 - f1_score: 0.8800 - val_loss: 0.3215 - val_accuracy: 0.8653 - val_f1_score: 0.8604\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3026 - accuracy: 0.8695 - f1_score: 0.8671 - val_loss: 0.3164 - val_accuracy: 0.8680 - val_f1_score: 0.8701\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2760 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3326 - val_accuracy: 0.8644 - val_f1_score: 0.8646\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2706 - accuracy: 0.8891 - f1_score: 0.8877 - val_loss: 0.3096 - val_accuracy: 0.8626 - val_f1_score: 0.8611\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2667 - accuracy: 0.8909 - f1_score: 0.8902 - val_loss: 0.3279 - val_accuracy: 0.8635 - val_f1_score: 0.8561\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2542 - accuracy: 0.8966 - f1_score: 0.8953 - val_loss: 0.3164 - val_accuracy: 0.8662 - val_f1_score: 0.8635\n","Epoch 15/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2708 - accuracy: 0.8873 - f1_score: 0.8871 - val_loss: 0.3167 - val_accuracy: 0.8707 - val_f1_score: 0.8680\n","Epoch 16/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2450 - accuracy: 0.9011 - f1_score: 0.9002 - val_loss: 0.3215 - val_accuracy: 0.8707 - val_f1_score: 0.8704\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2370 - accuracy: 0.9072 - f1_score: 0.9064 - val_loss: 0.3161 - val_accuracy: 0.8644 - val_f1_score: 0.8596\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8609 - f1_score: 0.8857\n","Epoch 1/20\n","104/104 [==============================] - 6s 25ms/step - loss: 0.6276 - accuracy: 0.6480 - f1_score: 0.6439 - val_loss: 0.5336 - val_accuracy: 0.7297 - val_f1_score: 0.7128\n","Epoch 2/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.4988 - accuracy: 0.7679 - f1_score: 0.7563 - val_loss: 0.4630 - val_accuracy: 0.7812 - val_f1_score: 0.7784\n","Epoch 3/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.4190 - accuracy: 0.8119 - f1_score: 0.8069 - val_loss: 0.3959 - val_accuracy: 0.8192 - val_f1_score: 0.8214\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3740 - accuracy: 0.8397 - f1_score: 0.8374 - val_loss: 0.3904 - val_accuracy: 0.8237 - val_f1_score: 0.8357\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3554 - accuracy: 0.8424 - f1_score: 0.8402 - val_loss: 0.3506 - val_accuracy: 0.8382 - val_f1_score: 0.8434\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3089 - accuracy: 0.8713 - f1_score: 0.8702 - val_loss: 0.3504 - val_accuracy: 0.8481 - val_f1_score: 0.8581\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3068 - accuracy: 0.8716 - f1_score: 0.8708 - val_loss: 0.3533 - val_accuracy: 0.8526 - val_f1_score: 0.8617\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2882 - accuracy: 0.8861 - f1_score: 0.8853 - val_loss: 0.3176 - val_accuracy: 0.8680 - val_f1_score: 0.8648\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2854 - accuracy: 0.8837 - f1_score: 0.8825 - val_loss: 0.3439 - val_accuracy: 0.8590 - val_f1_score: 0.8664\n","Epoch 10/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2773 - accuracy: 0.8852 - f1_score: 0.8851 - val_loss: 0.3850 - val_accuracy: 0.8300 - val_f1_score: 0.8476\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2723 - accuracy: 0.8915 - f1_score: 0.8910 - val_loss: 0.3785 - val_accuracy: 0.8345 - val_f1_score: 0.8484\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2686 - accuracy: 0.8879 - f1_score: 0.8867 - val_loss: 0.3271 - val_accuracy: 0.8698 - val_f1_score: 0.8728\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2543 - accuracy: 0.8945 - f1_score: 0.8938 - val_loss: 0.3211 - val_accuracy: 0.8761 - val_f1_score: 0.8765\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3734 - accuracy: 0.8467 - f1_score: 0.8695\n","Epoch 1/20\n","104/104 [==============================] - 9s 37ms/step - loss: 0.6324 - accuracy: 0.6380 - f1_score: 0.6215 - val_loss: 0.5694 - val_accuracy: 0.7043 - val_f1_score: 0.6371\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4878 - accuracy: 0.7743 - f1_score: 0.7617 - val_loss: 0.4717 - val_accuracy: 0.7676 - val_f1_score: 0.7337\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3962 - accuracy: 0.8225 - f1_score: 0.8158 - val_loss: 0.3611 - val_accuracy: 0.8400 - val_f1_score: 0.8398\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3519 - accuracy: 0.8520 - f1_score: 0.8499 - val_loss: 0.3309 - val_accuracy: 0.8553 - val_f1_score: 0.8577\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3437 - accuracy: 0.8508 - f1_score: 0.8494 - val_loss: 0.3734 - val_accuracy: 0.8336 - val_f1_score: 0.8489\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3165 - accuracy: 0.8695 - f1_score: 0.8686 - val_loss: 0.3198 - val_accuracy: 0.8635 - val_f1_score: 0.8626\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3025 - accuracy: 0.8740 - f1_score: 0.8722 - val_loss: 0.3224 - val_accuracy: 0.8644 - val_f1_score: 0.8552\n","Epoch 8/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3070 - accuracy: 0.8752 - f1_score: 0.8737 - val_loss: 0.3854 - val_accuracy: 0.8318 - val_f1_score: 0.8062\n","Epoch 9/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2949 - accuracy: 0.8813 - f1_score: 0.8802 - val_loss: 0.3235 - val_accuracy: 0.8635 - val_f1_score: 0.8521\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2701 - accuracy: 0.8915 - f1_score: 0.8904 - val_loss: 0.4111 - val_accuracy: 0.8165 - val_f1_score: 0.7834\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2929 - accuracy: 0.8810 - f1_score: 0.8784 - val_loss: 0.3141 - val_accuracy: 0.8662 - val_f1_score: 0.8604\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2733 - accuracy: 0.8852 - f1_score: 0.8830 - val_loss: 0.3302 - val_accuracy: 0.8590 - val_f1_score: 0.8477\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2553 - accuracy: 0.8945 - f1_score: 0.8925 - val_loss: 0.3092 - val_accuracy: 0.8725 - val_f1_score: 0.8696\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2432 - accuracy: 0.8999 - f1_score: 0.8985 - val_loss: 0.3919 - val_accuracy: 0.8363 - val_f1_score: 0.8124\n","Epoch 15/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2592 - accuracy: 0.8912 - f1_score: 0.8894 - val_loss: 0.3177 - val_accuracy: 0.8734 - val_f1_score: 0.8720\n","Epoch 16/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2341 - accuracy: 0.9069 - f1_score: 0.9051 - val_loss: 0.3396 - val_accuracy: 0.8725 - val_f1_score: 0.8648\n","Epoch 17/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2243 - accuracy: 0.9132 - f1_score: 0.9116 - val_loss: 0.3342 - val_accuracy: 0.8761 - val_f1_score: 0.8740\n","Epoch 18/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2339 - accuracy: 0.9072 - f1_score: 0.9051 - val_loss: 0.3357 - val_accuracy: 0.8716 - val_f1_score: 0.8704\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3441 - accuracy: 0.8528 - f1_score: 0.8759\n","Epoch 1/20\n","104/104 [==============================] - 8s 35ms/step - loss: 0.6362 - accuracy: 0.6404 - f1_score: 0.6173 - val_loss: 0.5332 - val_accuracy: 0.7604 - val_f1_score: 0.7636\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5075 - accuracy: 0.7592 - f1_score: 0.7469 - val_loss: 0.4427 - val_accuracy: 0.7893 - val_f1_score: 0.7534\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4446 - accuracy: 0.7990 - f1_score: 0.7890 - val_loss: 0.3964 - val_accuracy: 0.8273 - val_f1_score: 0.8053\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3854 - accuracy: 0.8351 - f1_score: 0.8322 - val_loss: 0.3477 - val_accuracy: 0.8499 - val_f1_score: 0.8483\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3502 - accuracy: 0.8529 - f1_score: 0.8520 - val_loss: 0.3302 - val_accuracy: 0.8617 - val_f1_score: 0.8558\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3304 - accuracy: 0.8674 - f1_score: 0.8657 - val_loss: 0.3186 - val_accuracy: 0.8635 - val_f1_score: 0.8601\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3150 - accuracy: 0.8731 - f1_score: 0.8715 - val_loss: 0.3232 - val_accuracy: 0.8653 - val_f1_score: 0.8555\n","Epoch 8/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3073 - accuracy: 0.8791 - f1_score: 0.8773 - val_loss: 0.3223 - val_accuracy: 0.8653 - val_f1_score: 0.8555\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2889 - accuracy: 0.8822 - f1_score: 0.8810 - val_loss: 0.3147 - val_accuracy: 0.8671 - val_f1_score: 0.8604\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2797 - accuracy: 0.8861 - f1_score: 0.8854 - val_loss: 0.3092 - val_accuracy: 0.8761 - val_f1_score: 0.8686\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2764 - accuracy: 0.8891 - f1_score: 0.8881 - val_loss: 0.3001 - val_accuracy: 0.8807 - val_f1_score: 0.8811\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2782 - accuracy: 0.8888 - f1_score: 0.8875 - val_loss: 0.2955 - val_accuracy: 0.8807 - val_f1_score: 0.8764\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2460 - accuracy: 0.9042 - f1_score: 0.9035 - val_loss: 0.3031 - val_accuracy: 0.8816 - val_f1_score: 0.8775\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2344 - accuracy: 0.9135 - f1_score: 0.9131 - val_loss: 0.3957 - val_accuracy: 0.8300 - val_f1_score: 0.8074\n","Epoch 15/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2341 - accuracy: 0.9105 - f1_score: 0.9098 - val_loss: 0.3434 - val_accuracy: 0.8761 - val_f1_score: 0.8740\n","Epoch 16/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2331 - accuracy: 0.9114 - f1_score: 0.9107 - val_loss: 0.3332 - val_accuracy: 0.8635 - val_f1_score: 0.8579\n","Epoch 17/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2415 - accuracy: 0.9039 - f1_score: 0.9034 - val_loss: 0.3423 - val_accuracy: 0.8617 - val_f1_score: 0.8507\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3455 - accuracy: 0.8629 - f1_score: 0.8872\n","Epoch 1/20\n","104/104 [==============================] - 6s 24ms/step - loss: 0.6159 - accuracy: 0.6615 - f1_score: 0.6456 - val_loss: 0.5323 - val_accuracy: 0.7360 - val_f1_score: 0.7132\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4856 - accuracy: 0.7728 - f1_score: 0.7641 - val_loss: 0.4505 - val_accuracy: 0.7884 - val_f1_score: 0.7697\n","Epoch 3/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3962 - accuracy: 0.8228 - f1_score: 0.8173 - val_loss: 0.4151 - val_accuracy: 0.8137 - val_f1_score: 0.8212\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3625 - accuracy: 0.8472 - f1_score: 0.8457 - val_loss: 0.4126 - val_accuracy: 0.8264 - val_f1_score: 0.8091\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3432 - accuracy: 0.8541 - f1_score: 0.8532 - val_loss: 0.4252 - val_accuracy: 0.8101 - val_f1_score: 0.8295\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3361 - accuracy: 0.8641 - f1_score: 0.8630 - val_loss: 0.3562 - val_accuracy: 0.8535 - val_f1_score: 0.8508\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3142 - accuracy: 0.8650 - f1_score: 0.8637 - val_loss: 0.3434 - val_accuracy: 0.8599 - val_f1_score: 0.8582\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2912 - accuracy: 0.8840 - f1_score: 0.8827 - val_loss: 0.3463 - val_accuracy: 0.8571 - val_f1_score: 0.8512\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2889 - accuracy: 0.8840 - f1_score: 0.8830 - val_loss: 0.3366 - val_accuracy: 0.8590 - val_f1_score: 0.8564\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2830 - accuracy: 0.8828 - f1_score: 0.8819 - val_loss: 0.3358 - val_accuracy: 0.8608 - val_f1_score: 0.8525\n","Epoch 11/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2761 - accuracy: 0.8942 - f1_score: 0.8935 - val_loss: 0.3431 - val_accuracy: 0.8644 - val_f1_score: 0.8552\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2634 - accuracy: 0.8996 - f1_score: 0.8995 - val_loss: 0.3489 - val_accuracy: 0.8562 - val_f1_score: 0.8481\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2602 - accuracy: 0.8990 - f1_score: 0.8980 - val_loss: 0.3349 - val_accuracy: 0.8662 - val_f1_score: 0.8645\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2583 - accuracy: 0.9033 - f1_score: 0.9016 - val_loss: 0.3346 - val_accuracy: 0.8689 - val_f1_score: 0.8707\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2433 - accuracy: 0.9081 - f1_score: 0.9067 - val_loss: 0.3478 - val_accuracy: 0.8671 - val_f1_score: 0.8635\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2387 - accuracy: 0.9075 - f1_score: 0.9063 - val_loss: 0.3550 - val_accuracy: 0.8635 - val_f1_score: 0.8590\n","Epoch 17/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2275 - accuracy: 0.9144 - f1_score: 0.9134 - val_loss: 0.3510 - val_accuracy: 0.8571 - val_f1_score: 0.8501\n","Epoch 18/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2304 - accuracy: 0.9126 - f1_score: 0.9116 - val_loss: 0.4238 - val_accuracy: 0.8400 - val_f1_score: 0.8221\n","Epoch 19/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2170 - accuracy: 0.9180 - f1_score: 0.9172 - val_loss: 0.4277 - val_accuracy: 0.8409 - val_f1_score: 0.8233\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3196 - accuracy: 0.8542 - f1_score: 0.8834\n","Epoch 1/20\n","104/104 [==============================] - 8s 27ms/step - loss: 0.6371 - accuracy: 0.6444 - f1_score: 0.6181 - val_loss: 0.5175 - val_accuracy: 0.7631 - val_f1_score: 0.7757\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5081 - accuracy: 0.7628 - f1_score: 0.7531 - val_loss: 0.4628 - val_accuracy: 0.7767 - val_f1_score: 0.7271\n","Epoch 3/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4226 - accuracy: 0.8020 - f1_score: 0.7969 - val_loss: 0.3172 - val_accuracy: 0.8662 - val_f1_score: 0.8655\n","Epoch 4/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3766 - accuracy: 0.8330 - f1_score: 0.8291 - val_loss: 0.3606 - val_accuracy: 0.8445 - val_f1_score: 0.8599\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3438 - accuracy: 0.8532 - f1_score: 0.8518 - val_loss: 0.2979 - val_accuracy: 0.8725 - val_f1_score: 0.8786\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3285 - accuracy: 0.8683 - f1_score: 0.8675 - val_loss: 0.2787 - val_accuracy: 0.8816 - val_f1_score: 0.8825\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3284 - accuracy: 0.8553 - f1_score: 0.8542 - val_loss: 0.2769 - val_accuracy: 0.8888 - val_f1_score: 0.8887\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3114 - accuracy: 0.8716 - f1_score: 0.8704 - val_loss: 0.2732 - val_accuracy: 0.8906 - val_f1_score: 0.8899\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3041 - accuracy: 0.8728 - f1_score: 0.8725 - val_loss: 0.3011 - val_accuracy: 0.8770 - val_f1_score: 0.8847\n","Epoch 10/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2850 - accuracy: 0.8831 - f1_score: 0.8819 - val_loss: 0.3049 - val_accuracy: 0.8770 - val_f1_score: 0.8847\n","Epoch 11/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2861 - accuracy: 0.8797 - f1_score: 0.8787 - val_loss: 0.2780 - val_accuracy: 0.8870 - val_f1_score: 0.8797\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2890 - accuracy: 0.8803 - f1_score: 0.8777 - val_loss: 0.2860 - val_accuracy: 0.8843 - val_f1_score: 0.8897\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2767 - accuracy: 0.8933 - f1_score: 0.8922 - val_loss: 0.2628 - val_accuracy: 0.9060 - val_f1_score: 0.9042\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2720 - accuracy: 0.8891 - f1_score: 0.8878 - val_loss: 0.3353 - val_accuracy: 0.8698 - val_f1_score: 0.8792\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2637 - accuracy: 0.8927 - f1_score: 0.8923 - val_loss: 0.2945 - val_accuracy: 0.8906 - val_f1_score: 0.8829\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2416 - accuracy: 0.9078 - f1_score: 0.9065 - val_loss: 0.2998 - val_accuracy: 0.8870 - val_f1_score: 0.8879\n","Epoch 17/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2454 - accuracy: 0.9002 - f1_score: 0.8991 - val_loss: 0.2936 - val_accuracy: 0.8906 - val_f1_score: 0.8862\n","Epoch 18/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2364 - accuracy: 0.9117 - f1_score: 0.9110 - val_loss: 0.2914 - val_accuracy: 0.8960 - val_f1_score: 0.8942\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.8555 - f1_score: 0.8798\n","Epoch 1/20\n","104/104 [==============================] - 6s 25ms/step - loss: 0.6418 - accuracy: 0.6350 - f1_score: 0.6261 - val_loss: 0.5439 - val_accuracy: 0.7206 - val_f1_score: 0.6844\n","Epoch 2/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.4639 - accuracy: 0.7896 - f1_score: 0.7840 - val_loss: 0.3854 - val_accuracy: 0.8309 - val_f1_score: 0.8260\n","Epoch 3/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3748 - accuracy: 0.8475 - f1_score: 0.8460 - val_loss: 0.3688 - val_accuracy: 0.8490 - val_f1_score: 0.8544\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3333 - accuracy: 0.8683 - f1_score: 0.8675 - val_loss: 0.3477 - val_accuracy: 0.8535 - val_f1_score: 0.8569\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3239 - accuracy: 0.8743 - f1_score: 0.8738 - val_loss: 0.3444 - val_accuracy: 0.8653 - val_f1_score: 0.8622\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3146 - accuracy: 0.8755 - f1_score: 0.8748 - val_loss: 0.3289 - val_accuracy: 0.8608 - val_f1_score: 0.8608\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2987 - accuracy: 0.8846 - f1_score: 0.8836 - val_loss: 0.3353 - val_accuracy: 0.8626 - val_f1_score: 0.8618\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2926 - accuracy: 0.8828 - f1_score: 0.8811 - val_loss: 0.3256 - val_accuracy: 0.8662 - val_f1_score: 0.8659\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2794 - accuracy: 0.8906 - f1_score: 0.8897 - val_loss: 0.3284 - val_accuracy: 0.8671 - val_f1_score: 0.8672\n","Epoch 10/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2747 - accuracy: 0.8912 - f1_score: 0.8899 - val_loss: 0.3344 - val_accuracy: 0.8707 - val_f1_score: 0.8701\n","Epoch 11/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2812 - accuracy: 0.8906 - f1_score: 0.8888 - val_loss: 0.3364 - val_accuracy: 0.8653 - val_f1_score: 0.8664\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2728 - accuracy: 0.8996 - f1_score: 0.8985 - val_loss: 0.3714 - val_accuracy: 0.8490 - val_f1_score: 0.8576\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2582 - accuracy: 0.9005 - f1_score: 0.8991 - val_loss: 0.3328 - val_accuracy: 0.8662 - val_f1_score: 0.8642\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8737 - f1_score: 0.8955\n","Epoch 1/20\n","104/104 [==============================] - 7s 35ms/step - loss: 0.6428 - accuracy: 0.6203 - f1_score: 0.6147 - val_loss: 0.5647 - val_accuracy: 0.7089 - val_f1_score: 0.7062\n","Epoch 2/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.5154 - accuracy: 0.7547 - f1_score: 0.7463 - val_loss: 0.4644 - val_accuracy: 0.7948 - val_f1_score: 0.8084\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4189 - accuracy: 0.8146 - f1_score: 0.8094 - val_loss: 0.3920 - val_accuracy: 0.8400 - val_f1_score: 0.8398\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3603 - accuracy: 0.8463 - f1_score: 0.8443 - val_loss: 0.3759 - val_accuracy: 0.8499 - val_f1_score: 0.8507\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3330 - accuracy: 0.8605 - f1_score: 0.8586 - val_loss: 0.3639 - val_accuracy: 0.8508 - val_f1_score: 0.8496\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3196 - accuracy: 0.8635 - f1_score: 0.8618 - val_loss: 0.3714 - val_accuracy: 0.8436 - val_f1_score: 0.8486\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3065 - accuracy: 0.8758 - f1_score: 0.8750 - val_loss: 0.3564 - val_accuracy: 0.8608 - val_f1_score: 0.8605\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2810 - accuracy: 0.8876 - f1_score: 0.8863 - val_loss: 0.3713 - val_accuracy: 0.8508 - val_f1_score: 0.8520\n","Epoch 9/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2830 - accuracy: 0.8894 - f1_score: 0.8884 - val_loss: 0.3790 - val_accuracy: 0.8400 - val_f1_score: 0.8506\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2708 - accuracy: 0.8948 - f1_score: 0.8943 - val_loss: 0.5241 - val_accuracy: 0.7622 - val_f1_score: 0.8006\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2735 - accuracy: 0.9002 - f1_score: 0.8992 - val_loss: 0.3702 - val_accuracy: 0.8481 - val_f1_score: 0.8547\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2710 - accuracy: 0.8867 - f1_score: 0.8860 - val_loss: 0.3728 - val_accuracy: 0.8535 - val_f1_score: 0.8591\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.8521 - f1_score: 0.8745\n","Epoch 1/20\n","104/104 [==============================] - 9s 43ms/step - loss: 0.6412 - accuracy: 0.6368 - f1_score: 0.6365 - val_loss: 0.5343 - val_accuracy: 0.7577 - val_f1_score: 0.7481\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4932 - accuracy: 0.7670 - f1_score: 0.7561 - val_loss: 0.4250 - val_accuracy: 0.7984 - val_f1_score: 0.7731\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4176 - accuracy: 0.8131 - f1_score: 0.8077 - val_loss: 0.3994 - val_accuracy: 0.8309 - val_f1_score: 0.8158\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3614 - accuracy: 0.8403 - f1_score: 0.8378 - val_loss: 0.4488 - val_accuracy: 0.7929 - val_f1_score: 0.8187\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3402 - accuracy: 0.8559 - f1_score: 0.8545 - val_loss: 0.3520 - val_accuracy: 0.8617 - val_f1_score: 0.8650\n","Epoch 6/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3273 - accuracy: 0.8635 - f1_score: 0.8627 - val_loss: 0.3324 - val_accuracy: 0.8617 - val_f1_score: 0.8536\n","Epoch 7/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3093 - accuracy: 0.8737 - f1_score: 0.8717 - val_loss: 0.3304 - val_accuracy: 0.8689 - val_f1_score: 0.8673\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3105 - accuracy: 0.8761 - f1_score: 0.8751 - val_loss: 0.3197 - val_accuracy: 0.8725 - val_f1_score: 0.8664\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2965 - accuracy: 0.8794 - f1_score: 0.8783 - val_loss: 0.3235 - val_accuracy: 0.8707 - val_f1_score: 0.8731\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2784 - accuracy: 0.8858 - f1_score: 0.8846 - val_loss: 0.3443 - val_accuracy: 0.8544 - val_f1_score: 0.8404\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2805 - accuracy: 0.8882 - f1_score: 0.8865 - val_loss: 0.3713 - val_accuracy: 0.8454 - val_f1_score: 0.8583\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2916 - accuracy: 0.8828 - f1_score: 0.8822 - val_loss: 0.3409 - val_accuracy: 0.8562 - val_f1_score: 0.8649\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2749 - accuracy: 0.8894 - f1_score: 0.8877 - val_loss: 0.3244 - val_accuracy: 0.8698 - val_f1_score: 0.8703\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3763 - accuracy: 0.8346 - f1_score: 0.8543\n","Epoch 1/20\n","104/104 [==============================] - 7s 25ms/step - loss: 0.6411 - accuracy: 0.6257 - f1_score: 0.6107 - val_loss: 0.5526 - val_accuracy: 0.7098 - val_f1_score: 0.6922\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5097 - accuracy: 0.7589 - f1_score: 0.7484 - val_loss: 0.4440 - val_accuracy: 0.8029 - val_f1_score: 0.8000\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4266 - accuracy: 0.8020 - f1_score: 0.7960 - val_loss: 0.3960 - val_accuracy: 0.8327 - val_f1_score: 0.8226\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3699 - accuracy: 0.8366 - f1_score: 0.8359 - val_loss: 0.3593 - val_accuracy: 0.8580 - val_f1_score: 0.8589\n","Epoch 5/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3397 - accuracy: 0.8617 - f1_score: 0.8606 - val_loss: 0.3389 - val_accuracy: 0.8680 - val_f1_score: 0.8643\n","Epoch 6/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3221 - accuracy: 0.8629 - f1_score: 0.8617 - val_loss: 0.3378 - val_accuracy: 0.8608 - val_f1_score: 0.8640\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3136 - accuracy: 0.8689 - f1_score: 0.8685 - val_loss: 0.3901 - val_accuracy: 0.8336 - val_f1_score: 0.8103\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3076 - accuracy: 0.8722 - f1_score: 0.8719 - val_loss: 0.3270 - val_accuracy: 0.8743 - val_f1_score: 0.8707\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2837 - accuracy: 0.8861 - f1_score: 0.8861 - val_loss: 0.3189 - val_accuracy: 0.8734 - val_f1_score: 0.8754\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2827 - accuracy: 0.8825 - f1_score: 0.8822 - val_loss: 0.3281 - val_accuracy: 0.8734 - val_f1_score: 0.8654\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2814 - accuracy: 0.8888 - f1_score: 0.8883 - val_loss: 0.3212 - val_accuracy: 0.8761 - val_f1_score: 0.8714\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2743 - accuracy: 0.8876 - f1_score: 0.8867 - val_loss: 0.3474 - val_accuracy: 0.8517 - val_f1_score: 0.8389\n","Epoch 13/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2611 - accuracy: 0.8951 - f1_score: 0.8945 - val_loss: 0.3635 - val_accuracy: 0.8544 - val_f1_score: 0.8404\n","Epoch 14/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2633 - accuracy: 0.8996 - f1_score: 0.8988 - val_loss: 0.3678 - val_accuracy: 0.8490 - val_f1_score: 0.8301\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3011 - accuracy: 0.8737 - f1_score: 0.8997\n","Epoch 1/20\n","104/104 [==============================] - 6s 27ms/step - loss: 0.6456 - accuracy: 0.6344 - f1_score: 0.6014 - val_loss: 0.5323 - val_accuracy: 0.7495 - val_f1_score: 0.7364\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.5016 - accuracy: 0.7715 - f1_score: 0.7646 - val_loss: 0.4038 - val_accuracy: 0.8228 - val_f1_score: 0.8016\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4065 - accuracy: 0.8237 - f1_score: 0.8220 - val_loss: 0.3469 - val_accuracy: 0.8590 - val_f1_score: 0.8511\n","Epoch 4/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3618 - accuracy: 0.8484 - f1_score: 0.8467 - val_loss: 0.3444 - val_accuracy: 0.8508 - val_f1_score: 0.8608\n","Epoch 5/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3394 - accuracy: 0.8571 - f1_score: 0.8572 - val_loss: 0.3303 - val_accuracy: 0.8689 - val_f1_score: 0.8610\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3214 - accuracy: 0.8719 - f1_score: 0.8711 - val_loss: 0.3131 - val_accuracy: 0.8770 - val_f1_score: 0.8766\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3184 - accuracy: 0.8650 - f1_score: 0.8644 - val_loss: 0.3242 - val_accuracy: 0.8743 - val_f1_score: 0.8675\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2922 - accuracy: 0.8813 - f1_score: 0.8800 - val_loss: 0.3320 - val_accuracy: 0.8626 - val_f1_score: 0.8690\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3018 - accuracy: 0.8767 - f1_score: 0.8762 - val_loss: 0.3077 - val_accuracy: 0.8779 - val_f1_score: 0.8758\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2923 - accuracy: 0.8807 - f1_score: 0.8795 - val_loss: 0.3164 - val_accuracy: 0.8797 - val_f1_score: 0.8746\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2731 - accuracy: 0.8915 - f1_score: 0.8906 - val_loss: 0.3187 - val_accuracy: 0.8807 - val_f1_score: 0.8748\n","Epoch 12/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2658 - accuracy: 0.8933 - f1_score: 0.8921 - val_loss: 0.3135 - val_accuracy: 0.8734 - val_f1_score: 0.8727\n","Epoch 13/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2662 - accuracy: 0.8963 - f1_score: 0.8954 - val_loss: 0.3167 - val_accuracy: 0.8680 - val_f1_score: 0.8638\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2685 - accuracy: 0.8927 - f1_score: 0.8919 - val_loss: 0.3086 - val_accuracy: 0.8788 - val_f1_score: 0.8750\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8697 - f1_score: 0.8922\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XhYEODOukUIH","executionInfo":{"status":"ok","timestamp":1689845592307,"user_tz":-330,"elapsed":37,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8ce954ee-b3b8-4050-edcd-26cb0beef968"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8514516949653625, 0.8595543503761292, 0.8514516949653625, 0.8386225700378418, 0.8426738977432251, 0.8548278212547302, 0.8602295517921448, 0.8453747630119324, 0.8629304766654968, 0.8615800142288208, 0.8683322072029114, 0.8575286865234375, 0.8575286865234375, 0.8710330724716187, 0.8609048128128052, 0.8467251658439636, 0.8528021574020386, 0.8629304766654968, 0.8541526198387146, 0.8555030226707458, 0.8737339377403259, 0.852126955986023, 0.8345712423324585, 0.8737339377403259, 0.8696826696395874]\n","0.8567994594573974\n","0.010147031817740343\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwt1Tq9ckULc","executionInfo":{"status":"ok","timestamp":1689845592308,"user_tz":-330,"elapsed":13,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"fe608988-3a5d-4ccc-948f-ef5799d6b908"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3518603444099426, 0.3270189166069031, 0.3449121117591858, 0.3749750256538391, 0.36729246377944946, 0.330608993768692, 0.33895525336265564, 0.3801100254058838, 0.3409336805343628, 0.32807227969169617, 0.30909264087677, 0.32836368680000305, 0.3283960521221161, 0.3183499574661255, 0.3272249400615692, 0.3733740448951721, 0.34405550360679626, 0.3454760015010834, 0.3196156919002533, 0.33541661500930786, 0.3197874426841736, 0.3483831584453583, 0.3762950897216797, 0.30107662081718445, 0.3176347315311432]\n","0.33909125089645387\n","0.021342405862100688\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3gO74ohkUOU","executionInfo":{"status":"ok","timestamp":1689845592308,"user_tz":-330,"elapsed":8,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d2af4cc1-4c53-4b08-ef7e-33f0dc4bd49f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8737083077430725, 0.882882833480835, 0.8744291663169861, 0.8589969873428345, 0.8642981052398682, 0.880754292011261, 0.8833801746368408, 0.8687678575515747, 0.8856337666511536, 0.8857937455177307, 0.8939641118049622, 0.8881822228431702, 0.8828427791595459, 0.893355667591095, 0.8856824636459351, 0.869465172290802, 0.8758541941642761, 0.8871594667434692, 0.8833692669868469, 0.8797752261161804, 0.8954722285270691, 0.8744984865188599, 0.854253351688385, 0.8997318148612976, 0.8922389149665833]\n","0.8805796241760254\n","0.011144277745214072\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kRLuilakkURB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=3, strides=1)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vk-YZJuekUUB","executionInfo":{"status":"ok","timestamp":1689846768047,"user_tz":-330,"elapsed":1175745,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1e70a86a-8493-4e33-9ab3-c5da61d6bf6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 14s 47ms/step - loss: 0.6473 - accuracy: 0.6305 - f1_score: 0.6377 - val_loss: 0.5186 - val_accuracy: 0.7568 - val_f1_score: 0.7450\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5104 - accuracy: 0.7532 - f1_score: 0.7312 - val_loss: 0.4280 - val_accuracy: 0.7975 - val_f1_score: 0.7627\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4382 - accuracy: 0.7978 - f1_score: 0.7868 - val_loss: 0.3517 - val_accuracy: 0.8481 - val_f1_score: 0.8529\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3863 - accuracy: 0.8300 - f1_score: 0.8267 - val_loss: 0.3363 - val_accuracy: 0.8508 - val_f1_score: 0.8378\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3584 - accuracy: 0.8481 - f1_score: 0.8449 - val_loss: 0.3054 - val_accuracy: 0.8626 - val_f1_score: 0.8618\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3380 - accuracy: 0.8617 - f1_score: 0.8584 - val_loss: 0.3005 - val_accuracy: 0.8698 - val_f1_score: 0.8693\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3180 - accuracy: 0.8722 - f1_score: 0.8698 - val_loss: 0.3319 - val_accuracy: 0.8472 - val_f1_score: 0.8308\n","Epoch 8/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3185 - accuracy: 0.8722 - f1_score: 0.8701 - val_loss: 0.2979 - val_accuracy: 0.8716 - val_f1_score: 0.8670\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3104 - accuracy: 0.8725 - f1_score: 0.8702 - val_loss: 0.3139 - val_accuracy: 0.8653 - val_f1_score: 0.8546\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3099 - accuracy: 0.8758 - f1_score: 0.8732 - val_loss: 0.3104 - val_accuracy: 0.8571 - val_f1_score: 0.8472\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2962 - accuracy: 0.8822 - f1_score: 0.8794 - val_loss: 0.2911 - val_accuracy: 0.8779 - val_f1_score: 0.8772\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2854 - accuracy: 0.8870 - f1_score: 0.8844 - val_loss: 0.3012 - val_accuracy: 0.8698 - val_f1_score: 0.8657\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2938 - accuracy: 0.8819 - f1_score: 0.8785 - val_loss: 0.3000 - val_accuracy: 0.8779 - val_f1_score: 0.8772\n","Epoch 14/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2791 - accuracy: 0.8822 - f1_score: 0.8798 - val_loss: 0.2987 - val_accuracy: 0.8770 - val_f1_score: 0.8745\n","Epoch 15/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2669 - accuracy: 0.8951 - f1_score: 0.8925 - val_loss: 0.3056 - val_accuracy: 0.8761 - val_f1_score: 0.8747\n","Epoch 16/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2509 - accuracy: 0.8957 - f1_score: 0.8933 - val_loss: 0.3052 - val_accuracy: 0.8743 - val_f1_score: 0.8690\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3458 - accuracy: 0.8501 - f1_score: 0.8717\n","Epoch 1/20\n","104/104 [==============================] - 10s 29ms/step - loss: 0.6575 - accuracy: 0.5992 - f1_score: 0.5960 - val_loss: 0.5880 - val_accuracy: 0.6998 - val_f1_score: 0.6874\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5463 - accuracy: 0.7357 - f1_score: 0.7215 - val_loss: 0.5039 - val_accuracy: 0.7541 - val_f1_score: 0.7094\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4451 - accuracy: 0.7923 - f1_score: 0.7794 - val_loss: 0.4685 - val_accuracy: 0.7848 - val_f1_score: 0.8062\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4017 - accuracy: 0.8207 - f1_score: 0.8153 - val_loss: 0.4185 - val_accuracy: 0.8074 - val_f1_score: 0.8221\n","Epoch 5/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3536 - accuracy: 0.8505 - f1_score: 0.8488 - val_loss: 0.3627 - val_accuracy: 0.8499 - val_f1_score: 0.8480\n","Epoch 6/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3442 - accuracy: 0.8559 - f1_score: 0.8550 - val_loss: 0.3508 - val_accuracy: 0.8544 - val_f1_score: 0.8535\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3130 - accuracy: 0.8710 - f1_score: 0.8706 - val_loss: 0.3378 - val_accuracy: 0.8571 - val_f1_score: 0.8574\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3099 - accuracy: 0.8722 - f1_score: 0.8714 - val_loss: 0.3359 - val_accuracy: 0.8662 - val_f1_score: 0.8619\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3075 - accuracy: 0.8689 - f1_score: 0.8674 - val_loss: 0.3462 - val_accuracy: 0.8680 - val_f1_score: 0.8651\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3002 - accuracy: 0.8740 - f1_score: 0.8742 - val_loss: 0.3764 - val_accuracy: 0.8553 - val_f1_score: 0.8425\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2887 - accuracy: 0.8831 - f1_score: 0.8824 - val_loss: 0.3541 - val_accuracy: 0.8580 - val_f1_score: 0.8459\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2841 - accuracy: 0.8831 - f1_score: 0.8821 - val_loss: 0.3426 - val_accuracy: 0.8662 - val_f1_score: 0.8604\n","Epoch 13/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2725 - accuracy: 0.8885 - f1_score: 0.8882 - val_loss: 0.3317 - val_accuracy: 0.8716 - val_f1_score: 0.8685\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2589 - accuracy: 0.8927 - f1_score: 0.8913 - val_loss: 0.3433 - val_accuracy: 0.8689 - val_f1_score: 0.8649\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2650 - accuracy: 0.8915 - f1_score: 0.8908 - val_loss: 0.3691 - val_accuracy: 0.8508 - val_f1_score: 0.8365\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2491 - accuracy: 0.9014 - f1_score: 0.9001 - val_loss: 0.3659 - val_accuracy: 0.8553 - val_f1_score: 0.8579\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2440 - accuracy: 0.9033 - f1_score: 0.9019 - val_loss: 0.3662 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 18/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2284 - accuracy: 0.9129 - f1_score: 0.9118 - val_loss: 0.3965 - val_accuracy: 0.8635 - val_f1_score: 0.8579\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.8575 - f1_score: 0.8810\n","Epoch 1/20\n","104/104 [==============================] - 10s 46ms/step - loss: 0.6318 - accuracy: 0.6423 - f1_score: 0.6532 - val_loss: 0.5528 - val_accuracy: 0.7179 - val_f1_score: 0.7194\n","Epoch 2/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.4885 - accuracy: 0.7679 - f1_score: 0.7600 - val_loss: 0.4433 - val_accuracy: 0.7758 - val_f1_score: 0.7762\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4158 - accuracy: 0.8119 - f1_score: 0.8067 - val_loss: 0.4176 - val_accuracy: 0.8038 - val_f1_score: 0.7836\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3644 - accuracy: 0.8466 - f1_score: 0.8442 - val_loss: 0.3795 - val_accuracy: 0.8318 - val_f1_score: 0.8201\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3370 - accuracy: 0.8593 - f1_score: 0.8577 - val_loss: 0.3887 - val_accuracy: 0.8336 - val_f1_score: 0.8386\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3191 - accuracy: 0.8749 - f1_score: 0.8730 - val_loss: 0.3832 - val_accuracy: 0.8418 - val_f1_score: 0.8408\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3033 - accuracy: 0.8791 - f1_score: 0.8779 - val_loss: 0.3773 - val_accuracy: 0.8418 - val_f1_score: 0.8436\n","Epoch 8/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2947 - accuracy: 0.8840 - f1_score: 0.8829 - val_loss: 0.4280 - val_accuracy: 0.8137 - val_f1_score: 0.7923\n","Epoch 9/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3000 - accuracy: 0.8797 - f1_score: 0.8776 - val_loss: 0.3902 - val_accuracy: 0.8418 - val_f1_score: 0.8444\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2720 - accuracy: 0.8987 - f1_score: 0.8972 - val_loss: 0.4122 - val_accuracy: 0.8300 - val_f1_score: 0.8109\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2720 - accuracy: 0.8906 - f1_score: 0.8892 - val_loss: 0.3878 - val_accuracy: 0.8291 - val_f1_score: 0.8104\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2534 - accuracy: 0.9008 - f1_score: 0.8998 - val_loss: 0.3918 - val_accuracy: 0.8291 - val_f1_score: 0.8112\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.8582 - f1_score: 0.8833\n","Epoch 1/20\n","104/104 [==============================] - 10s 29ms/step - loss: 0.6538 - accuracy: 0.6118 - f1_score: 0.5746 - val_loss: 0.5727 - val_accuracy: 0.7116 - val_f1_score: 0.7257\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.5282 - accuracy: 0.7423 - f1_score: 0.7230 - val_loss: 0.4447 - val_accuracy: 0.7866 - val_f1_score: 0.7631\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4530 - accuracy: 0.7827 - f1_score: 0.7718 - val_loss: 0.4085 - val_accuracy: 0.8228 - val_f1_score: 0.8319\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3874 - accuracy: 0.8330 - f1_score: 0.8285 - val_loss: 0.3780 - val_accuracy: 0.8300 - val_f1_score: 0.8142\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3553 - accuracy: 0.8493 - f1_score: 0.8462 - val_loss: 0.3728 - val_accuracy: 0.8526 - val_f1_score: 0.8601\n","Epoch 6/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3410 - accuracy: 0.8590 - f1_score: 0.8554 - val_loss: 0.3629 - val_accuracy: 0.8373 - val_f1_score: 0.8204\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3268 - accuracy: 0.8707 - f1_score: 0.8686 - val_loss: 0.3329 - val_accuracy: 0.8635 - val_f1_score: 0.8595\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3093 - accuracy: 0.8713 - f1_score: 0.8691 - val_loss: 0.3449 - val_accuracy: 0.8608 - val_f1_score: 0.8550\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3322 - accuracy: 0.8574 - f1_score: 0.8546 - val_loss: 0.4275 - val_accuracy: 0.8002 - val_f1_score: 0.8250\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2932 - accuracy: 0.8894 - f1_score: 0.8871 - val_loss: 0.4114 - val_accuracy: 0.8327 - val_f1_score: 0.8482\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2875 - accuracy: 0.8891 - f1_score: 0.8875 - val_loss: 0.3554 - val_accuracy: 0.8644 - val_f1_score: 0.8585\n","Epoch 12/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2762 - accuracy: 0.8924 - f1_score: 0.8905 - val_loss: 0.3982 - val_accuracy: 0.8472 - val_f1_score: 0.8571\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3696 - accuracy: 0.8447 - f1_score: 0.8650\n","Epoch 1/20\n","104/104 [==============================] - 9s 30ms/step - loss: 0.6438 - accuracy: 0.6293 - f1_score: 0.6151 - val_loss: 0.5381 - val_accuracy: 0.7459 - val_f1_score: 0.7198\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5225 - accuracy: 0.7502 - f1_score: 0.7409 - val_loss: 0.4370 - val_accuracy: 0.7929 - val_f1_score: 0.7708\n","Epoch 3/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4190 - accuracy: 0.8107 - f1_score: 0.8050 - val_loss: 0.3755 - val_accuracy: 0.8391 - val_f1_score: 0.8458\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3833 - accuracy: 0.8324 - f1_score: 0.8309 - val_loss: 0.3450 - val_accuracy: 0.8580 - val_f1_score: 0.8558\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3525 - accuracy: 0.8550 - f1_score: 0.8530 - val_loss: 0.3711 - val_accuracy: 0.8373 - val_f1_score: 0.8477\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3339 - accuracy: 0.8638 - f1_score: 0.8624 - val_loss: 0.3433 - val_accuracy: 0.8580 - val_f1_score: 0.8622\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3166 - accuracy: 0.8662 - f1_score: 0.8640 - val_loss: 0.3325 - val_accuracy: 0.8680 - val_f1_score: 0.8638\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3124 - accuracy: 0.8713 - f1_score: 0.8685 - val_loss: 0.3237 - val_accuracy: 0.8680 - val_f1_score: 0.8661\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3014 - accuracy: 0.8737 - f1_score: 0.8716 - val_loss: 0.3620 - val_accuracy: 0.8526 - val_f1_score: 0.8394\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2908 - accuracy: 0.8800 - f1_score: 0.8774 - val_loss: 0.3564 - val_accuracy: 0.8490 - val_f1_score: 0.8576\n","Epoch 11/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2896 - accuracy: 0.8816 - f1_score: 0.8801 - val_loss: 0.3273 - val_accuracy: 0.8743 - val_f1_score: 0.8712\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2761 - accuracy: 0.8879 - f1_score: 0.8863 - val_loss: 0.3341 - val_accuracy: 0.8725 - val_f1_score: 0.8681\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2737 - accuracy: 0.8915 - f1_score: 0.8894 - val_loss: 0.3424 - val_accuracy: 0.8635 - val_f1_score: 0.8663\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3491 - accuracy: 0.8521 - f1_score: 0.8736\n","Epoch 1/20\n","104/104 [==============================] - 11s 37ms/step - loss: 0.6433 - accuracy: 0.6269 - f1_score: 0.6372 - val_loss: 0.5249 - val_accuracy: 0.7468 - val_f1_score: 0.7233\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4960 - accuracy: 0.7688 - f1_score: 0.7605 - val_loss: 0.4160 - val_accuracy: 0.8092 - val_f1_score: 0.7875\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4075 - accuracy: 0.8183 - f1_score: 0.8134 - val_loss: 0.3421 - val_accuracy: 0.8508 - val_f1_score: 0.8476\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3628 - accuracy: 0.8451 - f1_score: 0.8437 - val_loss: 0.3204 - val_accuracy: 0.8680 - val_f1_score: 0.8737\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3304 - accuracy: 0.8647 - f1_score: 0.8650 - val_loss: 0.3176 - val_accuracy: 0.8617 - val_f1_score: 0.8698\n","Epoch 6/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3382 - accuracy: 0.8599 - f1_score: 0.8595 - val_loss: 0.3109 - val_accuracy: 0.8707 - val_f1_score: 0.8764\n","Epoch 7/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3133 - accuracy: 0.8722 - f1_score: 0.8718 - val_loss: 0.3623 - val_accuracy: 0.8409 - val_f1_score: 0.8567\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3047 - accuracy: 0.8764 - f1_score: 0.8764 - val_loss: 0.2823 - val_accuracy: 0.8825 - val_f1_score: 0.8841\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2923 - accuracy: 0.8797 - f1_score: 0.8801 - val_loss: 0.2870 - val_accuracy: 0.8825 - val_f1_score: 0.8852\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2818 - accuracy: 0.8912 - f1_score: 0.8904 - val_loss: 0.3147 - val_accuracy: 0.8671 - val_f1_score: 0.8540\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2814 - accuracy: 0.8861 - f1_score: 0.8857 - val_loss: 0.2889 - val_accuracy: 0.8770 - val_f1_score: 0.8799\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2733 - accuracy: 0.8930 - f1_score: 0.8923 - val_loss: 0.2812 - val_accuracy: 0.8906 - val_f1_score: 0.8866\n","Epoch 13/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2604 - accuracy: 0.8951 - f1_score: 0.8944 - val_loss: 0.2892 - val_accuracy: 0.8770 - val_f1_score: 0.8757\n","Epoch 14/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2545 - accuracy: 0.8939 - f1_score: 0.8931 - val_loss: 0.2904 - val_accuracy: 0.8843 - val_f1_score: 0.8838\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2457 - accuracy: 0.8999 - f1_score: 0.8995 - val_loss: 0.3439 - val_accuracy: 0.8535 - val_f1_score: 0.8340\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2447 - accuracy: 0.9033 - f1_score: 0.9029 - val_loss: 0.3537 - val_accuracy: 0.8463 - val_f1_score: 0.8258\n","Epoch 17/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2409 - accuracy: 0.8987 - f1_score: 0.8978 - val_loss: 0.3366 - val_accuracy: 0.8626 - val_f1_score: 0.8483\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3601 - accuracy: 0.8521 - f1_score: 0.8756\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6426 - accuracy: 0.6148 - f1_score: 0.5885 - val_loss: 0.5646 - val_accuracy: 0.7206 - val_f1_score: 0.7037\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.5056 - accuracy: 0.7643 - f1_score: 0.7487 - val_loss: 0.4485 - val_accuracy: 0.7929 - val_f1_score: 0.7858\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4218 - accuracy: 0.8159 - f1_score: 0.8088 - val_loss: 0.3871 - val_accuracy: 0.8192 - val_f1_score: 0.8188\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3820 - accuracy: 0.8321 - f1_score: 0.8276 - val_loss: 0.3476 - val_accuracy: 0.8490 - val_f1_score: 0.8518\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3546 - accuracy: 0.8502 - f1_score: 0.8477 - val_loss: 0.3266 - val_accuracy: 0.8590 - val_f1_score: 0.8556\n","Epoch 6/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3489 - accuracy: 0.8502 - f1_score: 0.8475 - val_loss: 0.3601 - val_accuracy: 0.8436 - val_f1_score: 0.8564\n","Epoch 7/20\n","104/104 [==============================] - 3s 32ms/step - loss: 0.3099 - accuracy: 0.8791 - f1_score: 0.8777 - val_loss: 0.3048 - val_accuracy: 0.8725 - val_f1_score: 0.8731\n","Epoch 8/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2980 - accuracy: 0.8782 - f1_score: 0.8767 - val_loss: 0.3363 - val_accuracy: 0.8553 - val_f1_score: 0.8644\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2935 - accuracy: 0.8776 - f1_score: 0.8770 - val_loss: 0.3185 - val_accuracy: 0.8662 - val_f1_score: 0.8724\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2822 - accuracy: 0.8834 - f1_score: 0.8828 - val_loss: 0.3708 - val_accuracy: 0.8382 - val_f1_score: 0.8548\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2857 - accuracy: 0.8800 - f1_score: 0.8789 - val_loss: 0.4242 - val_accuracy: 0.8101 - val_f1_score: 0.8359\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2738 - accuracy: 0.8885 - f1_score: 0.8870 - val_loss: 0.3297 - val_accuracy: 0.8626 - val_f1_score: 0.8714\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3371 - accuracy: 0.8548 - f1_score: 0.8789\n","Epoch 1/20\n","104/104 [==============================] - 10s 30ms/step - loss: 0.6374 - accuracy: 0.6371 - f1_score: 0.5981 - val_loss: 0.5687 - val_accuracy: 0.6980 - val_f1_score: 0.6653\n","Epoch 2/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.5178 - accuracy: 0.7553 - f1_score: 0.7511 - val_loss: 0.4947 - val_accuracy: 0.7649 - val_f1_score: 0.7297\n","Epoch 3/20\n","104/104 [==============================] - 3s 31ms/step - loss: 0.4481 - accuracy: 0.7975 - f1_score: 0.7920 - val_loss: 0.4351 - val_accuracy: 0.8011 - val_f1_score: 0.7970\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4002 - accuracy: 0.8198 - f1_score: 0.8163 - val_loss: 0.4786 - val_accuracy: 0.7794 - val_f1_score: 0.8082\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3682 - accuracy: 0.8373 - f1_score: 0.8345 - val_loss: 0.3812 - val_accuracy: 0.8400 - val_f1_score: 0.8443\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3373 - accuracy: 0.8568 - f1_score: 0.8559 - val_loss: 0.3781 - val_accuracy: 0.8291 - val_f1_score: 0.8397\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3213 - accuracy: 0.8692 - f1_score: 0.8683 - val_loss: 0.4061 - val_accuracy: 0.8146 - val_f1_score: 0.8329\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3162 - accuracy: 0.8704 - f1_score: 0.8702 - val_loss: 0.3921 - val_accuracy: 0.8345 - val_f1_score: 0.8153\n","Epoch 9/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3001 - accuracy: 0.8746 - f1_score: 0.8740 - val_loss: 0.3306 - val_accuracy: 0.8662 - val_f1_score: 0.8642\n","Epoch 10/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2820 - accuracy: 0.8897 - f1_score: 0.8888 - val_loss: 0.3433 - val_accuracy: 0.8608 - val_f1_score: 0.8647\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2627 - accuracy: 0.8930 - f1_score: 0.8928 - val_loss: 0.3398 - val_accuracy: 0.8617 - val_f1_score: 0.8666\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2723 - accuracy: 0.8900 - f1_score: 0.8897 - val_loss: 0.4232 - val_accuracy: 0.7884 - val_f1_score: 0.8189\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2686 - accuracy: 0.8927 - f1_score: 0.8921 - val_loss: 0.3347 - val_accuracy: 0.8599 - val_f1_score: 0.8610\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2482 - accuracy: 0.8993 - f1_score: 0.8986 - val_loss: 0.3325 - val_accuracy: 0.8716 - val_f1_score: 0.8707\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3673 - accuracy: 0.8542 - f1_score: 0.8767\n","Epoch 1/20\n","104/104 [==============================] - 10s 28ms/step - loss: 0.6333 - accuracy: 0.6480 - f1_score: 0.6651 - val_loss: 0.5186 - val_accuracy: 0.7523 - val_f1_score: 0.7509\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4837 - accuracy: 0.7664 - f1_score: 0.7533 - val_loss: 0.4178 - val_accuracy: 0.8065 - val_f1_score: 0.7910\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4138 - accuracy: 0.8104 - f1_score: 0.8026 - val_loss: 0.3762 - val_accuracy: 0.8336 - val_f1_score: 0.8377\n","Epoch 4/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3855 - accuracy: 0.8300 - f1_score: 0.8260 - val_loss: 0.3862 - val_accuracy: 0.8382 - val_f1_score: 0.8510\n","Epoch 5/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3512 - accuracy: 0.8520 - f1_score: 0.8504 - val_loss: 0.3240 - val_accuracy: 0.8626 - val_f1_score: 0.8579\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3289 - accuracy: 0.8614 - f1_score: 0.8599 - val_loss: 0.3653 - val_accuracy: 0.8400 - val_f1_score: 0.8536\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3147 - accuracy: 0.8692 - f1_score: 0.8684 - val_loss: 0.3189 - val_accuracy: 0.8680 - val_f1_score: 0.8724\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3022 - accuracy: 0.8761 - f1_score: 0.8753 - val_loss: 0.3477 - val_accuracy: 0.8427 - val_f1_score: 0.8550\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2921 - accuracy: 0.8788 - f1_score: 0.8788 - val_loss: 0.3391 - val_accuracy: 0.8526 - val_f1_score: 0.8624\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2873 - accuracy: 0.8819 - f1_score: 0.8806 - val_loss: 0.3009 - val_accuracy: 0.8788 - val_f1_score: 0.8759\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2576 - accuracy: 0.8945 - f1_score: 0.8942 - val_loss: 0.3447 - val_accuracy: 0.8707 - val_f1_score: 0.8618\n","Epoch 12/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2710 - accuracy: 0.8900 - f1_score: 0.8891 - val_loss: 0.2992 - val_accuracy: 0.8825 - val_f1_score: 0.8803\n","Epoch 13/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2772 - accuracy: 0.8822 - f1_score: 0.8802 - val_loss: 0.3098 - val_accuracy: 0.8734 - val_f1_score: 0.8699\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2481 - accuracy: 0.9042 - f1_score: 0.9028 - val_loss: 0.3504 - val_accuracy: 0.8698 - val_f1_score: 0.8596\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2456 - accuracy: 0.9039 - f1_score: 0.9025 - val_loss: 0.3270 - val_accuracy: 0.8788 - val_f1_score: 0.8736\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2388 - accuracy: 0.9054 - f1_score: 0.9035 - val_loss: 0.3240 - val_accuracy: 0.8725 - val_f1_score: 0.8726\n","Epoch 17/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2390 - accuracy: 0.9030 - f1_score: 0.9016 - val_loss: 0.3479 - val_accuracy: 0.8689 - val_f1_score: 0.8727\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3399 - accuracy: 0.8582 - f1_score: 0.8827\n","Epoch 1/20\n","104/104 [==============================] - 12s 31ms/step - loss: 0.6549 - accuracy: 0.6139 - f1_score: 0.5757 - val_loss: 0.5636 - val_accuracy: 0.7016 - val_f1_score: 0.7373\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5172 - accuracy: 0.7511 - f1_score: 0.7468 - val_loss: 0.4457 - val_accuracy: 0.7957 - val_f1_score: 0.8045\n","Epoch 3/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.4166 - accuracy: 0.8140 - f1_score: 0.8103 - val_loss: 0.3728 - val_accuracy: 0.8282 - val_f1_score: 0.8201\n","Epoch 4/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3636 - accuracy: 0.8421 - f1_score: 0.8411 - val_loss: 0.3610 - val_accuracy: 0.8463 - val_f1_score: 0.8359\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3309 - accuracy: 0.8611 - f1_score: 0.8611 - val_loss: 0.3343 - val_accuracy: 0.8571 - val_f1_score: 0.8594\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3090 - accuracy: 0.8758 - f1_score: 0.8748 - val_loss: 0.3377 - val_accuracy: 0.8617 - val_f1_score: 0.8536\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3107 - accuracy: 0.8713 - f1_score: 0.8709 - val_loss: 0.3186 - val_accuracy: 0.8644 - val_f1_score: 0.8619\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2905 - accuracy: 0.8882 - f1_score: 0.8879 - val_loss: 0.3099 - val_accuracy: 0.8689 - val_f1_score: 0.8666\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2812 - accuracy: 0.8903 - f1_score: 0.8898 - val_loss: 0.3197 - val_accuracy: 0.8626 - val_f1_score: 0.8643\n","Epoch 10/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2742 - accuracy: 0.8882 - f1_score: 0.8882 - val_loss: 0.3026 - val_accuracy: 0.8698 - val_f1_score: 0.8674\n","Epoch 11/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2693 - accuracy: 0.8984 - f1_score: 0.8980 - val_loss: 0.3029 - val_accuracy: 0.8725 - val_f1_score: 0.8691\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2767 - accuracy: 0.8828 - f1_score: 0.8824 - val_loss: 0.2984 - val_accuracy: 0.8734 - val_f1_score: 0.8720\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2626 - accuracy: 0.8948 - f1_score: 0.8940 - val_loss: 0.3312 - val_accuracy: 0.8644 - val_f1_score: 0.8532\n","Epoch 14/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2479 - accuracy: 0.9017 - f1_score: 0.9015 - val_loss: 0.3241 - val_accuracy: 0.8743 - val_f1_score: 0.8654\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2420 - accuracy: 0.9072 - f1_score: 0.9069 - val_loss: 0.3238 - val_accuracy: 0.8707 - val_f1_score: 0.8634\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2327 - accuracy: 0.9105 - f1_score: 0.9100 - val_loss: 0.3159 - val_accuracy: 0.8725 - val_f1_score: 0.8738\n","Epoch 17/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2299 - accuracy: 0.9144 - f1_score: 0.9135 - val_loss: 0.3682 - val_accuracy: 0.8553 - val_f1_score: 0.8384\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3198 - accuracy: 0.8663 - f1_score: 0.8904\n","Epoch 1/20\n","104/104 [==============================] - 9s 30ms/step - loss: 0.6490 - accuracy: 0.6145 - f1_score: 0.6007 - val_loss: 0.5549 - val_accuracy: 0.7315 - val_f1_score: 0.6979\n","Epoch 2/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.5125 - accuracy: 0.7514 - f1_score: 0.7416 - val_loss: 0.4356 - val_accuracy: 0.8029 - val_f1_score: 0.8060\n","Epoch 3/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.4184 - accuracy: 0.8125 - f1_score: 0.8092 - val_loss: 0.4461 - val_accuracy: 0.7939 - val_f1_score: 0.8173\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3718 - accuracy: 0.8342 - f1_score: 0.8297 - val_loss: 0.4771 - val_accuracy: 0.7722 - val_f1_score: 0.8067\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3408 - accuracy: 0.8553 - f1_score: 0.8537 - val_loss: 0.3466 - val_accuracy: 0.8626 - val_f1_score: 0.8595\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3360 - accuracy: 0.8629 - f1_score: 0.8626 - val_loss: 0.3416 - val_accuracy: 0.8599 - val_f1_score: 0.8520\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3064 - accuracy: 0.8746 - f1_score: 0.8728 - val_loss: 0.3376 - val_accuracy: 0.8517 - val_f1_score: 0.8530\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2829 - accuracy: 0.8882 - f1_score: 0.8874 - val_loss: 0.3543 - val_accuracy: 0.8562 - val_f1_score: 0.8449\n","Epoch 9/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2858 - accuracy: 0.8810 - f1_score: 0.8796 - val_loss: 0.3393 - val_accuracy: 0.8644 - val_f1_score: 0.8663\n","Epoch 10/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2677 - accuracy: 0.8933 - f1_score: 0.8921 - val_loss: 0.3817 - val_accuracy: 0.8490 - val_f1_score: 0.8335\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2698 - accuracy: 0.8864 - f1_score: 0.8847 - val_loss: 0.3436 - val_accuracy: 0.8626 - val_f1_score: 0.8606\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2540 - accuracy: 0.8960 - f1_score: 0.8943 - val_loss: 0.3530 - val_accuracy: 0.8608 - val_f1_score: 0.8615\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3198 - accuracy: 0.8677 - f1_score: 0.8967\n","Epoch 1/20\n","104/104 [==============================] - 12s 31ms/step - loss: 0.6491 - accuracy: 0.6166 - f1_score: 0.6003 - val_loss: 0.5419 - val_accuracy: 0.7360 - val_f1_score: 0.7051\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5120 - accuracy: 0.7529 - f1_score: 0.7395 - val_loss: 0.4214 - val_accuracy: 0.7884 - val_f1_score: 0.7733\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4331 - accuracy: 0.7920 - f1_score: 0.7838 - val_loss: 0.3951 - val_accuracy: 0.8336 - val_f1_score: 0.8417\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3882 - accuracy: 0.8300 - f1_score: 0.8266 - val_loss: 0.3485 - val_accuracy: 0.8427 - val_f1_score: 0.8449\n","Epoch 5/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3424 - accuracy: 0.8565 - f1_score: 0.8543 - val_loss: 0.3486 - val_accuracy: 0.8526 - val_f1_score: 0.8554\n","Epoch 6/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3351 - accuracy: 0.8565 - f1_score: 0.8549 - val_loss: 0.3230 - val_accuracy: 0.8680 - val_f1_score: 0.8685\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3260 - accuracy: 0.8683 - f1_score: 0.8661 - val_loss: 0.3337 - val_accuracy: 0.8599 - val_f1_score: 0.8476\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3061 - accuracy: 0.8737 - f1_score: 0.8717 - val_loss: 0.3260 - val_accuracy: 0.8571 - val_f1_score: 0.8448\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2919 - accuracy: 0.8782 - f1_score: 0.8758 - val_loss: 0.3268 - val_accuracy: 0.8689 - val_f1_score: 0.8610\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2911 - accuracy: 0.8870 - f1_score: 0.8847 - val_loss: 0.3267 - val_accuracy: 0.8816 - val_f1_score: 0.8823\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2859 - accuracy: 0.8846 - f1_score: 0.8833 - val_loss: 0.3427 - val_accuracy: 0.8599 - val_f1_score: 0.8464\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3393 - accuracy: 0.8528 - f1_score: 0.8863\n","Epoch 1/20\n","104/104 [==============================] - 10s 30ms/step - loss: 0.6613 - accuracy: 0.5992 - f1_score: 0.5501 - val_loss: 0.5911 - val_accuracy: 0.6953 - val_f1_score: 0.6565\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5351 - accuracy: 0.7357 - f1_score: 0.7243 - val_loss: 0.4574 - val_accuracy: 0.7785 - val_f1_score: 0.7633\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4281 - accuracy: 0.7981 - f1_score: 0.7924 - val_loss: 0.3907 - val_accuracy: 0.8237 - val_f1_score: 0.8148\n","Epoch 4/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3664 - accuracy: 0.8481 - f1_score: 0.8455 - val_loss: 0.4079 - val_accuracy: 0.8345 - val_f1_score: 0.8466\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3183 - accuracy: 0.8629 - f1_score: 0.8614 - val_loss: 0.3704 - val_accuracy: 0.8571 - val_f1_score: 0.8587\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3160 - accuracy: 0.8707 - f1_score: 0.8678 - val_loss: 0.3794 - val_accuracy: 0.8481 - val_f1_score: 0.8564\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3023 - accuracy: 0.8785 - f1_score: 0.8769 - val_loss: 0.3599 - val_accuracy: 0.8653 - val_f1_score: 0.8652\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2955 - accuracy: 0.8797 - f1_score: 0.8777 - val_loss: 0.3514 - val_accuracy: 0.8608 - val_f1_score: 0.8595\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2848 - accuracy: 0.8788 - f1_score: 0.8766 - val_loss: 0.3587 - val_accuracy: 0.8608 - val_f1_score: 0.8579\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2787 - accuracy: 0.8807 - f1_score: 0.8796 - val_loss: 0.3591 - val_accuracy: 0.8680 - val_f1_score: 0.8722\n","Epoch 11/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2678 - accuracy: 0.8915 - f1_score: 0.8898 - val_loss: 0.3541 - val_accuracy: 0.8644 - val_f1_score: 0.8621\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2567 - accuracy: 0.8921 - f1_score: 0.8901 - val_loss: 0.3754 - val_accuracy: 0.8716 - val_f1_score: 0.8748\n","Epoch 13/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2559 - accuracy: 0.8924 - f1_score: 0.8907 - val_loss: 0.3721 - val_accuracy: 0.8626 - val_f1_score: 0.8652\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3240 - accuracy: 0.8670 - f1_score: 0.8907\n","Epoch 1/20\n","104/104 [==============================] - 12s 45ms/step - loss: 0.6421 - accuracy: 0.6257 - f1_score: 0.6128 - val_loss: 0.5414 - val_accuracy: 0.7315 - val_f1_score: 0.7232\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5229 - accuracy: 0.7495 - f1_score: 0.7344 - val_loss: 0.4618 - val_accuracy: 0.7740 - val_f1_score: 0.7417\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4402 - accuracy: 0.7966 - f1_score: 0.7863 - val_loss: 0.4082 - val_accuracy: 0.8156 - val_f1_score: 0.8268\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3752 - accuracy: 0.8354 - f1_score: 0.8308 - val_loss: 0.3461 - val_accuracy: 0.8553 - val_f1_score: 0.8532\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3547 - accuracy: 0.8433 - f1_score: 0.8392 - val_loss: 0.3398 - val_accuracy: 0.8590 - val_f1_score: 0.8629\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3357 - accuracy: 0.8599 - f1_score: 0.8582 - val_loss: 0.4015 - val_accuracy: 0.8146 - val_f1_score: 0.8367\n","Epoch 7/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3077 - accuracy: 0.8764 - f1_score: 0.8738 - val_loss: 0.3247 - val_accuracy: 0.8662 - val_f1_score: 0.8690\n","Epoch 8/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3008 - accuracy: 0.8755 - f1_score: 0.8733 - val_loss: 0.4433 - val_accuracy: 0.8047 - val_f1_score: 0.8291\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2897 - accuracy: 0.8828 - f1_score: 0.8806 - val_loss: 0.3108 - val_accuracy: 0.8761 - val_f1_score: 0.8704\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2896 - accuracy: 0.8740 - f1_score: 0.8728 - val_loss: 0.3286 - val_accuracy: 0.8626 - val_f1_score: 0.8650\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2714 - accuracy: 0.8900 - f1_score: 0.8878 - val_loss: 0.3420 - val_accuracy: 0.8599 - val_f1_score: 0.8683\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2730 - accuracy: 0.8894 - f1_score: 0.8875 - val_loss: 0.3334 - val_accuracy: 0.8653 - val_f1_score: 0.8712\n","Epoch 13/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2620 - accuracy: 0.8927 - f1_score: 0.8912 - val_loss: 0.3254 - val_accuracy: 0.8689 - val_f1_score: 0.8749\n","Epoch 14/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2466 - accuracy: 0.8954 - f1_score: 0.8938 - val_loss: 0.3945 - val_accuracy: 0.8264 - val_f1_score: 0.8418\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3607 - accuracy: 0.8474 - f1_score: 0.8688\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6348 - accuracy: 0.6435 - f1_score: 0.6332 - val_loss: 0.5538 - val_accuracy: 0.7260 - val_f1_score: 0.7166\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5026 - accuracy: 0.7640 - f1_score: 0.7499 - val_loss: 0.4301 - val_accuracy: 0.7957 - val_f1_score: 0.7784\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4337 - accuracy: 0.8005 - f1_score: 0.7926 - val_loss: 0.3923 - val_accuracy: 0.8246 - val_f1_score: 0.8274\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3807 - accuracy: 0.8388 - f1_score: 0.8365 - val_loss: 0.3638 - val_accuracy: 0.8454 - val_f1_score: 0.8385\n","Epoch 5/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.3779 - accuracy: 0.8369 - f1_score: 0.8346 - val_loss: 0.3683 - val_accuracy: 0.8345 - val_f1_score: 0.8463\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3363 - accuracy: 0.8599 - f1_score: 0.8580 - val_loss: 0.3436 - val_accuracy: 0.8490 - val_f1_score: 0.8429\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3177 - accuracy: 0.8626 - f1_score: 0.8616 - val_loss: 0.3583 - val_accuracy: 0.8373 - val_f1_score: 0.8510\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3012 - accuracy: 0.8813 - f1_score: 0.8794 - val_loss: 0.3193 - val_accuracy: 0.8644 - val_f1_score: 0.8626\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2871 - accuracy: 0.8852 - f1_score: 0.8848 - val_loss: 0.3298 - val_accuracy: 0.8626 - val_f1_score: 0.8544\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2841 - accuracy: 0.8831 - f1_score: 0.8816 - val_loss: 0.3365 - val_accuracy: 0.8427 - val_f1_score: 0.8552\n","Epoch 11/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2821 - accuracy: 0.8813 - f1_score: 0.8803 - val_loss: 0.3343 - val_accuracy: 0.8472 - val_f1_score: 0.8554\n","Epoch 12/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2773 - accuracy: 0.8927 - f1_score: 0.8926 - val_loss: 0.3403 - val_accuracy: 0.8472 - val_f1_score: 0.8576\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2569 - accuracy: 0.8978 - f1_score: 0.8972 - val_loss: 0.3593 - val_accuracy: 0.8662 - val_f1_score: 0.8558\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8609 - f1_score: 0.8853\n","Epoch 1/20\n","104/104 [==============================] - 10s 40ms/step - loss: 0.6439 - accuracy: 0.6296 - f1_score: 0.6057 - val_loss: 0.5422 - val_accuracy: 0.7405 - val_f1_score: 0.7216\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.5023 - accuracy: 0.7646 - f1_score: 0.7515 - val_loss: 0.4488 - val_accuracy: 0.8011 - val_f1_score: 0.8025\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4220 - accuracy: 0.8098 - f1_score: 0.7990 - val_loss: 0.4405 - val_accuracy: 0.8029 - val_f1_score: 0.8228\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3650 - accuracy: 0.8418 - f1_score: 0.8391 - val_loss: 0.3610 - val_accuracy: 0.8463 - val_f1_score: 0.8474\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3386 - accuracy: 0.8541 - f1_score: 0.8522 - val_loss: 0.3962 - val_accuracy: 0.8210 - val_f1_score: 0.8369\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3091 - accuracy: 0.8713 - f1_score: 0.8701 - val_loss: 0.4152 - val_accuracy: 0.8174 - val_f1_score: 0.8376\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2908 - accuracy: 0.8746 - f1_score: 0.8736 - val_loss: 0.3389 - val_accuracy: 0.8617 - val_f1_score: 0.8519\n","Epoch 8/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2922 - accuracy: 0.8794 - f1_score: 0.8779 - val_loss: 0.3560 - val_accuracy: 0.8472 - val_f1_score: 0.8574\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2662 - accuracy: 0.8918 - f1_score: 0.8908 - val_loss: 0.3300 - val_accuracy: 0.8653 - val_f1_score: 0.8622\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2732 - accuracy: 0.8897 - f1_score: 0.8887 - val_loss: 0.3332 - val_accuracy: 0.8590 - val_f1_score: 0.8634\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2516 - accuracy: 0.8978 - f1_score: 0.8962 - val_loss: 0.3876 - val_accuracy: 0.8336 - val_f1_score: 0.8469\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2643 - accuracy: 0.8888 - f1_score: 0.8874 - val_loss: 0.3311 - val_accuracy: 0.8590 - val_f1_score: 0.8612\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2856 - accuracy: 0.8876 - f1_score: 0.8864 - val_loss: 0.3233 - val_accuracy: 0.8725 - val_f1_score: 0.8686\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2355 - accuracy: 0.9060 - f1_score: 0.9048 - val_loss: 0.3503 - val_accuracy: 0.8590 - val_f1_score: 0.8653\n","Epoch 15/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2386 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3470 - val_accuracy: 0.8590 - val_f1_score: 0.8653\n","Epoch 16/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2235 - accuracy: 0.9114 - f1_score: 0.9107 - val_loss: 0.3732 - val_accuracy: 0.8617 - val_f1_score: 0.8664\n","Epoch 17/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2120 - accuracy: 0.9183 - f1_score: 0.9172 - val_loss: 0.3435 - val_accuracy: 0.8716 - val_f1_score: 0.8678\n","Epoch 18/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2099 - accuracy: 0.9216 - f1_score: 0.9207 - val_loss: 0.3478 - val_accuracy: 0.8626 - val_f1_score: 0.8652\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3737 - accuracy: 0.8420 - f1_score: 0.8647\n","Epoch 1/20\n","104/104 [==============================] - 11s 33ms/step - loss: 0.6217 - accuracy: 0.6564 - f1_score: 0.6287 - val_loss: 0.5796 - val_accuracy: 0.6935 - val_f1_score: 0.7422\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4747 - accuracy: 0.7833 - f1_score: 0.7737 - val_loss: 0.4419 - val_accuracy: 0.7902 - val_f1_score: 0.7730\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4112 - accuracy: 0.8159 - f1_score: 0.8096 - val_loss: 0.3824 - val_accuracy: 0.8201 - val_f1_score: 0.8186\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3777 - accuracy: 0.8394 - f1_score: 0.8346 - val_loss: 0.3986 - val_accuracy: 0.8273 - val_f1_score: 0.8394\n","Epoch 5/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3505 - accuracy: 0.8475 - f1_score: 0.8453 - val_loss: 0.3349 - val_accuracy: 0.8517 - val_f1_score: 0.8514\n","Epoch 6/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3279 - accuracy: 0.8638 - f1_score: 0.8608 - val_loss: 0.3316 - val_accuracy: 0.8590 - val_f1_score: 0.8542\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3262 - accuracy: 0.8647 - f1_score: 0.8633 - val_loss: 0.3500 - val_accuracy: 0.8544 - val_f1_score: 0.8404\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3116 - accuracy: 0.8695 - f1_score: 0.8671 - val_loss: 0.3302 - val_accuracy: 0.8590 - val_f1_score: 0.8471\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2982 - accuracy: 0.8725 - f1_score: 0.8708 - val_loss: 0.3388 - val_accuracy: 0.8490 - val_f1_score: 0.8574\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2891 - accuracy: 0.8797 - f1_score: 0.8785 - val_loss: 0.3311 - val_accuracy: 0.8571 - val_f1_score: 0.8454\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2854 - accuracy: 0.8897 - f1_score: 0.8875 - val_loss: 0.3171 - val_accuracy: 0.8752 - val_f1_score: 0.8768\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2706 - accuracy: 0.8879 - f1_score: 0.8865 - val_loss: 0.3321 - val_accuracy: 0.8662 - val_f1_score: 0.8582\n","Epoch 13/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2566 - accuracy: 0.8960 - f1_score: 0.8944 - val_loss: 0.3373 - val_accuracy: 0.8671 - val_f1_score: 0.8580\n","Epoch 14/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2581 - accuracy: 0.8909 - f1_score: 0.8898 - val_loss: 0.3314 - val_accuracy: 0.8653 - val_f1_score: 0.8582\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2418 - accuracy: 0.9024 - f1_score: 0.9009 - val_loss: 0.3424 - val_accuracy: 0.8571 - val_f1_score: 0.8454\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2454 - accuracy: 0.9048 - f1_score: 0.9034 - val_loss: 0.3287 - val_accuracy: 0.8716 - val_f1_score: 0.8695\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3249 - accuracy: 0.8643 - f1_score: 0.8902\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6313 - accuracy: 0.6362 - f1_score: 0.6480 - val_loss: 0.5072 - val_accuracy: 0.7785 - val_f1_score: 0.7678\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5082 - accuracy: 0.7610 - f1_score: 0.7467 - val_loss: 0.4148 - val_accuracy: 0.8165 - val_f1_score: 0.8035\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4221 - accuracy: 0.8074 - f1_score: 0.8011 - val_loss: 0.3670 - val_accuracy: 0.8445 - val_f1_score: 0.8413\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3701 - accuracy: 0.8363 - f1_score: 0.8366 - val_loss: 0.3508 - val_accuracy: 0.8427 - val_f1_score: 0.8503\n","Epoch 5/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3489 - accuracy: 0.8559 - f1_score: 0.8546 - val_loss: 0.3207 - val_accuracy: 0.8680 - val_f1_score: 0.8687\n","Epoch 6/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3180 - accuracy: 0.8683 - f1_score: 0.8673 - val_loss: 0.3019 - val_accuracy: 0.8797 - val_f1_score: 0.8783\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3077 - accuracy: 0.8797 - f1_score: 0.8785 - val_loss: 0.3202 - val_accuracy: 0.8535 - val_f1_score: 0.8618\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2980 - accuracy: 0.8794 - f1_score: 0.8790 - val_loss: 0.3508 - val_accuracy: 0.8571 - val_f1_score: 0.8442\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2830 - accuracy: 0.8867 - f1_score: 0.8857 - val_loss: 0.3332 - val_accuracy: 0.8553 - val_f1_score: 0.8419\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2812 - accuracy: 0.8909 - f1_score: 0.8900 - val_loss: 0.2956 - val_accuracy: 0.8761 - val_f1_score: 0.8704\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2773 - accuracy: 0.8900 - f1_score: 0.8888 - val_loss: 0.4078 - val_accuracy: 0.8336 - val_f1_score: 0.8079\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2591 - accuracy: 0.8996 - f1_score: 0.8988 - val_loss: 0.3297 - val_accuracy: 0.8562 - val_f1_score: 0.8487\n","Epoch 13/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2464 - accuracy: 0.9066 - f1_score: 0.9063 - val_loss: 0.3267 - val_accuracy: 0.8707 - val_f1_score: 0.8773\n","Epoch 14/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2623 - accuracy: 0.8948 - f1_score: 0.8940 - val_loss: 0.3272 - val_accuracy: 0.8653 - val_f1_score: 0.8563\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2639 - accuracy: 0.8918 - f1_score: 0.8902 - val_loss: 0.3041 - val_accuracy: 0.8689 - val_f1_score: 0.8604\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3363 - accuracy: 0.8575 - f1_score: 0.8817\n","Epoch 1/20\n","104/104 [==============================] - 11s 39ms/step - loss: 0.6241 - accuracy: 0.6465 - f1_score: 0.6340 - val_loss: 0.5424 - val_accuracy: 0.7405 - val_f1_score: 0.7480\n","Epoch 2/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.4843 - accuracy: 0.7821 - f1_score: 0.7730 - val_loss: 0.4531 - val_accuracy: 0.7785 - val_f1_score: 0.7538\n","Epoch 3/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4112 - accuracy: 0.8204 - f1_score: 0.8140 - val_loss: 0.3934 - val_accuracy: 0.8219 - val_f1_score: 0.8217\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3709 - accuracy: 0.8415 - f1_score: 0.8371 - val_loss: 0.3991 - val_accuracy: 0.8264 - val_f1_score: 0.8121\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3363 - accuracy: 0.8647 - f1_score: 0.8640 - val_loss: 0.3764 - val_accuracy: 0.8409 - val_f1_score: 0.8336\n","Epoch 6/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3151 - accuracy: 0.8710 - f1_score: 0.8701 - val_loss: 0.3592 - val_accuracy: 0.8553 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3035 - accuracy: 0.8752 - f1_score: 0.8745 - val_loss: 0.3694 - val_accuracy: 0.8499 - val_f1_score: 0.8369\n","Epoch 8/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2956 - accuracy: 0.8803 - f1_score: 0.8785 - val_loss: 0.3555 - val_accuracy: 0.8590 - val_f1_score: 0.8612\n","Epoch 9/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2853 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.4099 - val_accuracy: 0.8192 - val_f1_score: 0.7930\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2883 - accuracy: 0.8788 - f1_score: 0.8780 - val_loss: 0.4331 - val_accuracy: 0.8011 - val_f1_score: 0.7614\n","Epoch 11/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2806 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3507 - val_accuracy: 0.8490 - val_f1_score: 0.8377\n","Epoch 12/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2701 - accuracy: 0.8954 - f1_score: 0.8942 - val_loss: 0.4193 - val_accuracy: 0.8454 - val_f1_score: 0.8302\n","Epoch 13/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2508 - accuracy: 0.9045 - f1_score: 0.9037 - val_loss: 0.3638 - val_accuracy: 0.8508 - val_f1_score: 0.8445\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2488 - accuracy: 0.9002 - f1_score: 0.8989 - val_loss: 0.3669 - val_accuracy: 0.8653 - val_f1_score: 0.8588\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2318 - accuracy: 0.9081 - f1_score: 0.9066 - val_loss: 0.3491 - val_accuracy: 0.8689 - val_f1_score: 0.8681\n","Epoch 16/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2347 - accuracy: 0.9081 - f1_score: 0.9071 - val_loss: 0.4202 - val_accuracy: 0.8165 - val_f1_score: 0.7861\n","Epoch 17/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2375 - accuracy: 0.9063 - f1_score: 0.9050 - val_loss: 0.3755 - val_accuracy: 0.8761 - val_f1_score: 0.8776\n","Epoch 18/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2236 - accuracy: 0.9156 - f1_score: 0.9150 - val_loss: 0.3866 - val_accuracy: 0.8743 - val_f1_score: 0.8777\n","Epoch 19/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2094 - accuracy: 0.9171 - f1_score: 0.9165 - val_loss: 0.3821 - val_accuracy: 0.8698 - val_f1_score: 0.8669\n","Epoch 20/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2246 - accuracy: 0.9090 - f1_score: 0.9076 - val_loss: 0.4181 - val_accuracy: 0.8544 - val_f1_score: 0.8477\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3517 - accuracy: 0.8548 - f1_score: 0.8811\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6532 - accuracy: 0.6200 - f1_score: 0.6001 - val_loss: 0.5645 - val_accuracy: 0.6962 - val_f1_score: 0.7308\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5192 - accuracy: 0.7541 - f1_score: 0.7436 - val_loss: 0.4187 - val_accuracy: 0.8165 - val_f1_score: 0.8061\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4493 - accuracy: 0.7957 - f1_score: 0.7850 - val_loss: 0.3624 - val_accuracy: 0.8526 - val_f1_score: 0.8606\n","Epoch 4/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.4034 - accuracy: 0.8183 - f1_score: 0.8134 - val_loss: 0.3176 - val_accuracy: 0.8644 - val_f1_score: 0.8649\n","Epoch 5/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3595 - accuracy: 0.8478 - f1_score: 0.8469 - val_loss: 0.3317 - val_accuracy: 0.8562 - val_f1_score: 0.8672\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3280 - accuracy: 0.8662 - f1_score: 0.8644 - val_loss: 0.2961 - val_accuracy: 0.8779 - val_f1_score: 0.8817\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3314 - accuracy: 0.8605 - f1_score: 0.8589 - val_loss: 0.2794 - val_accuracy: 0.8915 - val_f1_score: 0.8930\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3194 - accuracy: 0.8713 - f1_score: 0.8703 - val_loss: 0.3226 - val_accuracy: 0.8617 - val_f1_score: 0.8722\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3058 - accuracy: 0.8764 - f1_score: 0.8744 - val_loss: 0.2752 - val_accuracy: 0.8933 - val_f1_score: 0.8887\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2901 - accuracy: 0.8822 - f1_score: 0.8814 - val_loss: 0.2871 - val_accuracy: 0.8861 - val_f1_score: 0.8906\n","Epoch 11/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2759 - accuracy: 0.8873 - f1_score: 0.8867 - val_loss: 0.2638 - val_accuracy: 0.9005 - val_f1_score: 0.8974\n","Epoch 12/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2753 - accuracy: 0.8882 - f1_score: 0.8866 - val_loss: 0.3731 - val_accuracy: 0.8436 - val_f1_score: 0.8590\n","Epoch 13/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2777 - accuracy: 0.8813 - f1_score: 0.8804 - val_loss: 0.3126 - val_accuracy: 0.8734 - val_f1_score: 0.8812\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2746 - accuracy: 0.8867 - f1_score: 0.8859 - val_loss: 0.3258 - val_accuracy: 0.8788 - val_f1_score: 0.8862\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2716 - accuracy: 0.8897 - f1_score: 0.8892 - val_loss: 0.2988 - val_accuracy: 0.8870 - val_f1_score: 0.8921\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2468 - accuracy: 0.9002 - f1_score: 0.8984 - val_loss: 0.2906 - val_accuracy: 0.8879 - val_f1_score: 0.8929\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3615 - accuracy: 0.8427 - f1_score: 0.8648\n","Epoch 1/20\n","104/104 [==============================] - 12s 42ms/step - loss: 0.6452 - accuracy: 0.6275 - f1_score: 0.6176 - val_loss: 0.5469 - val_accuracy: 0.7315 - val_f1_score: 0.6966\n","Epoch 2/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.4929 - accuracy: 0.7737 - f1_score: 0.7651 - val_loss: 0.4535 - val_accuracy: 0.7857 - val_f1_score: 0.7608\n","Epoch 3/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4215 - accuracy: 0.8149 - f1_score: 0.8134 - val_loss: 0.4218 - val_accuracy: 0.8110 - val_f1_score: 0.8283\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3634 - accuracy: 0.8436 - f1_score: 0.8415 - val_loss: 0.3633 - val_accuracy: 0.8472 - val_f1_score: 0.8498\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3495 - accuracy: 0.8586 - f1_score: 0.8567 - val_loss: 0.3543 - val_accuracy: 0.8544 - val_f1_score: 0.8586\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3270 - accuracy: 0.8647 - f1_score: 0.8630 - val_loss: 0.3579 - val_accuracy: 0.8508 - val_f1_score: 0.8569\n","Epoch 7/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3140 - accuracy: 0.8749 - f1_score: 0.8720 - val_loss: 0.3427 - val_accuracy: 0.8544 - val_f1_score: 0.8579\n","Epoch 8/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2952 - accuracy: 0.8828 - f1_score: 0.8808 - val_loss: 0.3699 - val_accuracy: 0.8382 - val_f1_score: 0.8201\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2855 - accuracy: 0.8843 - f1_score: 0.8820 - val_loss: 0.3620 - val_accuracy: 0.8436 - val_f1_score: 0.8289\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2904 - accuracy: 0.8852 - f1_score: 0.8829 - val_loss: 0.3274 - val_accuracy: 0.8653 - val_f1_score: 0.8652\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2828 - accuracy: 0.8897 - f1_score: 0.8883 - val_loss: 0.3381 - val_accuracy: 0.8644 - val_f1_score: 0.8580\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2762 - accuracy: 0.8966 - f1_score: 0.8951 - val_loss: 0.3360 - val_accuracy: 0.8653 - val_f1_score: 0.8639\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2575 - accuracy: 0.9024 - f1_score: 0.9010 - val_loss: 0.3438 - val_accuracy: 0.8662 - val_f1_score: 0.8625\n","Epoch 14/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2597 - accuracy: 0.8993 - f1_score: 0.8986 - val_loss: 0.3506 - val_accuracy: 0.8608 - val_f1_score: 0.8511\n","Epoch 15/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2570 - accuracy: 0.8987 - f1_score: 0.8979 - val_loss: 0.3416 - val_accuracy: 0.8599 - val_f1_score: 0.8561\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3142 - accuracy: 0.8717 - f1_score: 0.8944\n","Epoch 1/20\n","104/104 [==============================] - 11s 41ms/step - loss: 0.6374 - accuracy: 0.6392 - f1_score: 0.6391 - val_loss: 0.5304 - val_accuracy: 0.7441 - val_f1_score: 0.7271\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4980 - accuracy: 0.7649 - f1_score: 0.7552 - val_loss: 0.4428 - val_accuracy: 0.8065 - val_f1_score: 0.8168\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4015 - accuracy: 0.8246 - f1_score: 0.8181 - val_loss: 0.3796 - val_accuracy: 0.8463 - val_f1_score: 0.8423\n","Epoch 4/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3453 - accuracy: 0.8583 - f1_score: 0.8572 - val_loss: 0.3776 - val_accuracy: 0.8526 - val_f1_score: 0.8576\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3153 - accuracy: 0.8710 - f1_score: 0.8700 - val_loss: 0.3605 - val_accuracy: 0.8553 - val_f1_score: 0.8571\n","Epoch 6/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3084 - accuracy: 0.8758 - f1_score: 0.8759 - val_loss: 0.3497 - val_accuracy: 0.8590 - val_f1_score: 0.8584\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3022 - accuracy: 0.8755 - f1_score: 0.8741 - val_loss: 0.3611 - val_accuracy: 0.8499 - val_f1_score: 0.8536\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2792 - accuracy: 0.8915 - f1_score: 0.8900 - val_loss: 0.3533 - val_accuracy: 0.8653 - val_f1_score: 0.8632\n","Epoch 9/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2791 - accuracy: 0.8906 - f1_score: 0.8896 - val_loss: 0.3759 - val_accuracy: 0.8382 - val_f1_score: 0.8466\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2649 - accuracy: 0.8951 - f1_score: 0.8939 - val_loss: 0.3728 - val_accuracy: 0.8508 - val_f1_score: 0.8525\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2698 - accuracy: 0.8969 - f1_score: 0.8963 - val_loss: 0.3602 - val_accuracy: 0.8517 - val_f1_score: 0.8551\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3384 - accuracy: 0.8569 - f1_score: 0.8786\n","Epoch 1/20\n","104/104 [==============================] - 11s 31ms/step - loss: 0.6354 - accuracy: 0.6374 - f1_score: 0.6306 - val_loss: 0.5212 - val_accuracy: 0.7459 - val_f1_score: 0.7326\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4765 - accuracy: 0.7821 - f1_score: 0.7730 - val_loss: 0.4104 - val_accuracy: 0.8128 - val_f1_score: 0.8015\n","Epoch 3/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3997 - accuracy: 0.8249 - f1_score: 0.8219 - val_loss: 0.3826 - val_accuracy: 0.8445 - val_f1_score: 0.8499\n","Epoch 4/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3660 - accuracy: 0.8433 - f1_score: 0.8405 - val_loss: 0.4072 - val_accuracy: 0.8137 - val_f1_score: 0.8333\n","Epoch 5/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3327 - accuracy: 0.8656 - f1_score: 0.8641 - val_loss: 0.3511 - val_accuracy: 0.8599 - val_f1_score: 0.8620\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3236 - accuracy: 0.8701 - f1_score: 0.8686 - val_loss: 0.3678 - val_accuracy: 0.8472 - val_f1_score: 0.8583\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3141 - accuracy: 0.8668 - f1_score: 0.8650 - val_loss: 0.3135 - val_accuracy: 0.8816 - val_f1_score: 0.8770\n","Epoch 8/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3065 - accuracy: 0.8767 - f1_score: 0.8750 - val_loss: 0.3281 - val_accuracy: 0.8752 - val_f1_score: 0.8800\n","Epoch 9/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3013 - accuracy: 0.8803 - f1_score: 0.8795 - val_loss: 0.3152 - val_accuracy: 0.8752 - val_f1_score: 0.8698\n","Epoch 10/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2785 - accuracy: 0.8882 - f1_score: 0.8867 - val_loss: 0.3200 - val_accuracy: 0.8761 - val_f1_score: 0.8701\n","Epoch 11/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.2782 - accuracy: 0.8831 - f1_score: 0.8819 - val_loss: 0.3685 - val_accuracy: 0.8517 - val_f1_score: 0.8629\n","Epoch 12/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2747 - accuracy: 0.8894 - f1_score: 0.8887 - val_loss: 0.3127 - val_accuracy: 0.8698 - val_f1_score: 0.8681\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2682 - accuracy: 0.8951 - f1_score: 0.8938 - val_loss: 0.3782 - val_accuracy: 0.8409 - val_f1_score: 0.8538\n","Epoch 14/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2568 - accuracy: 0.9011 - f1_score: 0.8998 - val_loss: 0.3347 - val_accuracy: 0.8707 - val_f1_score: 0.8608\n","Epoch 15/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2508 - accuracy: 0.9027 - f1_score: 0.9013 - val_loss: 0.3221 - val_accuracy: 0.8752 - val_f1_score: 0.8710\n","Epoch 16/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2368 - accuracy: 0.9078 - f1_score: 0.9070 - val_loss: 0.3382 - val_accuracy: 0.8671 - val_f1_score: 0.8615\n","Epoch 17/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2375 - accuracy: 0.9063 - f1_score: 0.9050 - val_loss: 0.3443 - val_accuracy: 0.8653 - val_f1_score: 0.8571\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3371 - accuracy: 0.8677 - f1_score: 0.8885\n","Epoch 1/20\n","104/104 [==============================] - 10s 39ms/step - loss: 0.6384 - accuracy: 0.6347 - f1_score: 0.6303 - val_loss: 0.5642 - val_accuracy: 0.7143 - val_f1_score: 0.6631\n","Epoch 2/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.5021 - accuracy: 0.7622 - f1_score: 0.7482 - val_loss: 0.4479 - val_accuracy: 0.7975 - val_f1_score: 0.8052\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4130 - accuracy: 0.8137 - f1_score: 0.8072 - val_loss: 0.3824 - val_accuracy: 0.8373 - val_f1_score: 0.8242\n","Epoch 4/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3725 - accuracy: 0.8351 - f1_score: 0.8330 - val_loss: 0.3622 - val_accuracy: 0.8463 - val_f1_score: 0.8362\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3439 - accuracy: 0.8508 - f1_score: 0.8506 - val_loss: 0.3716 - val_accuracy: 0.8382 - val_f1_score: 0.8219\n","Epoch 6/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3333 - accuracy: 0.8614 - f1_score: 0.8597 - val_loss: 0.3356 - val_accuracy: 0.8626 - val_f1_score: 0.8563\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3166 - accuracy: 0.8659 - f1_score: 0.8645 - val_loss: 0.3262 - val_accuracy: 0.8825 - val_f1_score: 0.8785\n","Epoch 8/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2977 - accuracy: 0.8755 - f1_score: 0.8746 - val_loss: 0.3276 - val_accuracy: 0.8716 - val_f1_score: 0.8723\n","Epoch 9/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2881 - accuracy: 0.8843 - f1_score: 0.8833 - val_loss: 0.3388 - val_accuracy: 0.8689 - val_f1_score: 0.8612\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2753 - accuracy: 0.8912 - f1_score: 0.8902 - val_loss: 0.3790 - val_accuracy: 0.8517 - val_f1_score: 0.8353\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2813 - accuracy: 0.8813 - f1_score: 0.8798 - val_loss: 0.3227 - val_accuracy: 0.8807 - val_f1_score: 0.8759\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2794 - accuracy: 0.8843 - f1_score: 0.8820 - val_loss: 0.3301 - val_accuracy: 0.8770 - val_f1_score: 0.8710\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2603 - accuracy: 0.8987 - f1_score: 0.8971 - val_loss: 0.3396 - val_accuracy: 0.8644 - val_f1_score: 0.8665\n","Epoch 14/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2644 - accuracy: 0.8939 - f1_score: 0.8927 - val_loss: 0.3368 - val_accuracy: 0.8680 - val_f1_score: 0.8633\n","Epoch 15/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2423 - accuracy: 0.9087 - f1_score: 0.9080 - val_loss: 0.3713 - val_accuracy: 0.8454 - val_f1_score: 0.8288\n","Epoch 16/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2357 - accuracy: 0.9081 - f1_score: 0.9067 - val_loss: 0.3585 - val_accuracy: 0.8526 - val_f1_score: 0.8372\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3256 - accuracy: 0.8690 - f1_score: 0.8904\n","Epoch 1/20\n","104/104 [==============================] - 12s 38ms/step - loss: 0.6568 - accuracy: 0.6136 - f1_score: 0.6182 - val_loss: 0.5705 - val_accuracy: 0.7297 - val_f1_score: 0.7425\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.5228 - accuracy: 0.7495 - f1_score: 0.7386 - val_loss: 0.4090 - val_accuracy: 0.8246 - val_f1_score: 0.8307\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4455 - accuracy: 0.7969 - f1_score: 0.7916 - val_loss: 0.3845 - val_accuracy: 0.8373 - val_f1_score: 0.8207\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3835 - accuracy: 0.8373 - f1_score: 0.8339 - val_loss: 0.3434 - val_accuracy: 0.8580 - val_f1_score: 0.8526\n","Epoch 5/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3517 - accuracy: 0.8565 - f1_score: 0.8549 - val_loss: 0.3555 - val_accuracy: 0.8454 - val_f1_score: 0.8562\n","Epoch 6/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3315 - accuracy: 0.8689 - f1_score: 0.8670 - val_loss: 0.3206 - val_accuracy: 0.8716 - val_f1_score: 0.8690\n","Epoch 7/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3192 - accuracy: 0.8713 - f1_score: 0.8698 - val_loss: 0.3205 - val_accuracy: 0.8698 - val_f1_score: 0.8654\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3075 - accuracy: 0.8710 - f1_score: 0.8699 - val_loss: 0.3214 - val_accuracy: 0.8689 - val_f1_score: 0.8636\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3067 - accuracy: 0.8776 - f1_score: 0.8767 - val_loss: 0.3136 - val_accuracy: 0.8716 - val_f1_score: 0.8695\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3005 - accuracy: 0.8788 - f1_score: 0.8774 - val_loss: 0.4467 - val_accuracy: 0.7794 - val_f1_score: 0.7265\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2909 - accuracy: 0.8840 - f1_score: 0.8827 - val_loss: 0.3552 - val_accuracy: 0.8472 - val_f1_score: 0.8281\n","Epoch 12/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2743 - accuracy: 0.8939 - f1_score: 0.8929 - val_loss: 0.3642 - val_accuracy: 0.8409 - val_f1_score: 0.8197\n","Epoch 13/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2688 - accuracy: 0.8921 - f1_score: 0.8913 - val_loss: 0.3218 - val_accuracy: 0.8626 - val_f1_score: 0.8552\n","Epoch 14/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2595 - accuracy: 0.8987 - f1_score: 0.8977 - val_loss: 0.3213 - val_accuracy: 0.8707 - val_f1_score: 0.8724\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3312 - accuracy: 0.8623 - f1_score: 0.8838\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"229w8TdFkUWm","executionInfo":{"status":"ok","timestamp":1689846768047,"user_tz":-330,"elapsed":25,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"561dadd9-4c82-45f4-8b2d-2763da7faf4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8501012921333313, 0.8575286865234375, 0.8582038879394531, 0.844699501991272, 0.852126955986023, 0.852126955986023, 0.8548278212547302, 0.8541526198387146, 0.8582038879394531, 0.8663065433502197, 0.8676570057868958, 0.8528021574020386, 0.8669817447662354, 0.847400426864624, 0.8609048128128052, 0.8419986367225647, 0.8642808794975281, 0.8575286865234375, 0.8548278212547302, 0.8426738977432251, 0.871708333492279, 0.8568534851074219, 0.8676570057868958, 0.869007408618927, 0.8622552156448364]\n","0.857312626838684\n","0.008184861473551998\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzpQhK0vkbml","executionInfo":{"status":"ok","timestamp":1689846768047,"user_tz":-330,"elapsed":5,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"981fb250-f25d-434b-983e-e42e6dbe8ce5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.34583330154418945, 0.3234500586986542, 0.32877272367477417, 0.3695537745952606, 0.3491450846195221, 0.36006292700767517, 0.3371388614177704, 0.36726319789886475, 0.3398682773113251, 0.3198012709617615, 0.3198471665382385, 0.3393459618091583, 0.3240026831626892, 0.36067280173301697, 0.3305080831050873, 0.3737141788005829, 0.32494229078292847, 0.33627229928970337, 0.351713627576828, 0.3615265190601349, 0.31420135498046875, 0.338392436504364, 0.33711180090904236, 0.325648695230484, 0.3311693072319031]\n","0.3403983473777771\n","0.01681346986336728\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6S9EZ0wkbpa","executionInfo":{"status":"ok","timestamp":1689846768586,"user_tz":-330,"elapsed":542,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"00942481-c484-4358-e52f-7832ed91f680"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8716762661933899, 0.880992591381073, 0.8833333253860474, 0.8650233745574951, 0.8736294507980347, 0.8756387829780579, 0.8788731694221497, 0.8767122626304626, 0.8826814889907837, 0.8903654217720032, 0.8967333436012268, 0.8863399028778076, 0.8907375931739807, 0.8687571883201599, 0.8853005766868591, 0.8647398948669434, 0.8902238607406616, 0.8816601037979126, 0.8811498284339905, 0.8647706508636475, 0.8944444060325623, 0.8785795569419861, 0.8885095715522766, 0.89039546251297, 0.8838267922401428]\n","0.881003794670105\n","0.009024384851894274\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"KfyKmBtZkbsD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing  -\n","1. Accuracy - 0.856/0.010\n","2. Loss - 0.339/0.021\n","3. F1 score - 0.880/0.011\n","\n","LSTM CNN LSTM nothing -\n","1. Accuracy - 0.857/0.008\n","2. Loss - 0.340/0.016\n","3. F1 score - 0.881/0.009"],"metadata":{"id":"2zkzig2nzVzf"}},{"cell_type":"code","source":[],"metadata":{"id":"inllJ8qtzcrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iqhjIChfFBM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oTs3e_7vFBPn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## put a LSTM layer after applying CNN layer to emotions with maxpooling"],"metadata":{"id":"QnEMAzHaFOtJ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Concatenate, GlobalMaxPooling1D, Reshape, Dropout, MaxPooling1D, MaxPooling2D, MaxPooling3D, RepeatVector, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from imblearn.under_sampling import RandomUnderSampler"],"metadata":{"id":"6dV_93RfFSps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaling = MinMaxScaler(feature_range=(0,1))  # (0,1) is default"],"metadata":{"id":"CkWiMMhXFgPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to register f1 score separately in Keras (working)\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.metrics import Metric\n","\n","class F1Score(Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","        false_positives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * y_pred, 0, 1)))\n","        false_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * (1 - y_pred), 0, 1)))\n","\n","        self.true_positives.assign_add(true_positives)\n","        self.false_positives.assign_add(false_positives)\n","        self.false_negatives.assign_add(false_negatives)\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n","        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","        return f1"],"metadata":{"id":"npFI_q9kFgSK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"9UBrND88FgXL","executionInfo":{"status":"ok","timestamp":1690580405975,"user_tz":-330,"elapsed":18,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7b6e563f-bcf8-4484-b9a0-a68a9da2603e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  ...  anger_intensity  \\\n","0     1.789982  1.404625   7.134058  ...         0.415048   \n","1     1.780003  1.160577   6.596563  ...         0.530400   \n","2     1.776312  1.471099   6.926485  ...         0.428600   \n","3     1.780587  1.134592   6.539715  ...         0.567200   \n","4     1.775400  1.475707   6.684998  ...         0.487000   \n","...        ...       ...        ...  ...              ...   \n","7400  1.769552  1.720969   6.056923  ...         0.396000   \n","7401  1.688843  1.656772   5.678233  ...         0.000000   \n","7402  1.686590  1.781551   5.741636  ...         0.000000   \n","7403  1.702558  1.761993   5.793803  ...         0.344000   \n","7404  1.753800  1.507380   6.102673  ...         0.376750   \n","\n","      anticipation_intensity  disgust_intensity  fear_intensity  \\\n","0                   0.553423           0.272333        0.568205   \n","1                   0.519750           0.541250        0.432167   \n","2                   0.533500           0.228167        0.526192   \n","3                   0.533462           0.114667        0.501952   \n","4                   0.508000           0.482250        0.624833   \n","...                      ...                ...             ...   \n","7400                0.609000           0.484000        0.527500   \n","7401                0.000000           0.000000        0.156000   \n","7402                0.000000           0.000000        0.156000   \n","7403                0.528667           0.000000        0.414000   \n","7404                0.502500           0.422000        0.515333   \n","\n","      joy_intensity  sadness_intensity  surprise_intensity  trust_intensity  \\\n","0          0.409500           0.467625            0.434500         0.522773   \n","1          0.453429           0.315600            0.247333         0.508875   \n","2          0.413444           0.468533            0.348500         0.504500   \n","3          0.505000           0.522095            0.320500         0.593615   \n","4          0.489167           0.505333            0.000000         0.527167   \n","...             ...                ...                 ...              ...   \n","7400       0.434000           0.591000            0.793000         0.540800   \n","7401       0.000000           0.000000            0.000000         0.000000   \n","7402       0.000000           0.000000            0.000000         0.641000   \n","7403       0.515500           0.500000            0.363500         0.613000   \n","7404       0.431900           0.418800            0.316500         0.519286   \n","\n","      symptoms_ext_count  label  \n","0                      8      1  \n","1                      1      1  \n","2                      3      1  \n","3                      1      1  \n","4                      3      1  \n","...                  ...    ...  \n","7400                   1      0  \n","7401                   0      0  \n","7402                   0      0  \n","7403                   0      0  \n","7404                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-2eecd751-660e-4d0d-ba06-abed428bc238\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>...</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>...</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>...</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>...</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>...</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>...</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>...</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>...</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>...</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2eecd751-660e-4d0d-ba06-abed428bc238')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-298d5a72-8ce8-497a-9879-2ea7a46613ad\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-298d5a72-8ce8-497a-9879-2ea7a46613ad')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-298d5a72-8ce8-497a-9879-2ea7a46613ad button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2eecd751-660e-4d0d-ba06-abed428bc238 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2eecd751-660e-4d0d-ba06-abed428bc238');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[],"metadata":{"id":"xAHN0lsJFgZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"LbCpMmj-FgcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x8Bkvm0PFlq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_emotions = Input(shape=(28,))\n","reshaped_emotions = Reshape((28, 1))(input_emotions)\n","cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","cnn_emotions = Flatten()(cnn_emotions)"],"metadata":{"id":"WoDWto4zHLKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cnn_emotions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsIgNpQpQdLF","executionInfo":{"status":"ok","timestamp":1690577268871,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"fda5d483-d7ee-45f7-905c-769537d555c5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 1408])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["cnn_emotions.shape[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFTrpAHEHLNc","executionInfo":{"status":"ok","timestamp":1690577268871,"user_tz":-330,"elapsed":4,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"7225439e-084d-419a-8bdc-581bec013fc2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1408"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":[],"metadata":{"id":"F9WKAlZfHLT9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hhuoClWvHLX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=3, strides=1)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_lstm_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svpNnaRGFltl","executionInfo":{"status":"ok","timestamp":1690576096178,"user_tz":-330,"elapsed":693657,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"2df9d82d-636e-466e-a978-def8734c9cf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 23s 27ms/step - loss: 0.6354 - accuracy: 0.6365 - f1_score: 0.6105 - val_loss: 0.5068 - val_accuracy: 0.7731 - val_f1_score: 0.7914\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4972 - accuracy: 0.7568 - f1_score: 0.7504 - val_loss: 0.4030 - val_accuracy: 0.8309 - val_f1_score: 0.8387\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4173 - accuracy: 0.8116 - f1_score: 0.8073 - val_loss: 0.3555 - val_accuracy: 0.8418 - val_f1_score: 0.8319\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3928 - accuracy: 0.8267 - f1_score: 0.8231 - val_loss: 0.3438 - val_accuracy: 0.8526 - val_f1_score: 0.8584\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3501 - accuracy: 0.8565 - f1_score: 0.8535 - val_loss: 0.3337 - val_accuracy: 0.8617 - val_f1_score: 0.8693\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3203 - accuracy: 0.8719 - f1_score: 0.8703 - val_loss: 0.3439 - val_accuracy: 0.8490 - val_f1_score: 0.8351\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3211 - accuracy: 0.8722 - f1_score: 0.8706 - val_loss: 0.3066 - val_accuracy: 0.8689 - val_f1_score: 0.8666\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3048 - accuracy: 0.8761 - f1_score: 0.8740 - val_loss: 0.3143 - val_accuracy: 0.8707 - val_f1_score: 0.8733\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2884 - accuracy: 0.8861 - f1_score: 0.8844 - val_loss: 0.3113 - val_accuracy: 0.8662 - val_f1_score: 0.8585\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2845 - accuracy: 0.8873 - f1_score: 0.8851 - val_loss: 0.3175 - val_accuracy: 0.8716 - val_f1_score: 0.8761\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2596 - accuracy: 0.9036 - f1_score: 0.9028 - val_loss: 0.3008 - val_accuracy: 0.8707 - val_f1_score: 0.8657\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2603 - accuracy: 0.8960 - f1_score: 0.8945 - val_loss: 0.3391 - val_accuracy: 0.8653 - val_f1_score: 0.8560\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2511 - accuracy: 0.9093 - f1_score: 0.9080 - val_loss: 0.3312 - val_accuracy: 0.8653 - val_f1_score: 0.8671\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2289 - accuracy: 0.9078 - f1_score: 0.9070 - val_loss: 0.3476 - val_accuracy: 0.8526 - val_f1_score: 0.8446\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2272 - accuracy: 0.9132 - f1_score: 0.9121 - val_loss: 0.3538 - val_accuracy: 0.8698 - val_f1_score: 0.8698\n","Epoch 16/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2036 - accuracy: 0.9189 - f1_score: 0.9179 - val_loss: 0.4170 - val_accuracy: 0.8418 - val_f1_score: 0.8289\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3783 - accuracy: 0.8433 - f1_score: 0.8648\n","Epoch 1/20\n","104/104 [==============================] - 13s 26ms/step - loss: 0.6352 - accuracy: 0.6383 - f1_score: 0.5976 - val_loss: 0.5575 - val_accuracy: 0.7269 - val_f1_score: 0.7151\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4985 - accuracy: 0.7643 - f1_score: 0.7567 - val_loss: 0.4679 - val_accuracy: 0.7712 - val_f1_score: 0.7739\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4116 - accuracy: 0.8137 - f1_score: 0.8109 - val_loss: 0.4779 - val_accuracy: 0.7595 - val_f1_score: 0.7912\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3748 - accuracy: 0.8360 - f1_score: 0.8351 - val_loss: 0.3769 - val_accuracy: 0.8309 - val_f1_score: 0.8375\n","Epoch 5/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3486 - accuracy: 0.8472 - f1_score: 0.8466 - val_loss: 0.3536 - val_accuracy: 0.8481 - val_f1_score: 0.8500\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3264 - accuracy: 0.8638 - f1_score: 0.8633 - val_loss: 0.3575 - val_accuracy: 0.8436 - val_f1_score: 0.8486\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3134 - accuracy: 0.8716 - f1_score: 0.8707 - val_loss: 0.3582 - val_accuracy: 0.8490 - val_f1_score: 0.8386\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3057 - accuracy: 0.8785 - f1_score: 0.8782 - val_loss: 0.4197 - val_accuracy: 0.8128 - val_f1_score: 0.8307\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2950 - accuracy: 0.8807 - f1_score: 0.8806 - val_loss: 0.3458 - val_accuracy: 0.8544 - val_f1_score: 0.8538\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2773 - accuracy: 0.8879 - f1_score: 0.8881 - val_loss: 0.3517 - val_accuracy: 0.8517 - val_f1_score: 0.8559\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2641 - accuracy: 0.8969 - f1_score: 0.8964 - val_loss: 0.3620 - val_accuracy: 0.8599 - val_f1_score: 0.8528\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2656 - accuracy: 0.8879 - f1_score: 0.8877 - val_loss: 0.3596 - val_accuracy: 0.8499 - val_f1_score: 0.8505\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2461 - accuracy: 0.8987 - f1_score: 0.8988 - val_loss: 0.3654 - val_accuracy: 0.8571 - val_f1_score: 0.8548\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2383 - accuracy: 0.9063 - f1_score: 0.9056 - val_loss: 0.3740 - val_accuracy: 0.8454 - val_f1_score: 0.8501\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3406 - accuracy: 0.8609 - f1_score: 0.8841\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6149 - accuracy: 0.6634 - f1_score: 0.6669 - val_loss: 0.5413 - val_accuracy: 0.7333 - val_f1_score: 0.7428\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4852 - accuracy: 0.7700 - f1_score: 0.7628 - val_loss: 0.4457 - val_accuracy: 0.7875 - val_f1_score: 0.7933\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3927 - accuracy: 0.8270 - f1_score: 0.8236 - val_loss: 0.4573 - val_accuracy: 0.7948 - val_f1_score: 0.7613\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3636 - accuracy: 0.8424 - f1_score: 0.8391 - val_loss: 0.4126 - val_accuracy: 0.8183 - val_f1_score: 0.8307\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3394 - accuracy: 0.8553 - f1_score: 0.8532 - val_loss: 0.4074 - val_accuracy: 0.8192 - val_f1_score: 0.7967\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3110 - accuracy: 0.8761 - f1_score: 0.8741 - val_loss: 0.3977 - val_accuracy: 0.8273 - val_f1_score: 0.8118\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2919 - accuracy: 0.8816 - f1_score: 0.8807 - val_loss: 0.4513 - val_accuracy: 0.8056 - val_f1_score: 0.7739\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2837 - accuracy: 0.8846 - f1_score: 0.8829 - val_loss: 0.3848 - val_accuracy: 0.8382 - val_f1_score: 0.8310\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2753 - accuracy: 0.8927 - f1_score: 0.8917 - val_loss: 0.4245 - val_accuracy: 0.8255 - val_f1_score: 0.8087\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2500 - accuracy: 0.9024 - f1_score: 0.9009 - val_loss: 0.3962 - val_accuracy: 0.8327 - val_f1_score: 0.8384\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2414 - accuracy: 0.9096 - f1_score: 0.9089 - val_loss: 0.4185 - val_accuracy: 0.8400 - val_f1_score: 0.8273\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2417 - accuracy: 0.9072 - f1_score: 0.9064 - val_loss: 0.4430 - val_accuracy: 0.8300 - val_f1_score: 0.8393\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2339 - accuracy: 0.9087 - f1_score: 0.9075 - val_loss: 0.4580 - val_accuracy: 0.8309 - val_f1_score: 0.8400\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3617 - accuracy: 0.8569 - f1_score: 0.8775\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6238 - accuracy: 0.6528 - f1_score: 0.6468 - val_loss: 0.5314 - val_accuracy: 0.7396 - val_f1_score: 0.7522\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4963 - accuracy: 0.7610 - f1_score: 0.7520 - val_loss: 0.4103 - val_accuracy: 0.8210 - val_f1_score: 0.8251\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3996 - accuracy: 0.8222 - f1_score: 0.8185 - val_loss: 0.3713 - val_accuracy: 0.8363 - val_f1_score: 0.8241\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3659 - accuracy: 0.8415 - f1_score: 0.8379 - val_loss: 0.3908 - val_accuracy: 0.8291 - val_f1_score: 0.8426\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3354 - accuracy: 0.8583 - f1_score: 0.8571 - val_loss: 0.5492 - val_accuracy: 0.7586 - val_f1_score: 0.8006\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3304 - accuracy: 0.8668 - f1_score: 0.8649 - val_loss: 0.3535 - val_accuracy: 0.8580 - val_f1_score: 0.8604\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3169 - accuracy: 0.8752 - f1_score: 0.8736 - val_loss: 0.3420 - val_accuracy: 0.8653 - val_f1_score: 0.8606\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3032 - accuracy: 0.8779 - f1_score: 0.8755 - val_loss: 0.3408 - val_accuracy: 0.8662 - val_f1_score: 0.8652\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2694 - accuracy: 0.8912 - f1_score: 0.8894 - val_loss: 0.3596 - val_accuracy: 0.8580 - val_f1_score: 0.8622\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2678 - accuracy: 0.8963 - f1_score: 0.8949 - val_loss: 0.4519 - val_accuracy: 0.8065 - val_f1_score: 0.8293\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2561 - accuracy: 0.9014 - f1_score: 0.9007 - val_loss: 0.4028 - val_accuracy: 0.8309 - val_f1_score: 0.8128\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2404 - accuracy: 0.9081 - f1_score: 0.9068 - val_loss: 0.3738 - val_accuracy: 0.8571 - val_f1_score: 0.8604\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2406 - accuracy: 0.9042 - f1_score: 0.9030 - val_loss: 0.3868 - val_accuracy: 0.8535 - val_f1_score: 0.8508\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3559 - accuracy: 0.8508 - f1_score: 0.8722\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6347 - accuracy: 0.6429 - f1_score: 0.6319 - val_loss: 0.5606 - val_accuracy: 0.7206 - val_f1_score: 0.7550\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5065 - accuracy: 0.7489 - f1_score: 0.7449 - val_loss: 0.4396 - val_accuracy: 0.8011 - val_f1_score: 0.7796\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4354 - accuracy: 0.7975 - f1_score: 0.7936 - val_loss: 0.3947 - val_accuracy: 0.8291 - val_f1_score: 0.8372\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3807 - accuracy: 0.8306 - f1_score: 0.8292 - val_loss: 0.3667 - val_accuracy: 0.8454 - val_f1_score: 0.8512\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3507 - accuracy: 0.8481 - f1_score: 0.8475 - val_loss: 0.3484 - val_accuracy: 0.8662 - val_f1_score: 0.8614\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3126 - accuracy: 0.8722 - f1_score: 0.8724 - val_loss: 0.3532 - val_accuracy: 0.8544 - val_f1_score: 0.8538\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3071 - accuracy: 0.8719 - f1_score: 0.8698 - val_loss: 0.3860 - val_accuracy: 0.8445 - val_f1_score: 0.8540\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3019 - accuracy: 0.8776 - f1_score: 0.8764 - val_loss: 0.4028 - val_accuracy: 0.8228 - val_f1_score: 0.8401\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3043 - accuracy: 0.8746 - f1_score: 0.8733 - val_loss: 0.3588 - val_accuracy: 0.8590 - val_f1_score: 0.8646\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2708 - accuracy: 0.8885 - f1_score: 0.8879 - val_loss: 0.3499 - val_accuracy: 0.8608 - val_f1_score: 0.8558\n","47/47 [==============================] - 0s 4ms/step - loss: 0.4175 - accuracy: 0.8231 - f1_score: 0.8422\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6189 - accuracy: 0.6501 - f1_score: 0.6292 - val_loss: 0.5168 - val_accuracy: 0.7468 - val_f1_score: 0.7431\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4775 - accuracy: 0.7755 - f1_score: 0.7662 - val_loss: 0.4043 - val_accuracy: 0.8210 - val_f1_score: 0.8207\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4090 - accuracy: 0.8128 - f1_score: 0.8097 - val_loss: 0.3651 - val_accuracy: 0.8427 - val_f1_score: 0.8487\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3625 - accuracy: 0.8427 - f1_score: 0.8401 - val_loss: 0.3397 - val_accuracy: 0.8517 - val_f1_score: 0.8591\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3442 - accuracy: 0.8538 - f1_score: 0.8525 - val_loss: 0.3265 - val_accuracy: 0.8590 - val_f1_score: 0.8667\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3153 - accuracy: 0.8647 - f1_score: 0.8637 - val_loss: 0.2938 - val_accuracy: 0.8861 - val_f1_score: 0.8838\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3052 - accuracy: 0.8755 - f1_score: 0.8745 - val_loss: 0.3177 - val_accuracy: 0.8671 - val_f1_score: 0.8740\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2834 - accuracy: 0.8852 - f1_score: 0.8846 - val_loss: 0.3104 - val_accuracy: 0.8707 - val_f1_score: 0.8588\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2790 - accuracy: 0.8834 - f1_score: 0.8830 - val_loss: 0.2862 - val_accuracy: 0.8797 - val_f1_score: 0.8818\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2635 - accuracy: 0.8948 - f1_score: 0.8938 - val_loss: 0.3040 - val_accuracy: 0.8734 - val_f1_score: 0.8643\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2583 - accuracy: 0.8984 - f1_score: 0.8978 - val_loss: 0.2823 - val_accuracy: 0.8870 - val_f1_score: 0.8865\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2491 - accuracy: 0.8999 - f1_score: 0.8995 - val_loss: 0.2851 - val_accuracy: 0.8915 - val_f1_score: 0.8883\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2346 - accuracy: 0.9105 - f1_score: 0.9094 - val_loss: 0.2952 - val_accuracy: 0.8761 - val_f1_score: 0.8776\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2232 - accuracy: 0.9174 - f1_score: 0.9168 - val_loss: 0.2843 - val_accuracy: 0.8834 - val_f1_score: 0.8791\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2192 - accuracy: 0.9138 - f1_score: 0.9126 - val_loss: 0.2902 - val_accuracy: 0.8852 - val_f1_score: 0.8810\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2117 - accuracy: 0.9186 - f1_score: 0.9180 - val_loss: 0.3066 - val_accuracy: 0.8752 - val_f1_score: 0.8759\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3601 - accuracy: 0.8528 - f1_score: 0.8777\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6108 - accuracy: 0.6712 - f1_score: 0.6522 - val_loss: 0.5471 - val_accuracy: 0.7188 - val_f1_score: 0.7568\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4620 - accuracy: 0.7845 - f1_score: 0.7802 - val_loss: 0.5623 - val_accuracy: 0.7269 - val_f1_score: 0.7783\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3898 - accuracy: 0.8255 - f1_score: 0.8233 - val_loss: 0.4225 - val_accuracy: 0.8002 - val_f1_score: 0.8239\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3536 - accuracy: 0.8493 - f1_score: 0.8476 - val_loss: 0.4215 - val_accuracy: 0.8029 - val_f1_score: 0.8292\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3232 - accuracy: 0.8674 - f1_score: 0.8667 - val_loss: 0.3560 - val_accuracy: 0.8544 - val_f1_score: 0.8650\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3092 - accuracy: 0.8728 - f1_score: 0.8727 - val_loss: 0.3372 - val_accuracy: 0.8599 - val_f1_score: 0.8688\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2925 - accuracy: 0.8831 - f1_score: 0.8830 - val_loss: 0.3172 - val_accuracy: 0.8671 - val_f1_score: 0.8638\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2690 - accuracy: 0.8918 - f1_score: 0.8910 - val_loss: 0.3221 - val_accuracy: 0.8698 - val_f1_score: 0.8754\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2783 - accuracy: 0.8852 - f1_score: 0.8849 - val_loss: 0.3112 - val_accuracy: 0.8716 - val_f1_score: 0.8776\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2534 - accuracy: 0.9039 - f1_score: 0.9033 - val_loss: 0.3223 - val_accuracy: 0.8770 - val_f1_score: 0.8790\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2395 - accuracy: 0.9042 - f1_score: 0.9039 - val_loss: 0.3146 - val_accuracy: 0.8734 - val_f1_score: 0.8734\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2316 - accuracy: 0.9084 - f1_score: 0.9082 - val_loss: 0.3980 - val_accuracy: 0.8409 - val_f1_score: 0.8555\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2282 - accuracy: 0.9120 - f1_score: 0.9111 - val_loss: 0.3688 - val_accuracy: 0.8427 - val_f1_score: 0.8567\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2024 - accuracy: 0.9219 - f1_score: 0.9217 - val_loss: 0.3718 - val_accuracy: 0.8761 - val_f1_score: 0.8814\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8575 - f1_score: 0.8877\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6283 - accuracy: 0.6392 - f1_score: 0.6265 - val_loss: 0.5677 - val_accuracy: 0.7071 - val_f1_score: 0.7268\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4930 - accuracy: 0.7673 - f1_score: 0.7595 - val_loss: 0.4617 - val_accuracy: 0.7920 - val_f1_score: 0.7862\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4155 - accuracy: 0.8110 - f1_score: 0.8075 - val_loss: 0.4447 - val_accuracy: 0.7957 - val_f1_score: 0.8135\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3721 - accuracy: 0.8297 - f1_score: 0.8283 - val_loss: 0.3911 - val_accuracy: 0.8327 - val_f1_score: 0.8381\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3374 - accuracy: 0.8614 - f1_score: 0.8598 - val_loss: 0.3877 - val_accuracy: 0.8300 - val_f1_score: 0.8415\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3138 - accuracy: 0.8701 - f1_score: 0.8691 - val_loss: 0.4067 - val_accuracy: 0.8137 - val_f1_score: 0.8303\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2959 - accuracy: 0.8761 - f1_score: 0.8762 - val_loss: 0.3820 - val_accuracy: 0.8363 - val_f1_score: 0.8192\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2782 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.3893 - val_accuracy: 0.8418 - val_f1_score: 0.8531\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2679 - accuracy: 0.8927 - f1_score: 0.8921 - val_loss: 0.4001 - val_accuracy: 0.8309 - val_f1_score: 0.8456\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2558 - accuracy: 0.9042 - f1_score: 0.9038 - val_loss: 0.3727 - val_accuracy: 0.8580 - val_f1_score: 0.8643\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2439 - accuracy: 0.9057 - f1_score: 0.9052 - val_loss: 0.3707 - val_accuracy: 0.8617 - val_f1_score: 0.8671\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2290 - accuracy: 0.9060 - f1_score: 0.9058 - val_loss: 0.3491 - val_accuracy: 0.8698 - val_f1_score: 0.8728\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2259 - accuracy: 0.9096 - f1_score: 0.9093 - val_loss: 0.3541 - val_accuracy: 0.8680 - val_f1_score: 0.8708\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2141 - accuracy: 0.9150 - f1_score: 0.9144 - val_loss: 0.3783 - val_accuracy: 0.8535 - val_f1_score: 0.8608\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1934 - accuracy: 0.9295 - f1_score: 0.9296 - val_loss: 0.4183 - val_accuracy: 0.8382 - val_f1_score: 0.8497\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1885 - accuracy: 0.9241 - f1_score: 0.9238 - val_loss: 0.3953 - val_accuracy: 0.8608 - val_f1_score: 0.8663\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1765 - accuracy: 0.9325 - f1_score: 0.9325 - val_loss: 0.3958 - val_accuracy: 0.8635 - val_f1_score: 0.8598\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3569 - accuracy: 0.8515 - f1_score: 0.8795\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6182 - accuracy: 0.6537 - f1_score: 0.6507 - val_loss: 0.5206 - val_accuracy: 0.7396 - val_f1_score: 0.7343\n","Epoch 2/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.4773 - accuracy: 0.7725 - f1_score: 0.7668 - val_loss: 0.4204 - val_accuracy: 0.8137 - val_f1_score: 0.8000\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4234 - accuracy: 0.8080 - f1_score: 0.8021 - val_loss: 0.3995 - val_accuracy: 0.8137 - val_f1_score: 0.8275\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3685 - accuracy: 0.8330 - f1_score: 0.8303 - val_loss: 0.3340 - val_accuracy: 0.8562 - val_f1_score: 0.8532\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3410 - accuracy: 0.8547 - f1_score: 0.8535 - val_loss: 0.3583 - val_accuracy: 0.8354 - val_f1_score: 0.8147\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3142 - accuracy: 0.8695 - f1_score: 0.8684 - val_loss: 0.3107 - val_accuracy: 0.8807 - val_f1_score: 0.8824\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2978 - accuracy: 0.8791 - f1_score: 0.8780 - val_loss: 0.3121 - val_accuracy: 0.8779 - val_f1_score: 0.8815\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2838 - accuracy: 0.8828 - f1_score: 0.8823 - val_loss: 0.3069 - val_accuracy: 0.8716 - val_f1_score: 0.8725\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2807 - accuracy: 0.8873 - f1_score: 0.8865 - val_loss: 0.3071 - val_accuracy: 0.8915 - val_f1_score: 0.8923\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2563 - accuracy: 0.8990 - f1_score: 0.8986 - val_loss: 0.3514 - val_accuracy: 0.8544 - val_f1_score: 0.8414\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2646 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3220 - val_accuracy: 0.8590 - val_f1_score: 0.8653\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2412 - accuracy: 0.9072 - f1_score: 0.9066 - val_loss: 0.3296 - val_accuracy: 0.8644 - val_f1_score: 0.8702\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2241 - accuracy: 0.9129 - f1_score: 0.9125 - val_loss: 0.3416 - val_accuracy: 0.8590 - val_f1_score: 0.8488\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3389 - accuracy: 0.8602 - f1_score: 0.8866\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.6409 - accuracy: 0.6263 - f1_score: 0.5897 - val_loss: 0.5285 - val_accuracy: 0.7387 - val_f1_score: 0.7375\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.5079 - accuracy: 0.7498 - f1_score: 0.7421 - val_loss: 0.4377 - val_accuracy: 0.8002 - val_f1_score: 0.7873\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4188 - accuracy: 0.8165 - f1_score: 0.8127 - val_loss: 0.3996 - val_accuracy: 0.8201 - val_f1_score: 0.8088\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3855 - accuracy: 0.8303 - f1_score: 0.8272 - val_loss: 0.3953 - val_accuracy: 0.8264 - val_f1_score: 0.8378\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3333 - accuracy: 0.8650 - f1_score: 0.8637 - val_loss: 0.3675 - val_accuracy: 0.8409 - val_f1_score: 0.8376\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3259 - accuracy: 0.8677 - f1_score: 0.8668 - val_loss: 0.3400 - val_accuracy: 0.8526 - val_f1_score: 0.8500\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3050 - accuracy: 0.8770 - f1_score: 0.8764 - val_loss: 0.4495 - val_accuracy: 0.8219 - val_f1_score: 0.7928\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2848 - accuracy: 0.8876 - f1_score: 0.8867 - val_loss: 0.3215 - val_accuracy: 0.8734 - val_f1_score: 0.8723\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2654 - accuracy: 0.9014 - f1_score: 0.9006 - val_loss: 0.3358 - val_accuracy: 0.8689 - val_f1_score: 0.8638\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2617 - accuracy: 0.8924 - f1_score: 0.8918 - val_loss: 0.3379 - val_accuracy: 0.8418 - val_f1_score: 0.8503\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2476 - accuracy: 0.9054 - f1_score: 0.9054 - val_loss: 0.3179 - val_accuracy: 0.8662 - val_f1_score: 0.8630\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2481 - accuracy: 0.9057 - f1_score: 0.9053 - val_loss: 0.3142 - val_accuracy: 0.8626 - val_f1_score: 0.8631\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2206 - accuracy: 0.9198 - f1_score: 0.9198 - val_loss: 0.3404 - val_accuracy: 0.8571 - val_f1_score: 0.8481\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2141 - accuracy: 0.9207 - f1_score: 0.9203 - val_loss: 0.3429 - val_accuracy: 0.8644 - val_f1_score: 0.8574\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2137 - accuracy: 0.9183 - f1_score: 0.9179 - val_loss: 0.3280 - val_accuracy: 0.8644 - val_f1_score: 0.8656\n","Epoch 16/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1985 - accuracy: 0.9268 - f1_score: 0.9270 - val_loss: 0.3546 - val_accuracy: 0.8716 - val_f1_score: 0.8663\n","Epoch 17/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1862 - accuracy: 0.9313 - f1_score: 0.9312 - val_loss: 0.3664 - val_accuracy: 0.8635 - val_f1_score: 0.8616\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3369 - accuracy: 0.8609 - f1_score: 0.8868\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6154 - accuracy: 0.6525 - f1_score: 0.6516 - val_loss: 0.5306 - val_accuracy: 0.7423 - val_f1_score: 0.7034\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4835 - accuracy: 0.7688 - f1_score: 0.7592 - val_loss: 0.4975 - val_accuracy: 0.7568 - val_f1_score: 0.7913\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3943 - accuracy: 0.8243 - f1_score: 0.8196 - val_loss: 0.3836 - val_accuracy: 0.8373 - val_f1_score: 0.8413\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3554 - accuracy: 0.8490 - f1_score: 0.8461 - val_loss: 0.3824 - val_accuracy: 0.8345 - val_f1_score: 0.8418\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3640 - accuracy: 0.8415 - f1_score: 0.8387 - val_loss: 0.4197 - val_accuracy: 0.8038 - val_f1_score: 0.8254\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3100 - accuracy: 0.8731 - f1_score: 0.8719 - val_loss: 0.3421 - val_accuracy: 0.8499 - val_f1_score: 0.8477\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2923 - accuracy: 0.8813 - f1_score: 0.8805 - val_loss: 0.3521 - val_accuracy: 0.8499 - val_f1_score: 0.8528\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2812 - accuracy: 0.8831 - f1_score: 0.8825 - val_loss: 0.3505 - val_accuracy: 0.8662 - val_f1_score: 0.8630\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2603 - accuracy: 0.8960 - f1_score: 0.8957 - val_loss: 0.3575 - val_accuracy: 0.8608 - val_f1_score: 0.8511\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2587 - accuracy: 0.8954 - f1_score: 0.8943 - val_loss: 0.3540 - val_accuracy: 0.8590 - val_f1_score: 0.8579\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2594 - accuracy: 0.8942 - f1_score: 0.8930 - val_loss: 0.3602 - val_accuracy: 0.8463 - val_f1_score: 0.8506\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3243 - accuracy: 0.8602 - f1_score: 0.8873\n","Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6517 - accuracy: 0.6209 - f1_score: 0.6304 - val_loss: 0.5459 - val_accuracy: 0.7324 - val_f1_score: 0.6961\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5046 - accuracy: 0.7526 - f1_score: 0.7403 - val_loss: 0.4264 - val_accuracy: 0.8056 - val_f1_score: 0.8065\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4111 - accuracy: 0.8183 - f1_score: 0.8121 - val_loss: 0.3780 - val_accuracy: 0.8300 - val_f1_score: 0.8365\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3637 - accuracy: 0.8342 - f1_score: 0.8309 - val_loss: 0.3375 - val_accuracy: 0.8635 - val_f1_score: 0.8549\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3316 - accuracy: 0.8641 - f1_score: 0.8625 - val_loss: 0.3221 - val_accuracy: 0.8725 - val_f1_score: 0.8715\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3105 - accuracy: 0.8746 - f1_score: 0.8722 - val_loss: 0.3216 - val_accuracy: 0.8752 - val_f1_score: 0.8703\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3140 - accuracy: 0.8728 - f1_score: 0.8710 - val_loss: 0.3252 - val_accuracy: 0.8644 - val_f1_score: 0.8555\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3005 - accuracy: 0.8794 - f1_score: 0.8771 - val_loss: 0.3216 - val_accuracy: 0.8725 - val_f1_score: 0.8712\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2795 - accuracy: 0.8897 - f1_score: 0.8877 - val_loss: 0.3065 - val_accuracy: 0.8843 - val_f1_score: 0.8808\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2669 - accuracy: 0.8912 - f1_score: 0.8892 - val_loss: 0.3337 - val_accuracy: 0.8635 - val_f1_score: 0.8535\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2682 - accuracy: 0.8894 - f1_score: 0.8878 - val_loss: 0.3531 - val_accuracy: 0.8635 - val_f1_score: 0.8535\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2532 - accuracy: 0.8981 - f1_score: 0.8968 - val_loss: 0.3282 - val_accuracy: 0.8770 - val_f1_score: 0.8712\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2337 - accuracy: 0.9033 - f1_score: 0.9017 - val_loss: 0.3369 - val_accuracy: 0.8788 - val_f1_score: 0.8795\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2222 - accuracy: 0.9132 - f1_score: 0.9122 - val_loss: 0.3302 - val_accuracy: 0.8807 - val_f1_score: 0.8791\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3112 - accuracy: 0.8663 - f1_score: 0.8931\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6334 - accuracy: 0.6444 - f1_score: 0.6280 - val_loss: 0.5384 - val_accuracy: 0.7378 - val_f1_score: 0.7320\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5132 - accuracy: 0.7565 - f1_score: 0.7437 - val_loss: 0.5472 - val_accuracy: 0.7532 - val_f1_score: 0.7911\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4335 - accuracy: 0.7981 - f1_score: 0.7888 - val_loss: 0.4270 - val_accuracy: 0.8183 - val_f1_score: 0.8284\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3766 - accuracy: 0.8270 - f1_score: 0.8226 - val_loss: 0.3748 - val_accuracy: 0.8345 - val_f1_score: 0.8288\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3322 - accuracy: 0.8583 - f1_score: 0.8548 - val_loss: 0.4176 - val_accuracy: 0.8363 - val_f1_score: 0.8483\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3068 - accuracy: 0.8674 - f1_score: 0.8659 - val_loss: 0.3628 - val_accuracy: 0.8590 - val_f1_score: 0.8622\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3082 - accuracy: 0.8683 - f1_score: 0.8666 - val_loss: 0.3834 - val_accuracy: 0.8472 - val_f1_score: 0.8562\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2900 - accuracy: 0.8761 - f1_score: 0.8741 - val_loss: 0.3752 - val_accuracy: 0.8553 - val_f1_score: 0.8614\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3031 - accuracy: 0.8725 - f1_score: 0.8716 - val_loss: 0.4664 - val_accuracy: 0.8201 - val_f1_score: 0.8389\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2764 - accuracy: 0.8834 - f1_score: 0.8824 - val_loss: 0.3792 - val_accuracy: 0.8626 - val_f1_score: 0.8631\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2544 - accuracy: 0.8945 - f1_score: 0.8927 - val_loss: 0.3823 - val_accuracy: 0.8680 - val_f1_score: 0.8673\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3174 - accuracy: 0.8602 - f1_score: 0.8883\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6436 - accuracy: 0.6296 - f1_score: 0.6129 - val_loss: 0.6156 - val_accuracy: 0.6700 - val_f1_score: 0.5278\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5109 - accuracy: 0.7508 - f1_score: 0.7434 - val_loss: 0.4247 - val_accuracy: 0.8083 - val_f1_score: 0.8062\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4206 - accuracy: 0.8062 - f1_score: 0.8018 - val_loss: 0.3724 - val_accuracy: 0.8327 - val_f1_score: 0.8373\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3664 - accuracy: 0.8412 - f1_score: 0.8382 - val_loss: 0.3512 - val_accuracy: 0.8526 - val_f1_score: 0.8440\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3330 - accuracy: 0.8571 - f1_score: 0.8549 - val_loss: 0.3435 - val_accuracy: 0.8580 - val_f1_score: 0.8619\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3169 - accuracy: 0.8680 - f1_score: 0.8664 - val_loss: 0.4805 - val_accuracy: 0.7731 - val_f1_score: 0.8083\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3004 - accuracy: 0.8767 - f1_score: 0.8753 - val_loss: 0.3531 - val_accuracy: 0.8418 - val_f1_score: 0.8511\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2790 - accuracy: 0.8885 - f1_score: 0.8874 - val_loss: 0.3499 - val_accuracy: 0.8481 - val_f1_score: 0.8574\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2805 - accuracy: 0.8876 - f1_score: 0.8864 - val_loss: 0.3234 - val_accuracy: 0.8680 - val_f1_score: 0.8670\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2799 - accuracy: 0.8855 - f1_score: 0.8836 - val_loss: 0.4029 - val_accuracy: 0.8192 - val_f1_score: 0.8379\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2464 - accuracy: 0.9005 - f1_score: 0.8990 - val_loss: 0.3831 - val_accuracy: 0.8436 - val_f1_score: 0.8552\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2344 - accuracy: 0.9063 - f1_score: 0.9053 - val_loss: 0.4856 - val_accuracy: 0.8083 - val_f1_score: 0.8317\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2338 - accuracy: 0.9111 - f1_score: 0.9105 - val_loss: 0.3701 - val_accuracy: 0.8526 - val_f1_score: 0.8620\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2248 - accuracy: 0.9117 - f1_score: 0.9110 - val_loss: 0.3740 - val_accuracy: 0.8562 - val_f1_score: 0.8651\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3560 - accuracy: 0.8535 - f1_score: 0.8764\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6320 - accuracy: 0.6474 - f1_score: 0.6116 - val_loss: 0.5490 - val_accuracy: 0.7161 - val_f1_score: 0.7176\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5136 - accuracy: 0.7453 - f1_score: 0.7339 - val_loss: 0.5379 - val_accuracy: 0.7387 - val_f1_score: 0.7758\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4204 - accuracy: 0.8065 - f1_score: 0.8014 - val_loss: 0.4990 - val_accuracy: 0.7640 - val_f1_score: 0.7966\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3764 - accuracy: 0.8376 - f1_score: 0.8344 - val_loss: 0.3981 - val_accuracy: 0.8174 - val_f1_score: 0.8322\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3543 - accuracy: 0.8448 - f1_score: 0.8425 - val_loss: 0.3575 - val_accuracy: 0.8409 - val_f1_score: 0.8434\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3267 - accuracy: 0.8623 - f1_score: 0.8604 - val_loss: 0.3416 - val_accuracy: 0.8544 - val_f1_score: 0.8485\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3065 - accuracy: 0.8692 - f1_score: 0.8675 - val_loss: 0.3380 - val_accuracy: 0.8590 - val_f1_score: 0.8587\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2941 - accuracy: 0.8767 - f1_score: 0.8753 - val_loss: 0.3310 - val_accuracy: 0.8580 - val_f1_score: 0.8629\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2785 - accuracy: 0.8876 - f1_score: 0.8855 - val_loss: 0.3212 - val_accuracy: 0.8626 - val_f1_score: 0.8618\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2707 - accuracy: 0.8897 - f1_score: 0.8883 - val_loss: 0.3506 - val_accuracy: 0.8626 - val_f1_score: 0.8547\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2487 - accuracy: 0.8984 - f1_score: 0.8974 - val_loss: 0.3387 - val_accuracy: 0.8689 - val_f1_score: 0.8700\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2592 - accuracy: 0.8984 - f1_score: 0.8981 - val_loss: 0.3410 - val_accuracy: 0.8599 - val_f1_score: 0.8597\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2292 - accuracy: 0.9096 - f1_score: 0.9090 - val_loss: 0.3722 - val_accuracy: 0.8481 - val_f1_score: 0.8557\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2201 - accuracy: 0.9126 - f1_score: 0.9125 - val_loss: 0.3257 - val_accuracy: 0.8635 - val_f1_score: 0.8595\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3353 - accuracy: 0.8562 - f1_score: 0.8837\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6048 - accuracy: 0.6685 - f1_score: 0.6508 - val_loss: 0.5972 - val_accuracy: 0.6881 - val_f1_score: 0.7491\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4796 - accuracy: 0.7743 - f1_score: 0.7672 - val_loss: 0.4523 - val_accuracy: 0.8047 - val_f1_score: 0.8169\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4080 - accuracy: 0.8177 - f1_score: 0.8121 - val_loss: 0.3945 - val_accuracy: 0.8255 - val_f1_score: 0.8099\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3638 - accuracy: 0.8394 - f1_score: 0.8353 - val_loss: 0.3762 - val_accuracy: 0.8327 - val_f1_score: 0.8417\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3146 - accuracy: 0.8710 - f1_score: 0.8684 - val_loss: 0.3569 - val_accuracy: 0.8436 - val_f1_score: 0.8518\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3150 - accuracy: 0.8641 - f1_score: 0.8620 - val_loss: 0.3414 - val_accuracy: 0.8562 - val_f1_score: 0.8464\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2891 - accuracy: 0.8791 - f1_score: 0.8781 - val_loss: 0.3201 - val_accuracy: 0.8644 - val_f1_score: 0.8596\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2749 - accuracy: 0.8888 - f1_score: 0.8880 - val_loss: 0.3418 - val_accuracy: 0.8553 - val_f1_score: 0.8614\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2601 - accuracy: 0.8972 - f1_score: 0.8965 - val_loss: 0.3360 - val_accuracy: 0.8590 - val_f1_score: 0.8632\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2527 - accuracy: 0.8960 - f1_score: 0.8950 - val_loss: 0.3274 - val_accuracy: 0.8680 - val_f1_score: 0.8689\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2320 - accuracy: 0.9063 - f1_score: 0.9050 - val_loss: 0.3487 - val_accuracy: 0.8571 - val_f1_score: 0.8654\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2278 - accuracy: 0.9063 - f1_score: 0.9055 - val_loss: 0.3239 - val_accuracy: 0.8716 - val_f1_score: 0.8688\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3873 - accuracy: 0.8244 - f1_score: 0.8472\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6107 - accuracy: 0.6736 - f1_score: 0.6610 - val_loss: 0.5656 - val_accuracy: 0.7170 - val_f1_score: 0.6518\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4892 - accuracy: 0.7743 - f1_score: 0.7695 - val_loss: 0.4548 - val_accuracy: 0.7939 - val_f1_score: 0.7881\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4175 - accuracy: 0.8092 - f1_score: 0.8037 - val_loss: 0.4187 - val_accuracy: 0.7984 - val_f1_score: 0.7754\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3713 - accuracy: 0.8354 - f1_score: 0.8328 - val_loss: 0.3800 - val_accuracy: 0.8345 - val_f1_score: 0.8445\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3495 - accuracy: 0.8496 - f1_score: 0.8488 - val_loss: 0.3585 - val_accuracy: 0.8472 - val_f1_score: 0.8364\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3128 - accuracy: 0.8674 - f1_score: 0.8654 - val_loss: 0.3700 - val_accuracy: 0.8336 - val_f1_score: 0.8160\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2957 - accuracy: 0.8782 - f1_score: 0.8779 - val_loss: 0.3273 - val_accuracy: 0.8662 - val_f1_score: 0.8622\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2777 - accuracy: 0.8837 - f1_score: 0.8828 - val_loss: 0.3370 - val_accuracy: 0.8644 - val_f1_score: 0.8580\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2729 - accuracy: 0.8912 - f1_score: 0.8899 - val_loss: 0.3241 - val_accuracy: 0.8707 - val_f1_score: 0.8717\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2477 - accuracy: 0.9036 - f1_score: 0.9020 - val_loss: 0.3732 - val_accuracy: 0.8544 - val_f1_score: 0.8616\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2547 - accuracy: 0.8939 - f1_score: 0.8928 - val_loss: 0.3574 - val_accuracy: 0.8553 - val_f1_score: 0.8614\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2440 - accuracy: 0.9008 - f1_score: 0.8998 - val_loss: 0.3459 - val_accuracy: 0.8689 - val_f1_score: 0.8673\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2144 - accuracy: 0.9168 - f1_score: 0.9163 - val_loss: 0.3620 - val_accuracy: 0.8553 - val_f1_score: 0.8456\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2036 - accuracy: 0.9219 - f1_score: 0.9208 - val_loss: 0.3461 - val_accuracy: 0.8617 - val_f1_score: 0.8536\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.8623 - f1_score: 0.8879\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6210 - accuracy: 0.6579 - f1_score: 0.6619 - val_loss: 0.5068 - val_accuracy: 0.7740 - val_f1_score: 0.7664\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4943 - accuracy: 0.7646 - f1_score: 0.7569 - val_loss: 0.4478 - val_accuracy: 0.7812 - val_f1_score: 0.7392\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4214 - accuracy: 0.8116 - f1_score: 0.8020 - val_loss: 0.3820 - val_accuracy: 0.8418 - val_f1_score: 0.8248\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3741 - accuracy: 0.8369 - f1_score: 0.8340 - val_loss: 0.3464 - val_accuracy: 0.8418 - val_f1_score: 0.8433\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3591 - accuracy: 0.8505 - f1_score: 0.8480 - val_loss: 0.3271 - val_accuracy: 0.8553 - val_f1_score: 0.8556\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3186 - accuracy: 0.8677 - f1_score: 0.8662 - val_loss: 0.3297 - val_accuracy: 0.8590 - val_f1_score: 0.8491\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3098 - accuracy: 0.8725 - f1_score: 0.8719 - val_loss: 0.3233 - val_accuracy: 0.8517 - val_f1_score: 0.8593\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2857 - accuracy: 0.8861 - f1_score: 0.8846 - val_loss: 0.3074 - val_accuracy: 0.8743 - val_f1_score: 0.8697\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2776 - accuracy: 0.8909 - f1_score: 0.8899 - val_loss: 0.3007 - val_accuracy: 0.8743 - val_f1_score: 0.8775\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2569 - accuracy: 0.8999 - f1_score: 0.8986 - val_loss: 0.3123 - val_accuracy: 0.8770 - val_f1_score: 0.8719\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2615 - accuracy: 0.8960 - f1_score: 0.8962 - val_loss: 0.2949 - val_accuracy: 0.8816 - val_f1_score: 0.8827\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2546 - accuracy: 0.8972 - f1_score: 0.8964 - val_loss: 0.3085 - val_accuracy: 0.8752 - val_f1_score: 0.8715\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2279 - accuracy: 0.9114 - f1_score: 0.9107 - val_loss: 0.3091 - val_accuracy: 0.8752 - val_f1_score: 0.8734\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2143 - accuracy: 0.9210 - f1_score: 0.9207 - val_loss: 0.3963 - val_accuracy: 0.8336 - val_f1_score: 0.8103\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2171 - accuracy: 0.9219 - f1_score: 0.9212 - val_loss: 0.4253 - val_accuracy: 0.8354 - val_f1_score: 0.8139\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1973 - accuracy: 0.9277 - f1_score: 0.9273 - val_loss: 0.3679 - val_accuracy: 0.8590 - val_f1_score: 0.8491\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.8596 - f1_score: 0.8887\n","Epoch 1/20\n","104/104 [==============================] - 9s 21ms/step - loss: 0.5983 - accuracy: 0.6878 - f1_score: 0.6820 - val_loss: 0.5193 - val_accuracy: 0.7450 - val_f1_score: 0.7056\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4719 - accuracy: 0.7833 - f1_score: 0.7724 - val_loss: 0.4377 - val_accuracy: 0.7920 - val_f1_score: 0.7898\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3930 - accuracy: 0.8243 - f1_score: 0.8209 - val_loss: 0.4047 - val_accuracy: 0.8110 - val_f1_score: 0.8159\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3656 - accuracy: 0.8436 - f1_score: 0.8416 - val_loss: 0.3880 - val_accuracy: 0.8318 - val_f1_score: 0.8222\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3260 - accuracy: 0.8650 - f1_score: 0.8642 - val_loss: 0.3807 - val_accuracy: 0.8382 - val_f1_score: 0.8322\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3154 - accuracy: 0.8692 - f1_score: 0.8685 - val_loss: 0.3609 - val_accuracy: 0.8490 - val_f1_score: 0.8461\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2931 - accuracy: 0.8770 - f1_score: 0.8761 - val_loss: 0.3582 - val_accuracy: 0.8562 - val_f1_score: 0.8553\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2881 - accuracy: 0.8843 - f1_score: 0.8831 - val_loss: 0.3495 - val_accuracy: 0.8526 - val_f1_score: 0.8535\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2742 - accuracy: 0.8900 - f1_score: 0.8891 - val_loss: 0.3472 - val_accuracy: 0.8535 - val_f1_score: 0.8492\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2522 - accuracy: 0.9033 - f1_score: 0.9018 - val_loss: 0.3865 - val_accuracy: 0.8463 - val_f1_score: 0.8542\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2547 - accuracy: 0.9024 - f1_score: 0.9016 - val_loss: 0.3976 - val_accuracy: 0.8436 - val_f1_score: 0.8538\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2361 - accuracy: 0.9111 - f1_score: 0.9107 - val_loss: 0.3750 - val_accuracy: 0.8571 - val_f1_score: 0.8484\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2449 - accuracy: 0.9054 - f1_score: 0.9046 - val_loss: 0.4047 - val_accuracy: 0.8445 - val_f1_score: 0.8307\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2356 - accuracy: 0.9090 - f1_score: 0.9083 - val_loss: 0.4089 - val_accuracy: 0.8508 - val_f1_score: 0.8378\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3452 - accuracy: 0.8481 - f1_score: 0.8734\n","Epoch 1/20\n","104/104 [==============================] - 8s 23ms/step - loss: 0.6234 - accuracy: 0.6591 - f1_score: 0.6594 - val_loss: 0.5032 - val_accuracy: 0.7712 - val_f1_score: 0.7668\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4961 - accuracy: 0.7604 - f1_score: 0.7533 - val_loss: 0.4382 - val_accuracy: 0.7911 - val_f1_score: 0.7524\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4281 - accuracy: 0.8008 - f1_score: 0.7955 - val_loss: 0.3263 - val_accuracy: 0.8644 - val_f1_score: 0.8606\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3826 - accuracy: 0.8394 - f1_score: 0.8394 - val_loss: 0.3110 - val_accuracy: 0.8788 - val_f1_score: 0.8816\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3514 - accuracy: 0.8535 - f1_score: 0.8517 - val_loss: 0.3137 - val_accuracy: 0.8671 - val_f1_score: 0.8757\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3161 - accuracy: 0.8707 - f1_score: 0.8700 - val_loss: 0.2743 - val_accuracy: 0.8879 - val_f1_score: 0.8854\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3059 - accuracy: 0.8683 - f1_score: 0.8670 - val_loss: 0.2725 - val_accuracy: 0.8915 - val_f1_score: 0.8936\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3110 - accuracy: 0.8665 - f1_score: 0.8654 - val_loss: 0.3951 - val_accuracy: 0.8192 - val_f1_score: 0.8418\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3053 - accuracy: 0.8755 - f1_score: 0.8744 - val_loss: 0.2718 - val_accuracy: 0.8951 - val_f1_score: 0.8955\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2661 - accuracy: 0.8918 - f1_score: 0.8906 - val_loss: 0.3004 - val_accuracy: 0.8852 - val_f1_score: 0.8915\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2638 - accuracy: 0.8906 - f1_score: 0.8896 - val_loss: 0.2703 - val_accuracy: 0.8960 - val_f1_score: 0.8976\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2455 - accuracy: 0.9017 - f1_score: 0.9010 - val_loss: 0.2921 - val_accuracy: 0.8834 - val_f1_score: 0.8879\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2440 - accuracy: 0.9036 - f1_score: 0.9026 - val_loss: 0.3918 - val_accuracy: 0.8336 - val_f1_score: 0.8526\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2387 - accuracy: 0.9036 - f1_score: 0.9031 - val_loss: 0.2791 - val_accuracy: 0.8942 - val_f1_score: 0.8969\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2267 - accuracy: 0.9108 - f1_score: 0.9099 - val_loss: 0.2831 - val_accuracy: 0.9014 - val_f1_score: 0.8997\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2165 - accuracy: 0.9114 - f1_score: 0.9105 - val_loss: 0.3486 - val_accuracy: 0.8635 - val_f1_score: 0.8732\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3266 - accuracy: 0.8663 - f1_score: 0.8910\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6108 - accuracy: 0.6627 - f1_score: 0.6579 - val_loss: 0.5752 - val_accuracy: 0.6817 - val_f1_score: 0.7361\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4904 - accuracy: 0.7655 - f1_score: 0.7538 - val_loss: 0.4234 - val_accuracy: 0.8002 - val_f1_score: 0.7835\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3977 - accuracy: 0.8261 - f1_score: 0.8209 - val_loss: 0.3974 - val_accuracy: 0.8255 - val_f1_score: 0.8346\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3517 - accuracy: 0.8535 - f1_score: 0.8510 - val_loss: 0.3580 - val_accuracy: 0.8472 - val_f1_score: 0.8506\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3372 - accuracy: 0.8605 - f1_score: 0.8589 - val_loss: 0.3537 - val_accuracy: 0.8472 - val_f1_score: 0.8539\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3047 - accuracy: 0.8740 - f1_score: 0.8730 - val_loss: 0.4148 - val_accuracy: 0.8228 - val_f1_score: 0.8407\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2876 - accuracy: 0.8876 - f1_score: 0.8864 - val_loss: 0.3350 - val_accuracy: 0.8662 - val_f1_score: 0.8606\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2944 - accuracy: 0.8837 - f1_score: 0.8824 - val_loss: 0.3338 - val_accuracy: 0.8644 - val_f1_score: 0.8649\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2661 - accuracy: 0.8999 - f1_score: 0.8987 - val_loss: 0.3483 - val_accuracy: 0.8626 - val_f1_score: 0.8633\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2690 - accuracy: 0.8939 - f1_score: 0.8929 - val_loss: 0.3823 - val_accuracy: 0.8336 - val_f1_score: 0.8126\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2595 - accuracy: 0.8996 - f1_score: 0.8986 - val_loss: 0.3521 - val_accuracy: 0.8680 - val_f1_score: 0.8623\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2438 - accuracy: 0.9084 - f1_score: 0.9074 - val_loss: 0.3581 - val_accuracy: 0.8644 - val_f1_score: 0.8689\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2308 - accuracy: 0.9123 - f1_score: 0.9109 - val_loss: 0.3472 - val_accuracy: 0.8680 - val_f1_score: 0.8656\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8731 - f1_score: 0.8941\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6241 - accuracy: 0.6579 - f1_score: 0.6456 - val_loss: 0.5318 - val_accuracy: 0.7468 - val_f1_score: 0.7659\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4785 - accuracy: 0.7719 - f1_score: 0.7634 - val_loss: 0.4399 - val_accuracy: 0.8092 - val_f1_score: 0.8170\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4090 - accuracy: 0.8149 - f1_score: 0.8110 - val_loss: 0.6388 - val_accuracy: 0.6890 - val_f1_score: 0.7550\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3588 - accuracy: 0.8457 - f1_score: 0.8430 - val_loss: 0.5032 - val_accuracy: 0.7731 - val_f1_score: 0.8044\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3314 - accuracy: 0.8590 - f1_score: 0.8568 - val_loss: 0.4026 - val_accuracy: 0.8373 - val_f1_score: 0.8464\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3145 - accuracy: 0.8728 - f1_score: 0.8709 - val_loss: 0.3797 - val_accuracy: 0.8454 - val_f1_score: 0.8504\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2902 - accuracy: 0.8810 - f1_score: 0.8795 - val_loss: 0.3878 - val_accuracy: 0.8454 - val_f1_score: 0.8527\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2766 - accuracy: 0.8879 - f1_score: 0.8866 - val_loss: 0.4383 - val_accuracy: 0.8210 - val_f1_score: 0.8366\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2722 - accuracy: 0.8912 - f1_score: 0.8902 - val_loss: 0.3586 - val_accuracy: 0.8562 - val_f1_score: 0.8579\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2539 - accuracy: 0.9024 - f1_score: 0.9016 - val_loss: 0.4459 - val_accuracy: 0.8029 - val_f1_score: 0.8242\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2450 - accuracy: 0.9020 - f1_score: 0.9015 - val_loss: 0.4204 - val_accuracy: 0.8119 - val_f1_score: 0.8272\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2288 - accuracy: 0.9111 - f1_score: 0.9104 - val_loss: 0.4126 - val_accuracy: 0.8300 - val_f1_score: 0.8436\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2127 - accuracy: 0.9195 - f1_score: 0.9192 - val_loss: 0.4310 - val_accuracy: 0.8309 - val_f1_score: 0.8425\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2035 - accuracy: 0.9244 - f1_score: 0.9243 - val_loss: 0.4148 - val_accuracy: 0.8463 - val_f1_score: 0.8527\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3389 - accuracy: 0.8609 - f1_score: 0.8826\n","Epoch 1/20\n","104/104 [==============================] - 9s 20ms/step - loss: 0.6334 - accuracy: 0.6377 - f1_score: 0.5972 - val_loss: 0.5376 - val_accuracy: 0.7360 - val_f1_score: 0.7020\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4863 - accuracy: 0.7643 - f1_score: 0.7600 - val_loss: 0.4289 - val_accuracy: 0.8183 - val_f1_score: 0.8312\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3988 - accuracy: 0.8219 - f1_score: 0.8202 - val_loss: 0.3958 - val_accuracy: 0.8291 - val_f1_score: 0.8418\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3623 - accuracy: 0.8427 - f1_score: 0.8403 - val_loss: 0.4361 - val_accuracy: 0.7884 - val_f1_score: 0.8157\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3348 - accuracy: 0.8568 - f1_score: 0.8561 - val_loss: 0.3508 - val_accuracy: 0.8526 - val_f1_score: 0.8613\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3202 - accuracy: 0.8647 - f1_score: 0.8642 - val_loss: 0.3236 - val_accuracy: 0.8725 - val_f1_score: 0.8693\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3018 - accuracy: 0.8746 - f1_score: 0.8742 - val_loss: 0.3351 - val_accuracy: 0.8626 - val_f1_score: 0.8676\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2881 - accuracy: 0.8791 - f1_score: 0.8786 - val_loss: 0.3240 - val_accuracy: 0.8725 - val_f1_score: 0.8747\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2731 - accuracy: 0.8918 - f1_score: 0.8912 - val_loss: 0.2996 - val_accuracy: 0.8852 - val_f1_score: 0.8834\n","Epoch 10/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2644 - accuracy: 0.8957 - f1_score: 0.8949 - val_loss: 0.4509 - val_accuracy: 0.7929 - val_f1_score: 0.8193\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2592 - accuracy: 0.8948 - f1_score: 0.8945 - val_loss: 0.3284 - val_accuracy: 0.8671 - val_f1_score: 0.8585\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2516 - accuracy: 0.9011 - f1_score: 0.9004 - val_loss: 0.3181 - val_accuracy: 0.8761 - val_f1_score: 0.8735\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2371 - accuracy: 0.9078 - f1_score: 0.9070 - val_loss: 0.3249 - val_accuracy: 0.8770 - val_f1_score: 0.8761\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2360 - accuracy: 0.9090 - f1_score: 0.9082 - val_loss: 0.3547 - val_accuracy: 0.8571 - val_f1_score: 0.8663\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3710 - accuracy: 0.8460 - f1_score: 0.8674\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6350 - accuracy: 0.6344 - f1_score: 0.6116 - val_loss: 0.5779 - val_accuracy: 0.7080 - val_f1_score: 0.6516\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.5130 - accuracy: 0.7498 - f1_score: 0.7419 - val_loss: 0.4572 - val_accuracy: 0.7911 - val_f1_score: 0.7921\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4217 - accuracy: 0.8125 - f1_score: 0.8089 - val_loss: 0.3884 - val_accuracy: 0.8336 - val_f1_score: 0.8254\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3687 - accuracy: 0.8412 - f1_score: 0.8385 - val_loss: 0.4223 - val_accuracy: 0.8219 - val_f1_score: 0.7971\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3485 - accuracy: 0.8553 - f1_score: 0.8533 - val_loss: 0.3399 - val_accuracy: 0.8617 - val_f1_score: 0.8566\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3114 - accuracy: 0.8728 - f1_score: 0.8718 - val_loss: 0.3611 - val_accuracy: 0.8454 - val_f1_score: 0.8302\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3083 - accuracy: 0.8695 - f1_score: 0.8679 - val_loss: 0.3259 - val_accuracy: 0.8734 - val_f1_score: 0.8743\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2833 - accuracy: 0.8870 - f1_score: 0.8863 - val_loss: 0.3276 - val_accuracy: 0.8671 - val_f1_score: 0.8714\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2808 - accuracy: 0.8840 - f1_score: 0.8827 - val_loss: 0.3141 - val_accuracy: 0.8770 - val_f1_score: 0.8748\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2657 - accuracy: 0.8996 - f1_score: 0.8987 - val_loss: 0.3167 - val_accuracy: 0.8725 - val_f1_score: 0.8693\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2601 - accuracy: 0.8933 - f1_score: 0.8934 - val_loss: 0.4026 - val_accuracy: 0.8336 - val_f1_score: 0.8115\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2469 - accuracy: 0.9051 - f1_score: 0.9045 - val_loss: 0.3230 - val_accuracy: 0.8788 - val_f1_score: 0.8788\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2301 - accuracy: 0.9138 - f1_score: 0.9135 - val_loss: 0.3156 - val_accuracy: 0.8797 - val_f1_score: 0.8816\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2245 - accuracy: 0.9153 - f1_score: 0.9148 - val_loss: 0.3374 - val_accuracy: 0.8734 - val_f1_score: 0.8667\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8650 - f1_score: 0.8897\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.6238 - accuracy: 0.6519 - f1_score: 0.6443 - val_loss: 0.5069 - val_accuracy: 0.7568 - val_f1_score: 0.7488\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4975 - accuracy: 0.7655 - f1_score: 0.7561 - val_loss: 0.3948 - val_accuracy: 0.8336 - val_f1_score: 0.8324\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4068 - accuracy: 0.8137 - f1_score: 0.8088 - val_loss: 0.3617 - val_accuracy: 0.8445 - val_f1_score: 0.8491\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3572 - accuracy: 0.8505 - f1_score: 0.8490 - val_loss: 0.3786 - val_accuracy: 0.8309 - val_f1_score: 0.8074\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3474 - accuracy: 0.8514 - f1_score: 0.8500 - val_loss: 0.3219 - val_accuracy: 0.8752 - val_f1_score: 0.8705\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3222 - accuracy: 0.8701 - f1_score: 0.8684 - val_loss: 0.3175 - val_accuracy: 0.8698 - val_f1_score: 0.8710\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3145 - accuracy: 0.8674 - f1_score: 0.8659 - val_loss: 0.3141 - val_accuracy: 0.8743 - val_f1_score: 0.8758\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2951 - accuracy: 0.8794 - f1_score: 0.8782 - val_loss: 0.3157 - val_accuracy: 0.8716 - val_f1_score: 0.8685\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2639 - accuracy: 0.8963 - f1_score: 0.8960 - val_loss: 0.3172 - val_accuracy: 0.8743 - val_f1_score: 0.8687\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2571 - accuracy: 0.8927 - f1_score: 0.8920 - val_loss: 0.3453 - val_accuracy: 0.8608 - val_f1_score: 0.8490\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2510 - accuracy: 0.9005 - f1_score: 0.8990 - val_loss: 0.3244 - val_accuracy: 0.8671 - val_f1_score: 0.8674\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2388 - accuracy: 0.9042 - f1_score: 0.9034 - val_loss: 0.3973 - val_accuracy: 0.8373 - val_f1_score: 0.8167\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3159 - accuracy: 0.8636 - f1_score: 0.8894\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GaLPJAPtFlwH","executionInfo":{"status":"ok","timestamp":1690576096179,"user_tz":-330,"elapsed":36,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"16fa3902-df4b-42fe-eba3-349626c64b0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8433490991592407, 0.8609048128128052, 0.8568534851074219, 0.8507764935493469, 0.823092520236969, 0.8528021574020386, 0.8575286865234375, 0.8514516949653625, 0.8602295517921448, 0.8609048128128052, 0.8602295517921448, 0.8663065433502197, 0.8602295517921448, 0.8534773588180542, 0.8561782836914062, 0.8244429230690002, 0.8622552156448364, 0.8595543503761292, 0.8480756282806396, 0.8663065433502197, 0.8730587363243103, 0.8609048128128052, 0.846049964427948, 0.8649561405181885, 0.8636056780815125]\n","0.8553409838676452\n","0.011448154483931444\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3lVoX8XFlyo","executionInfo":{"status":"ok","timestamp":1690576096179,"user_tz":-330,"elapsed":10,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"f82da00f-dc2d-413f-e5ef-258e1bba0b42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3782650828361511, 0.34063559770584106, 0.3616879880428314, 0.3559492826461792, 0.4174801707267761, 0.3601188063621521, 0.3158615827560425, 0.3568853735923767, 0.3388562500476837, 0.3368663787841797, 0.3243078589439392, 0.3112417459487915, 0.3173860013484955, 0.3559519946575165, 0.3352625370025635, 0.38728636503219604, 0.3317582905292511, 0.3405842185020447, 0.3451906144618988, 0.32664865255355835, 0.32284483313560486, 0.33885785937309265, 0.37104615569114685, 0.3138754367828369, 0.31592002511024475]\n","0.3440307641029358\n","0.025268874113812016\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brT4hEYVFl1B","executionInfo":{"status":"ok","timestamp":1690576096179,"user_tz":-330,"elapsed":7,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"ca9427a5-ef3e-4d8d-b6b7-f6c8d9b9932f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8648017644882202, 0.884139358997345, 0.877456545829773, 0.8721804022789001, 0.8421685695648193, 0.8776655197143555, 0.8877061605453491, 0.8795180320739746, 0.8865753412246704, 0.8868131637573242, 0.8873162269592285, 0.8930884599685669, 0.888289213180542, 0.8763532042503357, 0.8836700916290283, 0.8472385406494141, 0.8879120349884033, 0.8886508941650391, 0.873382031917572, 0.8909690380096436, 0.8941440582275391, 0.8825541138648987, 0.8674418330192566, 0.889746367931366, 0.889375627040863]\n","0.8799662637710571\n","0.012874795396858308\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pPoejQZfFl4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=3, strides=1)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_lstm_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCRtd4E7Fl6U","executionInfo":{"status":"ok","timestamp":1690576924493,"user_tz":-330,"elapsed":828317,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"bc9b5684-5dfb-4bd8-a01d-d0fdc22f9ffc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 10s 23ms/step - loss: 0.6288 - accuracy: 0.6453 - f1_score: 0.6258 - val_loss: 0.4845 - val_accuracy: 0.7866 - val_f1_score: 0.7839\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4994 - accuracy: 0.7547 - f1_score: 0.7485 - val_loss: 0.4264 - val_accuracy: 0.8065 - val_f1_score: 0.8217\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4293 - accuracy: 0.8017 - f1_score: 0.7954 - val_loss: 0.4756 - val_accuracy: 0.7749 - val_f1_score: 0.8095\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3900 - accuracy: 0.8297 - f1_score: 0.8274 - val_loss: 0.3772 - val_accuracy: 0.8400 - val_f1_score: 0.8506\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3572 - accuracy: 0.8418 - f1_score: 0.8400 - val_loss: 0.3223 - val_accuracy: 0.8617 - val_f1_score: 0.8645\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3337 - accuracy: 0.8614 - f1_score: 0.8594 - val_loss: 0.3242 - val_accuracy: 0.8671 - val_f1_score: 0.8707\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3262 - accuracy: 0.8635 - f1_score: 0.8624 - val_loss: 0.3052 - val_accuracy: 0.8698 - val_f1_score: 0.8662\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2975 - accuracy: 0.8782 - f1_score: 0.8766 - val_loss: 0.4196 - val_accuracy: 0.8210 - val_f1_score: 0.8436\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2889 - accuracy: 0.8843 - f1_score: 0.8830 - val_loss: 0.3153 - val_accuracy: 0.8617 - val_f1_score: 0.8555\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2901 - accuracy: 0.8861 - f1_score: 0.8849 - val_loss: 0.3405 - val_accuracy: 0.8463 - val_f1_score: 0.8276\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2575 - accuracy: 0.8963 - f1_score: 0.8945 - val_loss: 0.3241 - val_accuracy: 0.8725 - val_f1_score: 0.8760\n","Epoch 12/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2681 - accuracy: 0.8942 - f1_score: 0.8927 - val_loss: 0.3047 - val_accuracy: 0.8680 - val_f1_score: 0.8641\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2512 - accuracy: 0.9002 - f1_score: 0.8986 - val_loss: 0.3719 - val_accuracy: 0.8590 - val_f1_score: 0.8667\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2356 - accuracy: 0.9090 - f1_score: 0.9078 - val_loss: 0.3121 - val_accuracy: 0.8779 - val_f1_score: 0.8758\n","Epoch 15/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2234 - accuracy: 0.9141 - f1_score: 0.9128 - val_loss: 0.3348 - val_accuracy: 0.8671 - val_f1_score: 0.8662\n","Epoch 16/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2234 - accuracy: 0.9165 - f1_score: 0.9153 - val_loss: 0.3256 - val_accuracy: 0.8788 - val_f1_score: 0.8775\n","Epoch 17/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.1960 - accuracy: 0.9228 - f1_score: 0.9221 - val_loss: 0.3551 - val_accuracy: 0.8698 - val_f1_score: 0.8669\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3997 - accuracy: 0.8379 - f1_score: 0.8578\n","Epoch 1/20\n","104/104 [==============================] - 16s 32ms/step - loss: 0.6234 - accuracy: 0.6555 - f1_score: 0.6458 - val_loss: 0.5592 - val_accuracy: 0.7161 - val_f1_score: 0.6667\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4987 - accuracy: 0.7649 - f1_score: 0.7588 - val_loss: 0.4551 - val_accuracy: 0.7893 - val_f1_score: 0.7841\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4217 - accuracy: 0.8062 - f1_score: 0.8037 - val_loss: 0.4070 - val_accuracy: 0.8255 - val_f1_score: 0.8335\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3723 - accuracy: 0.8376 - f1_score: 0.8378 - val_loss: 0.3712 - val_accuracy: 0.8526 - val_f1_score: 0.8506\n","Epoch 5/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3360 - accuracy: 0.8593 - f1_score: 0.8577 - val_loss: 0.3710 - val_accuracy: 0.8354 - val_f1_score: 0.8392\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3230 - accuracy: 0.8665 - f1_score: 0.8661 - val_loss: 0.4830 - val_accuracy: 0.7848 - val_f1_score: 0.8141\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3197 - accuracy: 0.8632 - f1_score: 0.8622 - val_loss: 0.3610 - val_accuracy: 0.8454 - val_f1_score: 0.8496\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2797 - accuracy: 0.8849 - f1_score: 0.8846 - val_loss: 0.3709 - val_accuracy: 0.8599 - val_f1_score: 0.8473\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2840 - accuracy: 0.8816 - f1_score: 0.8810 - val_loss: 0.3436 - val_accuracy: 0.8716 - val_f1_score: 0.8663\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2809 - accuracy: 0.8834 - f1_score: 0.8831 - val_loss: 0.3862 - val_accuracy: 0.8481 - val_f1_score: 0.8317\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2488 - accuracy: 0.9024 - f1_score: 0.9014 - val_loss: 0.3966 - val_accuracy: 0.8472 - val_f1_score: 0.8342\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2423 - accuracy: 0.9051 - f1_score: 0.9050 - val_loss: 0.3887 - val_accuracy: 0.8345 - val_f1_score: 0.8416\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2246 - accuracy: 0.9165 - f1_score: 0.9162 - val_loss: 0.3953 - val_accuracy: 0.8418 - val_f1_score: 0.8453\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2122 - accuracy: 0.9168 - f1_score: 0.9165 - val_loss: 0.3996 - val_accuracy: 0.8490 - val_f1_score: 0.8497\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3609 - accuracy: 0.8467 - f1_score: 0.8683\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6126 - accuracy: 0.6757 - f1_score: 0.6833 - val_loss: 0.5235 - val_accuracy: 0.7432 - val_f1_score: 0.7227\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4829 - accuracy: 0.7779 - f1_score: 0.7671 - val_loss: 0.4918 - val_accuracy: 0.7631 - val_f1_score: 0.7880\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4020 - accuracy: 0.8219 - f1_score: 0.8185 - val_loss: 0.4142 - val_accuracy: 0.8128 - val_f1_score: 0.8235\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3620 - accuracy: 0.8457 - f1_score: 0.8431 - val_loss: 0.3881 - val_accuracy: 0.8273 - val_f1_score: 0.8358\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3330 - accuracy: 0.8577 - f1_score: 0.8559 - val_loss: 0.3699 - val_accuracy: 0.8436 - val_f1_score: 0.8382\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3003 - accuracy: 0.8731 - f1_score: 0.8720 - val_loss: 0.3931 - val_accuracy: 0.8409 - val_f1_score: 0.8385\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2960 - accuracy: 0.8861 - f1_score: 0.8847 - val_loss: 0.3775 - val_accuracy: 0.8409 - val_f1_score: 0.8340\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2755 - accuracy: 0.8945 - f1_score: 0.8933 - val_loss: 0.3900 - val_accuracy: 0.8382 - val_f1_score: 0.8397\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2731 - accuracy: 0.8873 - f1_score: 0.8866 - val_loss: 0.4538 - val_accuracy: 0.8092 - val_f1_score: 0.7840\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2698 - accuracy: 0.8939 - f1_score: 0.8923 - val_loss: 0.3800 - val_accuracy: 0.8445 - val_f1_score: 0.8410\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3593 - accuracy: 0.8400 - f1_score: 0.8620\n","Epoch 1/20\n","104/104 [==============================] - 10s 22ms/step - loss: 0.6283 - accuracy: 0.6410 - f1_score: 0.6563 - val_loss: 0.5133 - val_accuracy: 0.7559 - val_f1_score: 0.7389\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4913 - accuracy: 0.7746 - f1_score: 0.7687 - val_loss: 0.4031 - val_accuracy: 0.8174 - val_f1_score: 0.8126\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4042 - accuracy: 0.8192 - f1_score: 0.8158 - val_loss: 0.3765 - val_accuracy: 0.8454 - val_f1_score: 0.8507\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3744 - accuracy: 0.8397 - f1_score: 0.8382 - val_loss: 0.3517 - val_accuracy: 0.8535 - val_f1_score: 0.8535\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3264 - accuracy: 0.8671 - f1_score: 0.8659 - val_loss: 0.3501 - val_accuracy: 0.8599 - val_f1_score: 0.8634\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3205 - accuracy: 0.8656 - f1_score: 0.8644 - val_loss: 0.3438 - val_accuracy: 0.8617 - val_f1_score: 0.8633\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3076 - accuracy: 0.8813 - f1_score: 0.8797 - val_loss: 0.3462 - val_accuracy: 0.8626 - val_f1_score: 0.8611\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2916 - accuracy: 0.8852 - f1_score: 0.8840 - val_loss: 0.3470 - val_accuracy: 0.8653 - val_f1_score: 0.8622\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2930 - accuracy: 0.8819 - f1_score: 0.8806 - val_loss: 0.3530 - val_accuracy: 0.8689 - val_f1_score: 0.8671\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2887 - accuracy: 0.8819 - f1_score: 0.8803 - val_loss: 0.3667 - val_accuracy: 0.8553 - val_f1_score: 0.8604\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2655 - accuracy: 0.8996 - f1_score: 0.8989 - val_loss: 0.3661 - val_accuracy: 0.8508 - val_f1_score: 0.8554\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3556 - accuracy: 0.8521 - f1_score: 0.8742\n","Epoch 1/20\n","104/104 [==============================] - 10s 22ms/step - loss: 0.6117 - accuracy: 0.6615 - f1_score: 0.6465 - val_loss: 0.5052 - val_accuracy: 0.7640 - val_f1_score: 0.7672\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4865 - accuracy: 0.7640 - f1_score: 0.7580 - val_loss: 0.4054 - val_accuracy: 0.8201 - val_f1_score: 0.8247\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4062 - accuracy: 0.8159 - f1_score: 0.8143 - val_loss: 0.3714 - val_accuracy: 0.8481 - val_f1_score: 0.8421\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3689 - accuracy: 0.8421 - f1_score: 0.8404 - val_loss: 0.3459 - val_accuracy: 0.8608 - val_f1_score: 0.8590\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3707 - accuracy: 0.8397 - f1_score: 0.8373 - val_loss: 0.3597 - val_accuracy: 0.8481 - val_f1_score: 0.8571\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3285 - accuracy: 0.8662 - f1_score: 0.8652 - val_loss: 0.4045 - val_accuracy: 0.8273 - val_f1_score: 0.8448\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3292 - accuracy: 0.8656 - f1_score: 0.8657 - val_loss: 0.4396 - val_accuracy: 0.8020 - val_f1_score: 0.7632\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2994 - accuracy: 0.8800 - f1_score: 0.8786 - val_loss: 0.3438 - val_accuracy: 0.8653 - val_f1_score: 0.8673\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3035 - accuracy: 0.8770 - f1_score: 0.8761 - val_loss: 0.3740 - val_accuracy: 0.8436 - val_f1_score: 0.8562\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2830 - accuracy: 0.8885 - f1_score: 0.8871 - val_loss: 0.3611 - val_accuracy: 0.8517 - val_f1_score: 0.8629\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2615 - accuracy: 0.8948 - f1_score: 0.8939 - val_loss: 0.3435 - val_accuracy: 0.8662 - val_f1_score: 0.8619\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2524 - accuracy: 0.9002 - f1_score: 0.8989 - val_loss: 0.3498 - val_accuracy: 0.8653 - val_f1_score: 0.8659\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2511 - accuracy: 0.9017 - f1_score: 0.9009 - val_loss: 0.3796 - val_accuracy: 0.8463 - val_f1_score: 0.8300\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2311 - accuracy: 0.9135 - f1_score: 0.9126 - val_loss: 0.3597 - val_accuracy: 0.8635 - val_f1_score: 0.8593\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2287 - accuracy: 0.9120 - f1_score: 0.9109 - val_loss: 0.3453 - val_accuracy: 0.8671 - val_f1_score: 0.8723\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2042 - accuracy: 0.9244 - f1_score: 0.9234 - val_loss: 0.3826 - val_accuracy: 0.8553 - val_f1_score: 0.8437\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8609 - f1_score: 0.8823\n","Epoch 1/20\n","104/104 [==============================] - 9s 23ms/step - loss: 0.6291 - accuracy: 0.6380 - f1_score: 0.6335 - val_loss: 0.5044 - val_accuracy: 0.7595 - val_f1_score: 0.7471\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4730 - accuracy: 0.7833 - f1_score: 0.7759 - val_loss: 0.4116 - val_accuracy: 0.8128 - val_f1_score: 0.8282\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4207 - accuracy: 0.8035 - f1_score: 0.8009 - val_loss: 0.4042 - val_accuracy: 0.8318 - val_f1_score: 0.8453\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3555 - accuracy: 0.8448 - f1_score: 0.8433 - val_loss: 0.3388 - val_accuracy: 0.8544 - val_f1_score: 0.8613\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3252 - accuracy: 0.8656 - f1_score: 0.8646 - val_loss: 0.3063 - val_accuracy: 0.8888 - val_f1_score: 0.8856\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3122 - accuracy: 0.8719 - f1_score: 0.8716 - val_loss: 0.3068 - val_accuracy: 0.8906 - val_f1_score: 0.8853\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2925 - accuracy: 0.8807 - f1_score: 0.8804 - val_loss: 0.2940 - val_accuracy: 0.8816 - val_f1_score: 0.8806\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2894 - accuracy: 0.8813 - f1_score: 0.8810 - val_loss: 0.3006 - val_accuracy: 0.8734 - val_f1_score: 0.8708\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2705 - accuracy: 0.8873 - f1_score: 0.8873 - val_loss: 0.2836 - val_accuracy: 0.8897 - val_f1_score: 0.8891\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2579 - accuracy: 0.8975 - f1_score: 0.8968 - val_loss: 0.3368 - val_accuracy: 0.8680 - val_f1_score: 0.8554\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2454 - accuracy: 0.9017 - f1_score: 0.9008 - val_loss: 0.2965 - val_accuracy: 0.8761 - val_f1_score: 0.8737\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2263 - accuracy: 0.9111 - f1_score: 0.9107 - val_loss: 0.3466 - val_accuracy: 0.8418 - val_f1_score: 0.8550\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2259 - accuracy: 0.9165 - f1_score: 0.9162 - val_loss: 0.2991 - val_accuracy: 0.8770 - val_f1_score: 0.8745\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2108 - accuracy: 0.9228 - f1_score: 0.9227 - val_loss: 0.3074 - val_accuracy: 0.8825 - val_f1_score: 0.8778\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3304 - accuracy: 0.8535 - f1_score: 0.8806\n","Epoch 1/20\n","104/104 [==============================] - 10s 24ms/step - loss: 0.6132 - accuracy: 0.6637 - f1_score: 0.6690 - val_loss: 0.5249 - val_accuracy: 0.7432 - val_f1_score: 0.7072\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4607 - accuracy: 0.7884 - f1_score: 0.7824 - val_loss: 0.4027 - val_accuracy: 0.8237 - val_f1_score: 0.8216\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3921 - accuracy: 0.8243 - f1_score: 0.8198 - val_loss: 0.3465 - val_accuracy: 0.8445 - val_f1_score: 0.8428\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3617 - accuracy: 0.8466 - f1_score: 0.8443 - val_loss: 0.3494 - val_accuracy: 0.8526 - val_f1_score: 0.8610\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3302 - accuracy: 0.8662 - f1_score: 0.8650 - val_loss: 0.3244 - val_accuracy: 0.8590 - val_f1_score: 0.8520\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3095 - accuracy: 0.8743 - f1_score: 0.8731 - val_loss: 0.3613 - val_accuracy: 0.8409 - val_f1_score: 0.8219\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3074 - accuracy: 0.8746 - f1_score: 0.8740 - val_loss: 0.3118 - val_accuracy: 0.8770 - val_f1_score: 0.8724\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2863 - accuracy: 0.8846 - f1_score: 0.8844 - val_loss: 0.2968 - val_accuracy: 0.8870 - val_f1_score: 0.8885\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2602 - accuracy: 0.8951 - f1_score: 0.8944 - val_loss: 0.3032 - val_accuracy: 0.8834 - val_f1_score: 0.8857\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2539 - accuracy: 0.9017 - f1_score: 0.9010 - val_loss: 0.3152 - val_accuracy: 0.8743 - val_f1_score: 0.8799\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2459 - accuracy: 0.9090 - f1_score: 0.9084 - val_loss: 0.3111 - val_accuracy: 0.8843 - val_f1_score: 0.8810\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2391 - accuracy: 0.9051 - f1_score: 0.9041 - val_loss: 0.3390 - val_accuracy: 0.8680 - val_f1_score: 0.8752\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2197 - accuracy: 0.9198 - f1_score: 0.9189 - val_loss: 0.3576 - val_accuracy: 0.8617 - val_f1_score: 0.8715\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3329 - accuracy: 0.8521 - f1_score: 0.8801\n","Epoch 1/20\n","104/104 [==============================] - 9s 29ms/step - loss: 0.6123 - accuracy: 0.6624 - f1_score: 0.6333 - val_loss: 0.5413 - val_accuracy: 0.7288 - val_f1_score: 0.7521\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4715 - accuracy: 0.7797 - f1_score: 0.7739 - val_loss: 0.4453 - val_accuracy: 0.7948 - val_f1_score: 0.7755\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4027 - accuracy: 0.8237 - f1_score: 0.8185 - val_loss: 0.4099 - val_accuracy: 0.8228 - val_f1_score: 0.8330\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3811 - accuracy: 0.8306 - f1_score: 0.8276 - val_loss: 0.3728 - val_accuracy: 0.8418 - val_f1_score: 0.8447\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3457 - accuracy: 0.8484 - f1_score: 0.8457 - val_loss: 0.3520 - val_accuracy: 0.8526 - val_f1_score: 0.8509\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3185 - accuracy: 0.8662 - f1_score: 0.8635 - val_loss: 0.3615 - val_accuracy: 0.8472 - val_f1_score: 0.8554\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3008 - accuracy: 0.8734 - f1_score: 0.8730 - val_loss: 0.3420 - val_accuracy: 0.8707 - val_f1_score: 0.8662\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2829 - accuracy: 0.8843 - f1_score: 0.8837 - val_loss: 0.3822 - val_accuracy: 0.8318 - val_f1_score: 0.8463\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2527 - accuracy: 0.9027 - f1_score: 0.9024 - val_loss: 0.3412 - val_accuracy: 0.8743 - val_f1_score: 0.8777\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2520 - accuracy: 0.9005 - f1_score: 0.9000 - val_loss: 0.3512 - val_accuracy: 0.8671 - val_f1_score: 0.8729\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2481 - accuracy: 0.9039 - f1_score: 0.9032 - val_loss: 0.3395 - val_accuracy: 0.8644 - val_f1_score: 0.8705\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2362 - accuracy: 0.9042 - f1_score: 0.9038 - val_loss: 0.3435 - val_accuracy: 0.8698 - val_f1_score: 0.8686\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2210 - accuracy: 0.9177 - f1_score: 0.9174 - val_loss: 0.3957 - val_accuracy: 0.8373 - val_f1_score: 0.8502\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2006 - accuracy: 0.9253 - f1_score: 0.9250 - val_loss: 0.3757 - val_accuracy: 0.8680 - val_f1_score: 0.8694\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1949 - accuracy: 0.9237 - f1_score: 0.9234 - val_loss: 0.3659 - val_accuracy: 0.8707 - val_f1_score: 0.8720\n","Epoch 16/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1794 - accuracy: 0.9334 - f1_score: 0.9330 - val_loss: 0.4087 - val_accuracy: 0.8680 - val_f1_score: 0.8701\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3382 - accuracy: 0.8623 - f1_score: 0.8902\n","Epoch 1/20\n","104/104 [==============================] - 10s 31ms/step - loss: 0.6172 - accuracy: 0.6585 - f1_score: 0.6609 - val_loss: 0.5131 - val_accuracy: 0.7640 - val_f1_score: 0.7323\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4661 - accuracy: 0.7824 - f1_score: 0.7761 - val_loss: 0.4489 - val_accuracy: 0.7957 - val_f1_score: 0.8183\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3987 - accuracy: 0.8159 - f1_score: 0.8121 - val_loss: 0.4262 - val_accuracy: 0.8065 - val_f1_score: 0.8288\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3499 - accuracy: 0.8508 - f1_score: 0.8505 - val_loss: 0.3467 - val_accuracy: 0.8562 - val_f1_score: 0.8619\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3429 - accuracy: 0.8529 - f1_score: 0.8527 - val_loss: 0.3354 - val_accuracy: 0.8590 - val_f1_score: 0.8643\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3062 - accuracy: 0.8773 - f1_score: 0.8772 - val_loss: 0.3193 - val_accuracy: 0.8689 - val_f1_score: 0.8702\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2907 - accuracy: 0.8779 - f1_score: 0.8785 - val_loss: 0.3127 - val_accuracy: 0.8716 - val_f1_score: 0.8728\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2775 - accuracy: 0.8858 - f1_score: 0.8856 - val_loss: 0.3215 - val_accuracy: 0.8734 - val_f1_score: 0.8694\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2597 - accuracy: 0.8990 - f1_score: 0.8986 - val_loss: 0.3261 - val_accuracy: 0.8707 - val_f1_score: 0.8670\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2695 - accuracy: 0.8879 - f1_score: 0.8883 - val_loss: 0.3561 - val_accuracy: 0.8490 - val_f1_score: 0.8332\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2494 - accuracy: 0.9005 - f1_score: 0.8997 - val_loss: 0.3733 - val_accuracy: 0.8463 - val_f1_score: 0.8574\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2323 - accuracy: 0.9090 - f1_score: 0.9093 - val_loss: 0.3692 - val_accuracy: 0.8517 - val_f1_score: 0.8435\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3336 - accuracy: 0.8562 - f1_score: 0.8843\n","Epoch 1/20\n","104/104 [==============================] - 10s 24ms/step - loss: 0.6350 - accuracy: 0.6356 - f1_score: 0.6068 - val_loss: 0.5082 - val_accuracy: 0.7604 - val_f1_score: 0.7410\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4796 - accuracy: 0.7755 - f1_score: 0.7700 - val_loss: 0.4313 - val_accuracy: 0.8065 - val_f1_score: 0.7877\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4040 - accuracy: 0.8201 - f1_score: 0.8172 - val_loss: 0.4417 - val_accuracy: 0.7975 - val_f1_score: 0.7612\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3591 - accuracy: 0.8544 - f1_score: 0.8528 - val_loss: 0.3959 - val_accuracy: 0.8228 - val_f1_score: 0.7992\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3306 - accuracy: 0.8623 - f1_score: 0.8609 - val_loss: 0.3790 - val_accuracy: 0.8454 - val_f1_score: 0.8315\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3019 - accuracy: 0.8797 - f1_score: 0.8786 - val_loss: 0.3790 - val_accuracy: 0.8345 - val_f1_score: 0.8466\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3083 - accuracy: 0.8701 - f1_score: 0.8705 - val_loss: 0.3470 - val_accuracy: 0.8463 - val_f1_score: 0.8545\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2947 - accuracy: 0.8797 - f1_score: 0.8794 - val_loss: 0.3385 - val_accuracy: 0.8544 - val_f1_score: 0.8511\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2701 - accuracy: 0.8945 - f1_score: 0.8940 - val_loss: 0.3588 - val_accuracy: 0.8373 - val_f1_score: 0.8480\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2635 - accuracy: 0.8963 - f1_score: 0.8962 - val_loss: 0.3549 - val_accuracy: 0.8617 - val_f1_score: 0.8541\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2465 - accuracy: 0.9030 - f1_score: 0.9025 - val_loss: 0.3506 - val_accuracy: 0.8571 - val_f1_score: 0.8484\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2382 - accuracy: 0.9051 - f1_score: 0.9048 - val_loss: 0.5213 - val_accuracy: 0.7929 - val_f1_score: 0.7486\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2255 - accuracy: 0.9150 - f1_score: 0.9145 - val_loss: 0.3414 - val_accuracy: 0.8599 - val_f1_score: 0.8482\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3530 - accuracy: 0.8447 - f1_score: 0.8714\n","Epoch 1/20\n","104/104 [==============================] - 9s 22ms/step - loss: 0.6200 - accuracy: 0.6621 - f1_score: 0.6719 - val_loss: 0.5418 - val_accuracy: 0.7369 - val_f1_score: 0.6727\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4874 - accuracy: 0.7694 - f1_score: 0.7618 - val_loss: 0.4131 - val_accuracy: 0.8219 - val_f1_score: 0.8082\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3921 - accuracy: 0.8255 - f1_score: 0.8209 - val_loss: 0.3780 - val_accuracy: 0.8382 - val_f1_score: 0.8257\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3585 - accuracy: 0.8475 - f1_score: 0.8453 - val_loss: 0.3832 - val_accuracy: 0.8282 - val_f1_score: 0.8403\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3186 - accuracy: 0.8746 - f1_score: 0.8727 - val_loss: 0.3518 - val_accuracy: 0.8571 - val_f1_score: 0.8529\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3082 - accuracy: 0.8764 - f1_score: 0.8759 - val_loss: 0.3390 - val_accuracy: 0.8599 - val_f1_score: 0.8615\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2870 - accuracy: 0.8882 - f1_score: 0.8864 - val_loss: 0.3471 - val_accuracy: 0.8590 - val_f1_score: 0.8517\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2910 - accuracy: 0.8807 - f1_score: 0.8801 - val_loss: 0.3293 - val_accuracy: 0.8644 - val_f1_score: 0.8636\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2591 - accuracy: 0.8975 - f1_score: 0.8968 - val_loss: 0.3704 - val_accuracy: 0.8635 - val_f1_score: 0.8547\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2571 - accuracy: 0.8990 - f1_score: 0.8982 - val_loss: 0.3962 - val_accuracy: 0.8571 - val_f1_score: 0.8448\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2520 - accuracy: 0.8975 - f1_score: 0.8962 - val_loss: 0.3573 - val_accuracy: 0.8544 - val_f1_score: 0.8589\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2354 - accuracy: 0.9069 - f1_score: 0.9062 - val_loss: 0.4105 - val_accuracy: 0.8436 - val_f1_score: 0.8538\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2249 - accuracy: 0.9075 - f1_score: 0.9067 - val_loss: 0.3995 - val_accuracy: 0.8562 - val_f1_score: 0.8430\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3203 - accuracy: 0.8636 - f1_score: 0.8895\n","Epoch 1/20\n","104/104 [==============================] - 10s 25ms/step - loss: 0.6362 - accuracy: 0.6374 - f1_score: 0.6498 - val_loss: 0.5453 - val_accuracy: 0.7233 - val_f1_score: 0.6499\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.5016 - accuracy: 0.7517 - f1_score: 0.7401 - val_loss: 0.4234 - val_accuracy: 0.8119 - val_f1_score: 0.8123\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4045 - accuracy: 0.8128 - f1_score: 0.8058 - val_loss: 0.3691 - val_accuracy: 0.8418 - val_f1_score: 0.8439\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3615 - accuracy: 0.8457 - f1_score: 0.8414 - val_loss: 0.3498 - val_accuracy: 0.8463 - val_f1_score: 0.8330\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3356 - accuracy: 0.8590 - f1_score: 0.8559 - val_loss: 0.3256 - val_accuracy: 0.8617 - val_f1_score: 0.8539\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3326 - accuracy: 0.8617 - f1_score: 0.8582 - val_loss: 0.3234 - val_accuracy: 0.8680 - val_f1_score: 0.8687\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3049 - accuracy: 0.8725 - f1_score: 0.8702 - val_loss: 0.3208 - val_accuracy: 0.8725 - val_f1_score: 0.8658\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2906 - accuracy: 0.8822 - f1_score: 0.8805 - val_loss: 0.3274 - val_accuracy: 0.8680 - val_f1_score: 0.8583\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2803 - accuracy: 0.8876 - f1_score: 0.8858 - val_loss: 0.3267 - val_accuracy: 0.8743 - val_f1_score: 0.8682\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2746 - accuracy: 0.8912 - f1_score: 0.8896 - val_loss: 0.3147 - val_accuracy: 0.8689 - val_f1_score: 0.8604\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2603 - accuracy: 0.8987 - f1_score: 0.8965 - val_loss: 0.3495 - val_accuracy: 0.8590 - val_f1_score: 0.8455\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2552 - accuracy: 0.8999 - f1_score: 0.8983 - val_loss: 0.3557 - val_accuracy: 0.8571 - val_f1_score: 0.8670\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2489 - accuracy: 0.8996 - f1_score: 0.8984 - val_loss: 0.3423 - val_accuracy: 0.8671 - val_f1_score: 0.8729\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2220 - accuracy: 0.9105 - f1_score: 0.9091 - val_loss: 0.3428 - val_accuracy: 0.8644 - val_f1_score: 0.8646\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2184 - accuracy: 0.9180 - f1_score: 0.9171 - val_loss: 0.3370 - val_accuracy: 0.8671 - val_f1_score: 0.8653\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.8670 - f1_score: 0.8916\n","Epoch 1/20\n","104/104 [==============================] - 11s 24ms/step - loss: 0.6171 - accuracy: 0.6588 - f1_score: 0.6594 - val_loss: 0.5726 - val_accuracy: 0.7170 - val_f1_score: 0.7553\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4831 - accuracy: 0.7719 - f1_score: 0.7585 - val_loss: 0.4492 - val_accuracy: 0.7984 - val_f1_score: 0.8083\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4094 - accuracy: 0.8149 - f1_score: 0.8088 - val_loss: 0.5180 - val_accuracy: 0.7803 - val_f1_score: 0.8091\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3666 - accuracy: 0.8376 - f1_score: 0.8330 - val_loss: 0.4565 - val_accuracy: 0.8174 - val_f1_score: 0.8336\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3403 - accuracy: 0.8538 - f1_score: 0.8491 - val_loss: 0.3841 - val_accuracy: 0.8454 - val_f1_score: 0.8466\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3041 - accuracy: 0.8752 - f1_score: 0.8722 - val_loss: 0.3814 - val_accuracy: 0.8608 - val_f1_score: 0.8647\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2957 - accuracy: 0.8755 - f1_score: 0.8726 - val_loss: 0.3697 - val_accuracy: 0.8626 - val_f1_score: 0.8667\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2785 - accuracy: 0.8810 - f1_score: 0.8783 - val_loss: 0.3595 - val_accuracy: 0.8599 - val_f1_score: 0.8602\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2657 - accuracy: 0.8870 - f1_score: 0.8851 - val_loss: 0.5245 - val_accuracy: 0.8047 - val_f1_score: 0.8280\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2521 - accuracy: 0.8945 - f1_score: 0.8924 - val_loss: 0.4120 - val_accuracy: 0.8626 - val_f1_score: 0.8676\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2311 - accuracy: 0.9090 - f1_score: 0.9075 - val_loss: 0.4265 - val_accuracy: 0.8535 - val_f1_score: 0.8608\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2231 - accuracy: 0.9132 - f1_score: 0.9119 - val_loss: 0.3845 - val_accuracy: 0.8617 - val_f1_score: 0.8638\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2157 - accuracy: 0.9159 - f1_score: 0.9151 - val_loss: 0.3664 - val_accuracy: 0.8662 - val_f1_score: 0.8640\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3282 - accuracy: 0.8636 - f1_score: 0.8888\n","Epoch 1/20\n","104/104 [==============================] - 10s 22ms/step - loss: 0.6145 - accuracy: 0.6615 - f1_score: 0.6801 - val_loss: 0.5189 - val_accuracy: 0.7333 - val_f1_score: 0.6852\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4784 - accuracy: 0.7709 - f1_score: 0.7618 - val_loss: 0.4047 - val_accuracy: 0.8156 - val_f1_score: 0.8012\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3926 - accuracy: 0.8252 - f1_score: 0.8209 - val_loss: 0.3651 - val_accuracy: 0.8436 - val_f1_score: 0.8351\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3538 - accuracy: 0.8541 - f1_score: 0.8505 - val_loss: 0.3384 - val_accuracy: 0.8526 - val_f1_score: 0.8514\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3250 - accuracy: 0.8623 - f1_score: 0.8602 - val_loss: 0.3971 - val_accuracy: 0.8128 - val_f1_score: 0.8337\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3271 - accuracy: 0.8653 - f1_score: 0.8635 - val_loss: 0.3323 - val_accuracy: 0.8671 - val_f1_score: 0.8625\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3018 - accuracy: 0.8761 - f1_score: 0.8742 - val_loss: 0.3226 - val_accuracy: 0.8698 - val_f1_score: 0.8634\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2875 - accuracy: 0.8819 - f1_score: 0.8803 - val_loss: 0.3221 - val_accuracy: 0.8590 - val_f1_score: 0.8632\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2723 - accuracy: 0.8876 - f1_score: 0.8861 - val_loss: 0.3965 - val_accuracy: 0.8354 - val_f1_score: 0.8498\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2714 - accuracy: 0.8888 - f1_score: 0.8878 - val_loss: 0.3101 - val_accuracy: 0.8797 - val_f1_score: 0.8765\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2506 - accuracy: 0.8984 - f1_score: 0.8972 - val_loss: 0.4393 - val_accuracy: 0.8137 - val_f1_score: 0.8352\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2404 - accuracy: 0.9084 - f1_score: 0.9070 - val_loss: 0.4995 - val_accuracy: 0.8092 - val_f1_score: 0.8337\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2426 - accuracy: 0.9030 - f1_score: 0.9022 - val_loss: 0.3468 - val_accuracy: 0.8608 - val_f1_score: 0.8649\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2321 - accuracy: 0.9090 - f1_score: 0.9079 - val_loss: 0.3457 - val_accuracy: 0.8662 - val_f1_score: 0.8622\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2147 - accuracy: 0.9162 - f1_score: 0.9149 - val_loss: 0.3370 - val_accuracy: 0.8734 - val_f1_score: 0.8718\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3578 - accuracy: 0.8542 - f1_score: 0.8751\n","Epoch 1/20\n","104/104 [==============================] - 10s 22ms/step - loss: 0.6066 - accuracy: 0.6703 - f1_score: 0.6656 - val_loss: 0.5129 - val_accuracy: 0.7514 - val_f1_score: 0.7475\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4616 - accuracy: 0.7905 - f1_score: 0.7842 - val_loss: 0.4082 - val_accuracy: 0.8137 - val_f1_score: 0.8057\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3962 - accuracy: 0.8219 - f1_score: 0.8148 - val_loss: 0.3790 - val_accuracy: 0.8418 - val_f1_score: 0.8447\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3537 - accuracy: 0.8424 - f1_score: 0.8412 - val_loss: 0.3614 - val_accuracy: 0.8418 - val_f1_score: 0.8458\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3512 - accuracy: 0.8493 - f1_score: 0.8469 - val_loss: 0.3506 - val_accuracy: 0.8553 - val_f1_score: 0.8616\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3112 - accuracy: 0.8674 - f1_score: 0.8646 - val_loss: 0.3282 - val_accuracy: 0.8689 - val_f1_score: 0.8697\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2962 - accuracy: 0.8782 - f1_score: 0.8770 - val_loss: 0.3464 - val_accuracy: 0.8445 - val_f1_score: 0.8297\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2826 - accuracy: 0.8816 - f1_score: 0.8798 - val_loss: 0.3293 - val_accuracy: 0.8626 - val_f1_score: 0.8536\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2759 - accuracy: 0.8927 - f1_score: 0.8912 - val_loss: 0.3402 - val_accuracy: 0.8517 - val_f1_score: 0.8373\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2585 - accuracy: 0.8975 - f1_score: 0.8963 - val_loss: 0.3279 - val_accuracy: 0.8662 - val_f1_score: 0.8711\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2452 - accuracy: 0.9014 - f1_score: 0.9005 - val_loss: 0.3217 - val_accuracy: 0.8689 - val_f1_score: 0.8638\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2492 - accuracy: 0.8987 - f1_score: 0.8976 - val_loss: 0.3348 - val_accuracy: 0.8562 - val_f1_score: 0.8458\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2307 - accuracy: 0.9111 - f1_score: 0.9100 - val_loss: 0.3235 - val_accuracy: 0.8698 - val_f1_score: 0.8710\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2240 - accuracy: 0.9075 - f1_score: 0.9064 - val_loss: 0.3486 - val_accuracy: 0.8644 - val_f1_score: 0.8529\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2207 - accuracy: 0.9090 - f1_score: 0.9079 - val_loss: 0.3474 - val_accuracy: 0.8644 - val_f1_score: 0.8555\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1832 - accuracy: 0.9286 - f1_score: 0.9277 - val_loss: 0.3611 - val_accuracy: 0.8644 - val_f1_score: 0.8654\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3761 - accuracy: 0.8488 - f1_score: 0.8736\n","Epoch 1/20\n","104/104 [==============================] - 10s 36ms/step - loss: 0.6167 - accuracy: 0.6694 - f1_score: 0.6487 - val_loss: 0.5157 - val_accuracy: 0.7631 - val_f1_score: 0.7765\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4828 - accuracy: 0.7767 - f1_score: 0.7714 - val_loss: 0.5712 - val_accuracy: 0.7260 - val_f1_score: 0.7780\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4206 - accuracy: 0.8065 - f1_score: 0.8047 - val_loss: 0.4207 - val_accuracy: 0.8174 - val_f1_score: 0.8297\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3571 - accuracy: 0.8460 - f1_score: 0.8448 - val_loss: 0.4491 - val_accuracy: 0.7866 - val_f1_score: 0.8136\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3397 - accuracy: 0.8571 - f1_score: 0.8565 - val_loss: 0.4763 - val_accuracy: 0.7839 - val_f1_score: 0.8137\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3150 - accuracy: 0.8689 - f1_score: 0.8687 - val_loss: 0.3673 - val_accuracy: 0.8499 - val_f1_score: 0.8559\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2990 - accuracy: 0.8803 - f1_score: 0.8794 - val_loss: 0.4088 - val_accuracy: 0.8255 - val_f1_score: 0.8427\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2793 - accuracy: 0.8870 - f1_score: 0.8860 - val_loss: 0.4157 - val_accuracy: 0.8165 - val_f1_score: 0.8369\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2711 - accuracy: 0.8888 - f1_score: 0.8881 - val_loss: 0.3409 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2650 - accuracy: 0.8933 - f1_score: 0.8929 - val_loss: 0.3369 - val_accuracy: 0.8635 - val_f1_score: 0.8621\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2449 - accuracy: 0.9033 - f1_score: 0.9028 - val_loss: 0.3836 - val_accuracy: 0.8427 - val_f1_score: 0.8557\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2363 - accuracy: 0.9042 - f1_score: 0.9038 - val_loss: 0.5065 - val_accuracy: 0.7966 - val_f1_score: 0.8238\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2360 - accuracy: 0.9008 - f1_score: 0.9003 - val_loss: 0.3834 - val_accuracy: 0.8472 - val_f1_score: 0.8569\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2264 - accuracy: 0.9153 - f1_score: 0.9147 - val_loss: 0.4143 - val_accuracy: 0.8309 - val_f1_score: 0.8473\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2191 - accuracy: 0.9129 - f1_score: 0.9127 - val_loss: 0.5658 - val_accuracy: 0.7722 - val_f1_score: 0.8091\n","47/47 [==============================] - 0s 5ms/step - loss: 0.4069 - accuracy: 0.8359 - f1_score: 0.8588\n","Epoch 1/20\n","104/104 [==============================] - 10s 23ms/step - loss: 0.6020 - accuracy: 0.6854 - f1_score: 0.6865 - val_loss: 0.5424 - val_accuracy: 0.7333 - val_f1_score: 0.7604\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4538 - accuracy: 0.7881 - f1_score: 0.7820 - val_loss: 0.4257 - val_accuracy: 0.7966 - val_f1_score: 0.8014\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3969 - accuracy: 0.8249 - f1_score: 0.8217 - val_loss: 0.3765 - val_accuracy: 0.8354 - val_f1_score: 0.8299\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3575 - accuracy: 0.8490 - f1_score: 0.8480 - val_loss: 0.3549 - val_accuracy: 0.8562 - val_f1_score: 0.8470\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3313 - accuracy: 0.8608 - f1_score: 0.8592 - val_loss: 0.3572 - val_accuracy: 0.8382 - val_f1_score: 0.8466\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3199 - accuracy: 0.8680 - f1_score: 0.8669 - val_loss: 0.3319 - val_accuracy: 0.8635 - val_f1_score: 0.8601\n","Epoch 7/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2961 - accuracy: 0.8810 - f1_score: 0.8799 - val_loss: 0.3322 - val_accuracy: 0.8571 - val_f1_score: 0.8489\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2789 - accuracy: 0.8900 - f1_score: 0.8889 - val_loss: 0.3351 - val_accuracy: 0.8671 - val_f1_score: 0.8591\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2686 - accuracy: 0.8885 - f1_score: 0.8871 - val_loss: 0.3236 - val_accuracy: 0.8626 - val_f1_score: 0.8603\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2593 - accuracy: 0.8948 - f1_score: 0.8932 - val_loss: 0.3297 - val_accuracy: 0.8644 - val_f1_score: 0.8574\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2358 - accuracy: 0.9075 - f1_score: 0.9065 - val_loss: 0.3505 - val_accuracy: 0.8662 - val_f1_score: 0.8693\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2318 - accuracy: 0.9099 - f1_score: 0.9088 - val_loss: 0.3483 - val_accuracy: 0.8662 - val_f1_score: 0.8647\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2146 - accuracy: 0.9192 - f1_score: 0.9183 - val_loss: 0.3835 - val_accuracy: 0.8472 - val_f1_score: 0.8547\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2169 - accuracy: 0.9132 - f1_score: 0.9119 - val_loss: 0.3914 - val_accuracy: 0.8336 - val_f1_score: 0.8175\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3770 - accuracy: 0.8508 - f1_score: 0.8741\n","Epoch 1/20\n","104/104 [==============================] - 11s 22ms/step - loss: 0.6220 - accuracy: 0.6588 - f1_score: 0.6631 - val_loss: 0.4974 - val_accuracy: 0.7794 - val_f1_score: 0.7837\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4847 - accuracy: 0.7697 - f1_score: 0.7601 - val_loss: 0.4008 - val_accuracy: 0.8282 - val_f1_score: 0.8327\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4117 - accuracy: 0.8140 - f1_score: 0.8084 - val_loss: 0.3564 - val_accuracy: 0.8400 - val_f1_score: 0.8401\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3610 - accuracy: 0.8391 - f1_score: 0.8370 - val_loss: 0.3454 - val_accuracy: 0.8571 - val_f1_score: 0.8472\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3295 - accuracy: 0.8674 - f1_score: 0.8662 - val_loss: 0.3158 - val_accuracy: 0.8725 - val_f1_score: 0.8676\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3252 - accuracy: 0.8698 - f1_score: 0.8688 - val_loss: 0.4738 - val_accuracy: 0.7984 - val_f1_score: 0.7547\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3116 - accuracy: 0.8674 - f1_score: 0.8663 - val_loss: 0.3029 - val_accuracy: 0.8743 - val_f1_score: 0.8702\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2879 - accuracy: 0.8861 - f1_score: 0.8849 - val_loss: 0.3072 - val_accuracy: 0.8752 - val_f1_score: 0.8720\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2696 - accuracy: 0.8912 - f1_score: 0.8900 - val_loss: 0.3126 - val_accuracy: 0.8734 - val_f1_score: 0.8711\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2548 - accuracy: 0.9033 - f1_score: 0.9022 - val_loss: 0.3367 - val_accuracy: 0.8644 - val_f1_score: 0.8547\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2592 - accuracy: 0.8972 - f1_score: 0.8966 - val_loss: 0.3486 - val_accuracy: 0.8517 - val_f1_score: 0.8360\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2384 - accuracy: 0.9072 - f1_score: 0.9059 - val_loss: 0.3750 - val_accuracy: 0.8454 - val_f1_score: 0.8292\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.8569 - f1_score: 0.8817\n","Epoch 1/20\n","104/104 [==============================] - 10s 22ms/step - loss: 0.6192 - accuracy: 0.6670 - f1_score: 0.6515 - val_loss: 0.5260 - val_accuracy: 0.7441 - val_f1_score: 0.7333\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4868 - accuracy: 0.7746 - f1_score: 0.7665 - val_loss: 0.4768 - val_accuracy: 0.7830 - val_f1_score: 0.8020\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4179 - accuracy: 0.8119 - f1_score: 0.8092 - val_loss: 0.4833 - val_accuracy: 0.7767 - val_f1_score: 0.8038\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3700 - accuracy: 0.8376 - f1_score: 0.8355 - val_loss: 0.4156 - val_accuracy: 0.8137 - val_f1_score: 0.8269\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3358 - accuracy: 0.8617 - f1_score: 0.8609 - val_loss: 0.3704 - val_accuracy: 0.8454 - val_f1_score: 0.8385\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3030 - accuracy: 0.8846 - f1_score: 0.8841 - val_loss: 0.3650 - val_accuracy: 0.8526 - val_f1_score: 0.8559\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3032 - accuracy: 0.8743 - f1_score: 0.8741 - val_loss: 0.3984 - val_accuracy: 0.8354 - val_f1_score: 0.8471\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2899 - accuracy: 0.8819 - f1_score: 0.8809 - val_loss: 0.3486 - val_accuracy: 0.8580 - val_f1_score: 0.8566\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2711 - accuracy: 0.8927 - f1_score: 0.8925 - val_loss: 0.3539 - val_accuracy: 0.8608 - val_f1_score: 0.8635\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2712 - accuracy: 0.8954 - f1_score: 0.8948 - val_loss: 0.3572 - val_accuracy: 0.8562 - val_f1_score: 0.8476\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2500 - accuracy: 0.9045 - f1_score: 0.9040 - val_loss: 0.4366 - val_accuracy: 0.8255 - val_f1_score: 0.8004\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2476 - accuracy: 0.9048 - f1_score: 0.9038 - val_loss: 0.3668 - val_accuracy: 0.8653 - val_f1_score: 0.8624\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2271 - accuracy: 0.9174 - f1_score: 0.9169 - val_loss: 0.3551 - val_accuracy: 0.8644 - val_f1_score: 0.8574\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8602 - f1_score: 0.8838\n","Epoch 1/20\n","104/104 [==============================] - 11s 23ms/step - loss: 0.6218 - accuracy: 0.6534 - f1_score: 0.6505 - val_loss: 0.5114 - val_accuracy: 0.7568 - val_f1_score: 0.6974\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4883 - accuracy: 0.7703 - f1_score: 0.7611 - val_loss: 0.3893 - val_accuracy: 0.8183 - val_f1_score: 0.8000\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4310 - accuracy: 0.8119 - f1_score: 0.8035 - val_loss: 0.3481 - val_accuracy: 0.8635 - val_f1_score: 0.8629\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3800 - accuracy: 0.8303 - f1_score: 0.8250 - val_loss: 0.3361 - val_accuracy: 0.8617 - val_f1_score: 0.8689\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3520 - accuracy: 0.8511 - f1_score: 0.8480 - val_loss: 0.2903 - val_accuracy: 0.8816 - val_f1_score: 0.8797\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3335 - accuracy: 0.8599 - f1_score: 0.8574 - val_loss: 0.3674 - val_accuracy: 0.8318 - val_f1_score: 0.8502\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3069 - accuracy: 0.8725 - f1_score: 0.8716 - val_loss: 0.2713 - val_accuracy: 0.9014 - val_f1_score: 0.9015\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2859 - accuracy: 0.8861 - f1_score: 0.8843 - val_loss: 0.2688 - val_accuracy: 0.8951 - val_f1_score: 0.8962\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2941 - accuracy: 0.8719 - f1_score: 0.8697 - val_loss: 0.2774 - val_accuracy: 0.8915 - val_f1_score: 0.8946\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2686 - accuracy: 0.8921 - f1_score: 0.8918 - val_loss: 0.2714 - val_accuracy: 0.8978 - val_f1_score: 0.8972\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2592 - accuracy: 0.8981 - f1_score: 0.8966 - val_loss: 0.3262 - val_accuracy: 0.8725 - val_f1_score: 0.8794\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2403 - accuracy: 0.9093 - f1_score: 0.9084 - val_loss: 0.2732 - val_accuracy: 0.8942 - val_f1_score: 0.8939\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2382 - accuracy: 0.9048 - f1_score: 0.9035 - val_loss: 0.3034 - val_accuracy: 0.8807 - val_f1_score: 0.8840\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8623 - f1_score: 0.8880\n","Epoch 1/20\n","104/104 [==============================] - 11s 30ms/step - loss: 0.6270 - accuracy: 0.6486 - f1_score: 0.6408 - val_loss: 0.5555 - val_accuracy: 0.7016 - val_f1_score: 0.7334\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.5050 - accuracy: 0.7607 - f1_score: 0.7489 - val_loss: 0.4397 - val_accuracy: 0.7929 - val_f1_score: 0.7779\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4240 - accuracy: 0.8053 - f1_score: 0.8026 - val_loss: 0.3973 - val_accuracy: 0.8273 - val_f1_score: 0.8338\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3789 - accuracy: 0.8336 - f1_score: 0.8309 - val_loss: 0.3801 - val_accuracy: 0.8345 - val_f1_score: 0.8218\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3514 - accuracy: 0.8529 - f1_score: 0.8518 - val_loss: 0.3575 - val_accuracy: 0.8544 - val_f1_score: 0.8522\n","Epoch 6/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3194 - accuracy: 0.8725 - f1_score: 0.8712 - val_loss: 0.3844 - val_accuracy: 0.8418 - val_f1_score: 0.8523\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3177 - accuracy: 0.8686 - f1_score: 0.8671 - val_loss: 0.4028 - val_accuracy: 0.8183 - val_f1_score: 0.7891\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2910 - accuracy: 0.8888 - f1_score: 0.8874 - val_loss: 0.3795 - val_accuracy: 0.8517 - val_f1_score: 0.8605\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2828 - accuracy: 0.8867 - f1_score: 0.8856 - val_loss: 0.3505 - val_accuracy: 0.8535 - val_f1_score: 0.8606\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2750 - accuracy: 0.8870 - f1_score: 0.8861 - val_loss: 0.3284 - val_accuracy: 0.8653 - val_f1_score: 0.8656\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2652 - accuracy: 0.8975 - f1_score: 0.8965 - val_loss: 0.3321 - val_accuracy: 0.8689 - val_f1_score: 0.8646\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2399 - accuracy: 0.9060 - f1_score: 0.9051 - val_loss: 0.3273 - val_accuracy: 0.8644 - val_f1_score: 0.8644\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2546 - accuracy: 0.8975 - f1_score: 0.8965 - val_loss: 0.3484 - val_accuracy: 0.8635 - val_f1_score: 0.8658\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2383 - accuracy: 0.9087 - f1_score: 0.9077 - val_loss: 0.3843 - val_accuracy: 0.8373 - val_f1_score: 0.8512\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2358 - accuracy: 0.9075 - f1_score: 0.9066 - val_loss: 0.3841 - val_accuracy: 0.8580 - val_f1_score: 0.8597\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2102 - accuracy: 0.9192 - f1_score: 0.9182 - val_loss: 0.4164 - val_accuracy: 0.8472 - val_f1_score: 0.8574\n","Epoch 17/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2069 - accuracy: 0.9192 - f1_score: 0.9186 - val_loss: 0.3827 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8704 - f1_score: 0.8926\n","Epoch 1/20\n","104/104 [==============================] - 10s 32ms/step - loss: 0.6050 - accuracy: 0.6757 - f1_score: 0.6904 - val_loss: 0.5085 - val_accuracy: 0.7550 - val_f1_score: 0.7132\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4336 - accuracy: 0.7996 - f1_score: 0.7925 - val_loss: 0.4013 - val_accuracy: 0.8427 - val_f1_score: 0.8466\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3652 - accuracy: 0.8421 - f1_score: 0.8392 - val_loss: 0.3731 - val_accuracy: 0.8544 - val_f1_score: 0.8579\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3346 - accuracy: 0.8577 - f1_score: 0.8557 - val_loss: 0.3713 - val_accuracy: 0.8544 - val_f1_score: 0.8511\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3155 - accuracy: 0.8722 - f1_score: 0.8707 - val_loss: 0.3686 - val_accuracy: 0.8418 - val_f1_score: 0.8296\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2969 - accuracy: 0.8840 - f1_score: 0.8823 - val_loss: 0.3909 - val_accuracy: 0.8427 - val_f1_score: 0.8515\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2872 - accuracy: 0.8882 - f1_score: 0.8867 - val_loss: 0.3574 - val_accuracy: 0.8535 - val_f1_score: 0.8516\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2671 - accuracy: 0.8984 - f1_score: 0.8976 - val_loss: 0.3666 - val_accuracy: 0.8517 - val_f1_score: 0.8559\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2553 - accuracy: 0.9024 - f1_score: 0.9014 - val_loss: 0.3564 - val_accuracy: 0.8508 - val_f1_score: 0.8531\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2451 - accuracy: 0.9069 - f1_score: 0.9060 - val_loss: 0.3522 - val_accuracy: 0.8526 - val_f1_score: 0.8551\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2349 - accuracy: 0.9120 - f1_score: 0.9111 - val_loss: 0.3727 - val_accuracy: 0.8517 - val_f1_score: 0.8536\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2394 - accuracy: 0.9024 - f1_score: 0.9016 - val_loss: 0.4302 - val_accuracy: 0.8400 - val_f1_score: 0.8501\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2319 - accuracy: 0.9084 - f1_score: 0.9077 - val_loss: 0.3721 - val_accuracy: 0.8544 - val_f1_score: 0.8516\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2110 - accuracy: 0.9219 - f1_score: 0.9208 - val_loss: 0.4171 - val_accuracy: 0.8526 - val_f1_score: 0.8561\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2022 - accuracy: 0.9241 - f1_score: 0.9233 - val_loss: 0.4356 - val_accuracy: 0.8363 - val_f1_score: 0.8467\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3228 - accuracy: 0.8683 - f1_score: 0.8914\n","Epoch 1/20\n","104/104 [==============================] - 11s 22ms/step - loss: 0.6227 - accuracy: 0.6567 - f1_score: 0.6419 - val_loss: 0.5628 - val_accuracy: 0.7116 - val_f1_score: 0.6338\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4705 - accuracy: 0.7794 - f1_score: 0.7721 - val_loss: 0.4182 - val_accuracy: 0.8119 - val_f1_score: 0.7903\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3886 - accuracy: 0.8228 - f1_score: 0.8195 - val_loss: 0.3722 - val_accuracy: 0.8418 - val_f1_score: 0.8279\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3470 - accuracy: 0.8565 - f1_score: 0.8552 - val_loss: 0.3439 - val_accuracy: 0.8590 - val_f1_score: 0.8531\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3202 - accuracy: 0.8629 - f1_score: 0.8614 - val_loss: 0.3344 - val_accuracy: 0.8644 - val_f1_score: 0.8689\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3366 - accuracy: 0.8614 - f1_score: 0.8609 - val_loss: 0.3298 - val_accuracy: 0.8707 - val_f1_score: 0.8760\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3035 - accuracy: 0.8758 - f1_score: 0.8744 - val_loss: 0.3421 - val_accuracy: 0.8617 - val_f1_score: 0.8687\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2948 - accuracy: 0.8837 - f1_score: 0.8827 - val_loss: 0.3130 - val_accuracy: 0.8725 - val_f1_score: 0.8698\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2688 - accuracy: 0.8990 - f1_score: 0.8983 - val_loss: 0.3098 - val_accuracy: 0.8752 - val_f1_score: 0.8708\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2674 - accuracy: 0.8972 - f1_score: 0.8952 - val_loss: 0.3448 - val_accuracy: 0.8608 - val_f1_score: 0.8656\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2568 - accuracy: 0.8990 - f1_score: 0.8983 - val_loss: 0.3426 - val_accuracy: 0.8644 - val_f1_score: 0.8696\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2489 - accuracy: 0.9020 - f1_score: 0.9008 - val_loss: 0.3262 - val_accuracy: 0.8689 - val_f1_score: 0.8720\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2289 - accuracy: 0.9150 - f1_score: 0.9141 - val_loss: 0.3406 - val_accuracy: 0.8689 - val_f1_score: 0.8615\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2359 - accuracy: 0.9096 - f1_score: 0.9086 - val_loss: 0.3469 - val_accuracy: 0.8544 - val_f1_score: 0.8616\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3461 - accuracy: 0.8596 - f1_score: 0.8807\n","Epoch 1/20\n","104/104 [==============================] - 9s 22ms/step - loss: 0.6291 - accuracy: 0.6483 - f1_score: 0.6350 - val_loss: 0.5315 - val_accuracy: 0.7550 - val_f1_score: 0.7402\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4837 - accuracy: 0.7691 - f1_score: 0.7628 - val_loss: 0.4409 - val_accuracy: 0.7866 - val_f1_score: 0.7645\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3942 - accuracy: 0.8318 - f1_score: 0.8298 - val_loss: 0.3846 - val_accuracy: 0.8391 - val_f1_score: 0.8275\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3560 - accuracy: 0.8499 - f1_score: 0.8484 - val_loss: 0.3675 - val_accuracy: 0.8562 - val_f1_score: 0.8470\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3294 - accuracy: 0.8559 - f1_score: 0.8550 - val_loss: 0.3491 - val_accuracy: 0.8580 - val_f1_score: 0.8500\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3072 - accuracy: 0.8704 - f1_score: 0.8687 - val_loss: 0.3439 - val_accuracy: 0.8580 - val_f1_score: 0.8503\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3002 - accuracy: 0.8758 - f1_score: 0.8745 - val_loss: 0.3525 - val_accuracy: 0.8590 - val_f1_score: 0.8465\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2770 - accuracy: 0.8918 - f1_score: 0.8914 - val_loss: 0.3380 - val_accuracy: 0.8635 - val_f1_score: 0.8681\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2729 - accuracy: 0.8870 - f1_score: 0.8858 - val_loss: 0.3342 - val_accuracy: 0.8725 - val_f1_score: 0.8766\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2572 - accuracy: 0.8954 - f1_score: 0.8951 - val_loss: 0.4009 - val_accuracy: 0.8454 - val_f1_score: 0.8271\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2439 - accuracy: 0.9020 - f1_score: 0.9009 - val_loss: 0.3262 - val_accuracy: 0.8707 - val_f1_score: 0.8711\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2410 - accuracy: 0.9017 - f1_score: 0.9009 - val_loss: 0.3300 - val_accuracy: 0.8680 - val_f1_score: 0.8706\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2226 - accuracy: 0.9138 - f1_score: 0.9130 - val_loss: 0.3492 - val_accuracy: 0.8689 - val_f1_score: 0.8607\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2326 - accuracy: 0.9123 - f1_score: 0.9110 - val_loss: 0.3225 - val_accuracy: 0.8761 - val_f1_score: 0.8762\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2095 - accuracy: 0.9186 - f1_score: 0.9184 - val_loss: 0.3538 - val_accuracy: 0.8734 - val_f1_score: 0.8732\n","Epoch 16/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.1926 - accuracy: 0.9289 - f1_score: 0.9282 - val_loss: 0.3590 - val_accuracy: 0.8635 - val_f1_score: 0.8655\n","Epoch 17/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1875 - accuracy: 0.9286 - f1_score: 0.9281 - val_loss: 0.3628 - val_accuracy: 0.8752 - val_f1_score: 0.8683\n","Epoch 18/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1710 - accuracy: 0.9346 - f1_score: 0.9340 - val_loss: 0.3806 - val_accuracy: 0.8725 - val_f1_score: 0.8671\n","Epoch 19/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1672 - accuracy: 0.9367 - f1_score: 0.9361 - val_loss: 0.3683 - val_accuracy: 0.8788 - val_f1_score: 0.8766\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3128 - accuracy: 0.8805 - f1_score: 0.9040\n","Epoch 1/20\n","104/104 [==============================] - 11s 23ms/step - loss: 0.6281 - accuracy: 0.6504 - f1_score: 0.6442 - val_loss: 0.4974 - val_accuracy: 0.7749 - val_f1_score: 0.7561\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4944 - accuracy: 0.7631 - f1_score: 0.7516 - val_loss: 0.3999 - val_accuracy: 0.8300 - val_f1_score: 0.8259\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4035 - accuracy: 0.8171 - f1_score: 0.8122 - val_loss: 0.3823 - val_accuracy: 0.8282 - val_f1_score: 0.8403\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3678 - accuracy: 0.8418 - f1_score: 0.8382 - val_loss: 0.3898 - val_accuracy: 0.8228 - val_f1_score: 0.8388\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3517 - accuracy: 0.8520 - f1_score: 0.8499 - val_loss: 0.3231 - val_accuracy: 0.8734 - val_f1_score: 0.8694\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3128 - accuracy: 0.8737 - f1_score: 0.8717 - val_loss: 0.3277 - val_accuracy: 0.8707 - val_f1_score: 0.8733\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2952 - accuracy: 0.8785 - f1_score: 0.8766 - val_loss: 0.3199 - val_accuracy: 0.8707 - val_f1_score: 0.8727\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2796 - accuracy: 0.8852 - f1_score: 0.8838 - val_loss: 0.3789 - val_accuracy: 0.8445 - val_f1_score: 0.8576\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2735 - accuracy: 0.8936 - f1_score: 0.8925 - val_loss: 0.3186 - val_accuracy: 0.8797 - val_f1_score: 0.8760\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2528 - accuracy: 0.8963 - f1_score: 0.8949 - val_loss: 0.3387 - val_accuracy: 0.8571 - val_f1_score: 0.8487\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2509 - accuracy: 0.9017 - f1_score: 0.9004 - val_loss: 0.3201 - val_accuracy: 0.8734 - val_f1_score: 0.8682\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2182 - accuracy: 0.9165 - f1_score: 0.9157 - val_loss: 0.3302 - val_accuracy: 0.8653 - val_f1_score: 0.8604\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2211 - accuracy: 0.9129 - f1_score: 0.9118 - val_loss: 0.3355 - val_accuracy: 0.8689 - val_f1_score: 0.8620\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2105 - accuracy: 0.9150 - f1_score: 0.9145 - val_loss: 0.3448 - val_accuracy: 0.8698 - val_f1_score: 0.8664\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3415 - accuracy: 0.8656 - f1_score: 0.8869\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"poBKIIntFl9U","executionInfo":{"status":"ok","timestamp":1690576924494,"user_tz":-330,"elapsed":30,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"ffb125b6-45f7-4bee-bab3-85496c8838ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8379473090171814, 0.8467251658439636, 0.839972972869873, 0.852126955986023, 0.8609048128128052, 0.8534773588180542, 0.852126955986023, 0.8622552156448364, 0.8561782836914062, 0.844699501991272, 0.8636056780815125, 0.8669817447662354, 0.8636056780815125, 0.8541526198387146, 0.8487508296966553, 0.8359216451644897, 0.8507764935493469, 0.8568534851074219, 0.8602295517921448, 0.8622552156448364, 0.870357871055603, 0.8683322072029114, 0.8595543503761292, 0.8804861307144165, 0.8656313419342041]\n","0.8565563750267029\n","0.010453622199789188\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycvokYraFmBS","executionInfo":{"status":"ok","timestamp":1690576924494,"user_tz":-330,"elapsed":10,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"61ac41a2-540c-48d6-83bf-284fb1b38d70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3997068405151367, 0.3608960211277008, 0.3593278229236603, 0.3555915951728821, 0.3736473321914673, 0.33036673069000244, 0.33292606472969055, 0.3382280468940735, 0.3336232304573059, 0.35300299525260925, 0.32032451033592224, 0.32081446051597595, 0.3281874358654022, 0.35784032940864563, 0.37608495354652405, 0.4069492518901825, 0.37704765796661377, 0.3344019055366516, 0.34934309124946594, 0.3230357766151428, 0.3311912715435028, 0.32279670238494873, 0.3461233675479889, 0.31279829144477844, 0.34151947498321533]\n","0.3474310064315796\n","0.02421170079697197\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hw6DQ_SEFmDs","executionInfo":{"status":"ok","timestamp":1690576924494,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"e4894035-3316-4197-d3e6-cc954254d9e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8578197956085205, 0.8682529926300049, 0.861968457698822, 0.8742101192474365, 0.8822856545448303, 0.8805722594261169, 0.8801313042640686, 0.8902044892311096, 0.8843019604682922, 0.8713645339012146, 0.8894966840744019, 0.8915794491767883, 0.8887665271759033, 0.8751444816589355, 0.8735890984535217, 0.8588029742240906, 0.8740739822387695, 0.8816962838172913, 0.883773148059845, 0.8880350589752197, 0.8926173448562622, 0.8913648724555969, 0.8807339072227478, 0.9039609432220459, 0.8868674635887146]\n","0.8804645514488221\n","0.011005612584019817\n"]}]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing -\n","1. Accuracy - 0.855/0.011\n","2. Loss - 0.344/0.025\n","3. F1 score - 0.879/0.012\n","\n","LSTM CNN LSTM nothing -\n","1. Accuracy - 0.856/0.010\n","2. Loss - 0.347/0.024\n","3. F1 score - 0.880/0.011"],"metadata":{"id":"VQ7SVMk3QnFS"}},{"cell_type":"code","source":[],"metadata":{"id":"bKQpvFLZJXv2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AWLlq2odQxJp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN layer to emotions with maxpooling (pool size 5 and stride 2) (without additional LSTM layer in emotions)"],"metadata":{"id":"H4ShX0w_Qxv4"}},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        #cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SnpaiSlAQ9pi","executionInfo":{"status":"ok","timestamp":1690578347173,"user_tz":-330,"elapsed":763469,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"9095c3fa-3753-4c50-d4e5-690e2256a674"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 12s 37ms/step - loss: 0.6395 - accuracy: 0.6447 - f1_score: 0.6331 - val_loss: 0.5144 - val_accuracy: 0.7586 - val_f1_score: 0.7327\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.5145 - accuracy: 0.7541 - f1_score: 0.7423 - val_loss: 0.4053 - val_accuracy: 0.8345 - val_f1_score: 0.8320\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4261 - accuracy: 0.8035 - f1_score: 0.7973 - val_loss: 0.3882 - val_accuracy: 0.8246 - val_f1_score: 0.7992\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4026 - accuracy: 0.8237 - f1_score: 0.8178 - val_loss: 0.3543 - val_accuracy: 0.8608 - val_f1_score: 0.8569\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3586 - accuracy: 0.8535 - f1_score: 0.8504 - val_loss: 0.3180 - val_accuracy: 0.8653 - val_f1_score: 0.8668\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3447 - accuracy: 0.8559 - f1_score: 0.8527 - val_loss: 0.3627 - val_accuracy: 0.8544 - val_f1_score: 0.8668\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3304 - accuracy: 0.8590 - f1_score: 0.8567 - val_loss: 0.3760 - val_accuracy: 0.8382 - val_f1_score: 0.8546\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3163 - accuracy: 0.8701 - f1_score: 0.8684 - val_loss: 0.2978 - val_accuracy: 0.8797 - val_f1_score: 0.8785\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3084 - accuracy: 0.8782 - f1_score: 0.8770 - val_loss: 0.2930 - val_accuracy: 0.8843 - val_f1_score: 0.8843\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3062 - accuracy: 0.8749 - f1_score: 0.8716 - val_loss: 0.3016 - val_accuracy: 0.8807 - val_f1_score: 0.8809\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3039 - accuracy: 0.8810 - f1_score: 0.8789 - val_loss: 0.3778 - val_accuracy: 0.8391 - val_f1_score: 0.8548\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2868 - accuracy: 0.8873 - f1_score: 0.8854 - val_loss: 0.2968 - val_accuracy: 0.8752 - val_f1_score: 0.8752\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2842 - accuracy: 0.8894 - f1_score: 0.8873 - val_loss: 0.2955 - val_accuracy: 0.8807 - val_f1_score: 0.8782\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2632 - accuracy: 0.8984 - f1_score: 0.8973 - val_loss: 0.3171 - val_accuracy: 0.8698 - val_f1_score: 0.8618\n","47/47 [==============================] - 1s 10ms/step - loss: 0.3407 - accuracy: 0.8562 - f1_score: 0.8795\n","Epoch 1/20\n","104/104 [==============================] - 11s 22ms/step - loss: 0.6319 - accuracy: 0.6504 - f1_score: 0.6065 - val_loss: 0.5704 - val_accuracy: 0.7179 - val_f1_score: 0.7301\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4999 - accuracy: 0.7673 - f1_score: 0.7613 - val_loss: 0.4611 - val_accuracy: 0.7776 - val_f1_score: 0.7788\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4190 - accuracy: 0.8107 - f1_score: 0.8064 - val_loss: 0.4065 - val_accuracy: 0.8246 - val_f1_score: 0.8220\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3825 - accuracy: 0.8327 - f1_score: 0.8321 - val_loss: 0.3764 - val_accuracy: 0.8418 - val_f1_score: 0.8384\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3511 - accuracy: 0.8496 - f1_score: 0.8487 - val_loss: 0.3966 - val_accuracy: 0.8183 - val_f1_score: 0.8326\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3324 - accuracy: 0.8590 - f1_score: 0.8578 - val_loss: 0.5017 - val_accuracy: 0.7920 - val_f1_score: 0.7467\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3303 - accuracy: 0.8577 - f1_score: 0.8566 - val_loss: 0.4535 - val_accuracy: 0.7911 - val_f1_score: 0.8168\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3126 - accuracy: 0.8686 - f1_score: 0.8687 - val_loss: 0.3575 - val_accuracy: 0.8544 - val_f1_score: 0.8426\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2996 - accuracy: 0.8764 - f1_score: 0.8760 - val_loss: 0.3322 - val_accuracy: 0.8635 - val_f1_score: 0.8621\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2893 - accuracy: 0.8867 - f1_score: 0.8863 - val_loss: 0.3562 - val_accuracy: 0.8526 - val_f1_score: 0.8576\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2869 - accuracy: 0.8761 - f1_score: 0.8749 - val_loss: 0.3495 - val_accuracy: 0.8508 - val_f1_score: 0.8536\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2724 - accuracy: 0.8852 - f1_score: 0.8849 - val_loss: 0.3604 - val_accuracy: 0.8599 - val_f1_score: 0.8612\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2832 - accuracy: 0.8822 - f1_score: 0.8814 - val_loss: 0.3516 - val_accuracy: 0.8571 - val_f1_score: 0.8582\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2649 - accuracy: 0.8870 - f1_score: 0.8863 - val_loss: 0.3843 - val_accuracy: 0.8481 - val_f1_score: 0.8340\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8623 - f1_score: 0.8854\n","Epoch 1/20\n","104/104 [==============================] - 9s 23ms/step - loss: 0.6288 - accuracy: 0.6501 - f1_score: 0.6375 - val_loss: 0.6274 - val_accuracy: 0.6401 - val_f1_score: 0.7065\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4962 - accuracy: 0.7604 - f1_score: 0.7485 - val_loss: 0.5122 - val_accuracy: 0.7450 - val_f1_score: 0.7573\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4277 - accuracy: 0.8065 - f1_score: 0.7998 - val_loss: 0.4130 - val_accuracy: 0.8101 - val_f1_score: 0.8008\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3808 - accuracy: 0.8354 - f1_score: 0.8306 - val_loss: 0.4124 - val_accuracy: 0.8083 - val_f1_score: 0.7876\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3495 - accuracy: 0.8562 - f1_score: 0.8538 - val_loss: 0.3900 - val_accuracy: 0.8282 - val_f1_score: 0.8177\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3217 - accuracy: 0.8653 - f1_score: 0.8628 - val_loss: 0.4020 - val_accuracy: 0.8156 - val_f1_score: 0.8253\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3301 - accuracy: 0.8605 - f1_score: 0.8591 - val_loss: 0.3767 - val_accuracy: 0.8382 - val_f1_score: 0.8353\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3081 - accuracy: 0.8816 - f1_score: 0.8799 - val_loss: 0.3962 - val_accuracy: 0.8309 - val_f1_score: 0.8244\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2926 - accuracy: 0.8873 - f1_score: 0.8865 - val_loss: 0.4002 - val_accuracy: 0.8336 - val_f1_score: 0.8354\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2849 - accuracy: 0.8894 - f1_score: 0.8881 - val_loss: 0.4050 - val_accuracy: 0.8282 - val_f1_score: 0.8304\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2770 - accuracy: 0.8873 - f1_score: 0.8869 - val_loss: 0.4553 - val_accuracy: 0.8146 - val_f1_score: 0.7884\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2801 - accuracy: 0.8855 - f1_score: 0.8836 - val_loss: 0.4094 - val_accuracy: 0.8228 - val_f1_score: 0.8130\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8535 - f1_score: 0.8765\n","Epoch 1/20\n","104/104 [==============================] - 11s 21ms/step - loss: 0.6403 - accuracy: 0.6293 - f1_score: 0.6125 - val_loss: 0.5313 - val_accuracy: 0.7459 - val_f1_score: 0.7493\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.5039 - accuracy: 0.7643 - f1_score: 0.7539 - val_loss: 0.4737 - val_accuracy: 0.7866 - val_f1_score: 0.8133\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4178 - accuracy: 0.8165 - f1_score: 0.8110 - val_loss: 0.4272 - val_accuracy: 0.8228 - val_f1_score: 0.8383\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3696 - accuracy: 0.8403 - f1_score: 0.8368 - val_loss: 0.3956 - val_accuracy: 0.8373 - val_f1_score: 0.8490\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3396 - accuracy: 0.8617 - f1_score: 0.8594 - val_loss: 0.3528 - val_accuracy: 0.8553 - val_f1_score: 0.8592\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3236 - accuracy: 0.8635 - f1_score: 0.8615 - val_loss: 0.4072 - val_accuracy: 0.8400 - val_f1_score: 0.8524\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3161 - accuracy: 0.8707 - f1_score: 0.8693 - val_loss: 0.3657 - val_accuracy: 0.8409 - val_f1_score: 0.8261\n","Epoch 8/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3026 - accuracy: 0.8794 - f1_score: 0.8775 - val_loss: 0.4022 - val_accuracy: 0.8228 - val_f1_score: 0.8404\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3039 - accuracy: 0.8764 - f1_score: 0.8752 - val_loss: 0.3631 - val_accuracy: 0.8499 - val_f1_score: 0.8588\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2784 - accuracy: 0.8909 - f1_score: 0.8897 - val_loss: 0.3462 - val_accuracy: 0.8653 - val_f1_score: 0.8639\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2849 - accuracy: 0.8867 - f1_score: 0.8856 - val_loss: 0.4305 - val_accuracy: 0.8156 - val_f1_score: 0.8360\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2724 - accuracy: 0.8906 - f1_score: 0.8896 - val_loss: 0.3546 - val_accuracy: 0.8671 - val_f1_score: 0.8638\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2638 - accuracy: 0.9011 - f1_score: 0.9002 - val_loss: 0.3566 - val_accuracy: 0.8635 - val_f1_score: 0.8561\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2587 - accuracy: 0.8948 - f1_score: 0.8936 - val_loss: 0.3328 - val_accuracy: 0.8689 - val_f1_score: 0.8681\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2450 - accuracy: 0.9072 - f1_score: 0.9059 - val_loss: 0.3521 - val_accuracy: 0.8626 - val_f1_score: 0.8660\n","Epoch 16/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2275 - accuracy: 0.9165 - f1_score: 0.9157 - val_loss: 0.3932 - val_accuracy: 0.8508 - val_f1_score: 0.8564\n","Epoch 17/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2323 - accuracy: 0.9108 - f1_score: 0.9101 - val_loss: 0.3602 - val_accuracy: 0.8526 - val_f1_score: 0.8549\n","Epoch 18/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2141 - accuracy: 0.9210 - f1_score: 0.9205 - val_loss: 0.3766 - val_accuracy: 0.8644 - val_f1_score: 0.8611\n","Epoch 19/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2156 - accuracy: 0.9186 - f1_score: 0.9175 - val_loss: 0.4768 - val_accuracy: 0.8345 - val_f1_score: 0.8494\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3500 - accuracy: 0.8548 - f1_score: 0.8759\n","Epoch 1/20\n","104/104 [==============================] - 9s 30ms/step - loss: 0.6268 - accuracy: 0.6495 - f1_score: 0.6376 - val_loss: 0.6329 - val_accuracy: 0.6438 - val_f1_score: 0.7229\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.5021 - accuracy: 0.7640 - f1_score: 0.7539 - val_loss: 0.4384 - val_accuracy: 0.8029 - val_f1_score: 0.8146\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4223 - accuracy: 0.8059 - f1_score: 0.7996 - val_loss: 0.3678 - val_accuracy: 0.8436 - val_f1_score: 0.8446\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3917 - accuracy: 0.8273 - f1_score: 0.8214 - val_loss: 0.3542 - val_accuracy: 0.8571 - val_f1_score: 0.8512\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3609 - accuracy: 0.8445 - f1_score: 0.8411 - val_loss: 0.3408 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3378 - accuracy: 0.8623 - f1_score: 0.8607 - val_loss: 0.3513 - val_accuracy: 0.8544 - val_f1_score: 0.8453\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3235 - accuracy: 0.8662 - f1_score: 0.8644 - val_loss: 0.3365 - val_accuracy: 0.8580 - val_f1_score: 0.8582\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3093 - accuracy: 0.8704 - f1_score: 0.8684 - val_loss: 0.3732 - val_accuracy: 0.8336 - val_f1_score: 0.8477\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2982 - accuracy: 0.8779 - f1_score: 0.8762 - val_loss: 0.3408 - val_accuracy: 0.8608 - val_f1_score: 0.8539\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2869 - accuracy: 0.8870 - f1_score: 0.8854 - val_loss: 0.3746 - val_accuracy: 0.8418 - val_f1_score: 0.8521\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2780 - accuracy: 0.8855 - f1_score: 0.8830 - val_loss: 0.3327 - val_accuracy: 0.8734 - val_f1_score: 0.8723\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2732 - accuracy: 0.8894 - f1_score: 0.8876 - val_loss: 0.3439 - val_accuracy: 0.8662 - val_f1_score: 0.8593\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2656 - accuracy: 0.8969 - f1_score: 0.8957 - val_loss: 0.3646 - val_accuracy: 0.8499 - val_f1_score: 0.8584\n","Epoch 14/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2541 - accuracy: 0.8915 - f1_score: 0.8900 - val_loss: 0.3408 - val_accuracy: 0.8716 - val_f1_score: 0.8678\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2602 - accuracy: 0.8963 - f1_score: 0.8945 - val_loss: 0.3605 - val_accuracy: 0.8608 - val_f1_score: 0.8496\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2488 - accuracy: 0.9020 - f1_score: 0.9002 - val_loss: 0.3350 - val_accuracy: 0.8689 - val_f1_score: 0.8681\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3688 - accuracy: 0.8481 - f1_score: 0.8700\n","Epoch 1/20\n","104/104 [==============================] - 9s 32ms/step - loss: 0.6288 - accuracy: 0.6338 - f1_score: 0.5954 - val_loss: 0.5269 - val_accuracy: 0.7405 - val_f1_score: 0.7216\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4801 - accuracy: 0.7776 - f1_score: 0.7700 - val_loss: 0.3975 - val_accuracy: 0.8219 - val_f1_score: 0.8129\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4110 - accuracy: 0.8177 - f1_score: 0.8136 - val_loss: 0.3482 - val_accuracy: 0.8463 - val_f1_score: 0.8426\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3711 - accuracy: 0.8388 - f1_score: 0.8375 - val_loss: 0.3285 - val_accuracy: 0.8635 - val_f1_score: 0.8643\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3375 - accuracy: 0.8556 - f1_score: 0.8553 - val_loss: 0.3073 - val_accuracy: 0.8779 - val_f1_score: 0.8756\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3204 - accuracy: 0.8635 - f1_score: 0.8633 - val_loss: 0.2990 - val_accuracy: 0.8852 - val_f1_score: 0.8829\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3002 - accuracy: 0.8743 - f1_score: 0.8744 - val_loss: 0.3396 - val_accuracy: 0.8517 - val_f1_score: 0.8647\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2998 - accuracy: 0.8840 - f1_score: 0.8838 - val_loss: 0.2840 - val_accuracy: 0.8879 - val_f1_score: 0.8852\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2880 - accuracy: 0.8849 - f1_score: 0.8853 - val_loss: 0.2845 - val_accuracy: 0.8870 - val_f1_score: 0.8848\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2822 - accuracy: 0.8879 - f1_score: 0.8873 - val_loss: 0.2937 - val_accuracy: 0.8788 - val_f1_score: 0.8812\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2838 - accuracy: 0.8882 - f1_score: 0.8875 - val_loss: 0.2991 - val_accuracy: 0.8716 - val_f1_score: 0.8709\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2641 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.2974 - val_accuracy: 0.8779 - val_f1_score: 0.8683\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2706 - accuracy: 0.8861 - f1_score: 0.8856 - val_loss: 0.3037 - val_accuracy: 0.8770 - val_f1_score: 0.8700\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3488 - accuracy: 0.8528 - f1_score: 0.8760\n","Epoch 1/20\n","104/104 [==============================] - 10s 21ms/step - loss: 0.6090 - accuracy: 0.6679 - f1_score: 0.6586 - val_loss: 0.5244 - val_accuracy: 0.7432 - val_f1_score: 0.7399\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4662 - accuracy: 0.7821 - f1_score: 0.7755 - val_loss: 0.3908 - val_accuracy: 0.8255 - val_f1_score: 0.8266\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3904 - accuracy: 0.8267 - f1_score: 0.8250 - val_loss: 0.4580 - val_accuracy: 0.7821 - val_f1_score: 0.8159\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3716 - accuracy: 0.8415 - f1_score: 0.8404 - val_loss: 0.3207 - val_accuracy: 0.8635 - val_f1_score: 0.8611\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3323 - accuracy: 0.8638 - f1_score: 0.8624 - val_loss: 0.3257 - val_accuracy: 0.8571 - val_f1_score: 0.8616\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3165 - accuracy: 0.8689 - f1_score: 0.8687 - val_loss: 0.3157 - val_accuracy: 0.8689 - val_f1_score: 0.8740\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3142 - accuracy: 0.8665 - f1_score: 0.8660 - val_loss: 0.3295 - val_accuracy: 0.8626 - val_f1_score: 0.8714\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2932 - accuracy: 0.8822 - f1_score: 0.8822 - val_loss: 0.3245 - val_accuracy: 0.8617 - val_f1_score: 0.8516\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2865 - accuracy: 0.8879 - f1_score: 0.8867 - val_loss: 0.3391 - val_accuracy: 0.8635 - val_f1_score: 0.8726\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2736 - accuracy: 0.8918 - f1_score: 0.8908 - val_loss: 0.3191 - val_accuracy: 0.8644 - val_f1_score: 0.8718\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2683 - accuracy: 0.8951 - f1_score: 0.8947 - val_loss: 0.3413 - val_accuracy: 0.8544 - val_f1_score: 0.8666\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3237 - accuracy: 0.8589 - f1_score: 0.8857\n","Epoch 1/20\n","104/104 [==============================] - 9s 30ms/step - loss: 0.6329 - accuracy: 0.6549 - f1_score: 0.6310 - val_loss: 0.5515 - val_accuracy: 0.7170 - val_f1_score: 0.7193\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4856 - accuracy: 0.7661 - f1_score: 0.7563 - val_loss: 0.4870 - val_accuracy: 0.7703 - val_f1_score: 0.7901\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4115 - accuracy: 0.8140 - f1_score: 0.8096 - val_loss: 0.4065 - val_accuracy: 0.8210 - val_f1_score: 0.8275\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3597 - accuracy: 0.8376 - f1_score: 0.8342 - val_loss: 0.3700 - val_accuracy: 0.8463 - val_f1_score: 0.8490\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3353 - accuracy: 0.8608 - f1_score: 0.8597 - val_loss: 0.3664 - val_accuracy: 0.8463 - val_f1_score: 0.8529\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3270 - accuracy: 0.8611 - f1_score: 0.8603 - val_loss: 0.3778 - val_accuracy: 0.8264 - val_f1_score: 0.8411\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2923 - accuracy: 0.8807 - f1_score: 0.8792 - val_loss: 0.3808 - val_accuracy: 0.8409 - val_f1_score: 0.8523\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2809 - accuracy: 0.8837 - f1_score: 0.8839 - val_loss: 0.3859 - val_accuracy: 0.8309 - val_f1_score: 0.8466\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2822 - accuracy: 0.8855 - f1_score: 0.8849 - val_loss: 0.3367 - val_accuracy: 0.8644 - val_f1_score: 0.8580\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2694 - accuracy: 0.8936 - f1_score: 0.8926 - val_loss: 0.3223 - val_accuracy: 0.8707 - val_f1_score: 0.8717\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2712 - accuracy: 0.8876 - f1_score: 0.8870 - val_loss: 0.3361 - val_accuracy: 0.8644 - val_f1_score: 0.8569\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2477 - accuracy: 0.9051 - f1_score: 0.9044 - val_loss: 0.3401 - val_accuracy: 0.8590 - val_f1_score: 0.8520\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2449 - accuracy: 0.8987 - f1_score: 0.8980 - val_loss: 0.3333 - val_accuracy: 0.8617 - val_f1_score: 0.8571\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2627 - accuracy: 0.8927 - f1_score: 0.8921 - val_loss: 0.3341 - val_accuracy: 0.8680 - val_f1_score: 0.8633\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2256 - accuracy: 0.9150 - f1_score: 0.9144 - val_loss: 0.3721 - val_accuracy: 0.8626 - val_f1_score: 0.8585\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3375 - accuracy: 0.8569 - f1_score: 0.8813\n","Epoch 1/20\n","104/104 [==============================] - 7s 14ms/step - loss: 0.6170 - accuracy: 0.6694 - f1_score: 0.6784 - val_loss: 0.5167 - val_accuracy: 0.7586 - val_f1_score: 0.7284\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4871 - accuracy: 0.7673 - f1_score: 0.7543 - val_loss: 0.5435 - val_accuracy: 0.7414 - val_f1_score: 0.7856\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4082 - accuracy: 0.8183 - f1_score: 0.8129 - val_loss: 0.3874 - val_accuracy: 0.8219 - val_f1_score: 0.8323\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3600 - accuracy: 0.8457 - f1_score: 0.8424 - val_loss: 0.3351 - val_accuracy: 0.8526 - val_f1_score: 0.8484\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3327 - accuracy: 0.8583 - f1_score: 0.8566 - val_loss: 0.3205 - val_accuracy: 0.8617 - val_f1_score: 0.8585\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3229 - accuracy: 0.8662 - f1_score: 0.8650 - val_loss: 0.3234 - val_accuracy: 0.8644 - val_f1_score: 0.8670\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3098 - accuracy: 0.8728 - f1_score: 0.8721 - val_loss: 0.3592 - val_accuracy: 0.8436 - val_f1_score: 0.8567\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2996 - accuracy: 0.8755 - f1_score: 0.8750 - val_loss: 0.3898 - val_accuracy: 0.8273 - val_f1_score: 0.8446\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2874 - accuracy: 0.8813 - f1_score: 0.8809 - val_loss: 0.3107 - val_accuracy: 0.8734 - val_f1_score: 0.8765\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2903 - accuracy: 0.8797 - f1_score: 0.8785 - val_loss: 0.3014 - val_accuracy: 0.8725 - val_f1_score: 0.8744\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2696 - accuracy: 0.8909 - f1_score: 0.8900 - val_loss: 0.3009 - val_accuracy: 0.8743 - val_f1_score: 0.8738\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2657 - accuracy: 0.8918 - f1_score: 0.8906 - val_loss: 0.3049 - val_accuracy: 0.8834 - val_f1_score: 0.8845\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2584 - accuracy: 0.8978 - f1_score: 0.8968 - val_loss: 0.3215 - val_accuracy: 0.8743 - val_f1_score: 0.8690\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2489 - accuracy: 0.9014 - f1_score: 0.9009 - val_loss: 0.3368 - val_accuracy: 0.8644 - val_f1_score: 0.8544\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2367 - accuracy: 0.9078 - f1_score: 0.9073 - val_loss: 0.3130 - val_accuracy: 0.8752 - val_f1_score: 0.8688\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2311 - accuracy: 0.9039 - f1_score: 0.9034 - val_loss: 0.3208 - val_accuracy: 0.8752 - val_f1_score: 0.8676\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3425 - accuracy: 0.8609 - f1_score: 0.8864\n","Epoch 1/20\n","104/104 [==============================] - 6s 19ms/step - loss: 0.6454 - accuracy: 0.6293 - f1_score: 0.5805 - val_loss: 0.5367 - val_accuracy: 0.7486 - val_f1_score: 0.7110\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4874 - accuracy: 0.7655 - f1_score: 0.7584 - val_loss: 0.4139 - val_accuracy: 0.8146 - val_f1_score: 0.8023\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4053 - accuracy: 0.8195 - f1_score: 0.8167 - val_loss: 0.3721 - val_accuracy: 0.8382 - val_f1_score: 0.8437\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3621 - accuracy: 0.8388 - f1_score: 0.8367 - val_loss: 0.3507 - val_accuracy: 0.8427 - val_f1_score: 0.8365\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3377 - accuracy: 0.8568 - f1_score: 0.8565 - val_loss: 0.3583 - val_accuracy: 0.8508 - val_f1_score: 0.8378\n","Epoch 6/20\n","104/104 [==============================] - 1s 7ms/step - loss: 0.3205 - accuracy: 0.8719 - f1_score: 0.8713 - val_loss: 0.3324 - val_accuracy: 0.8580 - val_f1_score: 0.8622\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3140 - accuracy: 0.8704 - f1_score: 0.8688 - val_loss: 0.3175 - val_accuracy: 0.8662 - val_f1_score: 0.8630\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3073 - accuracy: 0.8803 - f1_score: 0.8797 - val_loss: 0.3220 - val_accuracy: 0.8626 - val_f1_score: 0.8618\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2931 - accuracy: 0.8858 - f1_score: 0.8857 - val_loss: 0.3343 - val_accuracy: 0.8544 - val_f1_score: 0.8596\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2879 - accuracy: 0.8828 - f1_score: 0.8831 - val_loss: 0.3212 - val_accuracy: 0.8662 - val_f1_score: 0.8571\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2686 - accuracy: 0.8951 - f1_score: 0.8950 - val_loss: 0.3502 - val_accuracy: 0.8626 - val_f1_score: 0.8492\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2619 - accuracy: 0.8987 - f1_score: 0.8979 - val_loss: 0.3101 - val_accuracy: 0.8734 - val_f1_score: 0.8689\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2678 - accuracy: 0.8969 - f1_score: 0.8966 - val_loss: 0.3055 - val_accuracy: 0.8761 - val_f1_score: 0.8749\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2519 - accuracy: 0.9017 - f1_score: 0.9017 - val_loss: 0.3216 - val_accuracy: 0.8662 - val_f1_score: 0.8693\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2449 - accuracy: 0.9057 - f1_score: 0.9056 - val_loss: 0.3726 - val_accuracy: 0.8490 - val_f1_score: 0.8291\n","Epoch 16/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2367 - accuracy: 0.9087 - f1_score: 0.9085 - val_loss: 0.2971 - val_accuracy: 0.8797 - val_f1_score: 0.8779\n","Epoch 17/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2255 - accuracy: 0.9108 - f1_score: 0.9104 - val_loss: 0.3334 - val_accuracy: 0.8734 - val_f1_score: 0.8669\n","Epoch 18/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2181 - accuracy: 0.9171 - f1_score: 0.9167 - val_loss: 0.5623 - val_accuracy: 0.8065 - val_f1_score: 0.7648\n","Epoch 19/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2106 - accuracy: 0.9201 - f1_score: 0.9196 - val_loss: 0.3768 - val_accuracy: 0.8680 - val_f1_score: 0.8546\n","Epoch 20/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2072 - accuracy: 0.9183 - f1_score: 0.9177 - val_loss: 0.3162 - val_accuracy: 0.8725 - val_f1_score: 0.8726\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3740 - accuracy: 0.8636 - f1_score: 0.8902\n","Epoch 1/20\n","104/104 [==============================] - 6s 13ms/step - loss: 0.6320 - accuracy: 0.6389 - f1_score: 0.6044 - val_loss: 0.5200 - val_accuracy: 0.7514 - val_f1_score: 0.7594\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4884 - accuracy: 0.7619 - f1_score: 0.7539 - val_loss: 0.4637 - val_accuracy: 0.7920 - val_f1_score: 0.7594\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4084 - accuracy: 0.8125 - f1_score: 0.8083 - val_loss: 0.4206 - val_accuracy: 0.8110 - val_f1_score: 0.7848\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3577 - accuracy: 0.8400 - f1_score: 0.8372 - val_loss: 0.3616 - val_accuracy: 0.8400 - val_f1_score: 0.8378\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3447 - accuracy: 0.8535 - f1_score: 0.8524 - val_loss: 0.3504 - val_accuracy: 0.8463 - val_f1_score: 0.8375\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3102 - accuracy: 0.8719 - f1_score: 0.8714 - val_loss: 0.3764 - val_accuracy: 0.8526 - val_f1_score: 0.8388\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3032 - accuracy: 0.8734 - f1_score: 0.8726 - val_loss: 0.3786 - val_accuracy: 0.8363 - val_f1_score: 0.8470\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2999 - accuracy: 0.8816 - f1_score: 0.8801 - val_loss: 0.3440 - val_accuracy: 0.8644 - val_f1_score: 0.8549\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2763 - accuracy: 0.8903 - f1_score: 0.8892 - val_loss: 0.3370 - val_accuracy: 0.8626 - val_f1_score: 0.8631\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2776 - accuracy: 0.8858 - f1_score: 0.8837 - val_loss: 0.3342 - val_accuracy: 0.8671 - val_f1_score: 0.8638\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2671 - accuracy: 0.8918 - f1_score: 0.8912 - val_loss: 0.3361 - val_accuracy: 0.8671 - val_f1_score: 0.8674\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2536 - accuracy: 0.8987 - f1_score: 0.8975 - val_loss: 0.3410 - val_accuracy: 0.8644 - val_f1_score: 0.8639\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2475 - accuracy: 0.9027 - f1_score: 0.9020 - val_loss: 0.3559 - val_accuracy: 0.8617 - val_f1_score: 0.8536\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2493 - accuracy: 0.8999 - f1_score: 0.8993 - val_loss: 0.3373 - val_accuracy: 0.8707 - val_f1_score: 0.8660\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2393 - accuracy: 0.9072 - f1_score: 0.9060 - val_loss: 0.4495 - val_accuracy: 0.8192 - val_f1_score: 0.7881\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.8636 - f1_score: 0.8896\n","Epoch 1/20\n","104/104 [==============================] - 6s 13ms/step - loss: 0.6253 - accuracy: 0.6513 - f1_score: 0.6347 - val_loss: 0.5191 - val_accuracy: 0.7315 - val_f1_score: 0.7392\n","Epoch 2/20\n","104/104 [==============================] - 1s 7ms/step - loss: 0.4925 - accuracy: 0.7658 - f1_score: 0.7525 - val_loss: 0.4069 - val_accuracy: 0.8011 - val_f1_score: 0.7868\n","Epoch 3/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4088 - accuracy: 0.8153 - f1_score: 0.8093 - val_loss: 0.3739 - val_accuracy: 0.8391 - val_f1_score: 0.8238\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3616 - accuracy: 0.8442 - f1_score: 0.8392 - val_loss: 0.3415 - val_accuracy: 0.8544 - val_f1_score: 0.8513\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3530 - accuracy: 0.8436 - f1_score: 0.8387 - val_loss: 0.3370 - val_accuracy: 0.8590 - val_f1_score: 0.8485\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3160 - accuracy: 0.8719 - f1_score: 0.8691 - val_loss: 0.3417 - val_accuracy: 0.8571 - val_f1_score: 0.8445\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3278 - accuracy: 0.8617 - f1_score: 0.8587 - val_loss: 0.3116 - val_accuracy: 0.8770 - val_f1_score: 0.8731\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3059 - accuracy: 0.8686 - f1_score: 0.8665 - val_loss: 0.3132 - val_accuracy: 0.8761 - val_f1_score: 0.8753\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2966 - accuracy: 0.8743 - f1_score: 0.8718 - val_loss: 0.3334 - val_accuracy: 0.8544 - val_f1_score: 0.8401\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2879 - accuracy: 0.8825 - f1_score: 0.8794 - val_loss: 0.3143 - val_accuracy: 0.8644 - val_f1_score: 0.8544\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2817 - accuracy: 0.8858 - f1_score: 0.8842 - val_loss: 0.3531 - val_accuracy: 0.8562 - val_f1_score: 0.8427\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2815 - accuracy: 0.8912 - f1_score: 0.8889 - val_loss: 0.3105 - val_accuracy: 0.8843 - val_f1_score: 0.8790\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2629 - accuracy: 0.8906 - f1_score: 0.8885 - val_loss: 0.3101 - val_accuracy: 0.8752 - val_f1_score: 0.8693\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2587 - accuracy: 0.8984 - f1_score: 0.8961 - val_loss: 0.3166 - val_accuracy: 0.8761 - val_f1_score: 0.8701\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2529 - accuracy: 0.8978 - f1_score: 0.8962 - val_loss: 0.3777 - val_accuracy: 0.8400 - val_f1_score: 0.8173\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2562 - accuracy: 0.8903 - f1_score: 0.8879 - val_loss: 0.3241 - val_accuracy: 0.8761 - val_f1_score: 0.8765\n","Epoch 17/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2314 - accuracy: 0.9033 - f1_score: 0.9014 - val_loss: 0.3949 - val_accuracy: 0.8571 - val_f1_score: 0.8420\n","Epoch 18/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2330 - accuracy: 0.9039 - f1_score: 0.9021 - val_loss: 0.3210 - val_accuracy: 0.8779 - val_f1_score: 0.8780\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3176 - accuracy: 0.8650 - f1_score: 0.8918\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6349 - accuracy: 0.6395 - f1_score: 0.6343 - val_loss: 0.5313 - val_accuracy: 0.7396 - val_f1_score: 0.7313\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4990 - accuracy: 0.7595 - f1_score: 0.7459 - val_loss: 0.4446 - val_accuracy: 0.7993 - val_f1_score: 0.8070\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4109 - accuracy: 0.8162 - f1_score: 0.8095 - val_loss: 0.3898 - val_accuracy: 0.8282 - val_f1_score: 0.8276\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3669 - accuracy: 0.8382 - f1_score: 0.8345 - val_loss: 0.3807 - val_accuracy: 0.8445 - val_f1_score: 0.8475\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3437 - accuracy: 0.8502 - f1_score: 0.8469 - val_loss: 0.3667 - val_accuracy: 0.8599 - val_f1_score: 0.8632\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3100 - accuracy: 0.8656 - f1_score: 0.8634 - val_loss: 0.3701 - val_accuracy: 0.8544 - val_f1_score: 0.8502\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3052 - accuracy: 0.8716 - f1_score: 0.8692 - val_loss: 0.3560 - val_accuracy: 0.8635 - val_f1_score: 0.8636\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2921 - accuracy: 0.8837 - f1_score: 0.8818 - val_loss: 0.4152 - val_accuracy: 0.8345 - val_f1_score: 0.8486\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2889 - accuracy: 0.8843 - f1_score: 0.8830 - val_loss: 0.3421 - val_accuracy: 0.8680 - val_f1_score: 0.8682\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2658 - accuracy: 0.8903 - f1_score: 0.8887 - val_loss: 0.3494 - val_accuracy: 0.8635 - val_f1_score: 0.8626\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2675 - accuracy: 0.8888 - f1_score: 0.8877 - val_loss: 0.4294 - val_accuracy: 0.8282 - val_f1_score: 0.8437\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2482 - accuracy: 0.8990 - f1_score: 0.8984 - val_loss: 0.3841 - val_accuracy: 0.8626 - val_f1_score: 0.8696\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2538 - accuracy: 0.8981 - f1_score: 0.8972 - val_loss: 0.3729 - val_accuracy: 0.8680 - val_f1_score: 0.8728\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2320 - accuracy: 0.9078 - f1_score: 0.9072 - val_loss: 0.3782 - val_accuracy: 0.8608 - val_f1_score: 0.8610\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3104 - accuracy: 0.8643 - f1_score: 0.8897\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6255 - accuracy: 0.6507 - f1_score: 0.6345 - val_loss: 0.5284 - val_accuracy: 0.7351 - val_f1_score: 0.6866\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4881 - accuracy: 0.7673 - f1_score: 0.7521 - val_loss: 0.4161 - val_accuracy: 0.8137 - val_f1_score: 0.8147\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4191 - accuracy: 0.8104 - f1_score: 0.8040 - val_loss: 0.3663 - val_accuracy: 0.8363 - val_f1_score: 0.8380\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3671 - accuracy: 0.8354 - f1_score: 0.8313 - val_loss: 0.4816 - val_accuracy: 0.7902 - val_f1_score: 0.8176\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3409 - accuracy: 0.8559 - f1_score: 0.8520 - val_loss: 0.3298 - val_accuracy: 0.8562 - val_f1_score: 0.8571\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3235 - accuracy: 0.8665 - f1_score: 0.8633 - val_loss: 0.5113 - val_accuracy: 0.7667 - val_f1_score: 0.8048\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3159 - accuracy: 0.8644 - f1_score: 0.8613 - val_loss: 0.3721 - val_accuracy: 0.8291 - val_f1_score: 0.8442\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2957 - accuracy: 0.8791 - f1_score: 0.8771 - val_loss: 0.3310 - val_accuracy: 0.8562 - val_f1_score: 0.8614\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2953 - accuracy: 0.8713 - f1_score: 0.8684 - val_loss: 0.3279 - val_accuracy: 0.8635 - val_f1_score: 0.8658\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2772 - accuracy: 0.8867 - f1_score: 0.8844 - val_loss: 0.3436 - val_accuracy: 0.8571 - val_f1_score: 0.8638\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2656 - accuracy: 0.8957 - f1_score: 0.8943 - val_loss: 0.4828 - val_accuracy: 0.8020 - val_f1_score: 0.8282\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2740 - accuracy: 0.8855 - f1_score: 0.8841 - val_loss: 0.3532 - val_accuracy: 0.8463 - val_f1_score: 0.8579\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2524 - accuracy: 0.9024 - f1_score: 0.9016 - val_loss: 0.3983 - val_accuracy: 0.8391 - val_f1_score: 0.8531\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2525 - accuracy: 0.9011 - f1_score: 0.8998 - val_loss: 0.3411 - val_accuracy: 0.8680 - val_f1_score: 0.8744\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3049 - accuracy: 0.8643 - f1_score: 0.8894\n","Epoch 1/20\n","104/104 [==============================] - 6s 26ms/step - loss: 0.6408 - accuracy: 0.6215 - f1_score: 0.6013 - val_loss: 0.6260 - val_accuracy: 0.6564 - val_f1_score: 0.7198\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.5086 - accuracy: 0.7577 - f1_score: 0.7508 - val_loss: 0.4529 - val_accuracy: 0.7776 - val_f1_score: 0.7780\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4252 - accuracy: 0.8071 - f1_score: 0.8019 - val_loss: 0.3973 - val_accuracy: 0.8237 - val_f1_score: 0.8094\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3767 - accuracy: 0.8330 - f1_score: 0.8295 - val_loss: 0.3603 - val_accuracy: 0.8472 - val_f1_score: 0.8484\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3535 - accuracy: 0.8493 - f1_score: 0.8462 - val_loss: 0.3587 - val_accuracy: 0.8517 - val_f1_score: 0.8525\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3352 - accuracy: 0.8590 - f1_score: 0.8573 - val_loss: 0.3969 - val_accuracy: 0.8282 - val_f1_score: 0.8435\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3085 - accuracy: 0.8704 - f1_score: 0.8695 - val_loss: 0.3948 - val_accuracy: 0.8291 - val_f1_score: 0.8037\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3179 - accuracy: 0.8665 - f1_score: 0.8646 - val_loss: 0.3781 - val_accuracy: 0.8309 - val_f1_score: 0.8461\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3008 - accuracy: 0.8758 - f1_score: 0.8745 - val_loss: 0.3214 - val_accuracy: 0.8644 - val_f1_score: 0.8670\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2884 - accuracy: 0.8816 - f1_score: 0.8806 - val_loss: 0.3655 - val_accuracy: 0.8445 - val_f1_score: 0.8562\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2781 - accuracy: 0.8834 - f1_score: 0.8828 - val_loss: 0.3451 - val_accuracy: 0.8445 - val_f1_score: 0.8557\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2710 - accuracy: 0.8939 - f1_score: 0.8930 - val_loss: 0.3233 - val_accuracy: 0.8707 - val_f1_score: 0.8745\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2565 - accuracy: 0.9011 - f1_score: 0.9010 - val_loss: 0.3206 - val_accuracy: 0.8725 - val_f1_score: 0.8703\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2575 - accuracy: 0.8927 - f1_score: 0.8924 - val_loss: 0.3219 - val_accuracy: 0.8653 - val_f1_score: 0.8588\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2490 - accuracy: 0.9020 - f1_score: 0.9010 - val_loss: 0.3322 - val_accuracy: 0.8653 - val_f1_score: 0.8717\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2411 - accuracy: 0.9075 - f1_score: 0.9071 - val_loss: 0.3692 - val_accuracy: 0.8508 - val_f1_score: 0.8615\n","Epoch 17/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2478 - accuracy: 0.8969 - f1_score: 0.8959 - val_loss: 0.3184 - val_accuracy: 0.8680 - val_f1_score: 0.8696\n","Epoch 18/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2308 - accuracy: 0.9072 - f1_score: 0.9067 - val_loss: 0.3249 - val_accuracy: 0.8626 - val_f1_score: 0.8558\n","Epoch 19/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2171 - accuracy: 0.9123 - f1_score: 0.9118 - val_loss: 0.3191 - val_accuracy: 0.8680 - val_f1_score: 0.8651\n","Epoch 20/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2206 - accuracy: 0.9129 - f1_score: 0.9120 - val_loss: 0.3827 - val_accuracy: 0.8336 - val_f1_score: 0.8487\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3606 - accuracy: 0.8663 - f1_score: 0.8972\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6268 - accuracy: 0.6465 - f1_score: 0.6028 - val_loss: 0.5425 - val_accuracy: 0.7324 - val_f1_score: 0.7558\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4790 - accuracy: 0.7794 - f1_score: 0.7727 - val_loss: 0.4305 - val_accuracy: 0.8065 - val_f1_score: 0.8040\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3932 - accuracy: 0.8207 - f1_score: 0.8165 - val_loss: 0.4488 - val_accuracy: 0.7984 - val_f1_score: 0.8226\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3597 - accuracy: 0.8433 - f1_score: 0.8418 - val_loss: 0.3851 - val_accuracy: 0.8345 - val_f1_score: 0.8458\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3362 - accuracy: 0.8556 - f1_score: 0.8540 - val_loss: 0.4538 - val_accuracy: 0.7812 - val_f1_score: 0.8138\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3065 - accuracy: 0.8653 - f1_score: 0.8638 - val_loss: 0.3919 - val_accuracy: 0.8264 - val_f1_score: 0.8434\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2928 - accuracy: 0.8731 - f1_score: 0.8733 - val_loss: 0.3220 - val_accuracy: 0.8671 - val_f1_score: 0.8655\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2901 - accuracy: 0.8819 - f1_score: 0.8807 - val_loss: 0.3253 - val_accuracy: 0.8644 - val_f1_score: 0.8639\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2777 - accuracy: 0.8882 - f1_score: 0.8875 - val_loss: 0.3234 - val_accuracy: 0.8608 - val_f1_score: 0.8668\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2578 - accuracy: 0.8921 - f1_score: 0.8916 - val_loss: 0.3775 - val_accuracy: 0.8309 - val_f1_score: 0.8461\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2571 - accuracy: 0.8984 - f1_score: 0.8978 - val_loss: 0.3334 - val_accuracy: 0.8635 - val_f1_score: 0.8697\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2612 - accuracy: 0.8915 - f1_score: 0.8908 - val_loss: 0.3134 - val_accuracy: 0.8779 - val_f1_score: 0.8776\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2407 - accuracy: 0.8990 - f1_score: 0.8982 - val_loss: 0.3287 - val_accuracy: 0.8761 - val_f1_score: 0.8791\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2280 - accuracy: 0.9117 - f1_score: 0.9112 - val_loss: 0.3726 - val_accuracy: 0.8427 - val_f1_score: 0.8528\n","Epoch 15/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2291 - accuracy: 0.9105 - f1_score: 0.9100 - val_loss: 0.3575 - val_accuracy: 0.8680 - val_f1_score: 0.8574\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2175 - accuracy: 0.9132 - f1_score: 0.9122 - val_loss: 0.3396 - val_accuracy: 0.8725 - val_f1_score: 0.8691\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2128 - accuracy: 0.9183 - f1_score: 0.9181 - val_loss: 0.3709 - val_accuracy: 0.8508 - val_f1_score: 0.8576\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3510 - accuracy: 0.8602 - f1_score: 0.8847\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6228 - accuracy: 0.6471 - f1_score: 0.6512 - val_loss: 0.6344 - val_accuracy: 0.6483 - val_f1_score: 0.4875\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4845 - accuracy: 0.7725 - f1_score: 0.7610 - val_loss: 0.4521 - val_accuracy: 0.7884 - val_f1_score: 0.7759\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4282 - accuracy: 0.8080 - f1_score: 0.8015 - val_loss: 0.4123 - val_accuracy: 0.8020 - val_f1_score: 0.8029\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3814 - accuracy: 0.8342 - f1_score: 0.8312 - val_loss: 0.3754 - val_accuracy: 0.8309 - val_f1_score: 0.8323\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3389 - accuracy: 0.8514 - f1_score: 0.8489 - val_loss: 0.3491 - val_accuracy: 0.8373 - val_f1_score: 0.8330\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3191 - accuracy: 0.8674 - f1_score: 0.8661 - val_loss: 0.3375 - val_accuracy: 0.8535 - val_f1_score: 0.8466\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3020 - accuracy: 0.8728 - f1_score: 0.8709 - val_loss: 0.3472 - val_accuracy: 0.8526 - val_f1_score: 0.8385\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2906 - accuracy: 0.8791 - f1_score: 0.8780 - val_loss: 0.3145 - val_accuracy: 0.8680 - val_f1_score: 0.8628\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2768 - accuracy: 0.8858 - f1_score: 0.8847 - val_loss: 0.3484 - val_accuracy: 0.8454 - val_f1_score: 0.8564\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2750 - accuracy: 0.8879 - f1_score: 0.8868 - val_loss: 0.3188 - val_accuracy: 0.8671 - val_f1_score: 0.8607\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2561 - accuracy: 0.8984 - f1_score: 0.8973 - val_loss: 0.3236 - val_accuracy: 0.8716 - val_f1_score: 0.8734\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2353 - accuracy: 0.9126 - f1_score: 0.9116 - val_loss: 0.3550 - val_accuracy: 0.8571 - val_f1_score: 0.8597\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2335 - accuracy: 0.9060 - f1_score: 0.9051 - val_loss: 0.3260 - val_accuracy: 0.8671 - val_f1_score: 0.8599\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3702 - accuracy: 0.8413 - f1_score: 0.8644\n","Epoch 1/20\n","104/104 [==============================] - 5s 14ms/step - loss: 0.6269 - accuracy: 0.6516 - f1_score: 0.6510 - val_loss: 0.5527 - val_accuracy: 0.7206 - val_f1_score: 0.6547\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4966 - accuracy: 0.7658 - f1_score: 0.7536 - val_loss: 0.4412 - val_accuracy: 0.7893 - val_f1_score: 0.7565\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4370 - accuracy: 0.8062 - f1_score: 0.7970 - val_loss: 0.4350 - val_accuracy: 0.7857 - val_f1_score: 0.7404\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3820 - accuracy: 0.8363 - f1_score: 0.8321 - val_loss: 0.3830 - val_accuracy: 0.8300 - val_f1_score: 0.8082\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3548 - accuracy: 0.8508 - f1_score: 0.8486 - val_loss: 0.3322 - val_accuracy: 0.8526 - val_f1_score: 0.8541\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3224 - accuracy: 0.8656 - f1_score: 0.8647 - val_loss: 0.3228 - val_accuracy: 0.8689 - val_f1_score: 0.8633\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3076 - accuracy: 0.8752 - f1_score: 0.8745 - val_loss: 0.3162 - val_accuracy: 0.8716 - val_f1_score: 0.8690\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2986 - accuracy: 0.8810 - f1_score: 0.8799 - val_loss: 0.3030 - val_accuracy: 0.8797 - val_f1_score: 0.8749\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2863 - accuracy: 0.8897 - f1_score: 0.8885 - val_loss: 0.3110 - val_accuracy: 0.8752 - val_f1_score: 0.8701\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2959 - accuracy: 0.8767 - f1_score: 0.8753 - val_loss: 0.4318 - val_accuracy: 0.8074 - val_f1_score: 0.7677\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2737 - accuracy: 0.8909 - f1_score: 0.8898 - val_loss: 0.2963 - val_accuracy: 0.8797 - val_f1_score: 0.8781\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2588 - accuracy: 0.8990 - f1_score: 0.8977 - val_loss: 0.2964 - val_accuracy: 0.8779 - val_f1_score: 0.8767\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2648 - accuracy: 0.8936 - f1_score: 0.8934 - val_loss: 0.3466 - val_accuracy: 0.8454 - val_f1_score: 0.8271\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2504 - accuracy: 0.9008 - f1_score: 0.9003 - val_loss: 0.2964 - val_accuracy: 0.8852 - val_f1_score: 0.8829\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2407 - accuracy: 0.9069 - f1_score: 0.9063 - val_loss: 0.3232 - val_accuracy: 0.8788 - val_f1_score: 0.8726\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2248 - accuracy: 0.9171 - f1_score: 0.9165 - val_loss: 0.3216 - val_accuracy: 0.8707 - val_f1_score: 0.8672\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3373 - accuracy: 0.8670 - f1_score: 0.8930\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6067 - accuracy: 0.6721 - f1_score: 0.6465 - val_loss: 0.5088 - val_accuracy: 0.7559 - val_f1_score: 0.7581\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4670 - accuracy: 0.7827 - f1_score: 0.7757 - val_loss: 0.4460 - val_accuracy: 0.7857 - val_f1_score: 0.7604\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3854 - accuracy: 0.8330 - f1_score: 0.8301 - val_loss: 0.4511 - val_accuracy: 0.7939 - val_f1_score: 0.8164\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3473 - accuracy: 0.8550 - f1_score: 0.8529 - val_loss: 0.4654 - val_accuracy: 0.7857 - val_f1_score: 0.8129\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3187 - accuracy: 0.8716 - f1_score: 0.8707 - val_loss: 0.3530 - val_accuracy: 0.8454 - val_f1_score: 0.8458\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3075 - accuracy: 0.8719 - f1_score: 0.8714 - val_loss: 0.3616 - val_accuracy: 0.8526 - val_f1_score: 0.8498\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2973 - accuracy: 0.8788 - f1_score: 0.8780 - val_loss: 0.3642 - val_accuracy: 0.8517 - val_f1_score: 0.8586\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2894 - accuracy: 0.8864 - f1_score: 0.8857 - val_loss: 0.3703 - val_accuracy: 0.8517 - val_f1_score: 0.8596\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2766 - accuracy: 0.8915 - f1_score: 0.8904 - val_loss: 0.4029 - val_accuracy: 0.8445 - val_f1_score: 0.8283\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2809 - accuracy: 0.8906 - f1_score: 0.8892 - val_loss: 0.3380 - val_accuracy: 0.8662 - val_f1_score: 0.8657\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2545 - accuracy: 0.9017 - f1_score: 0.9007 - val_loss: 0.3500 - val_accuracy: 0.8535 - val_f1_score: 0.8569\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2629 - accuracy: 0.8966 - f1_score: 0.8962 - val_loss: 0.3715 - val_accuracy: 0.8499 - val_f1_score: 0.8385\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2511 - accuracy: 0.9072 - f1_score: 0.9063 - val_loss: 0.3615 - val_accuracy: 0.8599 - val_f1_score: 0.8494\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2374 - accuracy: 0.9078 - f1_score: 0.9070 - val_loss: 0.3505 - val_accuracy: 0.8571 - val_f1_score: 0.8592\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2376 - accuracy: 0.9093 - f1_score: 0.9084 - val_loss: 0.3431 - val_accuracy: 0.8734 - val_f1_score: 0.8711\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.8623 - f1_score: 0.8872\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6243 - accuracy: 0.6588 - f1_score: 0.6561 - val_loss: 0.4981 - val_accuracy: 0.7568 - val_f1_score: 0.7183\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4692 - accuracy: 0.7848 - f1_score: 0.7795 - val_loss: 0.4459 - val_accuracy: 0.7884 - val_f1_score: 0.7423\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3943 - accuracy: 0.8222 - f1_score: 0.8182 - val_loss: 0.3040 - val_accuracy: 0.8725 - val_f1_score: 0.8735\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3626 - accuracy: 0.8439 - f1_score: 0.8426 - val_loss: 0.3030 - val_accuracy: 0.8707 - val_f1_score: 0.8773\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3546 - accuracy: 0.8439 - f1_score: 0.8430 - val_loss: 0.3007 - val_accuracy: 0.8743 - val_f1_score: 0.8809\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3151 - accuracy: 0.8701 - f1_score: 0.8690 - val_loss: 0.3076 - val_accuracy: 0.8698 - val_f1_score: 0.8792\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3112 - accuracy: 0.8725 - f1_score: 0.8712 - val_loss: 0.3025 - val_accuracy: 0.8752 - val_f1_score: 0.8834\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2980 - accuracy: 0.8755 - f1_score: 0.8738 - val_loss: 0.2591 - val_accuracy: 0.9014 - val_f1_score: 0.8995\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2951 - accuracy: 0.8800 - f1_score: 0.8784 - val_loss: 0.3208 - val_accuracy: 0.8608 - val_f1_score: 0.8721\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2842 - accuracy: 0.8819 - f1_score: 0.8792 - val_loss: 0.2577 - val_accuracy: 0.9033 - val_f1_score: 0.9010\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2754 - accuracy: 0.8855 - f1_score: 0.8847 - val_loss: 0.2860 - val_accuracy: 0.8897 - val_f1_score: 0.8954\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2904 - accuracy: 0.8764 - f1_score: 0.8753 - val_loss: 0.2927 - val_accuracy: 0.8834 - val_f1_score: 0.8893\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2579 - accuracy: 0.8966 - f1_score: 0.8956 - val_loss: 0.2841 - val_accuracy: 0.8852 - val_f1_score: 0.8899\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2527 - accuracy: 0.8972 - f1_score: 0.8961 - val_loss: 0.2670 - val_accuracy: 0.8942 - val_f1_score: 0.8935\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2445 - accuracy: 0.9069 - f1_score: 0.9060 - val_loss: 0.2796 - val_accuracy: 0.8951 - val_f1_score: 0.8986\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.8602 - f1_score: 0.8830\n","Epoch 1/20\n","104/104 [==============================] - 5s 17ms/step - loss: 0.6213 - accuracy: 0.6582 - f1_score: 0.6515 - val_loss: 0.5331 - val_accuracy: 0.7315 - val_f1_score: 0.7437\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4822 - accuracy: 0.7728 - f1_score: 0.7630 - val_loss: 0.4286 - val_accuracy: 0.8002 - val_f1_score: 0.8046\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4074 - accuracy: 0.8101 - f1_score: 0.8056 - val_loss: 0.3842 - val_accuracy: 0.8345 - val_f1_score: 0.8326\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3809 - accuracy: 0.8409 - f1_score: 0.8377 - val_loss: 0.3749 - val_accuracy: 0.8463 - val_f1_score: 0.8496\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3436 - accuracy: 0.8608 - f1_score: 0.8581 - val_loss: 0.4050 - val_accuracy: 0.8291 - val_f1_score: 0.8421\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3331 - accuracy: 0.8662 - f1_score: 0.8646 - val_loss: 0.3735 - val_accuracy: 0.8436 - val_f1_score: 0.8538\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3183 - accuracy: 0.8728 - f1_score: 0.8704 - val_loss: 0.3606 - val_accuracy: 0.8400 - val_f1_score: 0.8481\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2972 - accuracy: 0.8831 - f1_score: 0.8813 - val_loss: 0.3822 - val_accuracy: 0.8345 - val_f1_score: 0.8458\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2929 - accuracy: 0.8846 - f1_score: 0.8836 - val_loss: 0.3341 - val_accuracy: 0.8653 - val_f1_score: 0.8642\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2846 - accuracy: 0.8888 - f1_score: 0.8875 - val_loss: 0.3431 - val_accuracy: 0.8608 - val_f1_score: 0.8640\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2729 - accuracy: 0.8948 - f1_score: 0.8938 - val_loss: 0.3363 - val_accuracy: 0.8626 - val_f1_score: 0.8558\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2856 - accuracy: 0.8891 - f1_score: 0.8875 - val_loss: 0.3476 - val_accuracy: 0.8526 - val_f1_score: 0.8571\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2670 - accuracy: 0.9008 - f1_score: 0.8999 - val_loss: 0.3612 - val_accuracy: 0.8626 - val_f1_score: 0.8563\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2494 - accuracy: 0.9030 - f1_score: 0.9016 - val_loss: 0.3352 - val_accuracy: 0.8590 - val_f1_score: 0.8602\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8717 - f1_score: 0.8936\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6392 - accuracy: 0.6347 - f1_score: 0.6026 - val_loss: 0.5535 - val_accuracy: 0.7260 - val_f1_score: 0.6827\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4910 - accuracy: 0.7725 - f1_score: 0.7647 - val_loss: 0.4253 - val_accuracy: 0.8038 - val_f1_score: 0.8088\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3952 - accuracy: 0.8231 - f1_score: 0.8200 - val_loss: 0.3977 - val_accuracy: 0.8427 - val_f1_score: 0.8487\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3550 - accuracy: 0.8460 - f1_score: 0.8442 - val_loss: 0.4257 - val_accuracy: 0.8192 - val_f1_score: 0.8344\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3307 - accuracy: 0.8605 - f1_score: 0.8594 - val_loss: 0.3733 - val_accuracy: 0.8472 - val_f1_score: 0.8404\n","Epoch 6/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3195 - accuracy: 0.8656 - f1_score: 0.8643 - val_loss: 0.3720 - val_accuracy: 0.8427 - val_f1_score: 0.8474\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3051 - accuracy: 0.8722 - f1_score: 0.8706 - val_loss: 0.4135 - val_accuracy: 0.8282 - val_f1_score: 0.8411\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2884 - accuracy: 0.8867 - f1_score: 0.8849 - val_loss: 0.4216 - val_accuracy: 0.8246 - val_f1_score: 0.8394\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2769 - accuracy: 0.8939 - f1_score: 0.8930 - val_loss: 0.4006 - val_accuracy: 0.8454 - val_f1_score: 0.8557\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2740 - accuracy: 0.8927 - f1_score: 0.8926 - val_loss: 0.3576 - val_accuracy: 0.8535 - val_f1_score: 0.8581\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2596 - accuracy: 0.8987 - f1_score: 0.8981 - val_loss: 0.3537 - val_accuracy: 0.8662 - val_f1_score: 0.8693\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2540 - accuracy: 0.9036 - f1_score: 0.9031 - val_loss: 0.3910 - val_accuracy: 0.8499 - val_f1_score: 0.8586\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2623 - accuracy: 0.8951 - f1_score: 0.8951 - val_loss: 0.3614 - val_accuracy: 0.8599 - val_f1_score: 0.8622\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2471 - accuracy: 0.9063 - f1_score: 0.9057 - val_loss: 0.3784 - val_accuracy: 0.8644 - val_f1_score: 0.8687\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2404 - accuracy: 0.9078 - f1_score: 0.9071 - val_loss: 0.3907 - val_accuracy: 0.8490 - val_f1_score: 0.8574\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2240 - accuracy: 0.9162 - f1_score: 0.9159 - val_loss: 0.3856 - val_accuracy: 0.8590 - val_f1_score: 0.8615\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3097 - accuracy: 0.8771 - f1_score: 0.8984\n","Epoch 1/20\n","104/104 [==============================] - 6s 14ms/step - loss: 0.6280 - accuracy: 0.6456 - f1_score: 0.6297 - val_loss: 0.5425 - val_accuracy: 0.7360 - val_f1_score: 0.7587\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4859 - accuracy: 0.7752 - f1_score: 0.7690 - val_loss: 0.4317 - val_accuracy: 0.8156 - val_f1_score: 0.8286\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4080 - accuracy: 0.8140 - f1_score: 0.8102 - val_loss: 0.3993 - val_accuracy: 0.8255 - val_f1_score: 0.8346\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3614 - accuracy: 0.8475 - f1_score: 0.8456 - val_loss: 0.3943 - val_accuracy: 0.8309 - val_f1_score: 0.8463\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3418 - accuracy: 0.8596 - f1_score: 0.8576 - val_loss: 0.3780 - val_accuracy: 0.8436 - val_f1_score: 0.8523\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3196 - accuracy: 0.8656 - f1_score: 0.8641 - val_loss: 0.4244 - val_accuracy: 0.8092 - val_f1_score: 0.8313\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3030 - accuracy: 0.8794 - f1_score: 0.8786 - val_loss: 0.3358 - val_accuracy: 0.8707 - val_f1_score: 0.8755\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3010 - accuracy: 0.8773 - f1_score: 0.8765 - val_loss: 0.4124 - val_accuracy: 0.8011 - val_f1_score: 0.8270\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2835 - accuracy: 0.8897 - f1_score: 0.8893 - val_loss: 0.3607 - val_accuracy: 0.8454 - val_f1_score: 0.8562\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2762 - accuracy: 0.8921 - f1_score: 0.8912 - val_loss: 0.3352 - val_accuracy: 0.8635 - val_f1_score: 0.8704\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2679 - accuracy: 0.8990 - f1_score: 0.8985 - val_loss: 0.3364 - val_accuracy: 0.8662 - val_f1_score: 0.8563\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2754 - accuracy: 0.8882 - f1_score: 0.8871 - val_loss: 0.3279 - val_accuracy: 0.8653 - val_f1_score: 0.8606\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2620 - accuracy: 0.8960 - f1_score: 0.8949 - val_loss: 0.3191 - val_accuracy: 0.8734 - val_f1_score: 0.8716\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2411 - accuracy: 0.9042 - f1_score: 0.9033 - val_loss: 0.3486 - val_accuracy: 0.8644 - val_f1_score: 0.8541\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2353 - accuracy: 0.9114 - f1_score: 0.9110 - val_loss: 0.3489 - val_accuracy: 0.8626 - val_f1_score: 0.8669\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2486 - accuracy: 0.8972 - f1_score: 0.8963 - val_loss: 0.3161 - val_accuracy: 0.8761 - val_f1_score: 0.8716\n","Epoch 17/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2233 - accuracy: 0.9129 - f1_score: 0.9125 - val_loss: 0.3434 - val_accuracy: 0.8671 - val_f1_score: 0.8691\n","Epoch 18/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2103 - accuracy: 0.9241 - f1_score: 0.9237 - val_loss: 0.3430 - val_accuracy: 0.8761 - val_f1_score: 0.8795\n","Epoch 19/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2088 - accuracy: 0.9237 - f1_score: 0.9229 - val_loss: 0.3743 - val_accuracy: 0.8580 - val_f1_score: 0.8650\n","Epoch 20/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2025 - accuracy: 0.9216 - f1_score: 0.9210 - val_loss: 0.3998 - val_accuracy: 0.8427 - val_f1_score: 0.8545\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3260 - accuracy: 0.8731 - f1_score: 0.8988\n","Epoch 1/20\n","104/104 [==============================] - 5s 13ms/step - loss: 0.6228 - accuracy: 0.6567 - f1_score: 0.6583 - val_loss: 0.5440 - val_accuracy: 0.7269 - val_f1_score: 0.6753\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4629 - accuracy: 0.7842 - f1_score: 0.7753 - val_loss: 0.4217 - val_accuracy: 0.8074 - val_f1_score: 0.7820\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3782 - accuracy: 0.8373 - f1_score: 0.8329 - val_loss: 0.3815 - val_accuracy: 0.8382 - val_f1_score: 0.8464\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3495 - accuracy: 0.8556 - f1_score: 0.8536 - val_loss: 0.3487 - val_accuracy: 0.8653 - val_f1_score: 0.8692\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3387 - accuracy: 0.8590 - f1_score: 0.8588 - val_loss: 0.3320 - val_accuracy: 0.8743 - val_f1_score: 0.8751\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3355 - accuracy: 0.8538 - f1_score: 0.8517 - val_loss: 0.3883 - val_accuracy: 0.8327 - val_f1_score: 0.8091\n","Epoch 7/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3026 - accuracy: 0.8785 - f1_score: 0.8777 - val_loss: 0.3234 - val_accuracy: 0.8752 - val_f1_score: 0.8717\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2875 - accuracy: 0.8852 - f1_score: 0.8839 - val_loss: 0.3198 - val_accuracy: 0.8761 - val_f1_score: 0.8709\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2872 - accuracy: 0.8855 - f1_score: 0.8843 - val_loss: 0.3205 - val_accuracy: 0.8770 - val_f1_score: 0.8752\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2827 - accuracy: 0.8837 - f1_score: 0.8826 - val_loss: 0.3258 - val_accuracy: 0.8734 - val_f1_score: 0.8654\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2721 - accuracy: 0.8903 - f1_score: 0.8896 - val_loss: 0.4217 - val_accuracy: 0.8210 - val_f1_score: 0.7925\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2635 - accuracy: 0.8972 - f1_score: 0.8964 - val_loss: 0.3104 - val_accuracy: 0.8797 - val_f1_score: 0.8756\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2564 - accuracy: 0.9017 - f1_score: 0.9010 - val_loss: 0.3600 - val_accuracy: 0.8698 - val_f1_score: 0.8602\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2499 - accuracy: 0.9057 - f1_score: 0.9049 - val_loss: 0.3120 - val_accuracy: 0.8779 - val_f1_score: 0.8749\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2461 - accuracy: 0.8993 - f1_score: 0.8982 - val_loss: 0.3298 - val_accuracy: 0.8680 - val_f1_score: 0.8706\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2340 - accuracy: 0.9081 - f1_score: 0.9070 - val_loss: 0.3391 - val_accuracy: 0.8707 - val_f1_score: 0.8602\n","Epoch 17/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2335 - accuracy: 0.9054 - f1_score: 0.9050 - val_loss: 0.3259 - val_accuracy: 0.8716 - val_f1_score: 0.8650\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8589 - f1_score: 0.8819\n","Epoch 1/20\n","104/104 [==============================] - 6s 21ms/step - loss: 0.6464 - accuracy: 0.6308 - f1_score: 0.6171 - val_loss: 0.5776 - val_accuracy: 0.7043 - val_f1_score: 0.6139\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5020 - accuracy: 0.7613 - f1_score: 0.7489 - val_loss: 0.3904 - val_accuracy: 0.8309 - val_f1_score: 0.8264\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4011 - accuracy: 0.8207 - f1_score: 0.8165 - val_loss: 0.3704 - val_accuracy: 0.8318 - val_f1_score: 0.8416\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3692 - accuracy: 0.8430 - f1_score: 0.8405 - val_loss: 0.3541 - val_accuracy: 0.8490 - val_f1_score: 0.8569\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3341 - accuracy: 0.8641 - f1_score: 0.8620 - val_loss: 0.3190 - val_accuracy: 0.8743 - val_f1_score: 0.8717\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3150 - accuracy: 0.8710 - f1_score: 0.8701 - val_loss: 0.3153 - val_accuracy: 0.8734 - val_f1_score: 0.8704\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3068 - accuracy: 0.8819 - f1_score: 0.8803 - val_loss: 0.3296 - val_accuracy: 0.8653 - val_f1_score: 0.8699\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2890 - accuracy: 0.8834 - f1_score: 0.8813 - val_loss: 0.3234 - val_accuracy: 0.8689 - val_f1_score: 0.8709\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3037 - accuracy: 0.8800 - f1_score: 0.8795 - val_loss: 0.3110 - val_accuracy: 0.8779 - val_f1_score: 0.8725\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2831 - accuracy: 0.8882 - f1_score: 0.8877 - val_loss: 0.3507 - val_accuracy: 0.8544 - val_f1_score: 0.8379\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2692 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3080 - val_accuracy: 0.8779 - val_f1_score: 0.8751\n","Epoch 12/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2524 - accuracy: 0.9027 - f1_score: 0.9016 - val_loss: 0.3121 - val_accuracy: 0.8761 - val_f1_score: 0.8716\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2512 - accuracy: 0.9020 - f1_score: 0.9009 - val_loss: 0.3103 - val_accuracy: 0.8734 - val_f1_score: 0.8689\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2406 - accuracy: 0.9102 - f1_score: 0.9090 - val_loss: 0.3584 - val_accuracy: 0.8517 - val_f1_score: 0.8357\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2350 - accuracy: 0.9090 - f1_score: 0.9078 - val_loss: 0.3150 - val_accuracy: 0.8707 - val_f1_score: 0.8699\n","Epoch 16/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2277 - accuracy: 0.9177 - f1_score: 0.9168 - val_loss: 0.3243 - val_accuracy: 0.8644 - val_f1_score: 0.8621\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.8670 - f1_score: 0.8888\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhcIK3GiREIg","executionInfo":{"status":"ok","timestamp":1690578347174,"user_tz":-330,"elapsed":123,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"9544bd21-8d98-4db0-e275-39d23fdb4b49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8561782836914062, 0.8622552156448364, 0.8534773588180542, 0.8548278212547302, 0.8480756282806396, 0.8528021574020386, 0.8588791489601135, 0.8568534851074219, 0.8609048128128052, 0.8636056780815125, 0.8636056780815125, 0.8649561405181885, 0.8642808794975281, 0.8642808794975281, 0.8663065433502197, 0.8602295517921448, 0.8413234353065491, 0.8669817447662354, 0.8622552156448364, 0.8602295517921448, 0.871708333492279, 0.8771100640296936, 0.8730587363243103, 0.8588791489601135, 0.8669817447662354]\n","0.861201889514923\n","0.007586725795784946\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SYLZtzRRELO","executionInfo":{"status":"ok","timestamp":1690578347175,"user_tz":-330,"elapsed":33,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"76e7789f-ab9d-4ceb-f8a6-425ed28c4db4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3406858444213867, 0.32010939717292786, 0.3390131890773773, 0.3500480055809021, 0.3687829375267029, 0.3488112688064575, 0.32372650504112244, 0.33751553297042847, 0.3425281345844269, 0.3740350604057312, 0.3207777142524719, 0.31764093041419983, 0.3104436993598938, 0.30492720007896423, 0.36060768365859985, 0.35104265809059143, 0.37017303705215454, 0.3372787833213806, 0.3372608721256256, 0.3361664414405823, 0.32932478189468384, 0.3097023367881775, 0.32603228092193604, 0.3187360465526581, 0.3223949372768402]\n","0.3359106111526489\n","0.0188368473818887\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHx53G9qREN4","executionInfo":{"status":"ok","timestamp":1690578347176,"user_tz":-330,"elapsed":24,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"aeeec7a7-d594-4988-a3c9-e4f80e660371"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8794566988945007, 0.8853931427001953, 0.8764939308166504, 0.8759376406669617, 0.8700172305107117, 0.875995397567749, 0.8857298493385315, 0.8812988996505737, 0.8864387273788452, 0.8902173042297363, 0.8896174430847168, 0.8917748332023621, 0.8897420763969421, 0.8893779516220093, 0.8971961736679077, 0.8846796751022339, 0.8643969297409058, 0.8929929137229919, 0.8871680498123169, 0.8829846978187561, 0.8936169147491455, 0.8984374403953552, 0.898815929889679, 0.8818540573120117, 0.8887633085250854]\n","0.885535888671875\n","0.008371099742137214\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4NY_oDiuREQU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j_ftUumERETA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        #cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIDcR3wqREVo","executionInfo":{"status":"ok","timestamp":1690579133455,"user_tz":-330,"elapsed":786290,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"b3bc25c6-59a6-43e9-fb8a-90a495f0ac77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6403 - accuracy: 0.6317 - f1_score: 0.6292 - val_loss: 0.5386 - val_accuracy: 0.7125 - val_f1_score: 0.6419\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5018 - accuracy: 0.7553 - f1_score: 0.7461 - val_loss: 0.3728 - val_accuracy: 0.8327 - val_f1_score: 0.8367\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4139 - accuracy: 0.8198 - f1_score: 0.8168 - val_loss: 0.3342 - val_accuracy: 0.8571 - val_f1_score: 0.8553\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3730 - accuracy: 0.8397 - f1_score: 0.8363 - val_loss: 0.4011 - val_accuracy: 0.8264 - val_f1_score: 0.8469\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3544 - accuracy: 0.8529 - f1_score: 0.8506 - val_loss: 0.3077 - val_accuracy: 0.8734 - val_f1_score: 0.8772\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3251 - accuracy: 0.8719 - f1_score: 0.8698 - val_loss: 0.2951 - val_accuracy: 0.8834 - val_f1_score: 0.8843\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3213 - accuracy: 0.8674 - f1_score: 0.8646 - val_loss: 0.2916 - val_accuracy: 0.8807 - val_f1_score: 0.8813\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3117 - accuracy: 0.8716 - f1_score: 0.8700 - val_loss: 0.2967 - val_accuracy: 0.8743 - val_f1_score: 0.8788\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3070 - accuracy: 0.8825 - f1_score: 0.8808 - val_loss: 0.3116 - val_accuracy: 0.8689 - val_f1_score: 0.8753\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3126 - accuracy: 0.8704 - f1_score: 0.8686 - val_loss: 0.3101 - val_accuracy: 0.8707 - val_f1_score: 0.8766\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2879 - accuracy: 0.8888 - f1_score: 0.8875 - val_loss: 0.3014 - val_accuracy: 0.8707 - val_f1_score: 0.8753\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2724 - accuracy: 0.8903 - f1_score: 0.8882 - val_loss: 0.2960 - val_accuracy: 0.8743 - val_f1_score: 0.8719\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3314 - accuracy: 0.8548 - f1_score: 0.8785\n","Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6138 - accuracy: 0.6579 - f1_score: 0.6519 - val_loss: 0.5571 - val_accuracy: 0.7306 - val_f1_score: 0.7372\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5017 - accuracy: 0.7652 - f1_score: 0.7560 - val_loss: 0.4710 - val_accuracy: 0.7875 - val_f1_score: 0.7798\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4262 - accuracy: 0.8032 - f1_score: 0.7981 - val_loss: 0.4274 - val_accuracy: 0.8119 - val_f1_score: 0.8237\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3874 - accuracy: 0.8279 - f1_score: 0.8240 - val_loss: 0.4033 - val_accuracy: 0.8228 - val_f1_score: 0.8328\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3462 - accuracy: 0.8481 - f1_score: 0.8466 - val_loss: 0.4026 - val_accuracy: 0.8174 - val_f1_score: 0.8317\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3557 - accuracy: 0.8475 - f1_score: 0.8474 - val_loss: 0.3763 - val_accuracy: 0.8237 - val_f1_score: 0.8360\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3242 - accuracy: 0.8638 - f1_score: 0.8634 - val_loss: 0.3634 - val_accuracy: 0.8309 - val_f1_score: 0.8392\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3174 - accuracy: 0.8647 - f1_score: 0.8637 - val_loss: 0.3496 - val_accuracy: 0.8644 - val_f1_score: 0.8601\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2938 - accuracy: 0.8731 - f1_score: 0.8722 - val_loss: 0.3547 - val_accuracy: 0.8436 - val_f1_score: 0.8500\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2996 - accuracy: 0.8791 - f1_score: 0.8788 - val_loss: 0.3423 - val_accuracy: 0.8671 - val_f1_score: 0.8645\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2993 - accuracy: 0.8725 - f1_score: 0.8715 - val_loss: 0.3474 - val_accuracy: 0.8580 - val_f1_score: 0.8592\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2801 - accuracy: 0.8816 - f1_score: 0.8808 - val_loss: 0.3467 - val_accuracy: 0.8571 - val_f1_score: 0.8566\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2781 - accuracy: 0.8855 - f1_score: 0.8850 - val_loss: 0.3536 - val_accuracy: 0.8662 - val_f1_score: 0.8596\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2758 - accuracy: 0.8891 - f1_score: 0.8873 - val_loss: 0.3637 - val_accuracy: 0.8571 - val_f1_score: 0.8604\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2630 - accuracy: 0.8969 - f1_score: 0.8964 - val_loss: 0.3904 - val_accuracy: 0.8517 - val_f1_score: 0.8386\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3353 - accuracy: 0.8582 - f1_score: 0.8818\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6241 - accuracy: 0.6573 - f1_score: 0.6423 - val_loss: 0.5535 - val_accuracy: 0.7089 - val_f1_score: 0.7290\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4660 - accuracy: 0.7857 - f1_score: 0.7817 - val_loss: 0.4983 - val_accuracy: 0.7803 - val_f1_score: 0.7356\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3734 - accuracy: 0.8385 - f1_score: 0.8364 - val_loss: 0.3869 - val_accuracy: 0.8246 - val_f1_score: 0.8207\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3371 - accuracy: 0.8559 - f1_score: 0.8548 - val_loss: 0.4080 - val_accuracy: 0.8156 - val_f1_score: 0.8306\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3115 - accuracy: 0.8788 - f1_score: 0.8774 - val_loss: 0.3826 - val_accuracy: 0.8373 - val_f1_score: 0.8299\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3034 - accuracy: 0.8758 - f1_score: 0.8744 - val_loss: 0.3923 - val_accuracy: 0.8318 - val_f1_score: 0.8180\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2915 - accuracy: 0.8767 - f1_score: 0.8759 - val_loss: 0.3977 - val_accuracy: 0.8354 - val_f1_score: 0.8202\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2952 - accuracy: 0.8816 - f1_score: 0.8800 - val_loss: 0.4113 - val_accuracy: 0.8201 - val_f1_score: 0.7984\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2894 - accuracy: 0.8810 - f1_score: 0.8802 - val_loss: 0.3817 - val_accuracy: 0.8391 - val_f1_score: 0.8311\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2839 - accuracy: 0.8897 - f1_score: 0.8885 - val_loss: 0.3676 - val_accuracy: 0.8445 - val_f1_score: 0.8390\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2775 - accuracy: 0.8870 - f1_score: 0.8851 - val_loss: 0.4042 - val_accuracy: 0.8382 - val_f1_score: 0.8300\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2703 - accuracy: 0.8903 - f1_score: 0.8890 - val_loss: 0.3939 - val_accuracy: 0.8363 - val_f1_score: 0.8356\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2632 - accuracy: 0.8972 - f1_score: 0.8964 - val_loss: 0.4255 - val_accuracy: 0.8291 - val_f1_score: 0.8170\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2459 - accuracy: 0.9045 - f1_score: 0.9034 - val_loss: 0.4089 - val_accuracy: 0.8255 - val_f1_score: 0.8224\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2435 - accuracy: 0.9042 - f1_score: 0.9030 - val_loss: 0.4406 - val_accuracy: 0.8264 - val_f1_score: 0.8095\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3348 - accuracy: 0.8623 - f1_score: 0.8840\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6382 - accuracy: 0.6359 - f1_score: 0.5880 - val_loss: 0.5193 - val_accuracy: 0.7523 - val_f1_score: 0.7420\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5063 - accuracy: 0.7595 - f1_score: 0.7508 - val_loss: 0.4371 - val_accuracy: 0.8174 - val_f1_score: 0.8265\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4254 - accuracy: 0.8122 - f1_score: 0.8051 - val_loss: 0.3776 - val_accuracy: 0.8363 - val_f1_score: 0.8356\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3786 - accuracy: 0.8345 - f1_score: 0.8299 - val_loss: 0.3601 - val_accuracy: 0.8499 - val_f1_score: 0.8483\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3536 - accuracy: 0.8538 - f1_score: 0.8514 - val_loss: 0.3698 - val_accuracy: 0.8418 - val_f1_score: 0.8508\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3259 - accuracy: 0.8701 - f1_score: 0.8693 - val_loss: 0.3439 - val_accuracy: 0.8671 - val_f1_score: 0.8648\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3163 - accuracy: 0.8722 - f1_score: 0.8704 - val_loss: 0.3674 - val_accuracy: 0.8463 - val_f1_score: 0.8547\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3083 - accuracy: 0.8797 - f1_score: 0.8785 - val_loss: 0.3529 - val_accuracy: 0.8562 - val_f1_score: 0.8606\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3008 - accuracy: 0.8764 - f1_score: 0.8746 - val_loss: 0.3490 - val_accuracy: 0.8535 - val_f1_score: 0.8589\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2973 - accuracy: 0.8782 - f1_score: 0.8767 - val_loss: 0.3393 - val_accuracy: 0.8635 - val_f1_score: 0.8606\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2800 - accuracy: 0.8900 - f1_score: 0.8883 - val_loss: 0.3945 - val_accuracy: 0.8463 - val_f1_score: 0.8569\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2751 - accuracy: 0.8978 - f1_score: 0.8961 - val_loss: 0.3789 - val_accuracy: 0.8517 - val_f1_score: 0.8605\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2679 - accuracy: 0.8966 - f1_score: 0.8950 - val_loss: 0.3636 - val_accuracy: 0.8590 - val_f1_score: 0.8634\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2528 - accuracy: 0.9042 - f1_score: 0.9030 - val_loss: 0.3892 - val_accuracy: 0.8562 - val_f1_score: 0.8623\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2429 - accuracy: 0.9045 - f1_score: 0.9030 - val_loss: 0.3744 - val_accuracy: 0.8553 - val_f1_score: 0.8604\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3670 - accuracy: 0.8440 - f1_score: 0.8653\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.6368 - accuracy: 0.6356 - f1_score: 0.6279 - val_loss: 0.5365 - val_accuracy: 0.7468 - val_f1_score: 0.7445\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5151 - accuracy: 0.7468 - f1_score: 0.7330 - val_loss: 0.4694 - val_accuracy: 0.7839 - val_f1_score: 0.7997\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4389 - accuracy: 0.7975 - f1_score: 0.7868 - val_loss: 0.3787 - val_accuracy: 0.8391 - val_f1_score: 0.8324\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3865 - accuracy: 0.8351 - f1_score: 0.8318 - val_loss: 0.3753 - val_accuracy: 0.8463 - val_f1_score: 0.8340\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3520 - accuracy: 0.8454 - f1_score: 0.8430 - val_loss: 0.3451 - val_accuracy: 0.8535 - val_f1_score: 0.8564\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3333 - accuracy: 0.8602 - f1_score: 0.8573 - val_loss: 0.3485 - val_accuracy: 0.8544 - val_f1_score: 0.8591\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3273 - accuracy: 0.8695 - f1_score: 0.8675 - val_loss: 0.3347 - val_accuracy: 0.8626 - val_f1_score: 0.8652\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3060 - accuracy: 0.8782 - f1_score: 0.8765 - val_loss: 0.3311 - val_accuracy: 0.8626 - val_f1_score: 0.8621\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3153 - accuracy: 0.8701 - f1_score: 0.8672 - val_loss: 0.3666 - val_accuracy: 0.8327 - val_f1_score: 0.8477\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2947 - accuracy: 0.8831 - f1_score: 0.8815 - val_loss: 0.4053 - val_accuracy: 0.8219 - val_f1_score: 0.7992\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2935 - accuracy: 0.8788 - f1_score: 0.8766 - val_loss: 0.3521 - val_accuracy: 0.8580 - val_f1_score: 0.8462\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2913 - accuracy: 0.8776 - f1_score: 0.8754 - val_loss: 0.3233 - val_accuracy: 0.8743 - val_f1_score: 0.8771\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2728 - accuracy: 0.8936 - f1_score: 0.8915 - val_loss: 0.3552 - val_accuracy: 0.8662 - val_f1_score: 0.8706\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2737 - accuracy: 0.8888 - f1_score: 0.8880 - val_loss: 0.3928 - val_accuracy: 0.8454 - val_f1_score: 0.8281\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2599 - accuracy: 0.8939 - f1_score: 0.8925 - val_loss: 0.3795 - val_accuracy: 0.8671 - val_f1_score: 0.8593\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2484 - accuracy: 0.9048 - f1_score: 0.9033 - val_loss: 0.3468 - val_accuracy: 0.8644 - val_f1_score: 0.8609\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2318 - accuracy: 0.9090 - f1_score: 0.9075 - val_loss: 0.4331 - val_accuracy: 0.8354 - val_f1_score: 0.8169\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8650 - f1_score: 0.8891\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6356 - accuracy: 0.6317 - f1_score: 0.6497 - val_loss: 0.5190 - val_accuracy: 0.7595 - val_f1_score: 0.7361\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4946 - accuracy: 0.7649 - f1_score: 0.7503 - val_loss: 0.4779 - val_accuracy: 0.7794 - val_f1_score: 0.8048\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4072 - accuracy: 0.8198 - f1_score: 0.8124 - val_loss: 0.3471 - val_accuracy: 0.8517 - val_f1_score: 0.8504\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3683 - accuracy: 0.8418 - f1_score: 0.8395 - val_loss: 0.3433 - val_accuracy: 0.8544 - val_f1_score: 0.8632\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3379 - accuracy: 0.8583 - f1_score: 0.8570 - val_loss: 0.3141 - val_accuracy: 0.8716 - val_f1_score: 0.8621\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3198 - accuracy: 0.8665 - f1_score: 0.8646 - val_loss: 0.2900 - val_accuracy: 0.8852 - val_f1_score: 0.8857\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3192 - accuracy: 0.8614 - f1_score: 0.8606 - val_loss: 0.3265 - val_accuracy: 0.8635 - val_f1_score: 0.8717\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2975 - accuracy: 0.8797 - f1_score: 0.8793 - val_loss: 0.2899 - val_accuracy: 0.8834 - val_f1_score: 0.8841\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2896 - accuracy: 0.8828 - f1_score: 0.8822 - val_loss: 0.3327 - val_accuracy: 0.8526 - val_f1_score: 0.8345\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2944 - accuracy: 0.8800 - f1_score: 0.8792 - val_loss: 0.2798 - val_accuracy: 0.8915 - val_f1_score: 0.8938\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2728 - accuracy: 0.8882 - f1_score: 0.8877 - val_loss: 0.3070 - val_accuracy: 0.8698 - val_f1_score: 0.8588\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2655 - accuracy: 0.8924 - f1_score: 0.8913 - val_loss: 0.2830 - val_accuracy: 0.8825 - val_f1_score: 0.8818\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2555 - accuracy: 0.8972 - f1_score: 0.8963 - val_loss: 0.2841 - val_accuracy: 0.8852 - val_f1_score: 0.8836\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2503 - accuracy: 0.8984 - f1_score: 0.8978 - val_loss: 0.2994 - val_accuracy: 0.8834 - val_f1_score: 0.8828\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2282 - accuracy: 0.9132 - f1_score: 0.9123 - val_loss: 0.3257 - val_accuracy: 0.8617 - val_f1_score: 0.8490\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3253 - accuracy: 0.8650 - f1_score: 0.8917\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6207 - accuracy: 0.6649 - f1_score: 0.6438 - val_loss: 0.5522 - val_accuracy: 0.7306 - val_f1_score: 0.6850\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4813 - accuracy: 0.7700 - f1_score: 0.7639 - val_loss: 0.4226 - val_accuracy: 0.8074 - val_f1_score: 0.7938\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4008 - accuracy: 0.8183 - f1_score: 0.8134 - val_loss: 0.3902 - val_accuracy: 0.8264 - val_f1_score: 0.8413\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3643 - accuracy: 0.8363 - f1_score: 0.8350 - val_loss: 0.3372 - val_accuracy: 0.8590 - val_f1_score: 0.8579\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3430 - accuracy: 0.8623 - f1_score: 0.8611 - val_loss: 0.3577 - val_accuracy: 0.8391 - val_f1_score: 0.8522\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3340 - accuracy: 0.8605 - f1_score: 0.8592 - val_loss: 0.3152 - val_accuracy: 0.8698 - val_f1_score: 0.8652\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3069 - accuracy: 0.8764 - f1_score: 0.8753 - val_loss: 0.3472 - val_accuracy: 0.8553 - val_f1_score: 0.8658\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2839 - accuracy: 0.8831 - f1_score: 0.8824 - val_loss: 0.3104 - val_accuracy: 0.8671 - val_f1_score: 0.8714\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2912 - accuracy: 0.8810 - f1_score: 0.8798 - val_loss: 0.2921 - val_accuracy: 0.8807 - val_f1_score: 0.8800\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2705 - accuracy: 0.8900 - f1_score: 0.8898 - val_loss: 0.3014 - val_accuracy: 0.8716 - val_f1_score: 0.8645\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2801 - accuracy: 0.8879 - f1_score: 0.8875 - val_loss: 0.2927 - val_accuracy: 0.8843 - val_f1_score: 0.8845\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2651 - accuracy: 0.8933 - f1_score: 0.8923 - val_loss: 0.3811 - val_accuracy: 0.8436 - val_f1_score: 0.8595\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2616 - accuracy: 0.8921 - f1_score: 0.8916 - val_loss: 0.3059 - val_accuracy: 0.8707 - val_f1_score: 0.8758\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2439 - accuracy: 0.9024 - f1_score: 0.9018 - val_loss: 0.3135 - val_accuracy: 0.8743 - val_f1_score: 0.8672\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3416 - accuracy: 0.8542 - f1_score: 0.8777\n","Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6144 - accuracy: 0.6640 - f1_score: 0.6626 - val_loss: 0.5114 - val_accuracy: 0.7532 - val_f1_score: 0.7479\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4614 - accuracy: 0.7869 - f1_score: 0.7766 - val_loss: 0.5184 - val_accuracy: 0.7532 - val_f1_score: 0.7879\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3842 - accuracy: 0.8252 - f1_score: 0.8205 - val_loss: 0.3778 - val_accuracy: 0.8409 - val_f1_score: 0.8333\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3526 - accuracy: 0.8490 - f1_score: 0.8465 - val_loss: 0.4621 - val_accuracy: 0.7875 - val_f1_score: 0.8160\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3284 - accuracy: 0.8683 - f1_score: 0.8664 - val_loss: 0.3399 - val_accuracy: 0.8725 - val_f1_score: 0.8717\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3129 - accuracy: 0.8671 - f1_score: 0.8653 - val_loss: 0.3330 - val_accuracy: 0.8635 - val_f1_score: 0.8598\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2952 - accuracy: 0.8797 - f1_score: 0.8783 - val_loss: 0.3276 - val_accuracy: 0.8698 - val_f1_score: 0.8652\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2873 - accuracy: 0.8822 - f1_score: 0.8811 - val_loss: 0.3247 - val_accuracy: 0.8707 - val_f1_score: 0.8682\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2719 - accuracy: 0.8921 - f1_score: 0.8914 - val_loss: 0.3253 - val_accuracy: 0.8689 - val_f1_score: 0.8649\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2580 - accuracy: 0.8963 - f1_score: 0.8952 - val_loss: 0.3289 - val_accuracy: 0.8725 - val_f1_score: 0.8722\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2561 - accuracy: 0.8984 - f1_score: 0.8976 - val_loss: 0.3376 - val_accuracy: 0.8644 - val_f1_score: 0.8569\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2488 - accuracy: 0.9027 - f1_score: 0.9013 - val_loss: 0.3337 - val_accuracy: 0.8599 - val_f1_score: 0.8505\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2342 - accuracy: 0.9087 - f1_score: 0.9079 - val_loss: 0.3457 - val_accuracy: 0.8608 - val_f1_score: 0.8644\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3596 - accuracy: 0.8386 - f1_score: 0.8637\n","Epoch 1/20\n","104/104 [==============================] - 10s 22ms/step - loss: 0.6161 - accuracy: 0.6564 - f1_score: 0.6711 - val_loss: 0.5127 - val_accuracy: 0.7459 - val_f1_score: 0.7285\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4861 - accuracy: 0.7712 - f1_score: 0.7624 - val_loss: 0.4409 - val_accuracy: 0.7911 - val_f1_score: 0.7664\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4169 - accuracy: 0.8125 - f1_score: 0.8077 - val_loss: 0.3881 - val_accuracy: 0.8291 - val_f1_score: 0.8355\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3754 - accuracy: 0.8376 - f1_score: 0.8332 - val_loss: 0.3721 - val_accuracy: 0.8327 - val_f1_score: 0.8431\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3569 - accuracy: 0.8490 - f1_score: 0.8468 - val_loss: 0.3496 - val_accuracy: 0.8517 - val_f1_score: 0.8584\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3266 - accuracy: 0.8638 - f1_score: 0.8632 - val_loss: 0.3260 - val_accuracy: 0.8580 - val_f1_score: 0.8626\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3139 - accuracy: 0.8665 - f1_score: 0.8657 - val_loss: 0.3249 - val_accuracy: 0.8689 - val_f1_score: 0.8729\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3109 - accuracy: 0.8671 - f1_score: 0.8653 - val_loss: 0.3130 - val_accuracy: 0.8716 - val_f1_score: 0.8692\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2901 - accuracy: 0.8831 - f1_score: 0.8824 - val_loss: 0.3291 - val_accuracy: 0.8644 - val_f1_score: 0.8563\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2915 - accuracy: 0.8764 - f1_score: 0.8752 - val_loss: 0.3413 - val_accuracy: 0.8580 - val_f1_score: 0.8652\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2699 - accuracy: 0.8870 - f1_score: 0.8863 - val_loss: 0.3024 - val_accuracy: 0.8743 - val_f1_score: 0.8731\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2641 - accuracy: 0.8933 - f1_score: 0.8920 - val_loss: 0.3459 - val_accuracy: 0.8626 - val_f1_score: 0.8701\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2725 - accuracy: 0.8915 - f1_score: 0.8906 - val_loss: 0.3264 - val_accuracy: 0.8698 - val_f1_score: 0.8594\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2489 - accuracy: 0.8990 - f1_score: 0.8976 - val_loss: 0.3344 - val_accuracy: 0.8716 - val_f1_score: 0.8763\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2439 - accuracy: 0.9051 - f1_score: 0.9041 - val_loss: 0.3441 - val_accuracy: 0.8608 - val_f1_score: 0.8490\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2365 - accuracy: 0.9051 - f1_score: 0.9040 - val_loss: 0.4111 - val_accuracy: 0.8165 - val_f1_score: 0.7829\n","47/47 [==============================] - 1s 9ms/step - loss: 0.3327 - accuracy: 0.8582 - f1_score: 0.8845\n","Epoch 1/20\n","104/104 [==============================] - 12s 36ms/step - loss: 0.6252 - accuracy: 0.6498 - f1_score: 0.6360 - val_loss: 0.5140 - val_accuracy: 0.7595 - val_f1_score: 0.7263\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4847 - accuracy: 0.7679 - f1_score: 0.7560 - val_loss: 0.4154 - val_accuracy: 0.8156 - val_f1_score: 0.8038\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3940 - accuracy: 0.8261 - f1_score: 0.8216 - val_loss: 0.3648 - val_accuracy: 0.8382 - val_f1_score: 0.8356\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3453 - accuracy: 0.8532 - f1_score: 0.8508 - val_loss: 0.3418 - val_accuracy: 0.8508 - val_f1_score: 0.8520\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3271 - accuracy: 0.8662 - f1_score: 0.8651 - val_loss: 0.3909 - val_accuracy: 0.8300 - val_f1_score: 0.8454\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3098 - accuracy: 0.8776 - f1_score: 0.8766 - val_loss: 0.3206 - val_accuracy: 0.8635 - val_f1_score: 0.8626\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3054 - accuracy: 0.8788 - f1_score: 0.8791 - val_loss: 0.3147 - val_accuracy: 0.8626 - val_f1_score: 0.8623\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2848 - accuracy: 0.8843 - f1_score: 0.8843 - val_loss: 0.3333 - val_accuracy: 0.8608 - val_f1_score: 0.8505\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2667 - accuracy: 0.8933 - f1_score: 0.8926 - val_loss: 0.3060 - val_accuracy: 0.8671 - val_f1_score: 0.8679\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2632 - accuracy: 0.9002 - f1_score: 0.8997 - val_loss: 0.3143 - val_accuracy: 0.8635 - val_f1_score: 0.8585\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2613 - accuracy: 0.8933 - f1_score: 0.8931 - val_loss: 0.3074 - val_accuracy: 0.8680 - val_f1_score: 0.8680\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2454 - accuracy: 0.9069 - f1_score: 0.9067 - val_loss: 0.3461 - val_accuracy: 0.8716 - val_f1_score: 0.8627\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2383 - accuracy: 0.9123 - f1_score: 0.9120 - val_loss: 0.3308 - val_accuracy: 0.8608 - val_f1_score: 0.8525\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2314 - accuracy: 0.9159 - f1_score: 0.9153 - val_loss: 0.4022 - val_accuracy: 0.8400 - val_f1_score: 0.8166\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3199 - accuracy: 0.8704 - f1_score: 0.8954\n","Epoch 1/20\n","104/104 [==============================] - 14s 27ms/step - loss: 0.6285 - accuracy: 0.6410 - f1_score: 0.6162 - val_loss: 0.5091 - val_accuracy: 0.7550 - val_f1_score: 0.7260\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4761 - accuracy: 0.7712 - f1_score: 0.7617 - val_loss: 0.4050 - val_accuracy: 0.8174 - val_f1_score: 0.8140\n","Epoch 3/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3794 - accuracy: 0.8267 - f1_score: 0.8229 - val_loss: 0.4181 - val_accuracy: 0.8110 - val_f1_score: 0.8299\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3518 - accuracy: 0.8469 - f1_score: 0.8458 - val_loss: 0.3475 - val_accuracy: 0.8517 - val_f1_score: 0.8450\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3206 - accuracy: 0.8659 - f1_score: 0.8641 - val_loss: 0.3423 - val_accuracy: 0.8517 - val_f1_score: 0.8556\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3002 - accuracy: 0.8713 - f1_score: 0.8703 - val_loss: 0.4325 - val_accuracy: 0.8373 - val_f1_score: 0.8156\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3099 - accuracy: 0.8758 - f1_score: 0.8747 - val_loss: 0.3422 - val_accuracy: 0.8626 - val_f1_score: 0.8550\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2821 - accuracy: 0.8834 - f1_score: 0.8816 - val_loss: 0.3805 - val_accuracy: 0.8418 - val_f1_score: 0.8231\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2627 - accuracy: 0.8972 - f1_score: 0.8958 - val_loss: 0.3342 - val_accuracy: 0.8617 - val_f1_score: 0.8620\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2590 - accuracy: 0.8951 - f1_score: 0.8934 - val_loss: 0.3490 - val_accuracy: 0.8517 - val_f1_score: 0.8566\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2532 - accuracy: 0.8990 - f1_score: 0.8980 - val_loss: 0.4720 - val_accuracy: 0.8210 - val_f1_score: 0.7925\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2448 - accuracy: 0.9005 - f1_score: 0.8995 - val_loss: 0.3772 - val_accuracy: 0.8635 - val_f1_score: 0.8587\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2438 - accuracy: 0.9048 - f1_score: 0.9038 - val_loss: 0.4282 - val_accuracy: 0.8608 - val_f1_score: 0.8502\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2245 - accuracy: 0.9132 - f1_score: 0.9122 - val_loss: 0.3712 - val_accuracy: 0.8671 - val_f1_score: 0.8686\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3226 - accuracy: 0.8677 - f1_score: 0.8961\n","Epoch 1/20\n","104/104 [==============================] - 12s 43ms/step - loss: 0.6414 - accuracy: 0.6263 - f1_score: 0.5979 - val_loss: 0.5329 - val_accuracy: 0.7378 - val_f1_score: 0.7206\n","Epoch 2/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.5162 - accuracy: 0.7477 - f1_score: 0.7323 - val_loss: 0.4471 - val_accuracy: 0.7712 - val_f1_score: 0.7378\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4297 - accuracy: 0.7960 - f1_score: 0.7857 - val_loss: 0.3929 - val_accuracy: 0.8237 - val_f1_score: 0.8300\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3964 - accuracy: 0.8222 - f1_score: 0.8182 - val_loss: 0.3743 - val_accuracy: 0.8327 - val_f1_score: 0.8426\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3626 - accuracy: 0.8391 - f1_score: 0.8334 - val_loss: 0.3421 - val_accuracy: 0.8571 - val_f1_score: 0.8602\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3396 - accuracy: 0.8511 - f1_score: 0.8474 - val_loss: 0.4018 - val_accuracy: 0.8101 - val_f1_score: 0.8312\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3176 - accuracy: 0.8671 - f1_score: 0.8642 - val_loss: 0.3576 - val_accuracy: 0.8517 - val_f1_score: 0.8620\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3034 - accuracy: 0.8698 - f1_score: 0.8675 - val_loss: 0.3227 - val_accuracy: 0.8743 - val_f1_score: 0.8740\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2845 - accuracy: 0.8822 - f1_score: 0.8801 - val_loss: 0.3293 - val_accuracy: 0.8716 - val_f1_score: 0.8660\n","Epoch 10/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2803 - accuracy: 0.8840 - f1_score: 0.8817 - val_loss: 0.3309 - val_accuracy: 0.8761 - val_f1_score: 0.8789\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2744 - accuracy: 0.8885 - f1_score: 0.8866 - val_loss: 0.3126 - val_accuracy: 0.8743 - val_f1_score: 0.8675\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2709 - accuracy: 0.8870 - f1_score: 0.8860 - val_loss: 0.4776 - val_accuracy: 0.8056 - val_f1_score: 0.7661\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2665 - accuracy: 0.8855 - f1_score: 0.8826 - val_loss: 0.3212 - val_accuracy: 0.8788 - val_f1_score: 0.8775\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2452 - accuracy: 0.8969 - f1_score: 0.8955 - val_loss: 0.3679 - val_accuracy: 0.8608 - val_f1_score: 0.8484\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2524 - accuracy: 0.8978 - f1_score: 0.8967 - val_loss: 0.3265 - val_accuracy: 0.8725 - val_f1_score: 0.8756\n","Epoch 16/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2332 - accuracy: 0.9063 - f1_score: 0.9045 - val_loss: 0.3521 - val_accuracy: 0.8725 - val_f1_score: 0.8632\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3170 - accuracy: 0.8629 - f1_score: 0.8897\n","Epoch 1/20\n","104/104 [==============================] - 13s 37ms/step - loss: 0.6357 - accuracy: 0.6344 - f1_score: 0.5931 - val_loss: 0.5322 - val_accuracy: 0.7468 - val_f1_score: 0.7233\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4852 - accuracy: 0.7679 - f1_score: 0.7582 - val_loss: 0.5643 - val_accuracy: 0.7260 - val_f1_score: 0.7767\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4035 - accuracy: 0.8122 - f1_score: 0.8087 - val_loss: 0.5101 - val_accuracy: 0.7694 - val_f1_score: 0.8034\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3709 - accuracy: 0.8388 - f1_score: 0.8369 - val_loss: 0.5110 - val_accuracy: 0.7839 - val_f1_score: 0.8143\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3360 - accuracy: 0.8596 - f1_score: 0.8577 - val_loss: 0.3928 - val_accuracy: 0.8345 - val_f1_score: 0.8463\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3134 - accuracy: 0.8740 - f1_score: 0.8719 - val_loss: 0.4734 - val_accuracy: 0.8047 - val_f1_score: 0.8283\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3012 - accuracy: 0.8737 - f1_score: 0.8718 - val_loss: 0.3845 - val_accuracy: 0.8418 - val_f1_score: 0.8511\n","Epoch 8/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2968 - accuracy: 0.8758 - f1_score: 0.8755 - val_loss: 0.3453 - val_accuracy: 0.8626 - val_f1_score: 0.8636\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2781 - accuracy: 0.8924 - f1_score: 0.8908 - val_loss: 0.3451 - val_accuracy: 0.8644 - val_f1_score: 0.8629\n","Epoch 10/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2753 - accuracy: 0.8828 - f1_score: 0.8815 - val_loss: 0.3805 - val_accuracy: 0.8590 - val_f1_score: 0.8641\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2607 - accuracy: 0.8999 - f1_score: 0.8982 - val_loss: 0.3482 - val_accuracy: 0.8698 - val_f1_score: 0.8723\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2474 - accuracy: 0.9014 - f1_score: 0.9006 - val_loss: 0.3611 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2593 - accuracy: 0.8909 - f1_score: 0.8902 - val_loss: 0.3696 - val_accuracy: 0.8590 - val_f1_score: 0.8550\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2403 - accuracy: 0.9011 - f1_score: 0.9004 - val_loss: 0.3809 - val_accuracy: 0.8707 - val_f1_score: 0.8731\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3407 - accuracy: 0.8521 - f1_score: 0.8773\n","Epoch 1/20\n","104/104 [==============================] - 13s 27ms/step - loss: 0.6381 - accuracy: 0.6341 - f1_score: 0.6180 - val_loss: 0.5490 - val_accuracy: 0.7224 - val_f1_score: 0.6623\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4908 - accuracy: 0.7676 - f1_score: 0.7561 - val_loss: 0.4243 - val_accuracy: 0.8002 - val_f1_score: 0.7861\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4317 - accuracy: 0.8008 - f1_score: 0.7890 - val_loss: 0.4489 - val_accuracy: 0.7929 - val_f1_score: 0.8172\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3809 - accuracy: 0.8333 - f1_score: 0.8299 - val_loss: 0.3688 - val_accuracy: 0.8382 - val_f1_score: 0.8431\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3472 - accuracy: 0.8502 - f1_score: 0.8458 - val_loss: 0.3821 - val_accuracy: 0.8382 - val_f1_score: 0.8479\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3320 - accuracy: 0.8623 - f1_score: 0.8599 - val_loss: 0.3422 - val_accuracy: 0.8472 - val_f1_score: 0.8345\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3203 - accuracy: 0.8671 - f1_score: 0.8637 - val_loss: 0.3375 - val_accuracy: 0.8571 - val_f1_score: 0.8609\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2910 - accuracy: 0.8843 - f1_score: 0.8822 - val_loss: 0.3239 - val_accuracy: 0.8716 - val_f1_score: 0.8709\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2877 - accuracy: 0.8791 - f1_score: 0.8767 - val_loss: 0.3216 - val_accuracy: 0.8689 - val_f1_score: 0.8729\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2804 - accuracy: 0.8816 - f1_score: 0.8799 - val_loss: 0.3270 - val_accuracy: 0.8626 - val_f1_score: 0.8664\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2603 - accuracy: 0.8978 - f1_score: 0.8963 - val_loss: 0.3164 - val_accuracy: 0.8761 - val_f1_score: 0.8747\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2556 - accuracy: 0.8978 - f1_score: 0.8965 - val_loss: 0.4036 - val_accuracy: 0.8300 - val_f1_score: 0.8456\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2624 - accuracy: 0.8921 - f1_score: 0.8902 - val_loss: 0.3706 - val_accuracy: 0.8463 - val_f1_score: 0.8567\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2268 - accuracy: 0.9108 - f1_score: 0.9096 - val_loss: 0.3498 - val_accuracy: 0.8517 - val_f1_score: 0.8598\n","Epoch 15/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2252 - accuracy: 0.9093 - f1_score: 0.9084 - val_loss: 0.5301 - val_accuracy: 0.7948 - val_f1_score: 0.8244\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2269 - accuracy: 0.9102 - f1_score: 0.9096 - val_loss: 0.3742 - val_accuracy: 0.8490 - val_f1_score: 0.8586\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3396 - accuracy: 0.8542 - f1_score: 0.8777\n","Epoch 1/20\n","104/104 [==============================] - 10s 18ms/step - loss: 0.6333 - accuracy: 0.6377 - f1_score: 0.6331 - val_loss: 0.5419 - val_accuracy: 0.7360 - val_f1_score: 0.7491\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4857 - accuracy: 0.7691 - f1_score: 0.7588 - val_loss: 0.4395 - val_accuracy: 0.7893 - val_f1_score: 0.7744\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4161 - accuracy: 0.8098 - f1_score: 0.8026 - val_loss: 0.4086 - val_accuracy: 0.8137 - val_f1_score: 0.8260\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3756 - accuracy: 0.8382 - f1_score: 0.8361 - val_loss: 0.4153 - val_accuracy: 0.8074 - val_f1_score: 0.8244\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3415 - accuracy: 0.8565 - f1_score: 0.8543 - val_loss: 0.4594 - val_accuracy: 0.7866 - val_f1_score: 0.8145\n","Epoch 6/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3422 - accuracy: 0.8529 - f1_score: 0.8525 - val_loss: 0.3430 - val_accuracy: 0.8571 - val_f1_score: 0.8574\n","Epoch 7/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3150 - accuracy: 0.8680 - f1_score: 0.8667 - val_loss: 0.4002 - val_accuracy: 0.8219 - val_f1_score: 0.8389\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3067 - accuracy: 0.8725 - f1_score: 0.8718 - val_loss: 0.3313 - val_accuracy: 0.8617 - val_f1_score: 0.8671\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2957 - accuracy: 0.8785 - f1_score: 0.8781 - val_loss: 0.3399 - val_accuracy: 0.8517 - val_f1_score: 0.8571\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2861 - accuracy: 0.8767 - f1_score: 0.8766 - val_loss: 0.4642 - val_accuracy: 0.7667 - val_f1_score: 0.8051\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2891 - accuracy: 0.8788 - f1_score: 0.8780 - val_loss: 0.3242 - val_accuracy: 0.8671 - val_f1_score: 0.8703\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2629 - accuracy: 0.8954 - f1_score: 0.8948 - val_loss: 0.3412 - val_accuracy: 0.8517 - val_f1_score: 0.8574\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2533 - accuracy: 0.8984 - f1_score: 0.8982 - val_loss: 0.3283 - val_accuracy: 0.8553 - val_f1_score: 0.8574\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2486 - accuracy: 0.9014 - f1_score: 0.9014 - val_loss: 0.3326 - val_accuracy: 0.8590 - val_f1_score: 0.8641\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2467 - accuracy: 0.9069 - f1_score: 0.9062 - val_loss: 0.3624 - val_accuracy: 0.8300 - val_f1_score: 0.8454\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2321 - accuracy: 0.9090 - f1_score: 0.9087 - val_loss: 0.3358 - val_accuracy: 0.8707 - val_f1_score: 0.8652\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3375 - accuracy: 0.8569 - f1_score: 0.8844\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6319 - accuracy: 0.6477 - f1_score: 0.6439 - val_loss: 0.5399 - val_accuracy: 0.7360 - val_f1_score: 0.7420\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4961 - accuracy: 0.7694 - f1_score: 0.7609 - val_loss: 0.4481 - val_accuracy: 0.7893 - val_f1_score: 0.7791\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4037 - accuracy: 0.8201 - f1_score: 0.8163 - val_loss: 0.4524 - val_accuracy: 0.7920 - val_f1_score: 0.8183\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3690 - accuracy: 0.8430 - f1_score: 0.8419 - val_loss: 0.3839 - val_accuracy: 0.8345 - val_f1_score: 0.8458\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3377 - accuracy: 0.8593 - f1_score: 0.8587 - val_loss: 0.4193 - val_accuracy: 0.8083 - val_f1_score: 0.8317\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3252 - accuracy: 0.8644 - f1_score: 0.8639 - val_loss: 0.3684 - val_accuracy: 0.8400 - val_f1_score: 0.8526\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3143 - accuracy: 0.8740 - f1_score: 0.8733 - val_loss: 0.3502 - val_accuracy: 0.8472 - val_f1_score: 0.8564\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3032 - accuracy: 0.8755 - f1_score: 0.8756 - val_loss: 0.3235 - val_accuracy: 0.8562 - val_f1_score: 0.8561\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2811 - accuracy: 0.8846 - f1_score: 0.8833 - val_loss: 0.3368 - val_accuracy: 0.8562 - val_f1_score: 0.8604\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2812 - accuracy: 0.8852 - f1_score: 0.8839 - val_loss: 0.3216 - val_accuracy: 0.8698 - val_f1_score: 0.8684\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2712 - accuracy: 0.8879 - f1_score: 0.8871 - val_loss: 0.3281 - val_accuracy: 0.8725 - val_f1_score: 0.8683\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2829 - accuracy: 0.8849 - f1_score: 0.8839 - val_loss: 0.3554 - val_accuracy: 0.8580 - val_f1_score: 0.8441\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2609 - accuracy: 0.8915 - f1_score: 0.8900 - val_loss: 0.3879 - val_accuracy: 0.8282 - val_f1_score: 0.8453\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2426 - accuracy: 0.9066 - f1_score: 0.9066 - val_loss: 0.3409 - val_accuracy: 0.8617 - val_f1_score: 0.8661\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2450 - accuracy: 0.8975 - f1_score: 0.8964 - val_loss: 0.4599 - val_accuracy: 0.7948 - val_f1_score: 0.8220\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3575 - accuracy: 0.8521 - f1_score: 0.8759\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6241 - accuracy: 0.6528 - f1_score: 0.6296 - val_loss: 0.5531 - val_accuracy: 0.7342 - val_f1_score: 0.7525\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4786 - accuracy: 0.7797 - f1_score: 0.7715 - val_loss: 0.4483 - val_accuracy: 0.7884 - val_f1_score: 0.7776\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4176 - accuracy: 0.8065 - f1_score: 0.8005 - val_loss: 0.3932 - val_accuracy: 0.8228 - val_f1_score: 0.8161\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3558 - accuracy: 0.8454 - f1_score: 0.8446 - val_loss: 0.3719 - val_accuracy: 0.8409 - val_f1_score: 0.8240\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3354 - accuracy: 0.8577 - f1_score: 0.8554 - val_loss: 0.3435 - val_accuracy: 0.8454 - val_f1_score: 0.8504\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3176 - accuracy: 0.8662 - f1_score: 0.8642 - val_loss: 0.3317 - val_accuracy: 0.8580 - val_f1_score: 0.8645\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3015 - accuracy: 0.8728 - f1_score: 0.8713 - val_loss: 0.3848 - val_accuracy: 0.8318 - val_f1_score: 0.8483\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2974 - accuracy: 0.8800 - f1_score: 0.8798 - val_loss: 0.3161 - val_accuracy: 0.8689 - val_f1_score: 0.8646\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2862 - accuracy: 0.8867 - f1_score: 0.8858 - val_loss: 0.3396 - val_accuracy: 0.8472 - val_f1_score: 0.8516\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2703 - accuracy: 0.8852 - f1_score: 0.8837 - val_loss: 0.3219 - val_accuracy: 0.8644 - val_f1_score: 0.8680\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2743 - accuracy: 0.8834 - f1_score: 0.8824 - val_loss: 0.3452 - val_accuracy: 0.8580 - val_f1_score: 0.8453\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2649 - accuracy: 0.8921 - f1_score: 0.8908 - val_loss: 0.3211 - val_accuracy: 0.8680 - val_f1_score: 0.8628\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2439 - accuracy: 0.8963 - f1_score: 0.8949 - val_loss: 0.4089 - val_accuracy: 0.8201 - val_f1_score: 0.7867\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3690 - accuracy: 0.8460 - f1_score: 0.8690\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.6319 - accuracy: 0.6407 - f1_score: 0.6310 - val_loss: 0.5173 - val_accuracy: 0.7722 - val_f1_score: 0.7662\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4847 - accuracy: 0.7737 - f1_score: 0.7620 - val_loss: 0.4145 - val_accuracy: 0.8065 - val_f1_score: 0.7902\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4147 - accuracy: 0.8149 - f1_score: 0.8078 - val_loss: 0.3919 - val_accuracy: 0.8318 - val_f1_score: 0.8117\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3600 - accuracy: 0.8430 - f1_score: 0.8410 - val_loss: 0.3276 - val_accuracy: 0.8635 - val_f1_score: 0.8595\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3218 - accuracy: 0.8641 - f1_score: 0.8621 - val_loss: 0.3408 - val_accuracy: 0.8562 - val_f1_score: 0.8446\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3166 - accuracy: 0.8731 - f1_score: 0.8715 - val_loss: 0.3943 - val_accuracy: 0.8318 - val_f1_score: 0.8067\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3053 - accuracy: 0.8749 - f1_score: 0.8738 - val_loss: 0.3015 - val_accuracy: 0.8734 - val_f1_score: 0.8746\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2888 - accuracy: 0.8807 - f1_score: 0.8796 - val_loss: 0.3743 - val_accuracy: 0.8436 - val_f1_score: 0.8233\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2758 - accuracy: 0.8930 - f1_score: 0.8916 - val_loss: 0.3227 - val_accuracy: 0.8716 - val_f1_score: 0.8660\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2809 - accuracy: 0.8873 - f1_score: 0.8867 - val_loss: 0.3633 - val_accuracy: 0.8418 - val_f1_score: 0.8209\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2607 - accuracy: 0.8963 - f1_score: 0.8952 - val_loss: 0.2988 - val_accuracy: 0.8843 - val_f1_score: 0.8826\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2499 - accuracy: 0.9017 - f1_score: 0.9004 - val_loss: 0.3303 - val_accuracy: 0.8608 - val_f1_score: 0.8508\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2568 - accuracy: 0.8999 - f1_score: 0.8991 - val_loss: 0.3118 - val_accuracy: 0.8698 - val_f1_score: 0.8659\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2459 - accuracy: 0.8996 - f1_score: 0.8989 - val_loss: 0.3264 - val_accuracy: 0.8671 - val_f1_score: 0.8607\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2268 - accuracy: 0.9150 - f1_score: 0.9139 - val_loss: 0.3551 - val_accuracy: 0.8463 - val_f1_score: 0.8327\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2248 - accuracy: 0.9186 - f1_score: 0.9176 - val_loss: 0.3104 - val_accuracy: 0.8770 - val_f1_score: 0.8719\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3535 - accuracy: 0.8643 - f1_score: 0.8894\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6261 - accuracy: 0.6579 - f1_score: 0.6491 - val_loss: 0.5424 - val_accuracy: 0.7378 - val_f1_score: 0.7023\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4921 - accuracy: 0.7703 - f1_score: 0.7636 - val_loss: 0.4435 - val_accuracy: 0.7902 - val_f1_score: 0.7968\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3995 - accuracy: 0.8243 - f1_score: 0.8215 - val_loss: 0.3914 - val_accuracy: 0.8282 - val_f1_score: 0.8276\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3365 - accuracy: 0.8541 - f1_score: 0.8526 - val_loss: 0.3902 - val_accuracy: 0.8400 - val_f1_score: 0.8329\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3229 - accuracy: 0.8680 - f1_score: 0.8674 - val_loss: 0.3718 - val_accuracy: 0.8517 - val_f1_score: 0.8402\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2987 - accuracy: 0.8825 - f1_score: 0.8822 - val_loss: 0.3415 - val_accuracy: 0.8617 - val_f1_score: 0.8603\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2848 - accuracy: 0.8858 - f1_score: 0.8848 - val_loss: 0.3515 - val_accuracy: 0.8617 - val_f1_score: 0.8625\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2862 - accuracy: 0.8846 - f1_score: 0.8834 - val_loss: 0.3337 - val_accuracy: 0.8644 - val_f1_score: 0.8603\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2789 - accuracy: 0.8894 - f1_score: 0.8883 - val_loss: 0.3371 - val_accuracy: 0.8617 - val_f1_score: 0.8659\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2689 - accuracy: 0.8951 - f1_score: 0.8945 - val_loss: 0.3273 - val_accuracy: 0.8734 - val_f1_score: 0.8701\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2584 - accuracy: 0.9014 - f1_score: 0.9006 - val_loss: 0.3533 - val_accuracy: 0.8617 - val_f1_score: 0.8525\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2441 - accuracy: 0.9042 - f1_score: 0.9038 - val_loss: 0.3463 - val_accuracy: 0.8617 - val_f1_score: 0.8577\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2324 - accuracy: 0.9117 - f1_score: 0.9114 - val_loss: 0.3482 - val_accuracy: 0.8689 - val_f1_score: 0.8707\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2430 - accuracy: 0.9039 - f1_score: 0.9032 - val_loss: 0.3377 - val_accuracy: 0.8743 - val_f1_score: 0.8753\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2286 - accuracy: 0.9102 - f1_score: 0.9100 - val_loss: 0.3936 - val_accuracy: 0.8580 - val_f1_score: 0.8438\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3416 - accuracy: 0.8569 - f1_score: 0.8826\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6186 - accuracy: 0.6618 - f1_score: 0.6445 - val_loss: 0.4849 - val_accuracy: 0.7731 - val_f1_score: 0.7708\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4880 - accuracy: 0.7676 - f1_score: 0.7599 - val_loss: 0.3755 - val_accuracy: 0.8273 - val_f1_score: 0.8133\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4217 - accuracy: 0.8116 - f1_score: 0.8075 - val_loss: 0.3365 - val_accuracy: 0.8599 - val_f1_score: 0.8547\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3670 - accuracy: 0.8373 - f1_score: 0.8349 - val_loss: 0.3028 - val_accuracy: 0.8761 - val_f1_score: 0.8776\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3363 - accuracy: 0.8611 - f1_score: 0.8592 - val_loss: 0.3067 - val_accuracy: 0.8707 - val_f1_score: 0.8787\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3268 - accuracy: 0.8653 - f1_score: 0.8628 - val_loss: 0.3140 - val_accuracy: 0.8689 - val_f1_score: 0.8774\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3111 - accuracy: 0.8698 - f1_score: 0.8688 - val_loss: 0.2958 - val_accuracy: 0.8807 - val_f1_score: 0.8874\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2969 - accuracy: 0.8770 - f1_score: 0.8762 - val_loss: 0.2622 - val_accuracy: 0.8978 - val_f1_score: 0.8970\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2899 - accuracy: 0.8807 - f1_score: 0.8793 - val_loss: 0.2872 - val_accuracy: 0.8825 - val_f1_score: 0.8885\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2798 - accuracy: 0.8837 - f1_score: 0.8825 - val_loss: 0.2626 - val_accuracy: 0.9024 - val_f1_score: 0.8996\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2712 - accuracy: 0.8924 - f1_score: 0.8913 - val_loss: 0.2745 - val_accuracy: 0.8915 - val_f1_score: 0.8949\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2591 - accuracy: 0.8921 - f1_score: 0.8911 - val_loss: 0.2619 - val_accuracy: 0.9005 - val_f1_score: 0.9005\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2589 - accuracy: 0.8945 - f1_score: 0.8933 - val_loss: 0.2917 - val_accuracy: 0.8915 - val_f1_score: 0.8976\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2499 - accuracy: 0.8963 - f1_score: 0.8952 - val_loss: 0.2839 - val_accuracy: 0.8915 - val_f1_score: 0.8883\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2310 - accuracy: 0.9126 - f1_score: 0.9111 - val_loss: 0.3583 - val_accuracy: 0.8617 - val_f1_score: 0.8743\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2310 - accuracy: 0.9054 - f1_score: 0.9044 - val_loss: 0.2728 - val_accuracy: 0.9042 - val_f1_score: 0.9038\n","Epoch 17/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2183 - accuracy: 0.9165 - f1_score: 0.9160 - val_loss: 0.3030 - val_accuracy: 0.8870 - val_f1_score: 0.8785\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3411 - accuracy: 0.8582 - f1_score: 0.8818\n","Epoch 1/20\n","104/104 [==============================] - 9s 21ms/step - loss: 0.6249 - accuracy: 0.6486 - f1_score: 0.6217 - val_loss: 0.5424 - val_accuracy: 0.7432 - val_f1_score: 0.7253\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4898 - accuracy: 0.7833 - f1_score: 0.7798 - val_loss: 0.4423 - val_accuracy: 0.7884 - val_f1_score: 0.8060\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4106 - accuracy: 0.8213 - f1_score: 0.8206 - val_loss: 0.3985 - val_accuracy: 0.8273 - val_f1_score: 0.8137\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3759 - accuracy: 0.8403 - f1_score: 0.8391 - val_loss: 0.3750 - val_accuracy: 0.8391 - val_f1_score: 0.8311\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3316 - accuracy: 0.8632 - f1_score: 0.8618 - val_loss: 0.3891 - val_accuracy: 0.8345 - val_f1_score: 0.8461\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3290 - accuracy: 0.8626 - f1_score: 0.8617 - val_loss: 0.3700 - val_accuracy: 0.8436 - val_f1_score: 0.8520\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3189 - accuracy: 0.8698 - f1_score: 0.8681 - val_loss: 0.3994 - val_accuracy: 0.8264 - val_f1_score: 0.8426\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3087 - accuracy: 0.8755 - f1_score: 0.8742 - val_loss: 0.3565 - val_accuracy: 0.8662 - val_f1_score: 0.8667\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3069 - accuracy: 0.8785 - f1_score: 0.8769 - val_loss: 0.3418 - val_accuracy: 0.8608 - val_f1_score: 0.8620\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2840 - accuracy: 0.8870 - f1_score: 0.8858 - val_loss: 0.3918 - val_accuracy: 0.8382 - val_f1_score: 0.8474\n","Epoch 11/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2868 - accuracy: 0.8888 - f1_score: 0.8878 - val_loss: 0.3366 - val_accuracy: 0.8698 - val_f1_score: 0.8672\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2702 - accuracy: 0.8963 - f1_score: 0.8956 - val_loss: 0.3368 - val_accuracy: 0.8662 - val_f1_score: 0.8625\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2635 - accuracy: 0.8972 - f1_score: 0.8960 - val_loss: 0.3462 - val_accuracy: 0.8617 - val_f1_score: 0.8610\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2506 - accuracy: 0.9060 - f1_score: 0.9049 - val_loss: 0.3537 - val_accuracy: 0.8580 - val_f1_score: 0.8599\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2354 - accuracy: 0.9069 - f1_score: 0.9056 - val_loss: 0.3701 - val_accuracy: 0.8544 - val_f1_score: 0.8571\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2386 - accuracy: 0.9060 - f1_score: 0.9055 - val_loss: 0.3472 - val_accuracy: 0.8617 - val_f1_score: 0.8625\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8589 - f1_score: 0.8815\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6381 - accuracy: 0.6374 - f1_score: 0.6065 - val_loss: 0.5648 - val_accuracy: 0.7116 - val_f1_score: 0.6387\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4845 - accuracy: 0.7719 - f1_score: 0.7642 - val_loss: 0.4604 - val_accuracy: 0.7920 - val_f1_score: 0.8148\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3977 - accuracy: 0.8267 - f1_score: 0.8242 - val_loss: 0.4432 - val_accuracy: 0.8174 - val_f1_score: 0.8339\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3596 - accuracy: 0.8445 - f1_score: 0.8413 - val_loss: 0.3714 - val_accuracy: 0.8427 - val_f1_score: 0.8368\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3284 - accuracy: 0.8629 - f1_score: 0.8614 - val_loss: 0.3714 - val_accuracy: 0.8490 - val_f1_score: 0.8472\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3009 - accuracy: 0.8788 - f1_score: 0.8779 - val_loss: 0.3967 - val_accuracy: 0.8373 - val_f1_score: 0.8485\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2976 - accuracy: 0.8788 - f1_score: 0.8775 - val_loss: 0.3547 - val_accuracy: 0.8508 - val_f1_score: 0.8551\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2783 - accuracy: 0.8963 - f1_score: 0.8958 - val_loss: 0.3564 - val_accuracy: 0.8499 - val_f1_score: 0.8546\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2662 - accuracy: 0.8948 - f1_score: 0.8940 - val_loss: 0.3821 - val_accuracy: 0.8427 - val_f1_score: 0.8260\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2797 - accuracy: 0.8921 - f1_score: 0.8904 - val_loss: 0.3386 - val_accuracy: 0.8608 - val_f1_score: 0.8566\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2675 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3622 - val_accuracy: 0.8544 - val_f1_score: 0.8556\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2512 - accuracy: 0.9060 - f1_score: 0.9052 - val_loss: 0.3529 - val_accuracy: 0.8571 - val_f1_score: 0.8558\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2589 - accuracy: 0.9002 - f1_score: 0.8990 - val_loss: 0.4256 - val_accuracy: 0.8363 - val_f1_score: 0.8473\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2402 - accuracy: 0.9069 - f1_score: 0.9058 - val_loss: 0.3703 - val_accuracy: 0.8517 - val_f1_score: 0.8559\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2210 - accuracy: 0.9153 - f1_score: 0.9146 - val_loss: 0.3675 - val_accuracy: 0.8617 - val_f1_score: 0.8577\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3566 - accuracy: 0.8427 - f1_score: 0.8635\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6143 - accuracy: 0.6670 - f1_score: 0.6622 - val_loss: 0.4885 - val_accuracy: 0.7676 - val_f1_score: 0.7498\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4561 - accuracy: 0.7899 - f1_score: 0.7818 - val_loss: 0.3955 - val_accuracy: 0.8291 - val_f1_score: 0.8283\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3837 - accuracy: 0.8333 - f1_score: 0.8304 - val_loss: 0.3679 - val_accuracy: 0.8544 - val_f1_score: 0.8596\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3572 - accuracy: 0.8472 - f1_score: 0.8453 - val_loss: 0.3532 - val_accuracy: 0.8544 - val_f1_score: 0.8447\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3345 - accuracy: 0.8571 - f1_score: 0.8564 - val_loss: 0.3273 - val_accuracy: 0.8707 - val_f1_score: 0.8675\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3174 - accuracy: 0.8722 - f1_score: 0.8714 - val_loss: 0.3278 - val_accuracy: 0.8698 - val_f1_score: 0.8754\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3090 - accuracy: 0.8819 - f1_score: 0.8809 - val_loss: 0.3328 - val_accuracy: 0.8599 - val_f1_score: 0.8634\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3141 - accuracy: 0.8698 - f1_score: 0.8681 - val_loss: 0.3080 - val_accuracy: 0.8779 - val_f1_score: 0.8763\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2885 - accuracy: 0.8870 - f1_score: 0.8859 - val_loss: 0.3404 - val_accuracy: 0.8653 - val_f1_score: 0.8558\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2939 - accuracy: 0.8782 - f1_score: 0.8771 - val_loss: 0.3106 - val_accuracy: 0.8807 - val_f1_score: 0.8791\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2646 - accuracy: 0.8963 - f1_score: 0.8951 - val_loss: 0.3252 - val_accuracy: 0.8644 - val_f1_score: 0.8680\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2565 - accuracy: 0.8993 - f1_score: 0.8986 - val_loss: 0.3203 - val_accuracy: 0.8734 - val_f1_score: 0.8759\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2621 - accuracy: 0.8975 - f1_score: 0.8970 - val_loss: 0.3314 - val_accuracy: 0.8635 - val_f1_score: 0.8648\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3394 - accuracy: 0.8616 - f1_score: 0.8828\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6301 - accuracy: 0.6435 - f1_score: 0.6474 - val_loss: 0.5477 - val_accuracy: 0.7224 - val_f1_score: 0.6933\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4839 - accuracy: 0.7728 - f1_score: 0.7651 - val_loss: 0.4208 - val_accuracy: 0.8119 - val_f1_score: 0.8088\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4174 - accuracy: 0.8162 - f1_score: 0.8127 - val_loss: 0.3878 - val_accuracy: 0.8354 - val_f1_score: 0.8302\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3617 - accuracy: 0.8397 - f1_score: 0.8372 - val_loss: 0.3571 - val_accuracy: 0.8535 - val_f1_score: 0.8530\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3413 - accuracy: 0.8565 - f1_score: 0.8549 - val_loss: 0.3414 - val_accuracy: 0.8626 - val_f1_score: 0.8593\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3156 - accuracy: 0.8731 - f1_score: 0.8720 - val_loss: 0.3343 - val_accuracy: 0.8671 - val_f1_score: 0.8617\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3032 - accuracy: 0.8794 - f1_score: 0.8791 - val_loss: 0.3262 - val_accuracy: 0.8716 - val_f1_score: 0.8670\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2889 - accuracy: 0.8819 - f1_score: 0.8809 - val_loss: 0.3184 - val_accuracy: 0.8788 - val_f1_score: 0.8733\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2931 - accuracy: 0.8746 - f1_score: 0.8739 - val_loss: 0.3430 - val_accuracy: 0.8635 - val_f1_score: 0.8518\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2913 - accuracy: 0.8849 - f1_score: 0.8840 - val_loss: 0.3336 - val_accuracy: 0.8662 - val_f1_score: 0.8549\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2724 - accuracy: 0.8927 - f1_score: 0.8918 - val_loss: 0.3081 - val_accuracy: 0.8870 - val_f1_score: 0.8844\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2563 - accuracy: 0.8966 - f1_score: 0.8965 - val_loss: 0.3042 - val_accuracy: 0.8807 - val_f1_score: 0.8796\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2604 - accuracy: 0.8963 - f1_score: 0.8956 - val_loss: 0.3194 - val_accuracy: 0.8770 - val_f1_score: 0.8705\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2522 - accuracy: 0.8990 - f1_score: 0.8984 - val_loss: 0.3131 - val_accuracy: 0.8834 - val_f1_score: 0.8791\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2379 - accuracy: 0.9075 - f1_score: 0.9068 - val_loss: 0.3505 - val_accuracy: 0.8635 - val_f1_score: 0.8533\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2382 - accuracy: 0.9093 - f1_score: 0.9088 - val_loss: 0.3190 - val_accuracy: 0.8834 - val_f1_score: 0.8789\n","Epoch 17/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2253 - accuracy: 0.9117 - f1_score: 0.9111 - val_loss: 0.3270 - val_accuracy: 0.8707 - val_f1_score: 0.8642\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3116 - accuracy: 0.8778 - f1_score: 0.8997\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6295 - accuracy: 0.6492 - f1_score: 0.6410 - val_loss: 0.5264 - val_accuracy: 0.7396 - val_f1_score: 0.6930\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4975 - accuracy: 0.7622 - f1_score: 0.7524 - val_loss: 0.4185 - val_accuracy: 0.8192 - val_f1_score: 0.8322\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3957 - accuracy: 0.8300 - f1_score: 0.8272 - val_loss: 0.3638 - val_accuracy: 0.8526 - val_f1_score: 0.8397\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3700 - accuracy: 0.8451 - f1_score: 0.8420 - val_loss: 0.3984 - val_accuracy: 0.8219 - val_f1_score: 0.8405\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3390 - accuracy: 0.8680 - f1_score: 0.8669 - val_loss: 0.3219 - val_accuracy: 0.8707 - val_f1_score: 0.8720\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3210 - accuracy: 0.8665 - f1_score: 0.8651 - val_loss: 0.3331 - val_accuracy: 0.8590 - val_f1_score: 0.8655\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3050 - accuracy: 0.8813 - f1_score: 0.8794 - val_loss: 0.3347 - val_accuracy: 0.8671 - val_f1_score: 0.8552\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3098 - accuracy: 0.8713 - f1_score: 0.8695 - val_loss: 0.3122 - val_accuracy: 0.8752 - val_f1_score: 0.8770\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2848 - accuracy: 0.8864 - f1_score: 0.8854 - val_loss: 0.3047 - val_accuracy: 0.8816 - val_f1_score: 0.8790\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2920 - accuracy: 0.8825 - f1_score: 0.8815 - val_loss: 0.3156 - val_accuracy: 0.8743 - val_f1_score: 0.8657\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2786 - accuracy: 0.8867 - f1_score: 0.8853 - val_loss: 0.3275 - val_accuracy: 0.8725 - val_f1_score: 0.8632\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2631 - accuracy: 0.8996 - f1_score: 0.8984 - val_loss: 0.3607 - val_accuracy: 0.8445 - val_f1_score: 0.8241\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2537 - accuracy: 0.9005 - f1_score: 0.8993 - val_loss: 0.3252 - val_accuracy: 0.8562 - val_f1_score: 0.8449\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2462 - accuracy: 0.9057 - f1_score: 0.9045 - val_loss: 0.3204 - val_accuracy: 0.8635 - val_f1_score: 0.8653\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3217 - accuracy: 0.8609 - f1_score: 0.8831\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFnid5-SREYK","executionInfo":{"status":"ok","timestamp":1690579133456,"user_tz":-330,"elapsed":58,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"17e3c124-e7e5-46f0-b914-18a7ebd61609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8548278212547302, 0.8582038879394531, 0.8622552156448364, 0.8440243005752563, 0.8649561405181885, 0.8649561405181885, 0.8541526198387146, 0.8386225700378418, 0.8582038879394531, 0.870357871055603, 0.8676570057868958, 0.8629304766654968, 0.852126955986023, 0.8541526198387146, 0.8568534851074219, 0.852126955986023, 0.846049964427948, 0.8642808794975281, 0.8568534851074219, 0.8582038879394531, 0.8588791489601135, 0.8426738977432251, 0.8615800142288208, 0.8777852654457092, 0.8609048128128052]\n","0.8577447724342346\n","0.008709480074542284\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfZ4s_uSRI0Y","executionInfo":{"status":"ok","timestamp":1690579133456,"user_tz":-330,"elapsed":16,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"e21f21f1-9f6b-4cd3-8c88-9da5125d85f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3313765227794647, 0.3353334367275238, 0.3348371386528015, 0.3669673502445221, 0.31540554761886597, 0.3253156244754791, 0.3416121304035187, 0.35957202315330505, 0.33273035287857056, 0.3198983073234558, 0.32261669635772705, 0.3169587552547455, 0.3406760096549988, 0.33956408500671387, 0.337548166513443, 0.35751962661743164, 0.36898863315582275, 0.3535274565219879, 0.3415670692920685, 0.3411063551902771, 0.33435824513435364, 0.3566383123397827, 0.33936476707458496, 0.3115939199924469, 0.32170039415359497]\n","0.33787107706069947\n","0.015523359563889049\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhkDOQDHRI4H","executionInfo":{"status":"ok","timestamp":1690579133457,"user_tz":-330,"elapsed":11,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"897c5f40-65f8-460b-b64a-ef5ed2ec8b24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8784623146057129, 0.8817567229270935, 0.8839589953422546, 0.8653060793876648, 0.8891352415084839, 0.8916575312614441, 0.8776895999908447, 0.8636622428894043, 0.8844884037971497, 0.8954247832298279, 0.8960762619972229, 0.8897337317466736, 0.8773108720779419, 0.8776895999908447, 0.8844056725502014, 0.8759205937385559, 0.8689655065536499, 0.8893779516220093, 0.882613480091095, 0.8817567229270935, 0.8814520239830017, 0.8635031580924988, 0.8827900886535645, 0.8997228741645813, 0.8830873966217041]\n","0.8818379139900208\n","0.00938992109575628\n"]}]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing -\n","1. Accuracy - 0.861/0.007\n","2. Loss - 0.335/0.018\n","3. F1 score - 0.885/0.008\n","\n","LSTM CNN LSTM nothing -\n","1. Accuracy - 0.857/0.008\n","2. Loss - 0.337/0.015\n","3. F1 score - 0.881/0.009"],"metadata":{"id":"JkLeiVdLSI-X"}},{"cell_type":"code","source":[],"metadata":{"id":"am-XUNn1SIcI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AW7R3pnFSIfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZJ465fpKSIhs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lsW9RMQ2SIkg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ELw1enzRsJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8tNVy1v4Rtn1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## put a LSTM layer after applying CNN layer to emotions with maxpooling (pool size 5 and stride 2) (with additional LSTM layer in emotions)"],"metadata":{"id":"UuPJoUDNRtvx"}},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_lstm_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-JQ5BFgRzK9","executionInfo":{"status":"ok","timestamp":1690579790726,"user_tz":-330,"elapsed":657274,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"383754bf-0340-4a41-da72-702639ff5246"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6328 - accuracy: 0.6386 - f1_score: 0.6284 - val_loss: 0.4979 - val_accuracy: 0.7848 - val_f1_score: 0.7746\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5159 - accuracy: 0.7520 - f1_score: 0.7408 - val_loss: 0.4071 - val_accuracy: 0.8237 - val_f1_score: 0.8141\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4350 - accuracy: 0.8014 - f1_score: 0.7932 - val_loss: 0.4357 - val_accuracy: 0.8083 - val_f1_score: 0.8285\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3778 - accuracy: 0.8360 - f1_score: 0.8316 - val_loss: 0.3626 - val_accuracy: 0.8481 - val_f1_score: 0.8330\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3586 - accuracy: 0.8421 - f1_score: 0.8390 - val_loss: 0.3266 - val_accuracy: 0.8608 - val_f1_score: 0.8613\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3386 - accuracy: 0.8556 - f1_score: 0.8527 - val_loss: 0.3541 - val_accuracy: 0.8571 - val_f1_score: 0.8661\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3207 - accuracy: 0.8695 - f1_score: 0.8657 - val_loss: 0.3294 - val_accuracy: 0.8626 - val_f1_score: 0.8694\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3142 - accuracy: 0.8689 - f1_score: 0.8677 - val_loss: 0.3093 - val_accuracy: 0.8662 - val_f1_score: 0.8582\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3066 - accuracy: 0.8779 - f1_score: 0.8756 - val_loss: 0.3132 - val_accuracy: 0.8698 - val_f1_score: 0.8739\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2802 - accuracy: 0.8900 - f1_score: 0.8887 - val_loss: 0.3237 - val_accuracy: 0.8580 - val_f1_score: 0.8500\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2727 - accuracy: 0.8930 - f1_score: 0.8911 - val_loss: 0.3263 - val_accuracy: 0.8617 - val_f1_score: 0.8539\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2624 - accuracy: 0.8969 - f1_score: 0.8953 - val_loss: 0.2969 - val_accuracy: 0.8861 - val_f1_score: 0.8844\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2443 - accuracy: 0.9060 - f1_score: 0.9047 - val_loss: 0.3331 - val_accuracy: 0.8770 - val_f1_score: 0.8809\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2395 - accuracy: 0.9147 - f1_score: 0.9132 - val_loss: 0.3432 - val_accuracy: 0.8653 - val_f1_score: 0.8639\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2379 - accuracy: 0.9093 - f1_score: 0.9076 - val_loss: 0.3295 - val_accuracy: 0.8725 - val_f1_score: 0.8744\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2151 - accuracy: 0.9174 - f1_score: 0.9165 - val_loss: 0.3446 - val_accuracy: 0.8635 - val_f1_score: 0.8648\n","Epoch 17/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2112 - accuracy: 0.9241 - f1_score: 0.9230 - val_loss: 0.3567 - val_accuracy: 0.8626 - val_f1_score: 0.8519\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3482 - accuracy: 0.8562 - f1_score: 0.8774\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6107 - accuracy: 0.6661 - f1_score: 0.6695 - val_loss: 0.5548 - val_accuracy: 0.7152 - val_f1_score: 0.7249\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4920 - accuracy: 0.7709 - f1_score: 0.7638 - val_loss: 0.4828 - val_accuracy: 0.7703 - val_f1_score: 0.7455\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4295 - accuracy: 0.8068 - f1_score: 0.8000 - val_loss: 0.4360 - val_accuracy: 0.8038 - val_f1_score: 0.8137\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3771 - accuracy: 0.8303 - f1_score: 0.8285 - val_loss: 0.3973 - val_accuracy: 0.8336 - val_f1_score: 0.8354\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3442 - accuracy: 0.8454 - f1_score: 0.8440 - val_loss: 0.3781 - val_accuracy: 0.8273 - val_f1_score: 0.8329\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3340 - accuracy: 0.8611 - f1_score: 0.8598 - val_loss: 0.3710 - val_accuracy: 0.8472 - val_f1_score: 0.8451\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3049 - accuracy: 0.8752 - f1_score: 0.8745 - val_loss: 0.3685 - val_accuracy: 0.8472 - val_f1_score: 0.8459\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3281 - accuracy: 0.8671 - f1_score: 0.8654 - val_loss: 0.3854 - val_accuracy: 0.8318 - val_f1_score: 0.8410\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2842 - accuracy: 0.8858 - f1_score: 0.8854 - val_loss: 0.3517 - val_accuracy: 0.8571 - val_f1_score: 0.8558\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2720 - accuracy: 0.8879 - f1_score: 0.8873 - val_loss: 0.3656 - val_accuracy: 0.8472 - val_f1_score: 0.8516\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2626 - accuracy: 0.8954 - f1_score: 0.8945 - val_loss: 0.3780 - val_accuracy: 0.8571 - val_f1_score: 0.8463\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2619 - accuracy: 0.8960 - f1_score: 0.8947 - val_loss: 0.3972 - val_accuracy: 0.8463 - val_f1_score: 0.8333\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2554 - accuracy: 0.8990 - f1_score: 0.8988 - val_loss: 0.3774 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2348 - accuracy: 0.9096 - f1_score: 0.9086 - val_loss: 0.3723 - val_accuracy: 0.8580 - val_f1_score: 0.8579\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3237 - accuracy: 0.8636 - f1_score: 0.8860\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6240 - accuracy: 0.6555 - f1_score: 0.6440 - val_loss: 0.5572 - val_accuracy: 0.7369 - val_f1_score: 0.6834\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4899 - accuracy: 0.7700 - f1_score: 0.7609 - val_loss: 0.5200 - val_accuracy: 0.7622 - val_f1_score: 0.7075\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4212 - accuracy: 0.8119 - f1_score: 0.8065 - val_loss: 0.4013 - val_accuracy: 0.8092 - val_f1_score: 0.8059\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3675 - accuracy: 0.8478 - f1_score: 0.8459 - val_loss: 0.4207 - val_accuracy: 0.8119 - val_f1_score: 0.8284\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3317 - accuracy: 0.8593 - f1_score: 0.8578 - val_loss: 0.4136 - val_accuracy: 0.8083 - val_f1_score: 0.7819\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3252 - accuracy: 0.8635 - f1_score: 0.8622 - val_loss: 0.4017 - val_accuracy: 0.8192 - val_f1_score: 0.8035\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2984 - accuracy: 0.8779 - f1_score: 0.8768 - val_loss: 0.3964 - val_accuracy: 0.8345 - val_f1_score: 0.8407\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2891 - accuracy: 0.8819 - f1_score: 0.8800 - val_loss: 0.4012 - val_accuracy: 0.8300 - val_f1_score: 0.8223\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2763 - accuracy: 0.8867 - f1_score: 0.8855 - val_loss: 0.4016 - val_accuracy: 0.8327 - val_f1_score: 0.8341\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2770 - accuracy: 0.8903 - f1_score: 0.8893 - val_loss: 0.3914 - val_accuracy: 0.8418 - val_f1_score: 0.8363\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2554 - accuracy: 0.8999 - f1_score: 0.8988 - val_loss: 0.4093 - val_accuracy: 0.8291 - val_f1_score: 0.8338\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2468 - accuracy: 0.9036 - f1_score: 0.9025 - val_loss: 0.4117 - val_accuracy: 0.8282 - val_f1_score: 0.8159\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2483 - accuracy: 0.9008 - f1_score: 0.8996 - val_loss: 0.4219 - val_accuracy: 0.8210 - val_f1_score: 0.8040\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2331 - accuracy: 0.9102 - f1_score: 0.9094 - val_loss: 0.4258 - val_accuracy: 0.8336 - val_f1_score: 0.8254\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2177 - accuracy: 0.9102 - f1_score: 0.9093 - val_loss: 0.4611 - val_accuracy: 0.8219 - val_f1_score: 0.8264\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3474 - accuracy: 0.8569 - f1_score: 0.8791\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6215 - accuracy: 0.6516 - f1_score: 0.6247 - val_loss: 0.5015 - val_accuracy: 0.7640 - val_f1_score: 0.7721\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4884 - accuracy: 0.7682 - f1_score: 0.7561 - val_loss: 0.4146 - val_accuracy: 0.8309 - val_f1_score: 0.8302\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4106 - accuracy: 0.8137 - f1_score: 0.8091 - val_loss: 0.3798 - val_accuracy: 0.8454 - val_f1_score: 0.8512\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3744 - accuracy: 0.8385 - f1_score: 0.8365 - val_loss: 0.3637 - val_accuracy: 0.8400 - val_f1_score: 0.8488\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3365 - accuracy: 0.8614 - f1_score: 0.8594 - val_loss: 0.3427 - val_accuracy: 0.8626 - val_f1_score: 0.8626\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3247 - accuracy: 0.8674 - f1_score: 0.8659 - val_loss: 0.3491 - val_accuracy: 0.8608 - val_f1_score: 0.8661\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3111 - accuracy: 0.8770 - f1_score: 0.8755 - val_loss: 0.3947 - val_accuracy: 0.8327 - val_f1_score: 0.8482\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2937 - accuracy: 0.8800 - f1_score: 0.8792 - val_loss: 0.3628 - val_accuracy: 0.8590 - val_f1_score: 0.8634\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3027 - accuracy: 0.8749 - f1_score: 0.8737 - val_loss: 0.3605 - val_accuracy: 0.8553 - val_f1_score: 0.8488\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2671 - accuracy: 0.8981 - f1_score: 0.8971 - val_loss: 0.3705 - val_accuracy: 0.8499 - val_f1_score: 0.8574\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3594 - accuracy: 0.8460 - f1_score: 0.8691\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.6170 - accuracy: 0.6603 - f1_score: 0.6477 - val_loss: 0.5227 - val_accuracy: 0.7495 - val_f1_score: 0.7025\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4860 - accuracy: 0.7743 - f1_score: 0.7645 - val_loss: 0.3961 - val_accuracy: 0.8318 - val_f1_score: 0.8262\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3962 - accuracy: 0.8267 - f1_score: 0.8230 - val_loss: 0.3655 - val_accuracy: 0.8382 - val_f1_score: 0.8442\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3556 - accuracy: 0.8499 - f1_score: 0.8471 - val_loss: 0.4560 - val_accuracy: 0.8047 - val_f1_score: 0.8283\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3404 - accuracy: 0.8514 - f1_score: 0.8495 - val_loss: 0.4116 - val_accuracy: 0.8128 - val_f1_score: 0.8329\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3162 - accuracy: 0.8701 - f1_score: 0.8682 - val_loss: 0.3575 - val_accuracy: 0.8517 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3149 - accuracy: 0.8698 - f1_score: 0.8681 - val_loss: 0.3986 - val_accuracy: 0.8354 - val_f1_score: 0.8503\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3086 - accuracy: 0.8707 - f1_score: 0.8689 - val_loss: 0.3809 - val_accuracy: 0.8391 - val_f1_score: 0.8224\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3042 - accuracy: 0.8743 - f1_score: 0.8723 - val_loss: 0.3417 - val_accuracy: 0.8571 - val_f1_score: 0.8607\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2753 - accuracy: 0.8912 - f1_score: 0.8904 - val_loss: 0.3634 - val_accuracy: 0.8499 - val_f1_score: 0.8549\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2624 - accuracy: 0.8936 - f1_score: 0.8923 - val_loss: 0.3650 - val_accuracy: 0.8617 - val_f1_score: 0.8553\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2662 - accuracy: 0.8933 - f1_score: 0.8915 - val_loss: 0.3317 - val_accuracy: 0.8617 - val_f1_score: 0.8608\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2629 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3508 - val_accuracy: 0.8671 - val_f1_score: 0.8609\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2383 - accuracy: 0.9069 - f1_score: 0.9057 - val_loss: 0.3607 - val_accuracy: 0.8599 - val_f1_score: 0.8642\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2244 - accuracy: 0.9090 - f1_score: 0.9079 - val_loss: 0.3687 - val_accuracy: 0.8409 - val_f1_score: 0.8488\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2227 - accuracy: 0.9099 - f1_score: 0.9091 - val_loss: 0.3603 - val_accuracy: 0.8671 - val_f1_score: 0.8660\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2130 - accuracy: 0.9180 - f1_score: 0.9172 - val_loss: 0.4050 - val_accuracy: 0.8481 - val_f1_score: 0.8544\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3402 - accuracy: 0.8569 - f1_score: 0.8782\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6251 - accuracy: 0.6576 - f1_score: 0.6423 - val_loss: 0.5121 - val_accuracy: 0.7505 - val_f1_score: 0.7625\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4814 - accuracy: 0.7706 - f1_score: 0.7645 - val_loss: 0.4026 - val_accuracy: 0.8246 - val_f1_score: 0.8307\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3935 - accuracy: 0.8228 - f1_score: 0.8197 - val_loss: 0.3546 - val_accuracy: 0.8463 - val_f1_score: 0.8540\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3633 - accuracy: 0.8369 - f1_score: 0.8370 - val_loss: 0.3208 - val_accuracy: 0.8698 - val_f1_score: 0.8626\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3238 - accuracy: 0.8602 - f1_score: 0.8593 - val_loss: 0.3123 - val_accuracy: 0.8752 - val_f1_score: 0.8691\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3152 - accuracy: 0.8734 - f1_score: 0.8723 - val_loss: 0.3602 - val_accuracy: 0.8391 - val_f1_score: 0.8546\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3058 - accuracy: 0.8719 - f1_score: 0.8719 - val_loss: 0.3050 - val_accuracy: 0.8807 - val_f1_score: 0.8757\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2908 - accuracy: 0.8807 - f1_score: 0.8801 - val_loss: 0.3115 - val_accuracy: 0.8716 - val_f1_score: 0.8632\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2956 - accuracy: 0.8803 - f1_score: 0.8791 - val_loss: 0.2893 - val_accuracy: 0.8870 - val_f1_score: 0.8854\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2629 - accuracy: 0.8960 - f1_score: 0.8955 - val_loss: 0.2983 - val_accuracy: 0.8825 - val_f1_score: 0.8783\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2511 - accuracy: 0.9048 - f1_score: 0.9042 - val_loss: 0.2993 - val_accuracy: 0.8788 - val_f1_score: 0.8716\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2426 - accuracy: 0.9017 - f1_score: 0.9015 - val_loss: 0.3227 - val_accuracy: 0.8608 - val_f1_score: 0.8475\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2347 - accuracy: 0.9093 - f1_score: 0.9083 - val_loss: 0.3153 - val_accuracy: 0.8734 - val_f1_score: 0.8656\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2195 - accuracy: 0.9192 - f1_score: 0.9187 - val_loss: 0.3010 - val_accuracy: 0.8797 - val_f1_score: 0.8760\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.8555 - f1_score: 0.8795\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6095 - accuracy: 0.6700 - f1_score: 0.6615 - val_loss: 0.5256 - val_accuracy: 0.7441 - val_f1_score: 0.7502\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4747 - accuracy: 0.7776 - f1_score: 0.7731 - val_loss: 0.4327 - val_accuracy: 0.8047 - val_f1_score: 0.8200\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4024 - accuracy: 0.8243 - f1_score: 0.8213 - val_loss: 0.3567 - val_accuracy: 0.8445 - val_f1_score: 0.8436\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3646 - accuracy: 0.8445 - f1_score: 0.8429 - val_loss: 0.3886 - val_accuracy: 0.8345 - val_f1_score: 0.8504\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3278 - accuracy: 0.8620 - f1_score: 0.8604 - val_loss: 0.3309 - val_accuracy: 0.8644 - val_f1_score: 0.8566\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3145 - accuracy: 0.8716 - f1_score: 0.8712 - val_loss: 0.3482 - val_accuracy: 0.8580 - val_f1_score: 0.8664\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2972 - accuracy: 0.8837 - f1_score: 0.8830 - val_loss: 0.3669 - val_accuracy: 0.8481 - val_f1_score: 0.8602\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2861 - accuracy: 0.8846 - f1_score: 0.8845 - val_loss: 0.3080 - val_accuracy: 0.8779 - val_f1_score: 0.8753\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2858 - accuracy: 0.8888 - f1_score: 0.8884 - val_loss: 0.3297 - val_accuracy: 0.8662 - val_f1_score: 0.8722\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2653 - accuracy: 0.8975 - f1_score: 0.8970 - val_loss: 0.3446 - val_accuracy: 0.8644 - val_f1_score: 0.8727\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2573 - accuracy: 0.8936 - f1_score: 0.8932 - val_loss: 0.3134 - val_accuracy: 0.8725 - val_f1_score: 0.8722\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2485 - accuracy: 0.9027 - f1_score: 0.9021 - val_loss: 0.3128 - val_accuracy: 0.8698 - val_f1_score: 0.8714\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2400 - accuracy: 0.9096 - f1_score: 0.9089 - val_loss: 0.3085 - val_accuracy: 0.8725 - val_f1_score: 0.8715\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3684 - accuracy: 0.8454 - f1_score: 0.8676\n","Epoch 1/20\n","104/104 [==============================] - 9s 18ms/step - loss: 0.6180 - accuracy: 0.6606 - f1_score: 0.6486 - val_loss: 0.5282 - val_accuracy: 0.7423 - val_f1_score: 0.7246\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4924 - accuracy: 0.7688 - f1_score: 0.7616 - val_loss: 0.4864 - val_accuracy: 0.7722 - val_f1_score: 0.7928\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4011 - accuracy: 0.8171 - f1_score: 0.8136 - val_loss: 0.4029 - val_accuracy: 0.8101 - val_f1_score: 0.7989\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3593 - accuracy: 0.8406 - f1_score: 0.8373 - val_loss: 0.3812 - val_accuracy: 0.8300 - val_f1_score: 0.8189\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3259 - accuracy: 0.8623 - f1_score: 0.8611 - val_loss: 0.4285 - val_accuracy: 0.8002 - val_f1_score: 0.8231\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3149 - accuracy: 0.8671 - f1_score: 0.8662 - val_loss: 0.3579 - val_accuracy: 0.8499 - val_f1_score: 0.8382\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2913 - accuracy: 0.8776 - f1_score: 0.8761 - val_loss: 0.3487 - val_accuracy: 0.8580 - val_f1_score: 0.8631\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2857 - accuracy: 0.8864 - f1_score: 0.8855 - val_loss: 0.3606 - val_accuracy: 0.8445 - val_f1_score: 0.8537\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2747 - accuracy: 0.8900 - f1_score: 0.8894 - val_loss: 0.3398 - val_accuracy: 0.8662 - val_f1_score: 0.8688\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2656 - accuracy: 0.8927 - f1_score: 0.8917 - val_loss: 0.3772 - val_accuracy: 0.8517 - val_f1_score: 0.8591\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2728 - accuracy: 0.8840 - f1_score: 0.8838 - val_loss: 0.3321 - val_accuracy: 0.8644 - val_f1_score: 0.8601\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2518 - accuracy: 0.8972 - f1_score: 0.8963 - val_loss: 0.3455 - val_accuracy: 0.8490 - val_f1_score: 0.8571\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2257 - accuracy: 0.9153 - f1_score: 0.9150 - val_loss: 0.3546 - val_accuracy: 0.8698 - val_f1_score: 0.8730\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2347 - accuracy: 0.9048 - f1_score: 0.9043 - val_loss: 0.3578 - val_accuracy: 0.8553 - val_f1_score: 0.8444\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2244 - accuracy: 0.9114 - f1_score: 0.9110 - val_loss: 0.3764 - val_accuracy: 0.8599 - val_f1_score: 0.8522\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2100 - accuracy: 0.9162 - f1_score: 0.9152 - val_loss: 0.3859 - val_accuracy: 0.8472 - val_f1_score: 0.8514\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3666 - accuracy: 0.8494 - f1_score: 0.8728\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6165 - accuracy: 0.6588 - f1_score: 0.6418 - val_loss: 0.5130 - val_accuracy: 0.7568 - val_f1_score: 0.7574\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4716 - accuracy: 0.7782 - f1_score: 0.7700 - val_loss: 0.4150 - val_accuracy: 0.8101 - val_f1_score: 0.8077\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4081 - accuracy: 0.8165 - f1_score: 0.8109 - val_loss: 0.3695 - val_accuracy: 0.8273 - val_f1_score: 0.8246\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3759 - accuracy: 0.8348 - f1_score: 0.8330 - val_loss: 0.3563 - val_accuracy: 0.8436 - val_f1_score: 0.8332\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3348 - accuracy: 0.8590 - f1_score: 0.8567 - val_loss: 0.3583 - val_accuracy: 0.8418 - val_f1_score: 0.8521\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3314 - accuracy: 0.8590 - f1_score: 0.8584 - val_loss: 0.3199 - val_accuracy: 0.8725 - val_f1_score: 0.8717\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2998 - accuracy: 0.8767 - f1_score: 0.8756 - val_loss: 0.3664 - val_accuracy: 0.8436 - val_f1_score: 0.8261\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3081 - accuracy: 0.8758 - f1_score: 0.8750 - val_loss: 0.3294 - val_accuracy: 0.8635 - val_f1_score: 0.8530\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2920 - accuracy: 0.8840 - f1_score: 0.8837 - val_loss: 0.3358 - val_accuracy: 0.8608 - val_f1_score: 0.8490\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2764 - accuracy: 0.8849 - f1_score: 0.8843 - val_loss: 0.3285 - val_accuracy: 0.8671 - val_f1_score: 0.8577\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2710 - accuracy: 0.8879 - f1_score: 0.8871 - val_loss: 0.3316 - val_accuracy: 0.8562 - val_f1_score: 0.8452\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3488 - accuracy: 0.8454 - f1_score: 0.8724\n","Epoch 1/20\n","104/104 [==============================] - 9s 19ms/step - loss: 0.6134 - accuracy: 0.6612 - f1_score: 0.6268 - val_loss: 0.5452 - val_accuracy: 0.7269 - val_f1_score: 0.7648\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4692 - accuracy: 0.7833 - f1_score: 0.7768 - val_loss: 0.4296 - val_accuracy: 0.8047 - val_f1_score: 0.8138\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4052 - accuracy: 0.8204 - f1_score: 0.8155 - val_loss: 0.3864 - val_accuracy: 0.8282 - val_f1_score: 0.8348\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3542 - accuracy: 0.8511 - f1_score: 0.8476 - val_loss: 0.3436 - val_accuracy: 0.8535 - val_f1_score: 0.8543\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3268 - accuracy: 0.8605 - f1_score: 0.8595 - val_loss: 0.3408 - val_accuracy: 0.8490 - val_f1_score: 0.8408\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3055 - accuracy: 0.8761 - f1_score: 0.8746 - val_loss: 0.3404 - val_accuracy: 0.8608 - val_f1_score: 0.8531\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2824 - accuracy: 0.8855 - f1_score: 0.8840 - val_loss: 0.3137 - val_accuracy: 0.8626 - val_f1_score: 0.8628\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2768 - accuracy: 0.8945 - f1_score: 0.8940 - val_loss: 0.3140 - val_accuracy: 0.8680 - val_f1_score: 0.8651\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2580 - accuracy: 0.8981 - f1_score: 0.8979 - val_loss: 0.3295 - val_accuracy: 0.8590 - val_f1_score: 0.8462\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2556 - accuracy: 0.8987 - f1_score: 0.8986 - val_loss: 0.3249 - val_accuracy: 0.8644 - val_f1_score: 0.8566\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2375 - accuracy: 0.9129 - f1_score: 0.9127 - val_loss: 0.3376 - val_accuracy: 0.8580 - val_f1_score: 0.8498\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2309 - accuracy: 0.9159 - f1_score: 0.9154 - val_loss: 0.3281 - val_accuracy: 0.8626 - val_f1_score: 0.8538\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3294 - accuracy: 0.8535 - f1_score: 0.8812\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6446 - accuracy: 0.6272 - f1_score: 0.6275 - val_loss: 0.5500 - val_accuracy: 0.7459 - val_f1_score: 0.7520\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.5074 - accuracy: 0.7577 - f1_score: 0.7475 - val_loss: 0.4517 - val_accuracy: 0.7875 - val_f1_score: 0.8003\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4185 - accuracy: 0.8047 - f1_score: 0.7989 - val_loss: 0.4060 - val_accuracy: 0.8192 - val_f1_score: 0.8039\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3574 - accuracy: 0.8391 - f1_score: 0.8360 - val_loss: 0.3928 - val_accuracy: 0.8228 - val_f1_score: 0.8356\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3258 - accuracy: 0.8593 - f1_score: 0.8576 - val_loss: 0.3941 - val_accuracy: 0.8291 - val_f1_score: 0.8432\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3119 - accuracy: 0.8734 - f1_score: 0.8722 - val_loss: 0.3438 - val_accuracy: 0.8571 - val_f1_score: 0.8597\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2887 - accuracy: 0.8785 - f1_score: 0.8778 - val_loss: 0.3399 - val_accuracy: 0.8626 - val_f1_score: 0.8633\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2919 - accuracy: 0.8800 - f1_score: 0.8789 - val_loss: 0.3647 - val_accuracy: 0.8517 - val_f1_score: 0.8360\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2696 - accuracy: 0.8951 - f1_score: 0.8952 - val_loss: 0.3428 - val_accuracy: 0.8562 - val_f1_score: 0.8535\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2560 - accuracy: 0.8978 - f1_score: 0.8971 - val_loss: 0.3386 - val_accuracy: 0.8644 - val_f1_score: 0.8563\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2360 - accuracy: 0.9096 - f1_score: 0.9093 - val_loss: 0.3385 - val_accuracy: 0.8734 - val_f1_score: 0.8725\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2484 - accuracy: 0.9011 - f1_score: 0.9007 - val_loss: 0.3370 - val_accuracy: 0.8716 - val_f1_score: 0.8700\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2255 - accuracy: 0.9135 - f1_score: 0.9125 - val_loss: 0.3552 - val_accuracy: 0.8761 - val_f1_score: 0.8765\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2306 - accuracy: 0.9081 - f1_score: 0.9077 - val_loss: 0.3952 - val_accuracy: 0.8553 - val_f1_score: 0.8428\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2046 - accuracy: 0.9210 - f1_score: 0.9205 - val_loss: 0.3920 - val_accuracy: 0.8580 - val_f1_score: 0.8648\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1909 - accuracy: 0.9262 - f1_score: 0.9257 - val_loss: 0.3908 - val_accuracy: 0.8653 - val_f1_score: 0.8611\n","Epoch 17/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1852 - accuracy: 0.9295 - f1_score: 0.9290 - val_loss: 0.4528 - val_accuracy: 0.8436 - val_f1_score: 0.8275\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8623 - f1_score: 0.8898\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6250 - accuracy: 0.6537 - f1_score: 0.6492 - val_loss: 0.5346 - val_accuracy: 0.7378 - val_f1_score: 0.6749\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4888 - accuracy: 0.7649 - f1_score: 0.7558 - val_loss: 0.4132 - val_accuracy: 0.8174 - val_f1_score: 0.8109\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4022 - accuracy: 0.8171 - f1_score: 0.8105 - val_loss: 0.3532 - val_accuracy: 0.8445 - val_f1_score: 0.8401\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3648 - accuracy: 0.8360 - f1_score: 0.8325 - val_loss: 0.3458 - val_accuracy: 0.8571 - val_f1_score: 0.8463\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3249 - accuracy: 0.8596 - f1_score: 0.8577 - val_loss: 0.3264 - val_accuracy: 0.8752 - val_f1_score: 0.8705\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3184 - accuracy: 0.8683 - f1_score: 0.8659 - val_loss: 0.3241 - val_accuracy: 0.8707 - val_f1_score: 0.8632\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3012 - accuracy: 0.8692 - f1_score: 0.8669 - val_loss: 0.3353 - val_accuracy: 0.8662 - val_f1_score: 0.8558\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2903 - accuracy: 0.8825 - f1_score: 0.8810 - val_loss: 0.3405 - val_accuracy: 0.8635 - val_f1_score: 0.8512\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2812 - accuracy: 0.8825 - f1_score: 0.8810 - val_loss: 0.3278 - val_accuracy: 0.8752 - val_f1_score: 0.8676\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2763 - accuracy: 0.8816 - f1_score: 0.8805 - val_loss: 0.3040 - val_accuracy: 0.8888 - val_f1_score: 0.8877\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2584 - accuracy: 0.8972 - f1_score: 0.8957 - val_loss: 0.3531 - val_accuracy: 0.8653 - val_f1_score: 0.8523\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2524 - accuracy: 0.8966 - f1_score: 0.8957 - val_loss: 0.3255 - val_accuracy: 0.8807 - val_f1_score: 0.8743\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2400 - accuracy: 0.9011 - f1_score: 0.9001 - val_loss: 0.3276 - val_accuracy: 0.8834 - val_f1_score: 0.8820\n","Epoch 14/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2335 - accuracy: 0.9111 - f1_score: 0.9100 - val_loss: 0.4091 - val_accuracy: 0.8327 - val_f1_score: 0.8067\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2440 - accuracy: 0.8975 - f1_score: 0.8956 - val_loss: 0.4002 - val_accuracy: 0.8345 - val_f1_score: 0.8111\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8677 - f1_score: 0.8957\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6229 - accuracy: 0.6525 - f1_score: 0.6567 - val_loss: 0.5439 - val_accuracy: 0.7351 - val_f1_score: 0.7405\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.5022 - accuracy: 0.7655 - f1_score: 0.7522 - val_loss: 0.4987 - val_accuracy: 0.7902 - val_f1_score: 0.8101\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4079 - accuracy: 0.8168 - f1_score: 0.8120 - val_loss: 0.4232 - val_accuracy: 0.8137 - val_f1_score: 0.8260\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3698 - accuracy: 0.8333 - f1_score: 0.8310 - val_loss: 0.3914 - val_accuracy: 0.8246 - val_f1_score: 0.8079\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3320 - accuracy: 0.8538 - f1_score: 0.8506 - val_loss: 0.3944 - val_accuracy: 0.8445 - val_f1_score: 0.8550\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3046 - accuracy: 0.8716 - f1_score: 0.8696 - val_loss: 0.3637 - val_accuracy: 0.8617 - val_f1_score: 0.8608\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2857 - accuracy: 0.8816 - f1_score: 0.8793 - val_loss: 0.3762 - val_accuracy: 0.8544 - val_f1_score: 0.8613\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2783 - accuracy: 0.8882 - f1_score: 0.8867 - val_loss: 0.3662 - val_accuracy: 0.8617 - val_f1_score: 0.8615\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2663 - accuracy: 0.8882 - f1_score: 0.8865 - val_loss: 0.3699 - val_accuracy: 0.8580 - val_f1_score: 0.8534\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2672 - accuracy: 0.8933 - f1_score: 0.8919 - val_loss: 0.4242 - val_accuracy: 0.8318 - val_f1_score: 0.8460\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2420 - accuracy: 0.9033 - f1_score: 0.9017 - val_loss: 0.3768 - val_accuracy: 0.8653 - val_f1_score: 0.8685\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.8528 - f1_score: 0.8788\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6200 - accuracy: 0.6606 - f1_score: 0.6582 - val_loss: 0.5490 - val_accuracy: 0.7179 - val_f1_score: 0.6372\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4896 - accuracy: 0.7631 - f1_score: 0.7564 - val_loss: 0.4033 - val_accuracy: 0.8219 - val_f1_score: 0.8126\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4246 - accuracy: 0.8050 - f1_score: 0.7996 - val_loss: 0.4749 - val_accuracy: 0.7767 - val_f1_score: 0.8069\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3615 - accuracy: 0.8385 - f1_score: 0.8358 - val_loss: 0.3423 - val_accuracy: 0.8608 - val_f1_score: 0.8569\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3558 - accuracy: 0.8457 - f1_score: 0.8422 - val_loss: 0.3972 - val_accuracy: 0.8237 - val_f1_score: 0.8400\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3156 - accuracy: 0.8677 - f1_score: 0.8657 - val_loss: 0.3666 - val_accuracy: 0.8318 - val_f1_score: 0.8432\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3082 - accuracy: 0.8704 - f1_score: 0.8692 - val_loss: 0.4370 - val_accuracy: 0.8101 - val_f1_score: 0.8315\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2829 - accuracy: 0.8855 - f1_score: 0.8836 - val_loss: 0.3438 - val_accuracy: 0.8580 - val_f1_score: 0.8641\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2697 - accuracy: 0.8918 - f1_score: 0.8900 - val_loss: 0.3916 - val_accuracy: 0.8363 - val_f1_score: 0.8508\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3734 - accuracy: 0.8427 - f1_score: 0.8674\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6209 - accuracy: 0.6519 - f1_score: 0.6299 - val_loss: 0.5387 - val_accuracy: 0.7378 - val_f1_score: 0.7563\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4905 - accuracy: 0.7655 - f1_score: 0.7599 - val_loss: 0.6590 - val_accuracy: 0.6465 - val_f1_score: 0.7353\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4145 - accuracy: 0.8165 - f1_score: 0.8138 - val_loss: 0.3920 - val_accuracy: 0.8201 - val_f1_score: 0.8268\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3541 - accuracy: 0.8460 - f1_score: 0.8441 - val_loss: 0.3546 - val_accuracy: 0.8445 - val_f1_score: 0.8475\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3300 - accuracy: 0.8611 - f1_score: 0.8600 - val_loss: 0.3628 - val_accuracy: 0.8445 - val_f1_score: 0.8515\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3105 - accuracy: 0.8716 - f1_score: 0.8708 - val_loss: 0.3552 - val_accuracy: 0.8490 - val_f1_score: 0.8559\n","Epoch 7/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2969 - accuracy: 0.8770 - f1_score: 0.8768 - val_loss: 0.3467 - val_accuracy: 0.8544 - val_f1_score: 0.8627\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2935 - accuracy: 0.8828 - f1_score: 0.8812 - val_loss: 0.3340 - val_accuracy: 0.8608 - val_f1_score: 0.8569\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2742 - accuracy: 0.8822 - f1_score: 0.8811 - val_loss: 0.3516 - val_accuracy: 0.8590 - val_f1_score: 0.8494\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2641 - accuracy: 0.8933 - f1_score: 0.8929 - val_loss: 0.3681 - val_accuracy: 0.8409 - val_f1_score: 0.8508\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2572 - accuracy: 0.8960 - f1_score: 0.8947 - val_loss: 0.3341 - val_accuracy: 0.8590 - val_f1_score: 0.8561\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2364 - accuracy: 0.9084 - f1_score: 0.9080 - val_loss: 0.3544 - val_accuracy: 0.8599 - val_f1_score: 0.8534\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2268 - accuracy: 0.9066 - f1_score: 0.9058 - val_loss: 0.3471 - val_accuracy: 0.8463 - val_f1_score: 0.8532\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3408 - accuracy: 0.8636 - f1_score: 0.8873\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6167 - accuracy: 0.6600 - f1_score: 0.6430 - val_loss: 0.5186 - val_accuracy: 0.7550 - val_f1_score: 0.7766\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4528 - accuracy: 0.7942 - f1_score: 0.7874 - val_loss: 0.4365 - val_accuracy: 0.8056 - val_f1_score: 0.8204\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3756 - accuracy: 0.8321 - f1_score: 0.8289 - val_loss: 0.3703 - val_accuracy: 0.8409 - val_f1_score: 0.8394\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3327 - accuracy: 0.8556 - f1_score: 0.8542 - val_loss: 0.5044 - val_accuracy: 0.7676 - val_f1_score: 0.8049\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3289 - accuracy: 0.8614 - f1_score: 0.8603 - val_loss: 0.3340 - val_accuracy: 0.8517 - val_f1_score: 0.8512\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3006 - accuracy: 0.8791 - f1_score: 0.8776 - val_loss: 0.3363 - val_accuracy: 0.8517 - val_f1_score: 0.8536\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2937 - accuracy: 0.8674 - f1_score: 0.8659 - val_loss: 0.3669 - val_accuracy: 0.8454 - val_f1_score: 0.8567\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2909 - accuracy: 0.8779 - f1_score: 0.8767 - val_loss: 0.3428 - val_accuracy: 0.8635 - val_f1_score: 0.8521\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2775 - accuracy: 0.8891 - f1_score: 0.8882 - val_loss: 0.3168 - val_accuracy: 0.8743 - val_f1_score: 0.8731\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2618 - accuracy: 0.8897 - f1_score: 0.8886 - val_loss: 0.3183 - val_accuracy: 0.8644 - val_f1_score: 0.8646\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2529 - accuracy: 0.8978 - f1_score: 0.8967 - val_loss: 0.3304 - val_accuracy: 0.8698 - val_f1_score: 0.8700\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2473 - accuracy: 0.9027 - f1_score: 0.9017 - val_loss: 0.3463 - val_accuracy: 0.8463 - val_f1_score: 0.8532\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2209 - accuracy: 0.9144 - f1_score: 0.9137 - val_loss: 0.3967 - val_accuracy: 0.8327 - val_f1_score: 0.8452\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2237 - accuracy: 0.9114 - f1_score: 0.9105 - val_loss: 0.3468 - val_accuracy: 0.8644 - val_f1_score: 0.8593\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3443 - accuracy: 0.8575 - f1_score: 0.8809\n","Epoch 1/20\n","104/104 [==============================] - 9s 18ms/step - loss: 0.6284 - accuracy: 0.6398 - f1_score: 0.6101 - val_loss: 0.5251 - val_accuracy: 0.7532 - val_f1_score: 0.7432\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4733 - accuracy: 0.7806 - f1_score: 0.7779 - val_loss: 0.4399 - val_accuracy: 0.7966 - val_f1_score: 0.7792\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4207 - accuracy: 0.8104 - f1_score: 0.8068 - val_loss: 0.3931 - val_accuracy: 0.8165 - val_f1_score: 0.8069\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3691 - accuracy: 0.8382 - f1_score: 0.8352 - val_loss: 0.3530 - val_accuracy: 0.8472 - val_f1_score: 0.8487\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3417 - accuracy: 0.8583 - f1_score: 0.8571 - val_loss: 0.3504 - val_accuracy: 0.8481 - val_f1_score: 0.8366\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3180 - accuracy: 0.8680 - f1_score: 0.8674 - val_loss: 0.3360 - val_accuracy: 0.8571 - val_f1_score: 0.8526\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3048 - accuracy: 0.8764 - f1_score: 0.8761 - val_loss: 0.3543 - val_accuracy: 0.8472 - val_f1_score: 0.8338\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2806 - accuracy: 0.8861 - f1_score: 0.8855 - val_loss: 0.3373 - val_accuracy: 0.8562 - val_f1_score: 0.8635\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2638 - accuracy: 0.8921 - f1_score: 0.8910 - val_loss: 0.3206 - val_accuracy: 0.8752 - val_f1_score: 0.8755\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2629 - accuracy: 0.8924 - f1_score: 0.8917 - val_loss: 0.3349 - val_accuracy: 0.8617 - val_f1_score: 0.8577\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2406 - accuracy: 0.9063 - f1_score: 0.9052 - val_loss: 0.3434 - val_accuracy: 0.8562 - val_f1_score: 0.8537\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2383 - accuracy: 0.9054 - f1_score: 0.9048 - val_loss: 0.3371 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2233 - accuracy: 0.9105 - f1_score: 0.9099 - val_loss: 0.3536 - val_accuracy: 0.8544 - val_f1_score: 0.8441\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2012 - accuracy: 0.9250 - f1_score: 0.9243 - val_loss: 0.3770 - val_accuracy: 0.8535 - val_f1_score: 0.8579\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3512 - accuracy: 0.8528 - f1_score: 0.8786\n","Epoch 1/20\n","104/104 [==============================] - 8s 25ms/step - loss: 0.6284 - accuracy: 0.6510 - f1_score: 0.6594 - val_loss: 0.5032 - val_accuracy: 0.7821 - val_f1_score: 0.7685\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4736 - accuracy: 0.7752 - f1_score: 0.7676 - val_loss: 0.3885 - val_accuracy: 0.8309 - val_f1_score: 0.8370\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3902 - accuracy: 0.8252 - f1_score: 0.8224 - val_loss: 0.3346 - val_accuracy: 0.8562 - val_f1_score: 0.8548\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3522 - accuracy: 0.8466 - f1_score: 0.8446 - val_loss: 0.3565 - val_accuracy: 0.8508 - val_f1_score: 0.8358\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3337 - accuracy: 0.8577 - f1_score: 0.8567 - val_loss: 0.3616 - val_accuracy: 0.8454 - val_f1_score: 0.8267\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3131 - accuracy: 0.8674 - f1_score: 0.8668 - val_loss: 0.3095 - val_accuracy: 0.8680 - val_f1_score: 0.8630\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3041 - accuracy: 0.8779 - f1_score: 0.8763 - val_loss: 0.3254 - val_accuracy: 0.8635 - val_f1_score: 0.8715\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2820 - accuracy: 0.8885 - f1_score: 0.8877 - val_loss: 0.3184 - val_accuracy: 0.8653 - val_f1_score: 0.8549\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2799 - accuracy: 0.8921 - f1_score: 0.8913 - val_loss: 0.3327 - val_accuracy: 0.8608 - val_f1_score: 0.8525\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2736 - accuracy: 0.8903 - f1_score: 0.8898 - val_loss: 0.3154 - val_accuracy: 0.8707 - val_f1_score: 0.8637\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2642 - accuracy: 0.8996 - f1_score: 0.8991 - val_loss: 0.3097 - val_accuracy: 0.8725 - val_f1_score: 0.8731\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3447 - accuracy: 0.8636 - f1_score: 0.8888\n","Epoch 1/20\n","104/104 [==============================] - 9s 26ms/step - loss: 0.6181 - accuracy: 0.6552 - f1_score: 0.6669 - val_loss: 0.5244 - val_accuracy: 0.7405 - val_f1_score: 0.7335\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4828 - accuracy: 0.7785 - f1_score: 0.7685 - val_loss: 0.4849 - val_accuracy: 0.7758 - val_f1_score: 0.7400\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4087 - accuracy: 0.8189 - f1_score: 0.8131 - val_loss: 0.5296 - val_accuracy: 0.7667 - val_f1_score: 0.7978\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3723 - accuracy: 0.8376 - f1_score: 0.8365 - val_loss: 0.3875 - val_accuracy: 0.8237 - val_f1_score: 0.8226\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3327 - accuracy: 0.8577 - f1_score: 0.8549 - val_loss: 0.4627 - val_accuracy: 0.8011 - val_f1_score: 0.8220\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3114 - accuracy: 0.8743 - f1_score: 0.8733 - val_loss: 0.3670 - val_accuracy: 0.8454 - val_f1_score: 0.8496\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3050 - accuracy: 0.8794 - f1_score: 0.8784 - val_loss: 0.3582 - val_accuracy: 0.8535 - val_f1_score: 0.8519\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2817 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3575 - val_accuracy: 0.8553 - val_f1_score: 0.8462\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2733 - accuracy: 0.8930 - f1_score: 0.8920 - val_loss: 0.3476 - val_accuracy: 0.8626 - val_f1_score: 0.8527\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2700 - accuracy: 0.8903 - f1_score: 0.8892 - val_loss: 0.3408 - val_accuracy: 0.8626 - val_f1_score: 0.8616\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2470 - accuracy: 0.9030 - f1_score: 0.9019 - val_loss: 0.3488 - val_accuracy: 0.8635 - val_f1_score: 0.8538\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2417 - accuracy: 0.9063 - f1_score: 0.9054 - val_loss: 0.3594 - val_accuracy: 0.8571 - val_f1_score: 0.8597\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2313 - accuracy: 0.9096 - f1_score: 0.9084 - val_loss: 0.3617 - val_accuracy: 0.8553 - val_f1_score: 0.8467\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2138 - accuracy: 0.9241 - f1_score: 0.9232 - val_loss: 0.3952 - val_accuracy: 0.8680 - val_f1_score: 0.8604\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2076 - accuracy: 0.9259 - f1_score: 0.9249 - val_loss: 0.4926 - val_accuracy: 0.8363 - val_f1_score: 0.8136\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3415 - accuracy: 0.8515 - f1_score: 0.8778\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6165 - accuracy: 0.6652 - f1_score: 0.6538 - val_loss: 0.5048 - val_accuracy: 0.7676 - val_f1_score: 0.7468\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5074 - accuracy: 0.7559 - f1_score: 0.7491 - val_loss: 0.4066 - val_accuracy: 0.8183 - val_f1_score: 0.8301\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4224 - accuracy: 0.8059 - f1_score: 0.8037 - val_loss: 0.3234 - val_accuracy: 0.8626 - val_f1_score: 0.8595\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3674 - accuracy: 0.8397 - f1_score: 0.8378 - val_loss: 0.3058 - val_accuracy: 0.8743 - val_f1_score: 0.8760\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3465 - accuracy: 0.8475 - f1_score: 0.8461 - val_loss: 0.3113 - val_accuracy: 0.8734 - val_f1_score: 0.8807\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3227 - accuracy: 0.8668 - f1_score: 0.8650 - val_loss: 0.3188 - val_accuracy: 0.8653 - val_f1_score: 0.8753\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3114 - accuracy: 0.8680 - f1_score: 0.8668 - val_loss: 0.3159 - val_accuracy: 0.8689 - val_f1_score: 0.8780\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2915 - accuracy: 0.8834 - f1_score: 0.8820 - val_loss: 0.3304 - val_accuracy: 0.8626 - val_f1_score: 0.8744\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2996 - accuracy: 0.8755 - f1_score: 0.8742 - val_loss: 0.3176 - val_accuracy: 0.8698 - val_f1_score: 0.8788\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8433 - f1_score: 0.8742\n","Epoch 1/20\n","104/104 [==============================] - 9s 18ms/step - loss: 0.6426 - accuracy: 0.6272 - f1_score: 0.6501 - val_loss: 0.6243 - val_accuracy: 0.6365 - val_f1_score: 0.7104\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5287 - accuracy: 0.7405 - f1_score: 0.7295 - val_loss: 0.4677 - val_accuracy: 0.7758 - val_f1_score: 0.7540\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4350 - accuracy: 0.8029 - f1_score: 0.7959 - val_loss: 0.4062 - val_accuracy: 0.8146 - val_f1_score: 0.8068\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3866 - accuracy: 0.8249 - f1_score: 0.8213 - val_loss: 0.4031 - val_accuracy: 0.8156 - val_f1_score: 0.7918\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3515 - accuracy: 0.8574 - f1_score: 0.8564 - val_loss: 0.3671 - val_accuracy: 0.8526 - val_f1_score: 0.8561\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3422 - accuracy: 0.8593 - f1_score: 0.8568 - val_loss: 0.3543 - val_accuracy: 0.8499 - val_f1_score: 0.8523\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3043 - accuracy: 0.8773 - f1_score: 0.8767 - val_loss: 0.3700 - val_accuracy: 0.8544 - val_f1_score: 0.8591\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3000 - accuracy: 0.8773 - f1_score: 0.8760 - val_loss: 0.3600 - val_accuracy: 0.8463 - val_f1_score: 0.8343\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2988 - accuracy: 0.8810 - f1_score: 0.8795 - val_loss: 0.5010 - val_accuracy: 0.7857 - val_f1_score: 0.8150\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2799 - accuracy: 0.8924 - f1_score: 0.8916 - val_loss: 0.3415 - val_accuracy: 0.8635 - val_f1_score: 0.8582\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2762 - accuracy: 0.8897 - f1_score: 0.8883 - val_loss: 0.3249 - val_accuracy: 0.8671 - val_f1_score: 0.8682\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2551 - accuracy: 0.9020 - f1_score: 0.9009 - val_loss: 0.3530 - val_accuracy: 0.8635 - val_f1_score: 0.8651\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2440 - accuracy: 0.9042 - f1_score: 0.9035 - val_loss: 0.3363 - val_accuracy: 0.8671 - val_f1_score: 0.8674\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2337 - accuracy: 0.9123 - f1_score: 0.9112 - val_loss: 0.3486 - val_accuracy: 0.8671 - val_f1_score: 0.8691\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2257 - accuracy: 0.9120 - f1_score: 0.9114 - val_loss: 0.3801 - val_accuracy: 0.8571 - val_f1_score: 0.8616\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2230 - accuracy: 0.9126 - f1_score: 0.9120 - val_loss: 0.3804 - val_accuracy: 0.8499 - val_f1_score: 0.8566\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3260 - accuracy: 0.8704 - f1_score: 0.8929\n","Epoch 1/20\n","104/104 [==============================] - 8s 18ms/step - loss: 0.6258 - accuracy: 0.6486 - f1_score: 0.6428 - val_loss: 0.5330 - val_accuracy: 0.7387 - val_f1_score: 0.7229\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4868 - accuracy: 0.7728 - f1_score: 0.7639 - val_loss: 0.4593 - val_accuracy: 0.7794 - val_f1_score: 0.7421\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4084 - accuracy: 0.8219 - f1_score: 0.8175 - val_loss: 0.4061 - val_accuracy: 0.8228 - val_f1_score: 0.8036\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3537 - accuracy: 0.8520 - f1_score: 0.8498 - val_loss: 0.3736 - val_accuracy: 0.8454 - val_f1_score: 0.8415\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3305 - accuracy: 0.8644 - f1_score: 0.8629 - val_loss: 0.4198 - val_accuracy: 0.8201 - val_f1_score: 0.8359\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3076 - accuracy: 0.8707 - f1_score: 0.8699 - val_loss: 0.3619 - val_accuracy: 0.8499 - val_f1_score: 0.8523\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2830 - accuracy: 0.8870 - f1_score: 0.8861 - val_loss: 0.3903 - val_accuracy: 0.8409 - val_f1_score: 0.8485\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2823 - accuracy: 0.8852 - f1_score: 0.8843 - val_loss: 0.3562 - val_accuracy: 0.8599 - val_f1_score: 0.8607\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2642 - accuracy: 0.9011 - f1_score: 0.9001 - val_loss: 0.3529 - val_accuracy: 0.8553 - val_f1_score: 0.8577\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2530 - accuracy: 0.9011 - f1_score: 0.8998 - val_loss: 0.3717 - val_accuracy: 0.8562 - val_f1_score: 0.8592\n","Epoch 11/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2421 - accuracy: 0.9069 - f1_score: 0.9063 - val_loss: 0.4059 - val_accuracy: 0.8544 - val_f1_score: 0.8616\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2512 - accuracy: 0.8981 - f1_score: 0.8975 - val_loss: 0.3571 - val_accuracy: 0.8626 - val_f1_score: 0.8616\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2221 - accuracy: 0.9219 - f1_score: 0.9215 - val_loss: 0.3779 - val_accuracy: 0.8580 - val_f1_score: 0.8619\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2359 - accuracy: 0.9036 - f1_score: 0.9031 - val_loss: 0.3758 - val_accuracy: 0.8562 - val_f1_score: 0.8579\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3234 - accuracy: 0.8629 - f1_score: 0.8867\n","Epoch 1/20\n","104/104 [==============================] - 8s 31ms/step - loss: 0.6374 - accuracy: 0.6389 - f1_score: 0.6218 - val_loss: 0.5472 - val_accuracy: 0.7396 - val_f1_score: 0.7198\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4971 - accuracy: 0.7670 - f1_score: 0.7570 - val_loss: 0.4414 - val_accuracy: 0.7975 - val_f1_score: 0.7791\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4266 - accuracy: 0.8113 - f1_score: 0.8058 - val_loss: 0.3808 - val_accuracy: 0.8454 - val_f1_score: 0.8444\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3734 - accuracy: 0.8318 - f1_score: 0.8283 - val_loss: 0.5713 - val_accuracy: 0.7378 - val_f1_score: 0.7842\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3605 - accuracy: 0.8463 - f1_score: 0.8437 - val_loss: 0.4079 - val_accuracy: 0.8201 - val_f1_score: 0.8383\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3346 - accuracy: 0.8635 - f1_score: 0.8615 - val_loss: 0.3282 - val_accuracy: 0.8779 - val_f1_score: 0.8778\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3121 - accuracy: 0.8707 - f1_score: 0.8696 - val_loss: 0.3215 - val_accuracy: 0.8770 - val_f1_score: 0.8757\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3002 - accuracy: 0.8779 - f1_score: 0.8773 - val_loss: 0.3126 - val_accuracy: 0.8807 - val_f1_score: 0.8800\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3064 - accuracy: 0.8656 - f1_score: 0.8642 - val_loss: 0.3422 - val_accuracy: 0.8626 - val_f1_score: 0.8707\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2793 - accuracy: 0.8855 - f1_score: 0.8849 - val_loss: 0.3186 - val_accuracy: 0.8788 - val_f1_score: 0.8818\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2726 - accuracy: 0.8900 - f1_score: 0.8898 - val_loss: 0.3248 - val_accuracy: 0.8743 - val_f1_score: 0.8771\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2552 - accuracy: 0.8978 - f1_score: 0.8967 - val_loss: 0.3128 - val_accuracy: 0.8897 - val_f1_score: 0.8883\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2547 - accuracy: 0.8984 - f1_score: 0.8977 - val_loss: 0.3252 - val_accuracy: 0.8825 - val_f1_score: 0.8822\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.8420 - f1_score: 0.8636\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6349 - accuracy: 0.6395 - f1_score: 0.6255 - val_loss: 0.5556 - val_accuracy: 0.7242 - val_f1_score: 0.7364\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4737 - accuracy: 0.7863 - f1_score: 0.7786 - val_loss: 0.4086 - val_accuracy: 0.8219 - val_f1_score: 0.8133\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3866 - accuracy: 0.8342 - f1_score: 0.8310 - val_loss: 0.3948 - val_accuracy: 0.8273 - val_f1_score: 0.8410\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3430 - accuracy: 0.8571 - f1_score: 0.8561 - val_loss: 0.3584 - val_accuracy: 0.8508 - val_f1_score: 0.8371\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3240 - accuracy: 0.8719 - f1_score: 0.8699 - val_loss: 0.3298 - val_accuracy: 0.8788 - val_f1_score: 0.8741\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3089 - accuracy: 0.8743 - f1_score: 0.8732 - val_loss: 0.3290 - val_accuracy: 0.8653 - val_f1_score: 0.8683\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3111 - accuracy: 0.8686 - f1_score: 0.8684 - val_loss: 0.4046 - val_accuracy: 0.8192 - val_f1_score: 0.7904\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2897 - accuracy: 0.8810 - f1_score: 0.8802 - val_loss: 0.3096 - val_accuracy: 0.8816 - val_f1_score: 0.8799\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2818 - accuracy: 0.8825 - f1_score: 0.8811 - val_loss: 0.3126 - val_accuracy: 0.8734 - val_f1_score: 0.8748\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2566 - accuracy: 0.8978 - f1_score: 0.8976 - val_loss: 0.3229 - val_accuracy: 0.8816 - val_f1_score: 0.8758\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2614 - accuracy: 0.8939 - f1_score: 0.8933 - val_loss: 0.3184 - val_accuracy: 0.8698 - val_f1_score: 0.8712\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2554 - accuracy: 0.8987 - f1_score: 0.8977 - val_loss: 0.3815 - val_accuracy: 0.8508 - val_f1_score: 0.8335\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2280 - accuracy: 0.9156 - f1_score: 0.9152 - val_loss: 0.3934 - val_accuracy: 0.8282 - val_f1_score: 0.8017\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3054 - accuracy: 0.8751 - f1_score: 0.8981\n","Epoch 1/20\n","104/104 [==============================] - 8s 19ms/step - loss: 0.6239 - accuracy: 0.6474 - f1_score: 0.6457 - val_loss: 0.4951 - val_accuracy: 0.7676 - val_f1_score: 0.7512\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5044 - accuracy: 0.7607 - f1_score: 0.7484 - val_loss: 0.4391 - val_accuracy: 0.8020 - val_f1_score: 0.8198\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4221 - accuracy: 0.8056 - f1_score: 0.8009 - val_loss: 0.4075 - val_accuracy: 0.8183 - val_f1_score: 0.7886\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3789 - accuracy: 0.8273 - f1_score: 0.8235 - val_loss: 0.3391 - val_accuracy: 0.8599 - val_f1_score: 0.8555\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3336 - accuracy: 0.8614 - f1_score: 0.8588 - val_loss: 0.3323 - val_accuracy: 0.8653 - val_f1_score: 0.8690\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3186 - accuracy: 0.8704 - f1_score: 0.8687 - val_loss: 0.3171 - val_accuracy: 0.8788 - val_f1_score: 0.8808\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2966 - accuracy: 0.8837 - f1_score: 0.8825 - val_loss: 0.3172 - val_accuracy: 0.8716 - val_f1_score: 0.8702\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2840 - accuracy: 0.8897 - f1_score: 0.8881 - val_loss: 0.3793 - val_accuracy: 0.8354 - val_f1_score: 0.8501\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2757 - accuracy: 0.8903 - f1_score: 0.8898 - val_loss: 0.3072 - val_accuracy: 0.8761 - val_f1_score: 0.8753\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2841 - accuracy: 0.8828 - f1_score: 0.8814 - val_loss: 0.3037 - val_accuracy: 0.8834 - val_f1_score: 0.8813\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2463 - accuracy: 0.9054 - f1_score: 0.9042 - val_loss: 0.3242 - val_accuracy: 0.8671 - val_f1_score: 0.8670\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2424 - accuracy: 0.9036 - f1_score: 0.9025 - val_loss: 0.3804 - val_accuracy: 0.8436 - val_f1_score: 0.8236\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2385 - accuracy: 0.9048 - f1_score: 0.9039 - val_loss: 0.3580 - val_accuracy: 0.8553 - val_f1_score: 0.8410\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2117 - accuracy: 0.9228 - f1_score: 0.9221 - val_loss: 0.3288 - val_accuracy: 0.8644 - val_f1_score: 0.8670\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2114 - accuracy: 0.9171 - f1_score: 0.9163 - val_loss: 0.3195 - val_accuracy: 0.8716 - val_f1_score: 0.8700\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8663 - f1_score: 0.8894\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4714BFUWRzk8","executionInfo":{"status":"ok","timestamp":1690579790727,"user_tz":-330,"elapsed":113,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"2e40b477-046e-441f-886f-4de14808415d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8561782836914062, 0.8636056780815125, 0.8568534851074219, 0.846049964427948, 0.8568534851074219, 0.8555030226707458, 0.8453747630119324, 0.8494260907173157, 0.8453747630119324, 0.8534773588180542, 0.8622552156448364, 0.8676570057868958, 0.8528021574020386, 0.8426738977432251, 0.8636056780815125, 0.8575286865234375, 0.8528021574020386, 0.8636056780815125, 0.8514516949653625, 0.8433490991592407, 0.870357871055603, 0.8629304766654968, 0.8419986367225647, 0.875084400177002, 0.8663065433502197]\n","0.856124243736267\n","0.009002312469709109\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtC369ydRznv","executionInfo":{"status":"ok","timestamp":1690579790729,"user_tz":-330,"elapsed":31,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"199b0ab9-7a94-4b57-e0e4-5347d6d69c2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3481762707233429, 0.3237308859825134, 0.34738075733184814, 0.35935238003730774, 0.34019598364830017, 0.34350207448005676, 0.36840516328811646, 0.3665972054004669, 0.34878867864608765, 0.32939672470092773, 0.3184303939342499, 0.3209386467933655, 0.3435383141040802, 0.3733553886413574, 0.34082356095314026, 0.34434637427330017, 0.35122382640838623, 0.3447222411632538, 0.3414669334888458, 0.3561026453971863, 0.325958251953125, 0.32344043254852295, 0.36634573340415955, 0.30543991923332214, 0.3186541795730591]\n","0.3420125186443329\n","0.017405761682143632\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"De4uy7sDRz0o","executionInfo":{"status":"ok","timestamp":1690579790730,"user_tz":-330,"elapsed":22,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"1fce45ea-a8c8-42a6-97ff-03c5d87b4fc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8773747086524963, 0.8860044479370117, 0.8791333436965942, 0.8691158890724182, 0.878160834312439, 0.8795044422149658, 0.8675534129142761, 0.8727894425392151, 0.8724233508110046, 0.8812260031700134, 0.8898487091064453, 0.8957446217536926, 0.8787540793418884, 0.8673874735832214, 0.8872767090797424, 0.8808582425117493, 0.8786190748214722, 0.8887665271759033, 0.8777776956558228, 0.8741865158081055, 0.8928570747375488, 0.8866554498672485, 0.8636362552642822, 0.8980715870857239, 0.8893854022026062]\n","0.8805244517326355\n","0.008929658215103031\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"blpEyI_eR0Bc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S4H_cKSKiHp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_lstm_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RGW4dmFwR0E8","executionInfo":{"status":"ok","timestamp":1690581221270,"user_tz":-330,"elapsed":644380,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"b046f935-c8d0-419a-b73f-40b3dd7f4310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6458 - accuracy: 0.6314 - f1_score: 0.6308 - val_loss: 0.5225 - val_accuracy: 0.7794 - val_f1_score: 0.7821\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.5236 - accuracy: 0.7435 - f1_score: 0.7308 - val_loss: 0.4095 - val_accuracy: 0.8255 - val_f1_score: 0.8250\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4293 - accuracy: 0.7990 - f1_score: 0.7914 - val_loss: 0.3549 - val_accuracy: 0.8445 - val_f1_score: 0.8346\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3824 - accuracy: 0.8327 - f1_score: 0.8284 - val_loss: 0.3895 - val_accuracy: 0.8373 - val_f1_score: 0.8525\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3584 - accuracy: 0.8466 - f1_score: 0.8436 - val_loss: 0.3485 - val_accuracy: 0.8580 - val_f1_score: 0.8657\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3290 - accuracy: 0.8656 - f1_score: 0.8640 - val_loss: 0.3411 - val_accuracy: 0.8626 - val_f1_score: 0.8707\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3278 - accuracy: 0.8647 - f1_score: 0.8626 - val_loss: 0.3078 - val_accuracy: 0.8761 - val_f1_score: 0.8737\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3144 - accuracy: 0.8704 - f1_score: 0.8687 - val_loss: 0.3349 - val_accuracy: 0.8680 - val_f1_score: 0.8748\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2924 - accuracy: 0.8776 - f1_score: 0.8756 - val_loss: 0.3267 - val_accuracy: 0.8788 - val_f1_score: 0.8845\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2853 - accuracy: 0.8810 - f1_score: 0.8792 - val_loss: 0.3110 - val_accuracy: 0.8761 - val_f1_score: 0.8784\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2787 - accuracy: 0.8885 - f1_score: 0.8864 - val_loss: 0.2948 - val_accuracy: 0.8816 - val_f1_score: 0.8790\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2695 - accuracy: 0.8942 - f1_score: 0.8919 - val_loss: 0.3448 - val_accuracy: 0.8752 - val_f1_score: 0.8818\n","Epoch 13/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2631 - accuracy: 0.8948 - f1_score: 0.8936 - val_loss: 0.3218 - val_accuracy: 0.8770 - val_f1_score: 0.8745\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2534 - accuracy: 0.9042 - f1_score: 0.9022 - val_loss: 0.3282 - val_accuracy: 0.8816 - val_f1_score: 0.8854\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2415 - accuracy: 0.9090 - f1_score: 0.9080 - val_loss: 0.3214 - val_accuracy: 0.8770 - val_f1_score: 0.8784\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2252 - accuracy: 0.9177 - f1_score: 0.9164 - val_loss: 0.3128 - val_accuracy: 0.8834 - val_f1_score: 0.8833\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8406 - f1_score: 0.8620\n","Epoch 1/20\n","104/104 [==============================] - 6s 17ms/step - loss: 0.6343 - accuracy: 0.6423 - f1_score: 0.6497 - val_loss: 0.6135 - val_accuracy: 0.6908 - val_f1_score: 0.6078\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4977 - accuracy: 0.7709 - f1_score: 0.7610 - val_loss: 0.4870 - val_accuracy: 0.7622 - val_f1_score: 0.7742\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4240 - accuracy: 0.8062 - f1_score: 0.8017 - val_loss: 0.4375 - val_accuracy: 0.8020 - val_f1_score: 0.8155\n","Epoch 4/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3728 - accuracy: 0.8291 - f1_score: 0.8253 - val_loss: 0.4806 - val_accuracy: 0.7812 - val_f1_score: 0.8082\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3452 - accuracy: 0.8586 - f1_score: 0.8575 - val_loss: 0.4367 - val_accuracy: 0.7875 - val_f1_score: 0.8139\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3448 - accuracy: 0.8541 - f1_score: 0.8519 - val_loss: 0.3640 - val_accuracy: 0.8391 - val_f1_score: 0.8452\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3149 - accuracy: 0.8680 - f1_score: 0.8670 - val_loss: 0.3968 - val_accuracy: 0.8192 - val_f1_score: 0.8342\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3105 - accuracy: 0.8695 - f1_score: 0.8688 - val_loss: 0.3532 - val_accuracy: 0.8590 - val_f1_score: 0.8579\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2889 - accuracy: 0.8843 - f1_score: 0.8836 - val_loss: 0.3571 - val_accuracy: 0.8526 - val_f1_score: 0.8554\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2749 - accuracy: 0.8912 - f1_score: 0.8906 - val_loss: 0.3769 - val_accuracy: 0.8472 - val_f1_score: 0.8547\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2519 - accuracy: 0.9017 - f1_score: 0.9007 - val_loss: 0.3920 - val_accuracy: 0.8472 - val_f1_score: 0.8527\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2473 - accuracy: 0.9011 - f1_score: 0.9008 - val_loss: 0.3778 - val_accuracy: 0.8463 - val_f1_score: 0.8365\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2441 - accuracy: 0.9024 - f1_score: 0.9016 - val_loss: 0.4156 - val_accuracy: 0.8327 - val_f1_score: 0.8444\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8528 - f1_score: 0.8767\n","Epoch 1/20\n","104/104 [==============================] - 8s 27ms/step - loss: 0.6206 - accuracy: 0.6567 - f1_score: 0.6313 - val_loss: 0.5445 - val_accuracy: 0.7423 - val_f1_score: 0.6932\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4792 - accuracy: 0.7691 - f1_score: 0.7636 - val_loss: 0.4394 - val_accuracy: 0.7875 - val_f1_score: 0.7818\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4027 - accuracy: 0.8168 - f1_score: 0.8111 - val_loss: 0.4068 - val_accuracy: 0.8047 - val_f1_score: 0.8102\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3689 - accuracy: 0.8424 - f1_score: 0.8401 - val_loss: 0.3880 - val_accuracy: 0.8255 - val_f1_score: 0.8287\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3389 - accuracy: 0.8605 - f1_score: 0.8584 - val_loss: 0.4034 - val_accuracy: 0.8201 - val_f1_score: 0.8000\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3263 - accuracy: 0.8692 - f1_score: 0.8669 - val_loss: 0.3690 - val_accuracy: 0.8409 - val_f1_score: 0.8379\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3106 - accuracy: 0.8776 - f1_score: 0.8762 - val_loss: 0.3846 - val_accuracy: 0.8400 - val_f1_score: 0.8350\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2831 - accuracy: 0.8831 - f1_score: 0.8818 - val_loss: 0.4187 - val_accuracy: 0.8183 - val_f1_score: 0.7968\n","Epoch 9/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2786 - accuracy: 0.8933 - f1_score: 0.8921 - val_loss: 0.3971 - val_accuracy: 0.8255 - val_f1_score: 0.8198\n","Epoch 10/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2686 - accuracy: 0.8936 - f1_score: 0.8926 - val_loss: 0.4030 - val_accuracy: 0.8264 - val_f1_score: 0.8313\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2579 - accuracy: 0.8954 - f1_score: 0.8943 - val_loss: 0.4187 - val_accuracy: 0.8327 - val_f1_score: 0.8373\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8542 - f1_score: 0.8763\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6448 - accuracy: 0.6329 - f1_score: 0.6184 - val_loss: 0.5323 - val_accuracy: 0.7577 - val_f1_score: 0.7505\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.5196 - accuracy: 0.7414 - f1_score: 0.7273 - val_loss: 0.4309 - val_accuracy: 0.8083 - val_f1_score: 0.8097\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4281 - accuracy: 0.8083 - f1_score: 0.8038 - val_loss: 0.3883 - val_accuracy: 0.8237 - val_f1_score: 0.8060\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3868 - accuracy: 0.8282 - f1_score: 0.8225 - val_loss: 0.3518 - val_accuracy: 0.8580 - val_f1_score: 0.8602\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3472 - accuracy: 0.8511 - f1_score: 0.8491 - val_loss: 0.3464 - val_accuracy: 0.8617 - val_f1_score: 0.8574\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3269 - accuracy: 0.8605 - f1_score: 0.8584 - val_loss: 0.3857 - val_accuracy: 0.8282 - val_f1_score: 0.8435\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3004 - accuracy: 0.8803 - f1_score: 0.8790 - val_loss: 0.3389 - val_accuracy: 0.8635 - val_f1_score: 0.8648\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2910 - accuracy: 0.8837 - f1_score: 0.8822 - val_loss: 0.3498 - val_accuracy: 0.8626 - val_f1_score: 0.8652\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2860 - accuracy: 0.8888 - f1_score: 0.8871 - val_loss: 0.3500 - val_accuracy: 0.8599 - val_f1_score: 0.8610\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2881 - accuracy: 0.8882 - f1_score: 0.8869 - val_loss: 0.3924 - val_accuracy: 0.8391 - val_f1_score: 0.8504\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2572 - accuracy: 0.9005 - f1_score: 0.8999 - val_loss: 0.3615 - val_accuracy: 0.8562 - val_f1_score: 0.8553\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2483 - accuracy: 0.9005 - f1_score: 0.8990 - val_loss: 0.3614 - val_accuracy: 0.8617 - val_f1_score: 0.8587\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3438 - accuracy: 0.8569 - f1_score: 0.8789\n","Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6299 - accuracy: 0.6347 - f1_score: 0.6259 - val_loss: 0.5181 - val_accuracy: 0.7541 - val_f1_score: 0.7410\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4936 - accuracy: 0.7637 - f1_score: 0.7553 - val_loss: 0.5080 - val_accuracy: 0.7586 - val_f1_score: 0.7951\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4167 - accuracy: 0.8086 - f1_score: 0.8047 - val_loss: 0.4146 - val_accuracy: 0.8110 - val_f1_score: 0.8294\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3763 - accuracy: 0.8379 - f1_score: 0.8364 - val_loss: 0.3847 - val_accuracy: 0.8237 - val_f1_score: 0.8382\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3509 - accuracy: 0.8556 - f1_score: 0.8546 - val_loss: 0.3744 - val_accuracy: 0.8382 - val_f1_score: 0.8492\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3246 - accuracy: 0.8671 - f1_score: 0.8666 - val_loss: 0.3416 - val_accuracy: 0.8626 - val_f1_score: 0.8574\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3099 - accuracy: 0.8704 - f1_score: 0.8688 - val_loss: 0.4171 - val_accuracy: 0.8183 - val_f1_score: 0.8396\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3012 - accuracy: 0.8713 - f1_score: 0.8702 - val_loss: 0.4035 - val_accuracy: 0.8192 - val_f1_score: 0.8377\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2956 - accuracy: 0.8803 - f1_score: 0.8789 - val_loss: 0.3365 - val_accuracy: 0.8635 - val_f1_score: 0.8658\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2785 - accuracy: 0.8825 - f1_score: 0.8818 - val_loss: 0.3395 - val_accuracy: 0.8617 - val_f1_score: 0.8668\n","Epoch 11/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2654 - accuracy: 0.8954 - f1_score: 0.8944 - val_loss: 0.3412 - val_accuracy: 0.8725 - val_f1_score: 0.8733\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2539 - accuracy: 0.9030 - f1_score: 0.9020 - val_loss: 0.3534 - val_accuracy: 0.8644 - val_f1_score: 0.8684\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2518 - accuracy: 0.8993 - f1_score: 0.8984 - val_loss: 0.3322 - val_accuracy: 0.8689 - val_f1_score: 0.8683\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2408 - accuracy: 0.9036 - f1_score: 0.9027 - val_loss: 0.3755 - val_accuracy: 0.8698 - val_f1_score: 0.8741\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2199 - accuracy: 0.9129 - f1_score: 0.9123 - val_loss: 0.4225 - val_accuracy: 0.8418 - val_f1_score: 0.8538\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2125 - accuracy: 0.9144 - f1_score: 0.9142 - val_loss: 0.3776 - val_accuracy: 0.8526 - val_f1_score: 0.8589\n","Epoch 17/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2049 - accuracy: 0.9153 - f1_score: 0.9149 - val_loss: 0.3929 - val_accuracy: 0.8734 - val_f1_score: 0.8757\n","Epoch 18/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2055 - accuracy: 0.9195 - f1_score: 0.9186 - val_loss: 0.4072 - val_accuracy: 0.8698 - val_f1_score: 0.8667\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3818 - accuracy: 0.8528 - f1_score: 0.8740\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6253 - accuracy: 0.6531 - f1_score: 0.6524 - val_loss: 0.5048 - val_accuracy: 0.7658 - val_f1_score: 0.7694\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4732 - accuracy: 0.7788 - f1_score: 0.7732 - val_loss: 0.3989 - val_accuracy: 0.8201 - val_f1_score: 0.8163\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4187 - accuracy: 0.8092 - f1_score: 0.8059 - val_loss: 0.3518 - val_accuracy: 0.8517 - val_f1_score: 0.8546\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3698 - accuracy: 0.8345 - f1_score: 0.8321 - val_loss: 0.3683 - val_accuracy: 0.8345 - val_f1_score: 0.8484\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3430 - accuracy: 0.8541 - f1_score: 0.8524 - val_loss: 0.3116 - val_accuracy: 0.8843 - val_f1_score: 0.8804\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3306 - accuracy: 0.8599 - f1_score: 0.8601 - val_loss: 0.2998 - val_accuracy: 0.8825 - val_f1_score: 0.8833\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3015 - accuracy: 0.8819 - f1_score: 0.8816 - val_loss: 0.3127 - val_accuracy: 0.8707 - val_f1_score: 0.8632\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3051 - accuracy: 0.8797 - f1_score: 0.8796 - val_loss: 0.3261 - val_accuracy: 0.8571 - val_f1_score: 0.8436\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3002 - accuracy: 0.8743 - f1_score: 0.8739 - val_loss: 0.2938 - val_accuracy: 0.8843 - val_f1_score: 0.8821\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2728 - accuracy: 0.8918 - f1_score: 0.8918 - val_loss: 0.3146 - val_accuracy: 0.8689 - val_f1_score: 0.8583\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2580 - accuracy: 0.8996 - f1_score: 0.8989 - val_loss: 0.3102 - val_accuracy: 0.8725 - val_f1_score: 0.8777\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2545 - accuracy: 0.9002 - f1_score: 0.9003 - val_loss: 0.3227 - val_accuracy: 0.8698 - val_f1_score: 0.8585\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2359 - accuracy: 0.9117 - f1_score: 0.9111 - val_loss: 0.3327 - val_accuracy: 0.8580 - val_f1_score: 0.8425\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2374 - accuracy: 0.9096 - f1_score: 0.9097 - val_loss: 0.2999 - val_accuracy: 0.8852 - val_f1_score: 0.8810\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8481 - f1_score: 0.8756\n","Epoch 1/20\n","104/104 [==============================] - 7s 18ms/step - loss: 0.6056 - accuracy: 0.6742 - f1_score: 0.6882 - val_loss: 0.5687 - val_accuracy: 0.7043 - val_f1_score: 0.7494\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4515 - accuracy: 0.7878 - f1_score: 0.7808 - val_loss: 0.4141 - val_accuracy: 0.8219 - val_f1_score: 0.8362\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4050 - accuracy: 0.8282 - f1_score: 0.8246 - val_loss: 0.3490 - val_accuracy: 0.8454 - val_f1_score: 0.8394\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3528 - accuracy: 0.8535 - f1_score: 0.8521 - val_loss: 0.3276 - val_accuracy: 0.8626 - val_f1_score: 0.8648\n","Epoch 5/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.3269 - accuracy: 0.8671 - f1_score: 0.8658 - val_loss: 0.3119 - val_accuracy: 0.8752 - val_f1_score: 0.8691\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3206 - accuracy: 0.8668 - f1_score: 0.8653 - val_loss: 0.3391 - val_accuracy: 0.8499 - val_f1_score: 0.8612\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3040 - accuracy: 0.8710 - f1_score: 0.8704 - val_loss: 0.3018 - val_accuracy: 0.8807 - val_f1_score: 0.8769\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2865 - accuracy: 0.8825 - f1_score: 0.8810 - val_loss: 0.3103 - val_accuracy: 0.8734 - val_f1_score: 0.8789\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2703 - accuracy: 0.8939 - f1_score: 0.8931 - val_loss: 0.3007 - val_accuracy: 0.8797 - val_f1_score: 0.8830\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2609 - accuracy: 0.8993 - f1_score: 0.8985 - val_loss: 0.3081 - val_accuracy: 0.8788 - val_f1_score: 0.8822\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2454 - accuracy: 0.9002 - f1_score: 0.8998 - val_loss: 0.2927 - val_accuracy: 0.8870 - val_f1_score: 0.8850\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2403 - accuracy: 0.9054 - f1_score: 0.9042 - val_loss: 0.3305 - val_accuracy: 0.8671 - val_f1_score: 0.8757\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2358 - accuracy: 0.9063 - f1_score: 0.9057 - val_loss: 0.2966 - val_accuracy: 0.8816 - val_f1_score: 0.8777\n","Epoch 14/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2289 - accuracy: 0.9120 - f1_score: 0.9111 - val_loss: 0.3178 - val_accuracy: 0.8807 - val_f1_score: 0.8832\n","Epoch 15/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2183 - accuracy: 0.9135 - f1_score: 0.9130 - val_loss: 0.3025 - val_accuracy: 0.8897 - val_f1_score: 0.8903\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2073 - accuracy: 0.9177 - f1_score: 0.9171 - val_loss: 0.3023 - val_accuracy: 0.8797 - val_f1_score: 0.8788\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8535 - f1_score: 0.8769\n","Epoch 1/20\n","104/104 [==============================] - 7s 17ms/step - loss: 0.6247 - accuracy: 0.6555 - f1_score: 0.6391 - val_loss: 0.5260 - val_accuracy: 0.7414 - val_f1_score: 0.7271\n","Epoch 2/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4768 - accuracy: 0.7740 - f1_score: 0.7684 - val_loss: 0.5043 - val_accuracy: 0.7658 - val_f1_score: 0.7930\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4052 - accuracy: 0.8180 - f1_score: 0.8142 - val_loss: 0.3984 - val_accuracy: 0.8246 - val_f1_score: 0.8310\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3628 - accuracy: 0.8430 - f1_score: 0.8417 - val_loss: 0.4801 - val_accuracy: 0.7631 - val_f1_score: 0.7985\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3348 - accuracy: 0.8499 - f1_score: 0.8489 - val_loss: 0.3675 - val_accuracy: 0.8427 - val_f1_score: 0.8525\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3223 - accuracy: 0.8656 - f1_score: 0.8646 - val_loss: 0.3584 - val_accuracy: 0.8544 - val_f1_score: 0.8630\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2875 - accuracy: 0.8885 - f1_score: 0.8881 - val_loss: 0.3462 - val_accuracy: 0.8544 - val_f1_score: 0.8625\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2805 - accuracy: 0.8885 - f1_score: 0.8881 - val_loss: 0.3812 - val_accuracy: 0.8373 - val_f1_score: 0.8520\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2740 - accuracy: 0.8936 - f1_score: 0.8938 - val_loss: 0.3426 - val_accuracy: 0.8617 - val_f1_score: 0.8684\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2496 - accuracy: 0.9027 - f1_score: 0.9022 - val_loss: 0.3281 - val_accuracy: 0.8770 - val_f1_score: 0.8781\n","Epoch 11/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2511 - accuracy: 0.9014 - f1_score: 0.9014 - val_loss: 0.3525 - val_accuracy: 0.8517 - val_f1_score: 0.8423\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2450 - accuracy: 0.9036 - f1_score: 0.9031 - val_loss: 0.3633 - val_accuracy: 0.8517 - val_f1_score: 0.8613\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2383 - accuracy: 0.9039 - f1_score: 0.9035 - val_loss: 0.3553 - val_accuracy: 0.8599 - val_f1_score: 0.8653\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2220 - accuracy: 0.9105 - f1_score: 0.9099 - val_loss: 0.3339 - val_accuracy: 0.8716 - val_f1_score: 0.8700\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2072 - accuracy: 0.9204 - f1_score: 0.9203 - val_loss: 0.3608 - val_accuracy: 0.8535 - val_f1_score: 0.8548\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3550 - accuracy: 0.8569 - f1_score: 0.8827\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.6125 - accuracy: 0.6582 - f1_score: 0.6363 - val_loss: 0.5205 - val_accuracy: 0.7568 - val_f1_score: 0.7235\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4726 - accuracy: 0.7800 - f1_score: 0.7720 - val_loss: 0.4246 - val_accuracy: 0.8020 - val_f1_score: 0.8070\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4133 - accuracy: 0.8149 - f1_score: 0.8118 - val_loss: 0.4186 - val_accuracy: 0.8101 - val_f1_score: 0.7826\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3710 - accuracy: 0.8357 - f1_score: 0.8326 - val_loss: 0.3440 - val_accuracy: 0.8571 - val_f1_score: 0.8512\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3326 - accuracy: 0.8586 - f1_score: 0.8571 - val_loss: 0.3372 - val_accuracy: 0.8617 - val_f1_score: 0.8664\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3322 - accuracy: 0.8568 - f1_score: 0.8561 - val_loss: 0.3175 - val_accuracy: 0.8807 - val_f1_score: 0.8778\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2979 - accuracy: 0.8822 - f1_score: 0.8821 - val_loss: 0.3071 - val_accuracy: 0.8779 - val_f1_score: 0.8789\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2970 - accuracy: 0.8797 - f1_score: 0.8794 - val_loss: 0.3157 - val_accuracy: 0.8743 - val_f1_score: 0.8700\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2740 - accuracy: 0.8915 - f1_score: 0.8908 - val_loss: 0.3358 - val_accuracy: 0.8580 - val_f1_score: 0.8459\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2599 - accuracy: 0.8957 - f1_score: 0.8957 - val_loss: 0.3156 - val_accuracy: 0.8725 - val_f1_score: 0.8729\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2527 - accuracy: 0.8948 - f1_score: 0.8936 - val_loss: 0.3174 - val_accuracy: 0.8797 - val_f1_score: 0.8758\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2492 - accuracy: 0.8966 - f1_score: 0.8961 - val_loss: 0.3127 - val_accuracy: 0.8734 - val_f1_score: 0.8704\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8542 - f1_score: 0.8812\n","Epoch 1/20\n","104/104 [==============================] - 7s 22ms/step - loss: 0.6384 - accuracy: 0.6444 - f1_score: 0.6588 - val_loss: 0.5095 - val_accuracy: 0.7550 - val_f1_score: 0.7451\n","Epoch 2/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4798 - accuracy: 0.7752 - f1_score: 0.7632 - val_loss: 0.4312 - val_accuracy: 0.8092 - val_f1_score: 0.8094\n","Epoch 3/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.4108 - accuracy: 0.8222 - f1_score: 0.8171 - val_loss: 0.3920 - val_accuracy: 0.8255 - val_f1_score: 0.8099\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3590 - accuracy: 0.8457 - f1_score: 0.8413 - val_loss: 0.4246 - val_accuracy: 0.8083 - val_f1_score: 0.8288\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3540 - accuracy: 0.8550 - f1_score: 0.8528 - val_loss: 0.3424 - val_accuracy: 0.8508 - val_f1_score: 0.8518\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3127 - accuracy: 0.8740 - f1_score: 0.8729 - val_loss: 0.3450 - val_accuracy: 0.8526 - val_f1_score: 0.8400\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3024 - accuracy: 0.8716 - f1_score: 0.8700 - val_loss: 0.3299 - val_accuracy: 0.8608 - val_f1_score: 0.8514\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2943 - accuracy: 0.8785 - f1_score: 0.8776 - val_loss: 0.3729 - val_accuracy: 0.8409 - val_f1_score: 0.8186\n","Epoch 9/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3009 - accuracy: 0.8737 - f1_score: 0.8719 - val_loss: 0.3117 - val_accuracy: 0.8725 - val_f1_score: 0.8676\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2623 - accuracy: 0.9005 - f1_score: 0.8995 - val_loss: 0.3098 - val_accuracy: 0.8590 - val_f1_score: 0.8542\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2618 - accuracy: 0.8936 - f1_score: 0.8929 - val_loss: 0.3169 - val_accuracy: 0.8608 - val_f1_score: 0.8603\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2543 - accuracy: 0.9030 - f1_score: 0.9018 - val_loss: 0.3084 - val_accuracy: 0.8689 - val_f1_score: 0.8659\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2324 - accuracy: 0.9114 - f1_score: 0.9108 - val_loss: 0.3250 - val_accuracy: 0.8635 - val_f1_score: 0.8670\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2284 - accuracy: 0.9111 - f1_score: 0.9104 - val_loss: 0.3234 - val_accuracy: 0.8680 - val_f1_score: 0.8604\n","Epoch 15/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2123 - accuracy: 0.9231 - f1_score: 0.9225 - val_loss: 0.3219 - val_accuracy: 0.8635 - val_f1_score: 0.8601\n","Epoch 16/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2083 - accuracy: 0.9234 - f1_score: 0.9229 - val_loss: 0.3491 - val_accuracy: 0.8463 - val_f1_score: 0.8545\n","Epoch 17/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1952 - accuracy: 0.9256 - f1_score: 0.9250 - val_loss: 0.3425 - val_accuracy: 0.8662 - val_f1_score: 0.8657\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8569 - f1_score: 0.8804\n","Epoch 1/20\n","104/104 [==============================] - 10s 23ms/step - loss: 0.6287 - accuracy: 0.6474 - f1_score: 0.6141 - val_loss: 0.5254 - val_accuracy: 0.7495 - val_f1_score: 0.7222\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4918 - accuracy: 0.7643 - f1_score: 0.7541 - val_loss: 0.4415 - val_accuracy: 0.7975 - val_f1_score: 0.7728\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4018 - accuracy: 0.8159 - f1_score: 0.8115 - val_loss: 0.4564 - val_accuracy: 0.7911 - val_f1_score: 0.8139\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3570 - accuracy: 0.8421 - f1_score: 0.8394 - val_loss: 0.3712 - val_accuracy: 0.8391 - val_f1_score: 0.8444\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3347 - accuracy: 0.8556 - f1_score: 0.8533 - val_loss: 0.3914 - val_accuracy: 0.8300 - val_f1_score: 0.8396\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3133 - accuracy: 0.8710 - f1_score: 0.8692 - val_loss: 0.3744 - val_accuracy: 0.8409 - val_f1_score: 0.8498\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3313 - accuracy: 0.8602 - f1_score: 0.8591 - val_loss: 0.3355 - val_accuracy: 0.8635 - val_f1_score: 0.8636\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2948 - accuracy: 0.8819 - f1_score: 0.8816 - val_loss: 0.3585 - val_accuracy: 0.8544 - val_f1_score: 0.8599\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2700 - accuracy: 0.8918 - f1_score: 0.8910 - val_loss: 0.3387 - val_accuracy: 0.8653 - val_f1_score: 0.8649\n","Epoch 10/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2582 - accuracy: 0.8999 - f1_score: 0.9000 - val_loss: 0.3723 - val_accuracy: 0.8599 - val_f1_score: 0.8485\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2559 - accuracy: 0.8909 - f1_score: 0.8906 - val_loss: 0.3377 - val_accuracy: 0.8635 - val_f1_score: 0.8648\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2368 - accuracy: 0.9081 - f1_score: 0.9078 - val_loss: 0.3488 - val_accuracy: 0.8580 - val_f1_score: 0.8506\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8623 - f1_score: 0.8908\n","Epoch 1/20\n","104/104 [==============================] - 8s 21ms/step - loss: 0.6302 - accuracy: 0.6579 - f1_score: 0.6342 - val_loss: 0.5411 - val_accuracy: 0.7306 - val_f1_score: 0.6761\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.5029 - accuracy: 0.7598 - f1_score: 0.7509 - val_loss: 0.4322 - val_accuracy: 0.8002 - val_f1_score: 0.8083\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4219 - accuracy: 0.8038 - f1_score: 0.7990 - val_loss: 0.4385 - val_accuracy: 0.7993 - val_f1_score: 0.8192\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3839 - accuracy: 0.8276 - f1_score: 0.8240 - val_loss: 0.3974 - val_accuracy: 0.8291 - val_f1_score: 0.8062\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3562 - accuracy: 0.8451 - f1_score: 0.8415 - val_loss: 0.3586 - val_accuracy: 0.8508 - val_f1_score: 0.8374\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3318 - accuracy: 0.8629 - f1_score: 0.8605 - val_loss: 0.3328 - val_accuracy: 0.8689 - val_f1_score: 0.8671\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3150 - accuracy: 0.8746 - f1_score: 0.8722 - val_loss: 0.3340 - val_accuracy: 0.8734 - val_f1_score: 0.8763\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2928 - accuracy: 0.8825 - f1_score: 0.8810 - val_loss: 0.3131 - val_accuracy: 0.8825 - val_f1_score: 0.8820\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2891 - accuracy: 0.8800 - f1_score: 0.8788 - val_loss: 0.3565 - val_accuracy: 0.8535 - val_f1_score: 0.8632\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2838 - accuracy: 0.8852 - f1_score: 0.8841 - val_loss: 0.3088 - val_accuracy: 0.8779 - val_f1_score: 0.8772\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2693 - accuracy: 0.8891 - f1_score: 0.8878 - val_loss: 0.3565 - val_accuracy: 0.8608 - val_f1_score: 0.8457\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2627 - accuracy: 0.8921 - f1_score: 0.8907 - val_loss: 0.3452 - val_accuracy: 0.8698 - val_f1_score: 0.8594\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2453 - accuracy: 0.9033 - f1_score: 0.9020 - val_loss: 0.3481 - val_accuracy: 0.8816 - val_f1_score: 0.8777\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2356 - accuracy: 0.9048 - f1_score: 0.9040 - val_loss: 0.3341 - val_accuracy: 0.8834 - val_f1_score: 0.8786\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2138 - accuracy: 0.9186 - f1_score: 0.9182 - val_loss: 0.3885 - val_accuracy: 0.8599 - val_f1_score: 0.8470\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8616 - f1_score: 0.8918\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6139 - accuracy: 0.6691 - f1_score: 0.6679 - val_loss: 0.5109 - val_accuracy: 0.7622 - val_f1_score: 0.7616\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4747 - accuracy: 0.7794 - f1_score: 0.7681 - val_loss: 0.4306 - val_accuracy: 0.7966 - val_f1_score: 0.7863\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.4028 - accuracy: 0.8134 - f1_score: 0.8067 - val_loss: 0.3973 - val_accuracy: 0.8309 - val_f1_score: 0.8326\n","Epoch 4/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3617 - accuracy: 0.8457 - f1_score: 0.8419 - val_loss: 0.6370 - val_accuracy: 0.7414 - val_f1_score: 0.7903\n","Epoch 5/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3377 - accuracy: 0.8535 - f1_score: 0.8504 - val_loss: 0.4571 - val_accuracy: 0.8264 - val_f1_score: 0.8431\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3062 - accuracy: 0.8683 - f1_score: 0.8662 - val_loss: 0.3806 - val_accuracy: 0.8526 - val_f1_score: 0.8603\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3200 - accuracy: 0.8683 - f1_score: 0.8655 - val_loss: 0.3726 - val_accuracy: 0.8517 - val_f1_score: 0.8579\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2956 - accuracy: 0.8788 - f1_score: 0.8768 - val_loss: 0.4374 - val_accuracy: 0.8237 - val_f1_score: 0.8416\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2751 - accuracy: 0.8834 - f1_score: 0.8810 - val_loss: 0.3694 - val_accuracy: 0.8644 - val_f1_score: 0.8668\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2837 - accuracy: 0.8852 - f1_score: 0.8835 - val_loss: 0.4590 - val_accuracy: 0.8128 - val_f1_score: 0.8351\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2413 - accuracy: 0.9017 - f1_score: 0.9005 - val_loss: 0.4013 - val_accuracy: 0.8599 - val_f1_score: 0.8656\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2421 - accuracy: 0.8990 - f1_score: 0.8976 - val_loss: 0.4737 - val_accuracy: 0.8409 - val_f1_score: 0.8541\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2325 - accuracy: 0.9054 - f1_score: 0.9042 - val_loss: 0.4473 - val_accuracy: 0.8445 - val_f1_score: 0.8562\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2295 - accuracy: 0.9090 - f1_score: 0.9078 - val_loss: 0.3731 - val_accuracy: 0.8653 - val_f1_score: 0.8699\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3255 - accuracy: 0.8562 - f1_score: 0.8851\n","Epoch 1/20\n","104/104 [==============================] - 8s 20ms/step - loss: 0.6344 - accuracy: 0.6359 - f1_score: 0.6177 - val_loss: 0.5271 - val_accuracy: 0.7468 - val_f1_score: 0.7487\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4903 - accuracy: 0.7625 - f1_score: 0.7500 - val_loss: 0.4259 - val_accuracy: 0.8219 - val_f1_score: 0.8273\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3980 - accuracy: 0.8131 - f1_score: 0.8044 - val_loss: 0.4319 - val_accuracy: 0.8110 - val_f1_score: 0.8310\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3662 - accuracy: 0.8388 - f1_score: 0.8351 - val_loss: 0.6374 - val_accuracy: 0.7061 - val_f1_score: 0.7677\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3490 - accuracy: 0.8466 - f1_score: 0.8435 - val_loss: 0.3596 - val_accuracy: 0.8436 - val_f1_score: 0.8525\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3096 - accuracy: 0.8743 - f1_score: 0.8714 - val_loss: 0.4566 - val_accuracy: 0.8083 - val_f1_score: 0.8320\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2976 - accuracy: 0.8779 - f1_score: 0.8766 - val_loss: 0.3149 - val_accuracy: 0.8743 - val_f1_score: 0.8742\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2864 - accuracy: 0.8870 - f1_score: 0.8847 - val_loss: 0.3546 - val_accuracy: 0.8490 - val_f1_score: 0.8586\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2852 - accuracy: 0.8825 - f1_score: 0.8807 - val_loss: 0.3962 - val_accuracy: 0.8336 - val_f1_score: 0.8492\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2632 - accuracy: 0.8951 - f1_score: 0.8936 - val_loss: 0.3434 - val_accuracy: 0.8553 - val_f1_score: 0.8618\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2594 - accuracy: 0.8936 - f1_score: 0.8922 - val_loss: 0.3473 - val_accuracy: 0.8626 - val_f1_score: 0.8681\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2514 - accuracy: 0.9008 - f1_score: 0.8995 - val_loss: 0.4295 - val_accuracy: 0.8237 - val_f1_score: 0.8416\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8609 - f1_score: 0.8849\n","Epoch 1/20\n","104/104 [==============================] - 7s 24ms/step - loss: 0.6229 - accuracy: 0.6528 - f1_score: 0.6701 - val_loss: 0.5406 - val_accuracy: 0.7260 - val_f1_score: 0.7248\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4862 - accuracy: 0.7737 - f1_score: 0.7572 - val_loss: 0.4636 - val_accuracy: 0.7893 - val_f1_score: 0.8047\n","Epoch 3/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3984 - accuracy: 0.8213 - f1_score: 0.8152 - val_loss: 0.4052 - val_accuracy: 0.8174 - val_f1_score: 0.8317\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3492 - accuracy: 0.8532 - f1_score: 0.8513 - val_loss: 0.3731 - val_accuracy: 0.8354 - val_f1_score: 0.8198\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3305 - accuracy: 0.8568 - f1_score: 0.8563 - val_loss: 0.3548 - val_accuracy: 0.8590 - val_f1_score: 0.8657\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3156 - accuracy: 0.8737 - f1_score: 0.8734 - val_loss: 0.3757 - val_accuracy: 0.8345 - val_f1_score: 0.8501\n","Epoch 7/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2965 - accuracy: 0.8819 - f1_score: 0.8815 - val_loss: 0.3469 - val_accuracy: 0.8499 - val_f1_score: 0.8605\n","Epoch 8/20\n","104/104 [==============================] - 1s 8ms/step - loss: 0.2838 - accuracy: 0.8843 - f1_score: 0.8844 - val_loss: 0.3613 - val_accuracy: 0.8400 - val_f1_score: 0.8534\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2842 - accuracy: 0.8828 - f1_score: 0.8819 - val_loss: 0.3228 - val_accuracy: 0.8662 - val_f1_score: 0.8693\n","Epoch 10/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2605 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3305 - val_accuracy: 0.8644 - val_f1_score: 0.8673\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2504 - accuracy: 0.8999 - f1_score: 0.8992 - val_loss: 0.3255 - val_accuracy: 0.8544 - val_f1_score: 0.8601\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2427 - accuracy: 0.9039 - f1_score: 0.9037 - val_loss: 0.3774 - val_accuracy: 0.8544 - val_f1_score: 0.8438\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2381 - accuracy: 0.9045 - f1_score: 0.9040 - val_loss: 0.3766 - val_accuracy: 0.8418 - val_f1_score: 0.8548\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2338 - accuracy: 0.9024 - f1_score: 0.9024 - val_loss: 0.3834 - val_accuracy: 0.8463 - val_f1_score: 0.8310\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8670 - f1_score: 0.8935\n","Epoch 1/20\n","104/104 [==============================] - 7s 27ms/step - loss: 0.6175 - accuracy: 0.6570 - f1_score: 0.6583 - val_loss: 0.5213 - val_accuracy: 0.7577 - val_f1_score: 0.7248\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4671 - accuracy: 0.7830 - f1_score: 0.7744 - val_loss: 0.4240 - val_accuracy: 0.8002 - val_f1_score: 0.7889\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4014 - accuracy: 0.8180 - f1_score: 0.8123 - val_loss: 0.3780 - val_accuracy: 0.8327 - val_f1_score: 0.8335\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3601 - accuracy: 0.8400 - f1_score: 0.8359 - val_loss: 0.4480 - val_accuracy: 0.8074 - val_f1_score: 0.8286\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3405 - accuracy: 0.8538 - f1_score: 0.8515 - val_loss: 0.3528 - val_accuracy: 0.8463 - val_f1_score: 0.8517\n","Epoch 6/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.3016 - accuracy: 0.8737 - f1_score: 0.8726 - val_loss: 0.3272 - val_accuracy: 0.8617 - val_f1_score: 0.8544\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2939 - accuracy: 0.8800 - f1_score: 0.8779 - val_loss: 0.4204 - val_accuracy: 0.8174 - val_f1_score: 0.8376\n","Epoch 8/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2877 - accuracy: 0.8788 - f1_score: 0.8774 - val_loss: 0.3256 - val_accuracy: 0.8653 - val_f1_score: 0.8678\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2685 - accuracy: 0.8894 - f1_score: 0.8877 - val_loss: 0.3273 - val_accuracy: 0.8526 - val_f1_score: 0.8511\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2555 - accuracy: 0.8960 - f1_score: 0.8950 - val_loss: 0.3717 - val_accuracy: 0.8409 - val_f1_score: 0.8538\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2390 - accuracy: 0.9054 - f1_score: 0.9046 - val_loss: 0.3161 - val_accuracy: 0.8752 - val_f1_score: 0.8722\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2381 - accuracy: 0.9024 - f1_score: 0.9016 - val_loss: 0.4253 - val_accuracy: 0.8201 - val_f1_score: 0.8391\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2259 - accuracy: 0.9066 - f1_score: 0.9061 - val_loss: 0.3422 - val_accuracy: 0.8644 - val_f1_score: 0.8644\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2064 - accuracy: 0.9168 - f1_score: 0.9161 - val_loss: 0.3504 - val_accuracy: 0.8689 - val_f1_score: 0.8688\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1921 - accuracy: 0.9289 - f1_score: 0.9284 - val_loss: 0.3801 - val_accuracy: 0.8526 - val_f1_score: 0.8574\n","Epoch 16/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.1907 - accuracy: 0.9262 - f1_score: 0.9260 - val_loss: 0.3471 - val_accuracy: 0.8707 - val_f1_score: 0.8701\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3983 - accuracy: 0.8420 - f1_score: 0.8644\n","Epoch 1/20\n","104/104 [==============================] - 8s 26ms/step - loss: 0.6197 - accuracy: 0.6621 - f1_score: 0.6522 - val_loss: 0.5450 - val_accuracy: 0.7315 - val_f1_score: 0.6844\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4762 - accuracy: 0.7866 - f1_score: 0.7803 - val_loss: 0.4385 - val_accuracy: 0.7984 - val_f1_score: 0.7986\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4206 - accuracy: 0.8101 - f1_score: 0.8040 - val_loss: 0.4167 - val_accuracy: 0.8011 - val_f1_score: 0.7813\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3878 - accuracy: 0.8273 - f1_score: 0.8228 - val_loss: 0.3746 - val_accuracy: 0.8345 - val_f1_score: 0.8275\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3442 - accuracy: 0.8574 - f1_score: 0.8547 - val_loss: 0.3730 - val_accuracy: 0.8327 - val_f1_score: 0.8393\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3283 - accuracy: 0.8629 - f1_score: 0.8605 - val_loss: 0.3384 - val_accuracy: 0.8562 - val_f1_score: 0.8543\n","Epoch 7/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3041 - accuracy: 0.8704 - f1_score: 0.8689 - val_loss: 0.3287 - val_accuracy: 0.8644 - val_f1_score: 0.8616\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2754 - accuracy: 0.8894 - f1_score: 0.8882 - val_loss: 0.3348 - val_accuracy: 0.8644 - val_f1_score: 0.8574\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2588 - accuracy: 0.9017 - f1_score: 0.9009 - val_loss: 0.3661 - val_accuracy: 0.8517 - val_f1_score: 0.8370\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2533 - accuracy: 0.9008 - f1_score: 0.8997 - val_loss: 0.3284 - val_accuracy: 0.8580 - val_f1_score: 0.8526\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2444 - accuracy: 0.9002 - f1_score: 0.8991 - val_loss: 0.3452 - val_accuracy: 0.8535 - val_f1_score: 0.8430\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2568 - accuracy: 0.8948 - f1_score: 0.8937 - val_loss: 0.3602 - val_accuracy: 0.8644 - val_f1_score: 0.8677\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2195 - accuracy: 0.9129 - f1_score: 0.9116 - val_loss: 0.3661 - val_accuracy: 0.8590 - val_f1_score: 0.8600\n","Epoch 14/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.1988 - accuracy: 0.9213 - f1_score: 0.9204 - val_loss: 0.3787 - val_accuracy: 0.8662 - val_f1_score: 0.8632\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1845 - accuracy: 0.9325 - f1_score: 0.9316 - val_loss: 0.3811 - val_accuracy: 0.8590 - val_f1_score: 0.8590\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8440 - f1_score: 0.8673\n","Epoch 1/20\n","104/104 [==============================] - 7s 23ms/step - loss: 0.6238 - accuracy: 0.6459 - f1_score: 0.6496 - val_loss: 0.5104 - val_accuracy: 0.7767 - val_f1_score: 0.7876\n","Epoch 2/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.4928 - accuracy: 0.7712 - f1_score: 0.7624 - val_loss: 0.4122 - val_accuracy: 0.8228 - val_f1_score: 0.8234\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4184 - accuracy: 0.8065 - f1_score: 0.8031 - val_loss: 0.3624 - val_accuracy: 0.8454 - val_f1_score: 0.8403\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3837 - accuracy: 0.8360 - f1_score: 0.8330 - val_loss: 0.3531 - val_accuracy: 0.8436 - val_f1_score: 0.8510\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3324 - accuracy: 0.8553 - f1_score: 0.8539 - val_loss: 0.3251 - val_accuracy: 0.8671 - val_f1_score: 0.8648\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3292 - accuracy: 0.8608 - f1_score: 0.8597 - val_loss: 0.3171 - val_accuracy: 0.8734 - val_f1_score: 0.8741\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2986 - accuracy: 0.8794 - f1_score: 0.8778 - val_loss: 0.3352 - val_accuracy: 0.8617 - val_f1_score: 0.8493\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2721 - accuracy: 0.8936 - f1_score: 0.8929 - val_loss: 0.3359 - val_accuracy: 0.8644 - val_f1_score: 0.8552\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2685 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3098 - val_accuracy: 0.8779 - val_f1_score: 0.8787\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2593 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.3336 - val_accuracy: 0.8562 - val_f1_score: 0.8433\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2417 - accuracy: 0.9084 - f1_score: 0.9075 - val_loss: 0.3370 - val_accuracy: 0.8608 - val_f1_score: 0.8516\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2368 - accuracy: 0.9084 - f1_score: 0.9079 - val_loss: 0.3554 - val_accuracy: 0.8626 - val_f1_score: 0.8524\n","Epoch 13/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2287 - accuracy: 0.9183 - f1_score: 0.9180 - val_loss: 0.3194 - val_accuracy: 0.8716 - val_f1_score: 0.8663\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2202 - accuracy: 0.9108 - f1_score: 0.9101 - val_loss: 0.3371 - val_accuracy: 0.8689 - val_f1_score: 0.8685\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8724 - f1_score: 0.8988\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6231 - accuracy: 0.6483 - f1_score: 0.6359 - val_loss: 0.5375 - val_accuracy: 0.7342 - val_f1_score: 0.6839\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4764 - accuracy: 0.7797 - f1_score: 0.7725 - val_loss: 0.4473 - val_accuracy: 0.7812 - val_f1_score: 0.7632\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4068 - accuracy: 0.8168 - f1_score: 0.8121 - val_loss: 0.3989 - val_accuracy: 0.8174 - val_f1_score: 0.8061\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3721 - accuracy: 0.8333 - f1_score: 0.8321 - val_loss: 0.4087 - val_accuracy: 0.8273 - val_f1_score: 0.8100\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3311 - accuracy: 0.8574 - f1_score: 0.8556 - val_loss: 0.3698 - val_accuracy: 0.8454 - val_f1_score: 0.8406\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3076 - accuracy: 0.8737 - f1_score: 0.8732 - val_loss: 0.3593 - val_accuracy: 0.8517 - val_f1_score: 0.8544\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2975 - accuracy: 0.8788 - f1_score: 0.8772 - val_loss: 0.3555 - val_accuracy: 0.8571 - val_f1_score: 0.8594\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2848 - accuracy: 0.8852 - f1_score: 0.8841 - val_loss: 0.3547 - val_accuracy: 0.8517 - val_f1_score: 0.8444\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2722 - accuracy: 0.8909 - f1_score: 0.8893 - val_loss: 0.3820 - val_accuracy: 0.8499 - val_f1_score: 0.8363\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2593 - accuracy: 0.8954 - f1_score: 0.8942 - val_loss: 0.3428 - val_accuracy: 0.8580 - val_f1_score: 0.8582\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2461 - accuracy: 0.9048 - f1_score: 0.9036 - val_loss: 0.3689 - val_accuracy: 0.8617 - val_f1_score: 0.8608\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2347 - accuracy: 0.9084 - f1_score: 0.9079 - val_loss: 0.3742 - val_accuracy: 0.8571 - val_f1_score: 0.8529\n","Epoch 13/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2289 - accuracy: 0.9153 - f1_score: 0.9146 - val_loss: 0.3667 - val_accuracy: 0.8626 - val_f1_score: 0.8648\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2144 - accuracy: 0.9168 - f1_score: 0.9161 - val_loss: 0.3558 - val_accuracy: 0.8608 - val_f1_score: 0.8635\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2108 - accuracy: 0.9237 - f1_score: 0.9230 - val_loss: 0.3646 - val_accuracy: 0.8608 - val_f1_score: 0.8566\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8596 - f1_score: 0.8865\n","Epoch 1/20\n","104/104 [==============================] - 7s 21ms/step - loss: 0.6280 - accuracy: 0.6501 - f1_score: 0.6815 - val_loss: 0.5115 - val_accuracy: 0.7505 - val_f1_score: 0.7760\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4807 - accuracy: 0.7661 - f1_score: 0.7573 - val_loss: 0.3738 - val_accuracy: 0.8300 - val_f1_score: 0.8164\n","Epoch 3/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.4223 - accuracy: 0.8146 - f1_score: 0.8092 - val_loss: 0.3275 - val_accuracy: 0.8716 - val_f1_score: 0.8683\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3736 - accuracy: 0.8415 - f1_score: 0.8375 - val_loss: 0.3203 - val_accuracy: 0.8635 - val_f1_score: 0.8704\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3564 - accuracy: 0.8442 - f1_score: 0.8426 - val_loss: 0.2914 - val_accuracy: 0.8797 - val_f1_score: 0.8742\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3285 - accuracy: 0.8596 - f1_score: 0.8582 - val_loss: 0.2782 - val_accuracy: 0.8924 - val_f1_score: 0.8881\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3025 - accuracy: 0.8725 - f1_score: 0.8712 - val_loss: 0.2709 - val_accuracy: 0.8924 - val_f1_score: 0.8927\n","Epoch 8/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2990 - accuracy: 0.8761 - f1_score: 0.8749 - val_loss: 0.2665 - val_accuracy: 0.8951 - val_f1_score: 0.8947\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3011 - accuracy: 0.8719 - f1_score: 0.8707 - val_loss: 0.3361 - val_accuracy: 0.8526 - val_f1_score: 0.8663\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2893 - accuracy: 0.8752 - f1_score: 0.8736 - val_loss: 0.3268 - val_accuracy: 0.8743 - val_f1_score: 0.8831\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2672 - accuracy: 0.8879 - f1_score: 0.8871 - val_loss: 0.2661 - val_accuracy: 0.9060 - val_f1_score: 0.9080\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2643 - accuracy: 0.8867 - f1_score: 0.8858 - val_loss: 0.2852 - val_accuracy: 0.8915 - val_f1_score: 0.8947\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2559 - accuracy: 0.8930 - f1_score: 0.8925 - val_loss: 0.2779 - val_accuracy: 0.9051 - val_f1_score: 0.9050\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2395 - accuracy: 0.9008 - f1_score: 0.8999 - val_loss: 0.3088 - val_accuracy: 0.8816 - val_f1_score: 0.8874\n","Epoch 15/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2277 - accuracy: 0.9108 - f1_score: 0.9102 - val_loss: 0.2998 - val_accuracy: 0.8951 - val_f1_score: 0.8981\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2214 - accuracy: 0.9129 - f1_score: 0.9121 - val_loss: 0.3130 - val_accuracy: 0.8933 - val_f1_score: 0.8979\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8670 - f1_score: 0.8912\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6085 - accuracy: 0.6652 - f1_score: 0.6614 - val_loss: 0.5078 - val_accuracy: 0.7523 - val_f1_score: 0.7335\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4707 - accuracy: 0.7824 - f1_score: 0.7777 - val_loss: 0.4118 - val_accuracy: 0.8065 - val_f1_score: 0.8015\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3959 - accuracy: 0.8189 - f1_score: 0.8166 - val_loss: 0.4000 - val_accuracy: 0.8228 - val_f1_score: 0.8367\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3586 - accuracy: 0.8541 - f1_score: 0.8517 - val_loss: 0.3683 - val_accuracy: 0.8490 - val_f1_score: 0.8539\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3207 - accuracy: 0.8671 - f1_score: 0.8653 - val_loss: 0.3526 - val_accuracy: 0.8526 - val_f1_score: 0.8569\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3159 - accuracy: 0.8758 - f1_score: 0.8741 - val_loss: 0.3566 - val_accuracy: 0.8454 - val_f1_score: 0.8530\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3054 - accuracy: 0.8770 - f1_score: 0.8757 - val_loss: 0.3461 - val_accuracy: 0.8626 - val_f1_score: 0.8544\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2860 - accuracy: 0.8873 - f1_score: 0.8863 - val_loss: 0.3541 - val_accuracy: 0.8526 - val_f1_score: 0.8394\n","Epoch 9/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2741 - accuracy: 0.8897 - f1_score: 0.8887 - val_loss: 0.3409 - val_accuracy: 0.8571 - val_f1_score: 0.8604\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2632 - accuracy: 0.8948 - f1_score: 0.8938 - val_loss: 0.3362 - val_accuracy: 0.8653 - val_f1_score: 0.8661\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2568 - accuracy: 0.9002 - f1_score: 0.8992 - val_loss: 0.3509 - val_accuracy: 0.8635 - val_f1_score: 0.8571\n","Epoch 12/20\n","104/104 [==============================] - 1s 9ms/step - loss: 0.2395 - accuracy: 0.9039 - f1_score: 0.9031 - val_loss: 0.3384 - val_accuracy: 0.8626 - val_f1_score: 0.8618\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2473 - accuracy: 0.9024 - f1_score: 0.9015 - val_loss: 0.3587 - val_accuracy: 0.8635 - val_f1_score: 0.8571\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2216 - accuracy: 0.9144 - f1_score: 0.9141 - val_loss: 0.4045 - val_accuracy: 0.8544 - val_f1_score: 0.8616\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2162 - accuracy: 0.9180 - f1_score: 0.9166 - val_loss: 0.3768 - val_accuracy: 0.8590 - val_f1_score: 0.8632\n","47/47 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8663 - f1_score: 0.8890\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.6174 - accuracy: 0.6634 - f1_score: 0.6258 - val_loss: 0.5341 - val_accuracy: 0.7405 - val_f1_score: 0.7545\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4786 - accuracy: 0.7770 - f1_score: 0.7726 - val_loss: 0.5024 - val_accuracy: 0.7740 - val_f1_score: 0.8028\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3873 - accuracy: 0.8279 - f1_score: 0.8240 - val_loss: 0.3890 - val_accuracy: 0.8472 - val_f1_score: 0.8498\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3332 - accuracy: 0.8617 - f1_score: 0.8610 - val_loss: 0.3923 - val_accuracy: 0.8418 - val_f1_score: 0.8485\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3136 - accuracy: 0.8692 - f1_score: 0.8682 - val_loss: 0.3794 - val_accuracy: 0.8517 - val_f1_score: 0.8551\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3021 - accuracy: 0.8819 - f1_score: 0.8804 - val_loss: 0.3784 - val_accuracy: 0.8445 - val_f1_score: 0.8462\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2844 - accuracy: 0.8909 - f1_score: 0.8896 - val_loss: 0.4254 - val_accuracy: 0.8237 - val_f1_score: 0.8392\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2694 - accuracy: 0.8945 - f1_score: 0.8936 - val_loss: 0.4383 - val_accuracy: 0.8174 - val_f1_score: 0.8360\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2616 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.3633 - val_accuracy: 0.8535 - val_f1_score: 0.8554\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2637 - accuracy: 0.8933 - f1_score: 0.8922 - val_loss: 0.3667 - val_accuracy: 0.8535 - val_f1_score: 0.8477\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2436 - accuracy: 0.9014 - f1_score: 0.9007 - val_loss: 0.3969 - val_accuracy: 0.8427 - val_f1_score: 0.8508\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2415 - accuracy: 0.9075 - f1_score: 0.9071 - val_loss: 0.3798 - val_accuracy: 0.8454 - val_f1_score: 0.8496\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2255 - accuracy: 0.9123 - f1_score: 0.9118 - val_loss: 0.4603 - val_accuracy: 0.8255 - val_f1_score: 0.8401\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2270 - accuracy: 0.9165 - f1_score: 0.9159 - val_loss: 0.3928 - val_accuracy: 0.8517 - val_f1_score: 0.8523\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3359 - accuracy: 0.8677 - f1_score: 0.8896\n","Epoch 1/20\n","104/104 [==============================] - 7s 19ms/step - loss: 0.6174 - accuracy: 0.6640 - f1_score: 0.6499 - val_loss: 0.5114 - val_accuracy: 0.7613 - val_f1_score: 0.7273\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4602 - accuracy: 0.7800 - f1_score: 0.7724 - val_loss: 0.4036 - val_accuracy: 0.8237 - val_f1_score: 0.8229\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3812 - accuracy: 0.8345 - f1_score: 0.8322 - val_loss: 0.4004 - val_accuracy: 0.8327 - val_f1_score: 0.8467\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3464 - accuracy: 0.8508 - f1_score: 0.8488 - val_loss: 0.4860 - val_accuracy: 0.7703 - val_f1_score: 0.8064\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3198 - accuracy: 0.8674 - f1_score: 0.8662 - val_loss: 0.3642 - val_accuracy: 0.8454 - val_f1_score: 0.8571\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3086 - accuracy: 0.8728 - f1_score: 0.8722 - val_loss: 0.5086 - val_accuracy: 0.7414 - val_f1_score: 0.7891\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2992 - accuracy: 0.8785 - f1_score: 0.8784 - val_loss: 0.3461 - val_accuracy: 0.8635 - val_f1_score: 0.8717\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2841 - accuracy: 0.8894 - f1_score: 0.8891 - val_loss: 0.3146 - val_accuracy: 0.8788 - val_f1_score: 0.8808\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2813 - accuracy: 0.8885 - f1_score: 0.8879 - val_loss: 0.3040 - val_accuracy: 0.8852 - val_f1_score: 0.8827\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2881 - accuracy: 0.8816 - f1_score: 0.8800 - val_loss: 0.3160 - val_accuracy: 0.8779 - val_f1_score: 0.8800\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2478 - accuracy: 0.9027 - f1_score: 0.9019 - val_loss: 0.3337 - val_accuracy: 0.8734 - val_f1_score: 0.8780\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2483 - accuracy: 0.8999 - f1_score: 0.8995 - val_loss: 0.3495 - val_accuracy: 0.8526 - val_f1_score: 0.8622\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2398 - accuracy: 0.9060 - f1_score: 0.9059 - val_loss: 0.3521 - val_accuracy: 0.8571 - val_f1_score: 0.8654\n","Epoch 14/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2258 - accuracy: 0.9159 - f1_score: 0.9159 - val_loss: 0.3405 - val_accuracy: 0.8752 - val_f1_score: 0.8701\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8582 - f1_score: 0.8799\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6166 - accuracy: 0.6676 - f1_score: 0.6533 - val_loss: 0.5266 - val_accuracy: 0.7450 - val_f1_score: 0.7469\n","Epoch 2/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4969 - accuracy: 0.7694 - f1_score: 0.7626 - val_loss: 0.4590 - val_accuracy: 0.7830 - val_f1_score: 0.7551\n","Epoch 3/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.4050 - accuracy: 0.8156 - f1_score: 0.8109 - val_loss: 0.4309 - val_accuracy: 0.8165 - val_f1_score: 0.8340\n","Epoch 4/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3707 - accuracy: 0.8403 - f1_score: 0.8386 - val_loss: 0.3532 - val_accuracy: 0.8535 - val_f1_score: 0.8480\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3329 - accuracy: 0.8577 - f1_score: 0.8557 - val_loss: 0.3330 - val_accuracy: 0.8635 - val_f1_score: 0.8655\n","Epoch 6/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3186 - accuracy: 0.8662 - f1_score: 0.8646 - val_loss: 0.3153 - val_accuracy: 0.8770 - val_f1_score: 0.8766\n","Epoch 7/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2955 - accuracy: 0.8725 - f1_score: 0.8714 - val_loss: 0.3628 - val_accuracy: 0.8490 - val_f1_score: 0.8322\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2973 - accuracy: 0.8825 - f1_score: 0.8811 - val_loss: 0.3589 - val_accuracy: 0.8562 - val_f1_score: 0.8427\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2697 - accuracy: 0.8954 - f1_score: 0.8947 - val_loss: 0.3136 - val_accuracy: 0.8797 - val_f1_score: 0.8781\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2630 - accuracy: 0.8972 - f1_score: 0.8961 - val_loss: 0.3244 - val_accuracy: 0.8797 - val_f1_score: 0.8725\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2537 - accuracy: 0.9011 - f1_score: 0.8999 - val_loss: 0.3137 - val_accuracy: 0.8888 - val_f1_score: 0.8862\n","Epoch 12/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2454 - accuracy: 0.9078 - f1_score: 0.9075 - val_loss: 0.3378 - val_accuracy: 0.8834 - val_f1_score: 0.8784\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2410 - accuracy: 0.9060 - f1_score: 0.9055 - val_loss: 0.3655 - val_accuracy: 0.8698 - val_f1_score: 0.8588\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2167 - accuracy: 0.9168 - f1_score: 0.9163 - val_loss: 0.3896 - val_accuracy: 0.8481 - val_f1_score: 0.8306\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.8710 - f1_score: 0.8944\n","Epoch 1/20\n","104/104 [==============================] - 8s 21ms/step - loss: 0.6281 - accuracy: 0.6522 - f1_score: 0.6270 - val_loss: 0.5050 - val_accuracy: 0.7586 - val_f1_score: 0.7311\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4926 - accuracy: 0.7700 - f1_score: 0.7628 - val_loss: 0.4197 - val_accuracy: 0.8083 - val_f1_score: 0.7778\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3947 - accuracy: 0.8219 - f1_score: 0.8181 - val_loss: 0.3555 - val_accuracy: 0.8472 - val_f1_score: 0.8395\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3562 - accuracy: 0.8463 - f1_score: 0.8441 - val_loss: 0.3635 - val_accuracy: 0.8526 - val_f1_score: 0.8375\n","Epoch 5/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.3345 - accuracy: 0.8602 - f1_score: 0.8582 - val_loss: 0.3284 - val_accuracy: 0.8671 - val_f1_score: 0.8662\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3138 - accuracy: 0.8698 - f1_score: 0.8683 - val_loss: 0.3190 - val_accuracy: 0.8644 - val_f1_score: 0.8663\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3028 - accuracy: 0.8752 - f1_score: 0.8745 - val_loss: 0.3431 - val_accuracy: 0.8608 - val_f1_score: 0.8472\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2908 - accuracy: 0.8876 - f1_score: 0.8867 - val_loss: 0.3523 - val_accuracy: 0.8580 - val_f1_score: 0.8438\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2777 - accuracy: 0.8858 - f1_score: 0.8852 - val_loss: 0.3497 - val_accuracy: 0.8562 - val_f1_score: 0.8408\n","Epoch 10/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2681 - accuracy: 0.8915 - f1_score: 0.8903 - val_loss: 0.3460 - val_accuracy: 0.8517 - val_f1_score: 0.8615\n","Epoch 11/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2619 - accuracy: 0.8984 - f1_score: 0.8971 - val_loss: 0.3149 - val_accuracy: 0.8689 - val_f1_score: 0.8618\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2518 - accuracy: 0.8975 - f1_score: 0.8962 - val_loss: 0.3703 - val_accuracy: 0.8463 - val_f1_score: 0.8283\n","Epoch 13/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2292 - accuracy: 0.9156 - f1_score: 0.9147 - val_loss: 0.3306 - val_accuracy: 0.8671 - val_f1_score: 0.8591\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2202 - accuracy: 0.9135 - f1_score: 0.9121 - val_loss: 0.3382 - val_accuracy: 0.8608 - val_f1_score: 0.8647\n","Epoch 15/20\n","104/104 [==============================] - 1s 10ms/step - loss: 0.2148 - accuracy: 0.9177 - f1_score: 0.9169 - val_loss: 0.3846 - val_accuracy: 0.8318 - val_f1_score: 0.8082\n","Epoch 16/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1993 - accuracy: 0.9222 - f1_score: 0.9213 - val_loss: 0.3449 - val_accuracy: 0.8698 - val_f1_score: 0.8691\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8656 - f1_score: 0.8864\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpxB2RCgR9vT","executionInfo":{"status":"ok","timestamp":1690581221271,"user_tz":-330,"elapsed":84,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8e986645-6ba0-4a20-c5bf-10b899367f66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8406482338905334, 0.8528021574020386, 0.8541526198387146, 0.8568534851074219, 0.8528021574020386, 0.8480756282806396, 0.8534773588180542, 0.8568534851074219, 0.8541526198387146, 0.8568534851074219, 0.8622552156448364, 0.8615800142288208, 0.8561782836914062, 0.8609048128128052, 0.8669817447662354, 0.8419986367225647, 0.8440243005752563, 0.8723835349082947, 0.8595543503761292, 0.8669817447662354, 0.8663065433502197, 0.8676570057868958, 0.8582038879394531, 0.8710330724716187, 0.8656313419342041]\n","0.857933828830719\n","0.008396643885408615\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1oIrYVO9R90e","executionInfo":{"status":"ok","timestamp":1690581221272,"user_tz":-330,"elapsed":30,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d6cb1dc2-bf1a-4b12-ba11-7363846b82a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.36812177300453186, 0.3374425172805786, 0.3438366651535034, 0.34379711747169495, 0.3818342387676239, 0.33773332834243774, 0.3535831570625305, 0.35498884320259094, 0.3376592993736267, 0.3462216854095459, 0.32075294852256775, 0.3197299540042877, 0.32548171281814575, 0.3312779366970062, 0.3166283071041107, 0.3982945680618286, 0.362510085105896, 0.3461480438709259, 0.33185887336730957, 0.33069565892219543, 0.3271618187427521, 0.33592697978019714, 0.3516855239868164, 0.3087334632873535, 0.32215458154678345]\n","0.34137036323547365\n","0.020309801014016103\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLYjQ7LsR0Hr","executionInfo":{"status":"ok","timestamp":1690581221273,"user_tz":-330,"elapsed":20,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8d1f0cd6-b67b-49d8-c741-524589584234"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8619882464408875, 0.876696765422821, 0.8762885928153992, 0.8788570761680603, 0.8739883899688721, 0.8756218552589417, 0.8769143223762512, 0.8827432990074158, 0.8811880350112915, 0.8803611397743225, 0.890792191028595, 0.8918205499649048, 0.8850512504577637, 0.8849161267280579, 0.8934558629989624, 0.8644263744354248, 0.8673175573348999, 0.8987680077552795, 0.8864628076553345, 0.8912202715873718, 0.889013409614563, 0.8896395564079285, 0.8798626661300659, 0.8944167494773865, 0.8863506317138672]\n","0.8823264694213867\n","0.009212118700408636\n"]}]},{"cell_type":"markdown","source":["LSTM CNN nothing nothing -\n","1. Accuracy - 0.856/0.009\n","2. Loss - 0.342/0.017\n","3. F1 score - 0.880/0.008\n","\n","LSTM CNN LSTM nothing -\n","1. Accuracy - 0.857/0.008\n","2. Loss - 0.341/0.020\n","3. F1 score - 0.882/0.009"],"metadata":{"id":"VEJ9lt3TSTvG"}},{"cell_type":"code","source":[],"metadata":{"id":"O_TKFtn6R0QZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tFv2czuriWld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KXeOJfDTiWsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1LNoTNTkiWxI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## To see how splitting is done"],"metadata":{"id":"OW3maS2si8kR"}},{"cell_type":"code","source":["X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)"],"metadata":{"id":"oa9rILP1iW3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_sentemb_remaining.shape, X_sentemb_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lib4euv0iW6o","executionInfo":{"status":"ok","timestamp":1690582092178,"user_tz":-330,"elapsed":69,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"f39d383e-586a-472f-a79f-3f862a8855f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((5924, 28), (1481, 28))"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3j_WlJ8iW_j","executionInfo":{"status":"ok","timestamp":1690582092179,"user_tz":-330,"elapsed":66,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"80b2ce50-060d-4a12-f2d9-c99800504e37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1481,)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["y_remaining.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qyjm5ySNigId","executionInfo":{"status":"ok","timestamp":1690582092181,"user_tz":-330,"elapsed":53,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"de3a4637-55a0-441a-d841-9180bae71d65"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    3712\n","0    2212\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["3712+2212"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXxNrxWOjKY2","executionInfo":{"status":"ok","timestamp":1690582174081,"user_tz":-330,"elapsed":725,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"01940f17-6ff5-443f-f8a1-4ab1259da5db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5924"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)"],"metadata":{"id":"0hDZSUy7igRI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtemp = final_rem_df.drop('label', axis=1)\n","ytemp = final_rem_df['label']\n","undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","final_rem_df = X_undersampled.copy()\n","final_rem_df['label'] = y_undersampled"],"metadata":{"id":"eL1ooGYgigUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_rem_df['label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15VifgtaigXd","executionInfo":{"status":"ok","timestamp":1690582092185,"user_tz":-330,"elapsed":39,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"973496f5-9291-4e05-88fd-9b8ac89190b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2212\n","1    2212\n","Name: label, dtype: int64"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# making different features X and y after undersampling\n","X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","y_remaining = final_rem_df['label']"],"metadata":{"id":"fprxkpUNigb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)"],"metadata":{"id":"9Z6EqM-jigg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_sentemb_train.shape, X_sentemb_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHCiJvGJigl4","executionInfo":{"status":"ok","timestamp":1690582093363,"user_tz":-330,"elapsed":7,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8dba4910-864a-42fc-ea37-d9adb12a91d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3318, 28), (1106, 28))"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["3318+1106  # this is ofc also 2212+2212"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgPQDX54i4Sp","executionInfo":{"status":"ok","timestamp":1690582097673,"user_tz":-330,"elapsed":693,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"0a55e5c1-eb09-4b4f-b632-8626e1a8ac42"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4424"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["total = 1481 + 3318 + 1106  # test + train + val\n","total"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pua42dLri5l1","executionInfo":{"status":"ok","timestamp":1690582268687,"user_tz":-330,"elapsed":470,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d24b0c32-1ca6-49cf-8316-a14d5c2dff92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5905"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["3712-2212  # rows gone in undersampling"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7oAtJeoijjaW","executionInfo":{"status":"ok","timestamp":1690582286818,"user_tz":-330,"elapsed":734,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"21940691-1639-412e-f0ed-20fa479cb890"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1500"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["5904+1500  # it should be 7405??? maybe during undersampling"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AQfIQqojnwu","executionInfo":{"status":"ok","timestamp":1690582299932,"user_tz":-330,"elapsed":855,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"74b965d9-98e1-44ce-c4ea-bef7e5d57871"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7404"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":[],"metadata":{"id":"hEPPT7jRj-Gz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nX4kgCpVX12W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PxQwFNg4X15-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m5Ml6StEX1-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QFiZgw5WX2B0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3_ax2D_MX2FS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## looking at top 5 values for emotions, set to 0 values in all positions different from positions of selected emotions, renormalize"],"metadata":{"id":"miA4wRVdX2SX"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Concatenate, GlobalMaxPooling1D, Reshape, Dropout, MaxPooling1D, MaxPooling2D, MaxPooling3D, RepeatVector, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from imblearn.under_sampling import RandomUnderSampler"],"metadata":{"id":"CsjiUIWFi6bu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaling = MinMaxScaler(feature_range=(0,1))  # (0,1) is default"],"metadata":{"id":"IrYPd-RWi6ez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to register f1 score separately in Keras (working)\n","\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.metrics import Metric\n","\n","class F1Score(Metric):\n","    def __init__(self, name='f1_score', **kwargs):\n","        super(F1Score, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n","        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n","        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","\n","        true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","        false_positives = tf.reduce_sum(tf.round(tf.clip_by_value((1 - y_true) * y_pred, 0, 1)))\n","        false_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * (1 - y_pred), 0, 1)))\n","\n","        self.true_positives.assign_add(true_positives)\n","        self.false_positives.assign_add(false_positives)\n","        self.false_negatives.assign_add(false_negatives)\n","\n","    def result(self):\n","        precision = self.true_positives / (self.true_positives + self.false_positives + K.epsilon())\n","        recall = self.true_positives / (self.true_positives + self.false_negatives + K.epsilon())\n","        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n","        return f1"],"metadata":{"id":"wClCHaZHi6iI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OOCHDxx-i6k3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jCnWB8jGi6u6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### How to extract top 5 emotion probs and set other to 0"],"metadata":{"id":"ehsyl2r1nPNZ"}},{"cell_type":"code","source":["temp_df = final_df.loc[:, 'admiration':'neutral']\n","temp_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"wld5L_v0X_Sc","executionInfo":{"status":"ok","timestamp":1690629829584,"user_tz":-330,"elapsed":563,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"f7df5852-6596-4750-f9bd-f27f60af161e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      admiration  amusement     anger  annoyance  approval    caring  \\\n","0       0.000016   0.000150  0.000705   0.001447  0.000643  0.009114   \n","1       0.000054   0.167888  0.011522   0.201494  0.000443  0.001504   \n","2       0.000101   0.000596  0.000428   0.001275  0.006295  0.013178   \n","3       0.000054   0.000639  0.043696   0.003672  0.000041  0.007463   \n","4       0.000004   0.000040  0.000176   0.000860  0.000181  0.000255   \n","...          ...        ...       ...        ...       ...       ...   \n","7400    0.000907   0.000017  0.000027   0.000027  0.000360  0.001514   \n","7401    0.000451   0.000075  0.000005   0.000022  0.000736  0.000157   \n","7402    0.000228   0.000024  0.000010   0.000034  0.000405  0.000364   \n","7403    0.000064   0.000080  0.000109   0.000155  0.012040  0.932658   \n","7404    0.003813   0.000036  0.000079   0.000095  0.039129  0.920536   \n","\n","      confusion  curiosity.1    desire  disappointment  ...      love  \\\n","0      0.048850     0.010012  0.000057        0.000440  ...  0.000210   \n","1      0.002532     0.000539  0.000486        0.053486  ...  0.003212   \n","2      0.412494     0.034596  0.000124        0.002020  ...  0.001481   \n","3      0.000333     0.000094  0.000294        0.002422  ...  0.005427   \n","4      0.001674     0.000182  0.000016        0.001535  ...  0.000145   \n","...         ...          ...       ...             ...  ...       ...   \n","7400   0.000162     0.000076  0.000035        0.000038  ...  0.000018   \n","7401   0.000070     0.000227  0.000052        0.000015  ...  0.000010   \n","7402   0.000123     0.000703  0.000040        0.000015  ...  0.000008   \n","7403   0.000593     0.001206  0.000311        0.000126  ...  0.000124   \n","7404   0.000142     0.000082  0.000070        0.000138  ...  0.000231   \n","\n","      nervousness  optimism     pride  realization    relief   remorse  \\\n","0        0.068864  0.000185  0.000041     0.000323  0.001267  0.000330   \n","1        0.013415  0.000246  0.000617     0.022932  0.000231  0.003261   \n","2        0.035853  0.000615  0.000224     0.336612  0.002241  0.001373   \n","3        0.010949  0.000076  0.000740     0.000619  0.000275  0.001561   \n","4        0.079422  0.000020  0.000017     0.000338  0.000211  0.000071   \n","...           ...       ...       ...          ...       ...       ...   \n","7400     0.000038  0.000060  0.000021     0.000021  0.000448  0.000136   \n","7401     0.000004  0.000037  0.000003     0.000018  0.000051  0.000023   \n","7402     0.000007  0.000077  0.000003     0.000009  0.000134  0.000023   \n","7403     0.016793  0.015985  0.000026     0.000285  0.005689  0.000256   \n","7404     0.000211  0.007032  0.000072     0.001102  0.007063  0.000425   \n","\n","       sadness  surprise   neutral  \n","0     0.000575  0.000205  0.000843  \n","1     0.416074  0.001216  0.003240  \n","2     0.007805  0.013074  0.041802  \n","3     0.057985  0.000332  0.000073  \n","4     0.002956  0.000103  0.000174  \n","...        ...       ...       ...  \n","7400  0.000022  0.000016  0.000068  \n","7401  0.000008  0.000011  0.001105  \n","7402  0.000008  0.000013  0.000393  \n","7403  0.000404  0.000174  0.003708  \n","7404  0.000141  0.000089  0.015839  \n","\n","[7405 rows x 28 columns]"],"text/html":["\n","\n","  <div id=\"df-6e4d8ef3-e814-49ca-a758-33aafd6cd771\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity.1</th>\n","      <th>desire</th>\n","      <th>disappointment</th>\n","      <th>...</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000016</td>\n","      <td>0.000150</td>\n","      <td>0.000705</td>\n","      <td>0.001447</td>\n","      <td>0.000643</td>\n","      <td>0.009114</td>\n","      <td>0.048850</td>\n","      <td>0.010012</td>\n","      <td>0.000057</td>\n","      <td>0.000440</td>\n","      <td>...</td>\n","      <td>0.000210</td>\n","      <td>0.068864</td>\n","      <td>0.000185</td>\n","      <td>0.000041</td>\n","      <td>0.000323</td>\n","      <td>0.001267</td>\n","      <td>0.000330</td>\n","      <td>0.000575</td>\n","      <td>0.000205</td>\n","      <td>0.000843</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000054</td>\n","      <td>0.167888</td>\n","      <td>0.011522</td>\n","      <td>0.201494</td>\n","      <td>0.000443</td>\n","      <td>0.001504</td>\n","      <td>0.002532</td>\n","      <td>0.000539</td>\n","      <td>0.000486</td>\n","      <td>0.053486</td>\n","      <td>...</td>\n","      <td>0.003212</td>\n","      <td>0.013415</td>\n","      <td>0.000246</td>\n","      <td>0.000617</td>\n","      <td>0.022932</td>\n","      <td>0.000231</td>\n","      <td>0.003261</td>\n","      <td>0.416074</td>\n","      <td>0.001216</td>\n","      <td>0.003240</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000101</td>\n","      <td>0.000596</td>\n","      <td>0.000428</td>\n","      <td>0.001275</td>\n","      <td>0.006295</td>\n","      <td>0.013178</td>\n","      <td>0.412494</td>\n","      <td>0.034596</td>\n","      <td>0.000124</td>\n","      <td>0.002020</td>\n","      <td>...</td>\n","      <td>0.001481</td>\n","      <td>0.035853</td>\n","      <td>0.000615</td>\n","      <td>0.000224</td>\n","      <td>0.336612</td>\n","      <td>0.002241</td>\n","      <td>0.001373</td>\n","      <td>0.007805</td>\n","      <td>0.013074</td>\n","      <td>0.041802</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000054</td>\n","      <td>0.000639</td>\n","      <td>0.043696</td>\n","      <td>0.003672</td>\n","      <td>0.000041</td>\n","      <td>0.007463</td>\n","      <td>0.000333</td>\n","      <td>0.000094</td>\n","      <td>0.000294</td>\n","      <td>0.002422</td>\n","      <td>...</td>\n","      <td>0.005427</td>\n","      <td>0.010949</td>\n","      <td>0.000076</td>\n","      <td>0.000740</td>\n","      <td>0.000619</td>\n","      <td>0.000275</td>\n","      <td>0.001561</td>\n","      <td>0.057985</td>\n","      <td>0.000332</td>\n","      <td>0.000073</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000004</td>\n","      <td>0.000040</td>\n","      <td>0.000176</td>\n","      <td>0.000860</td>\n","      <td>0.000181</td>\n","      <td>0.000255</td>\n","      <td>0.001674</td>\n","      <td>0.000182</td>\n","      <td>0.000016</td>\n","      <td>0.001535</td>\n","      <td>...</td>\n","      <td>0.000145</td>\n","      <td>0.079422</td>\n","      <td>0.000020</td>\n","      <td>0.000017</td>\n","      <td>0.000338</td>\n","      <td>0.000211</td>\n","      <td>0.000071</td>\n","      <td>0.002956</td>\n","      <td>0.000103</td>\n","      <td>0.000174</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>0.000907</td>\n","      <td>0.000017</td>\n","      <td>0.000027</td>\n","      <td>0.000027</td>\n","      <td>0.000360</td>\n","      <td>0.001514</td>\n","      <td>0.000162</td>\n","      <td>0.000076</td>\n","      <td>0.000035</td>\n","      <td>0.000038</td>\n","      <td>...</td>\n","      <td>0.000018</td>\n","      <td>0.000038</td>\n","      <td>0.000060</td>\n","      <td>0.000021</td>\n","      <td>0.000021</td>\n","      <td>0.000448</td>\n","      <td>0.000136</td>\n","      <td>0.000022</td>\n","      <td>0.000016</td>\n","      <td>0.000068</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>0.000451</td>\n","      <td>0.000075</td>\n","      <td>0.000005</td>\n","      <td>0.000022</td>\n","      <td>0.000736</td>\n","      <td>0.000157</td>\n","      <td>0.000070</td>\n","      <td>0.000227</td>\n","      <td>0.000052</td>\n","      <td>0.000015</td>\n","      <td>...</td>\n","      <td>0.000010</td>\n","      <td>0.000004</td>\n","      <td>0.000037</td>\n","      <td>0.000003</td>\n","      <td>0.000018</td>\n","      <td>0.000051</td>\n","      <td>0.000023</td>\n","      <td>0.000008</td>\n","      <td>0.000011</td>\n","      <td>0.001105</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>0.000228</td>\n","      <td>0.000024</td>\n","      <td>0.000010</td>\n","      <td>0.000034</td>\n","      <td>0.000405</td>\n","      <td>0.000364</td>\n","      <td>0.000123</td>\n","      <td>0.000703</td>\n","      <td>0.000040</td>\n","      <td>0.000015</td>\n","      <td>...</td>\n","      <td>0.000008</td>\n","      <td>0.000007</td>\n","      <td>0.000077</td>\n","      <td>0.000003</td>\n","      <td>0.000009</td>\n","      <td>0.000134</td>\n","      <td>0.000023</td>\n","      <td>0.000008</td>\n","      <td>0.000013</td>\n","      <td>0.000393</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>0.000064</td>\n","      <td>0.000080</td>\n","      <td>0.000109</td>\n","      <td>0.000155</td>\n","      <td>0.012040</td>\n","      <td>0.932658</td>\n","      <td>0.000593</td>\n","      <td>0.001206</td>\n","      <td>0.000311</td>\n","      <td>0.000126</td>\n","      <td>...</td>\n","      <td>0.000124</td>\n","      <td>0.016793</td>\n","      <td>0.015985</td>\n","      <td>0.000026</td>\n","      <td>0.000285</td>\n","      <td>0.005689</td>\n","      <td>0.000256</td>\n","      <td>0.000404</td>\n","      <td>0.000174</td>\n","      <td>0.003708</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>0.003813</td>\n","      <td>0.000036</td>\n","      <td>0.000079</td>\n","      <td>0.000095</td>\n","      <td>0.039129</td>\n","      <td>0.920536</td>\n","      <td>0.000142</td>\n","      <td>0.000082</td>\n","      <td>0.000070</td>\n","      <td>0.000138</td>\n","      <td>...</td>\n","      <td>0.000231</td>\n","      <td>0.000211</td>\n","      <td>0.007032</td>\n","      <td>0.000072</td>\n","      <td>0.001102</td>\n","      <td>0.007063</td>\n","      <td>0.000425</td>\n","      <td>0.000141</td>\n","      <td>0.000089</td>\n","      <td>0.015839</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e4d8ef3-e814-49ca-a758-33aafd6cd771')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-5ab08000-1814-4808-af14-859ea6ad428a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ab08000-1814-4808-af14-859ea6ad428a')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-5ab08000-1814-4808-af14-859ea6ad428a button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6e4d8ef3-e814-49ca-a758-33aafd6cd771 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6e4d8ef3-e814-49ca-a758-33aafd6cd771');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["for index, row in temp_df.head(5).iterrows():\n","  print(index, row)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJmLGCu3YUgo","executionInfo":{"status":"ok","timestamp":1690630004503,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"9ea12223-fd77-4ee6-d89e-72193228db9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 admiration        0.000016\n","amusement         0.000150\n","anger             0.000705\n","annoyance         0.001447\n","approval          0.000643\n","caring            0.009114\n","confusion         0.048850\n","curiosity.1       0.010012\n","desire            0.000057\n","disappointment    0.000440\n","disapproval       0.000339\n","disgust           0.000160\n","embarrassment     0.000265\n","excitement        0.000173\n","fear              0.854317\n","gratitude         0.000166\n","grief             0.000265\n","joy               0.000040\n","love              0.000210\n","nervousness       0.068864\n","optimism          0.000185\n","pride             0.000041\n","realization       0.000323\n","relief            0.001267\n","remorse           0.000330\n","sadness           0.000575\n","surprise          0.000205\n","neutral           0.000843\n","Name: 0, dtype: float64\n","1 admiration        0.000054\n","amusement         0.167888\n","anger             0.011522\n","annoyance         0.201494\n","approval          0.000443\n","caring            0.001504\n","confusion         0.002532\n","curiosity.1       0.000539\n","desire            0.000486\n","disappointment    0.053486\n","disapproval       0.012135\n","disgust           0.005403\n","embarrassment     0.012620\n","excitement        0.000473\n","fear              0.045313\n","gratitude         0.001156\n","grief             0.000709\n","joy               0.017801\n","love              0.003212\n","nervousness       0.013415\n","optimism          0.000246\n","pride             0.000617\n","realization       0.022932\n","relief            0.000231\n","remorse           0.003261\n","sadness           0.416074\n","surprise          0.001216\n","neutral           0.003240\n","Name: 1, dtype: float64\n","2 admiration        0.000101\n","amusement         0.000596\n","anger             0.000428\n","annoyance         0.001275\n","approval          0.006295\n","caring            0.013178\n","confusion         0.412494\n","curiosity.1       0.034596\n","desire            0.000124\n","disappointment    0.002020\n","disapproval       0.000344\n","disgust           0.000193\n","embarrassment     0.000744\n","excitement        0.000525\n","fear              0.081238\n","gratitude         0.000199\n","grief             0.004233\n","joy               0.000337\n","love              0.001481\n","nervousness       0.035853\n","optimism          0.000615\n","pride             0.000224\n","realization       0.336612\n","relief            0.002241\n","remorse           0.001373\n","sadness           0.007805\n","surprise          0.013074\n","neutral           0.041802\n","Name: 2, dtype: float64\n","3 admiration        0.000054\n","amusement         0.000639\n","anger             0.043696\n","annoyance         0.003672\n","approval          0.000041\n","caring            0.007463\n","confusion         0.000333\n","curiosity.1       0.000094\n","desire            0.000294\n","disappointment    0.002422\n","disapproval       0.000207\n","disgust           0.005001\n","embarrassment     0.000633\n","excitement        0.000092\n","fear              0.821281\n","gratitude         0.000076\n","grief             0.035779\n","joy               0.000188\n","love              0.005427\n","nervousness       0.010949\n","optimism          0.000076\n","pride             0.000740\n","realization       0.000619\n","relief            0.000275\n","remorse           0.001561\n","sadness           0.057985\n","surprise          0.000332\n","neutral           0.000073\n","Name: 3, dtype: float64\n","4 admiration        0.000004\n","amusement         0.000040\n","anger             0.000176\n","annoyance         0.000860\n","approval          0.000181\n","caring            0.000255\n","confusion         0.001674\n","curiosity.1       0.000182\n","desire            0.000016\n","disappointment    0.001535\n","disapproval       0.000209\n","disgust           0.000108\n","embarrassment     0.000199\n","excitement        0.000030\n","fear              0.910931\n","gratitude         0.000034\n","grief             0.000098\n","joy               0.000010\n","love              0.000145\n","nervousness       0.079422\n","optimism          0.000020\n","pride             0.000017\n","realization       0.000338\n","relief            0.000211\n","remorse           0.000071\n","sadness           0.002956\n","surprise          0.000103\n","neutral           0.000174\n","Name: 4, dtype: float64\n"]}]},{"cell_type":"code","source":["temp_df.iloc[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4wmXa08IZ1AK","executionInfo":{"status":"ok","timestamp":1690630303852,"user_tz":-330,"elapsed":518,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"bf8aa367-967f-42b2-c121-474aed20dd8a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["admiration        0.000016\n","amusement         0.000150\n","anger             0.000705\n","annoyance         0.001447\n","approval          0.000643\n","caring            0.009114\n","confusion         0.048850\n","curiosity.1       0.010012\n","desire            0.000057\n","disappointment    0.000440\n","disapproval       0.000339\n","disgust           0.000160\n","embarrassment     0.000265\n","excitement        0.000173\n","fear              0.854317\n","gratitude         0.000166\n","grief             0.000265\n","joy               0.000040\n","love              0.000210\n","nervousness       0.068864\n","optimism          0.000185\n","pride             0.000041\n","realization       0.000323\n","relief            0.001267\n","remorse           0.000330\n","sadness           0.000575\n","surprise          0.000205\n","neutral           0.000843\n","Name: 0, dtype: float64"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["temp_df.iloc[0].argsort()  # doesnt seem to give correct indices, like fear is the highest so it should be 27"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-_b7yNvaNWJ","executionInfo":{"status":"ok","timestamp":1690630305395,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"ea8d07ae-f0b0-48da-e64a-51a37b8e9f50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["admiration         0\n","amusement         17\n","anger             21\n","annoyance          8\n","approval           1\n","caring            11\n","confusion         15\n","curiosity.1       13\n","desire            20\n","disappointment    26\n","disapproval       18\n","disgust           12\n","embarrassment     16\n","excitement        22\n","fear              24\n","gratitude         10\n","grief              9\n","joy               25\n","love               4\n","nervousness        2\n","optimism          27\n","pride             23\n","realization        3\n","relief             5\n","remorse            7\n","sadness            6\n","surprise          19\n","neutral           14\n","Name: 0, dtype: int64"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["[1,2,3,4,5,6,7,8,9,3,5,6,78][-5:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VE7w1GV6Z2KH","executionInfo":{"status":"ok","timestamp":1690630131952,"user_tz":-330,"elapsed":7,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"685ad9c0-d69f-49aa-c3c3-f5319a2c0c9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[9, 3, 5, 6, 78]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["temp_df.iloc[0].argsort()[-5:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-m9NKhseZ93P","executionInfo":{"status":"ok","timestamp":1690630088819,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"c28d1b58-510a-4028-ffde-3ecfcb73bf58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["relief       5\n","remorse      7\n","sadness      6\n","surprise    19\n","neutral     14\n","Name: 0, dtype: int64"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["temp_df.iloc[0].argsort()[-5:][::-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpOidssBZgAG","executionInfo":{"status":"ok","timestamp":1690630055725,"user_tz":-330,"elapsed":435,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"48e2071a-8f13-4503-ca70-b8141ce46b53"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["neutral     14\n","surprise    19\n","sadness      6\n","remorse      7\n","relief       5\n","Name: 0, dtype: int64"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["temp_df.iloc[0].nlargest(5)  # this works"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-gZYInwb0zW","executionInfo":{"status":"ok","timestamp":1690630588463,"user_tz":-330,"elapsed":468,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"80d8b8a4-aedd-4eba-a0e5-78f66b2b098f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fear           0.854317\n","nervousness    0.068864\n","confusion      0.048850\n","curiosity.1    0.010012\n","caring         0.009114\n","Name: 0, dtype: float64"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["temp_df.iloc[0].nlargest(5).index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNuuCks9cFXA","executionInfo":{"status":"ok","timestamp":1690630647883,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"21d653b0-d4c4-469c-9b71-067c16b00148"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['fear', 'nervousness', 'confusion', 'curiosity.1', 'caring'], dtype='object')"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)"],"metadata":{"id":"5GATIUF7bQ9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prob_cols = temp_df.columns\n","\n","# Replace the probabilities with 0 for each row except the largest 5\n","new_data = []\n","for index, row in temp_df.iterrows():\n","    top_5_indices = row.nlargest(5).index  # Get indices of largest 5 probabilities\n","    row_data = {col: prob if col in top_5_indices else 0 for col, prob in zip(prob_cols, row)}\n","    new_data.append(row_data)\n","\n","# Create the new DataFrame with the modified probabilities\n","new_df = pd.DataFrame(new_data)"],"metadata":{"id":"4WMG_9FwaDZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"k_T8Rn_hbUc-","executionInfo":{"status":"ok","timestamp":1690630688428,"user_tz":-330,"elapsed":51,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"33b3a45c-0bd4-413e-8612-2d7b1e8ac7cd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      admiration  amusement     anger  annoyance  approval    caring  \\\n","0       0.000000   0.000000  0.000000   0.000000  0.000000  0.009114   \n","1       0.000000   0.167888  0.000000   0.201494  0.000000  0.000000   \n","2       0.000000   0.000000  0.000000   0.000000  0.000000  0.000000   \n","3       0.000000   0.000000  0.043696   0.000000  0.000000  0.000000   \n","4       0.000000   0.000000  0.000000   0.000000  0.000000  0.000000   \n","...          ...        ...       ...        ...       ...       ...   \n","7400    0.000907   0.000000  0.000000   0.000000  0.000360  0.001514   \n","7401    0.000451   0.000000  0.000000   0.000000  0.000736  0.000000   \n","7402    0.000000   0.000000  0.000000   0.000000  0.000405  0.000364   \n","7403    0.000000   0.000000  0.000000   0.000000  0.012040  0.932658   \n","7404    0.000000   0.000000  0.000000   0.000000  0.039129  0.920536   \n","\n","      confusion  curiosity.1  desire  disappointment  disapproval  disgust  \\\n","0      0.048850     0.010012     0.0        0.000000          0.0      0.0   \n","1      0.000000     0.000000     0.0        0.053486          0.0      0.0   \n","2      0.412494     0.000000     0.0        0.000000          0.0      0.0   \n","3      0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","4      0.001674     0.000000     0.0        0.001535          0.0      0.0   \n","...         ...          ...     ...             ...          ...      ...   \n","7400   0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","7401   0.000000     0.000227     0.0        0.000000          0.0      0.0   \n","7402   0.000000     0.000703     0.0        0.000000          0.0      0.0   \n","7403   0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","7404   0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","\n","      embarrassment  excitement      fear  gratitude     grief  joy  love  \\\n","0               0.0         0.0  0.854317   0.000000  0.000000  0.0   0.0   \n","1               0.0         0.0  0.045313   0.000000  0.000000  0.0   0.0   \n","2               0.0         0.0  0.081238   0.000000  0.000000  0.0   0.0   \n","3               0.0         0.0  0.821281   0.000000  0.035779  0.0   0.0   \n","4               0.0         0.0  0.910931   0.000000  0.000000  0.0   0.0   \n","...             ...         ...       ...        ...       ...  ...   ...   \n","7400            0.0         0.0  0.000000   0.995492  0.000000  0.0   0.0   \n","7401            0.0         0.0  0.000000   0.996667  0.000000  0.0   0.0   \n","7402            0.0         0.0  0.000000   0.997167  0.000000  0.0   0.0   \n","7403            0.0         0.0  0.007893   0.000000  0.000000  0.0   0.0   \n","7404            0.0         0.0  0.000000   0.000000  0.000000  0.0   0.0   \n","\n","      nervousness  optimism  pride  realization    relief  remorse   sadness  \\\n","0        0.068864  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","1        0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.416074   \n","2        0.035853  0.000000    0.0     0.336612  0.000000      0.0  0.000000   \n","3        0.010949  0.000000    0.0     0.000000  0.000000      0.0  0.057985   \n","4        0.079422  0.000000    0.0     0.000000  0.000000      0.0  0.002956   \n","...           ...       ...    ...          ...       ...      ...       ...   \n","7400     0.000000  0.000000    0.0     0.000000  0.000448      0.0  0.000000   \n","7401     0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","7402     0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","7403     0.016793  0.015985    0.0     0.000000  0.000000      0.0  0.000000   \n","7404     0.000000  0.007032    0.0     0.000000  0.007063      0.0  0.000000   \n","\n","      surprise   neutral  \n","0          0.0  0.000000  \n","1          0.0  0.000000  \n","2          0.0  0.041802  \n","3          0.0  0.000000  \n","4          0.0  0.000000  \n","...        ...       ...  \n","7400       0.0  0.000000  \n","7401       0.0  0.001105  \n","7402       0.0  0.000393  \n","7403       0.0  0.000000  \n","7404       0.0  0.015839  \n","\n","[7405 rows x 28 columns]"],"text/html":["\n","\n","  <div id=\"df-8f6eadff-57ee-4b39-8d30-86591664c559\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity.1</th>\n","      <th>desire</th>\n","      <th>disappointment</th>\n","      <th>disapproval</th>\n","      <th>disgust</th>\n","      <th>embarrassment</th>\n","      <th>excitement</th>\n","      <th>fear</th>\n","      <th>gratitude</th>\n","      <th>grief</th>\n","      <th>joy</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.009114</td>\n","      <td>0.048850</td>\n","      <td>0.010012</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.854317</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.068864</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.167888</td>\n","      <td>0.000000</td>\n","      <td>0.201494</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.053486</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.045313</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.416074</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.412494</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.081238</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.035853</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.336612</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.041802</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.043696</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.821281</td>\n","      <td>0.000000</td>\n","      <td>0.035779</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.010949</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.057985</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.001674</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.001535</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.910931</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.079422</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.002956</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>0.000907</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000360</td>\n","      <td>0.001514</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.995492</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000448</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>0.000451</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000736</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000227</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.996667</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.001105</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000405</td>\n","      <td>0.000364</td>\n","      <td>0.000000</td>\n","      <td>0.000703</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.997167</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000393</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.012040</td>\n","      <td>0.932658</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.007893</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.016793</td>\n","      <td>0.015985</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.039129</td>\n","      <td>0.920536</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007032</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007063</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.015839</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f6eadff-57ee-4b39-8d30-86591664c559')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-dcbdc315-93c4-4948-b5d6-ffa71ffe0f1a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcbdc315-93c4-4948-b5d6-ffa71ffe0f1a')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-dcbdc315-93c4-4948-b5d6-ffa71ffe0f1a button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8f6eadff-57ee-4b39-8d30-86591664c559 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8f6eadff-57ee-4b39-8d30-86591664c559');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["normalized_new_df = pd.DataFrame(scaling.fit_transform(new_df.values.reshape(-1, 1)).reshape(*new_df.shape), columns=prob_cols)\n","normalized_new_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"qII62Vd-jZ5r","executionInfo":{"status":"ok","timestamp":1690633771318,"user_tz":-330,"elapsed":13,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"a2741cef-3d3b-49c6-ed4c-0418aba680cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      admiration  amusement     anger  annoyance  approval    caring  \\\n","0       0.000000   0.000000  0.000000   0.000000  0.000000  0.009124   \n","1       0.000000   0.168074  0.000000   0.201718  0.000000  0.000000   \n","2       0.000000   0.000000  0.000000   0.000000  0.000000  0.000000   \n","3       0.000000   0.000000  0.043745   0.000000  0.000000  0.000000   \n","4       0.000000   0.000000  0.000000   0.000000  0.000000  0.000000   \n","...          ...        ...       ...        ...       ...       ...   \n","7400    0.000908   0.000000  0.000000   0.000000  0.000360  0.001515   \n","7401    0.000452   0.000000  0.000000   0.000000  0.000737  0.000000   \n","7402    0.000000   0.000000  0.000000   0.000000  0.000406  0.000365   \n","7403    0.000000   0.000000  0.000000   0.000000  0.012053  0.933693   \n","7404    0.000000   0.000000  0.000000   0.000000  0.039173  0.921558   \n","\n","      confusion  curiosity.1  desire  disappointment  disapproval  disgust  \\\n","0      0.048904     0.010023     0.0        0.000000          0.0      0.0   \n","1      0.000000     0.000000     0.0        0.053545          0.0      0.0   \n","2      0.412951     0.000000     0.0        0.000000          0.0      0.0   \n","3      0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","4      0.001675     0.000000     0.0        0.001537          0.0      0.0   \n","...         ...          ...     ...             ...          ...      ...   \n","7400   0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","7401   0.000000     0.000227     0.0        0.000000          0.0      0.0   \n","7402   0.000000     0.000704     0.0        0.000000          0.0      0.0   \n","7403   0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","7404   0.000000     0.000000     0.0        0.000000          0.0      0.0   \n","\n","      embarrassment  excitement      fear  gratitude     grief  joy  love  \\\n","0               0.0         0.0  0.855265   0.000000  0.000000  0.0   0.0   \n","1               0.0         0.0  0.045363   0.000000  0.000000  0.0   0.0   \n","2               0.0         0.0  0.081328   0.000000  0.000000  0.0   0.0   \n","3               0.0         0.0  0.822192   0.000000  0.035818  0.0   0.0   \n","4               0.0         0.0  0.911942   0.000000  0.000000  0.0   0.0   \n","...             ...         ...       ...        ...       ...  ...   ...   \n","7400            0.0         0.0  0.000000   0.996597  0.000000  0.0   0.0   \n","7401            0.0         0.0  0.000000   0.997774  0.000000  0.0   0.0   \n","7402            0.0         0.0  0.000000   0.998274  0.000000  0.0   0.0   \n","7403            0.0         0.0  0.007902   0.000000  0.000000  0.0   0.0   \n","7404            0.0         0.0  0.000000   0.000000  0.000000  0.0   0.0   \n","\n","      nervousness  optimism  pride  realization    relief  remorse   sadness  \\\n","0        0.068940  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","1        0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.416536   \n","2        0.035893  0.000000    0.0     0.336985  0.000000      0.0  0.000000   \n","3        0.010961  0.000000    0.0     0.000000  0.000000      0.0  0.058049   \n","4        0.079510  0.000000    0.0     0.000000  0.000000      0.0  0.002959   \n","...           ...       ...    ...          ...       ...      ...       ...   \n","7400     0.000000  0.000000    0.0     0.000000  0.000448      0.0  0.000000   \n","7401     0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","7402     0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","7403     0.016812  0.016002    0.0     0.000000  0.000000      0.0  0.000000   \n","7404     0.000000  0.007040    0.0     0.000000  0.007071      0.0  0.000000   \n","\n","      surprise   neutral  \n","0          0.0  0.000000  \n","1          0.0  0.000000  \n","2          0.0  0.041849  \n","3          0.0  0.000000  \n","4          0.0  0.000000  \n","...        ...       ...  \n","7400       0.0  0.000000  \n","7401       0.0  0.001106  \n","7402       0.0  0.000394  \n","7403       0.0  0.000000  \n","7404       0.0  0.015857  \n","\n","[7405 rows x 28 columns]"],"text/html":["\n","\n","  <div id=\"df-02adc1f9-0ae0-49e0-94dc-fa1e37820143\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity.1</th>\n","      <th>desire</th>\n","      <th>disappointment</th>\n","      <th>disapproval</th>\n","      <th>disgust</th>\n","      <th>embarrassment</th>\n","      <th>excitement</th>\n","      <th>fear</th>\n","      <th>gratitude</th>\n","      <th>grief</th>\n","      <th>joy</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.009124</td>\n","      <td>0.048904</td>\n","      <td>0.010023</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.855265</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.068940</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.168074</td>\n","      <td>0.000000</td>\n","      <td>0.201718</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.053545</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.045363</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.416536</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.412951</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.081328</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.035893</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.336985</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.041849</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.043745</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.822192</td>\n","      <td>0.000000</td>\n","      <td>0.035818</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.010961</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.058049</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.001675</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.001537</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.911942</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.079510</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.002959</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>0.000908</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000360</td>\n","      <td>0.001515</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.996597</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000448</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>0.000452</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000737</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000227</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.997774</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.001106</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000406</td>\n","      <td>0.000365</td>\n","      <td>0.000000</td>\n","      <td>0.000704</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.998274</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000394</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.012053</td>\n","      <td>0.933693</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.007902</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.016812</td>\n","      <td>0.016002</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.039173</td>\n","      <td>0.921558</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007040</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007071</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.015857</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02adc1f9-0ae0-49e0-94dc-fa1e37820143')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-cabac936-9636-46d7-b100-f5384702c0d7\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cabac936-9636-46d7-b100-f5384702c0d7')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-cabac936-9636-46d7-b100-f5384702c0d7 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-02adc1f9-0ae0-49e0-94dc-fa1e37820143 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-02adc1f9-0ae0-49e0-94dc-fa1e37820143');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":[],"metadata":{"id":"qOPHpj-Hiv7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df.loc[:, 'admiration':'neutral'] = normalized_new_df\n","final_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"NiZ-dX7Div_m","executionInfo":{"status":"ok","timestamp":1690633842369,"user_tz":-330,"elapsed":727,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"17ec9235-bf20-4705-b862-df50159f32c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sentemb1  sentemb2  sentemb3  sentemb4  sentemb5  sentemb6  sentemb7  \\\n","0     10.641940  5.043077  5.682403  3.366873  3.939331  3.919311  6.919125   \n","1     11.312859  5.364349  4.413650  3.407885  5.023540  4.247440  6.088093   \n","2     10.531799  4.894456  5.387705  3.325495  4.051551  4.119681  6.826452   \n","3     11.310531  5.330986  4.330414  3.431627  5.095810  4.284573  6.036744   \n","4     10.990587  5.196148  4.968183  2.576940  4.233728  4.119200  6.174312   \n","...         ...       ...       ...       ...       ...       ...       ...   \n","7400  10.507652  4.949894  4.014671  2.117899  4.715438  4.335719  5.412321   \n","7401  10.698858  4.546772  3.540320  3.286137  5.087534  5.156605  5.941041   \n","7402  10.346373  4.424756  3.581532  3.153925  5.020442  4.948900  5.897359   \n","7403  10.454275  4.564041  3.603564  3.078566  4.991286  4.955261  5.872911   \n","7404  11.222271  5.146849  4.005402  3.371954  4.682106  5.396935  6.367755   \n","\n","      sentemb8  sentemb9  sentemb10  sentemb11  sentemb12  sentemb13  \\\n","0     1.789982  1.404625   7.134058   4.420479   7.684002   2.286182   \n","1     1.780003  1.160577   6.596563   4.065295   7.592488   2.172122   \n","2     1.776312  1.471099   6.926485   4.441208   7.688570   2.225875   \n","3     1.780587  1.134592   6.539715   4.022463   7.585912   2.188976   \n","4     1.775400  1.475707   6.684998   4.217587   7.664200   1.999518   \n","...        ...       ...        ...        ...        ...        ...   \n","7400  1.769552  1.720969   6.056923   4.317115   7.701209   1.577826   \n","7401  1.688843  1.656772   5.678233   4.209131   7.742318   2.049652   \n","7402  1.686590  1.781551   5.741636   4.264494   7.766108   1.989159   \n","7403  1.702558  1.761993   5.793803   4.275763   7.760498   1.955059   \n","7404  1.753800  1.507380   6.102673   4.176505   7.723337   2.232829   \n","\n","      sentemb14  sentemb15  sentemb16  sentemb17  sentemb18  sentemb19  \\\n","0      2.857458   2.669546   4.379669   3.859759   5.403258   6.754729   \n","1      3.172726   2.680326   4.280528   3.796072   5.654799   7.253571   \n","2      2.871117   2.653050   4.511485   3.862097   5.338437   6.786397   \n","3      3.194212   2.678760   4.270244   3.784710   5.666665   7.298054   \n","4      2.998645   2.709311   4.489384   3.847829   5.320369   7.061465   \n","...         ...        ...        ...        ...        ...        ...   \n","7400   3.124527   2.587120   4.895798   3.797136   5.180048   7.375135   \n","7401   3.168222   2.397176   5.081987   3.717591   5.339126   7.371179   \n","7402   3.134499   2.402262   5.107181   3.737733   5.321551   7.339849   \n","7403   3.136624   2.410064   5.094350   3.744368   5.321893   7.350019   \n","7404   3.068840   2.387916   4.919335   3.758407   5.360199   7.279993   \n","\n","      sentemb20  sentemb21  sentemb22  sentemb23  sentemb24  sentemb25  \\\n","0      4.308768   2.225483   3.196717   5.969540   5.937778   5.176552   \n","1      4.070558   2.360492   3.181391   6.144005   5.662973   5.073269   \n","2      4.259876   2.223957   3.178261   5.996410   5.876356   5.186982   \n","3      4.057921   2.369854   3.181388   6.149067   5.650796   5.068528   \n","4      4.159153   2.327722   3.183620   6.181925   5.809379   5.113574   \n","...         ...        ...        ...        ...        ...        ...   \n","7400   3.952120   2.305240   3.210462   6.340523   5.595186   5.102685   \n","7401   3.950364   2.242772   3.236705   6.118145   5.502766   5.187657   \n","7402   3.958114   2.226620   3.229278   6.177763   5.498463   5.200507   \n","7403   3.961870   2.231675   3.226660   6.186529   5.515781   5.197260   \n","7404   4.084011   2.253345   3.232021   6.046115   5.681019   5.217414   \n","\n","      sentemb26  sentemb27  sentemb28   WC  Analytic  Clout  Authentic   Tone  \\\n","0      4.905828   6.870799   4.538727  607     55.22  35.35      48.82   1.00   \n","1      4.922340   6.847518   4.559060  401     54.08   1.31      99.00   8.57   \n","2      4.896872   6.836080   4.530069  446     25.83  93.36      75.79   1.00   \n","3      4.935538   6.840829   4.564339  525     30.63   2.97      96.06   6.86   \n","4      4.873700   6.771983   4.528791  323     21.57   1.00      99.00   1.12   \n","...         ...        ...        ...  ...       ...    ...        ...    ...   \n","7400   4.922517   6.645929   4.465776  237      5.24  20.52      53.23  15.51   \n","7401   5.083427   6.556334   4.512558   20     39.70   1.00      28.56  99.00   \n","7402   5.040123   6.552696   4.490976   19     36.67   3.34      70.28  99.00   \n","7403   5.032067   6.564847   4.491873   38     36.67  90.88      33.61  92.27   \n","7404   5.079988   6.634119   4.559704  186     26.78  52.89      86.48   2.39   \n","\n","        WPS  BigWords    Dic  Linguistic  function  pronoun  ppron      i  \\\n","0     26.39     25.86  93.74       68.86     57.50    12.85   6.26   3.13   \n","1     13.83     14.21  90.02       74.81     61.60    17.96  12.72  11.47   \n","2     12.74     15.47  95.74       75.34     59.64    17.26  13.00   2.91   \n","3     15.44     12.38  90.86       73.33     56.57    20.38  16.19  12.95   \n","4     13.46     15.48  93.50       75.85     60.99    20.12  14.86  13.00   \n","...     ...       ...    ...         ...       ...      ...    ...    ...   \n","7400  16.93     18.57  93.25       81.86     62.87    23.63  14.35   8.44   \n","7401  20.00     25.00  95.00       65.00     40.00    15.00  15.00  15.00   \n","7402  19.00     21.05  94.74       73.68     47.37    21.05  15.79  15.79   \n","7403  19.00     21.05  97.37       63.16     52.63    15.79  10.53   2.63   \n","7404  23.25     15.59  98.39       79.03     61.83    17.20   8.60   3.23   \n","\n","        we   you  shehe  they  ipron    det  article  number   prep  auxverb  \\\n","0     0.16  2.31   0.00  0.16   6.59  15.16     7.74    1.15  14.66     9.88   \n","1     0.00  0.25   0.00  0.75   5.24  15.46     7.98    1.25  15.71     6.98   \n","2     0.00  9.87   0.00  0.22   4.26  13.90     6.28    1.35  13.23     9.64   \n","3     0.00  1.14   1.90  0.19   4.19  13.52     5.71    0.76  14.67    10.29   \n","4     0.00  1.55   0.00  0.00   5.26  12.69     4.64    0.62  15.79    10.53   \n","...    ...   ...    ...   ...    ...    ...      ...     ...    ...      ...   \n","7400  0.00  3.80   0.42  1.27   9.28  14.35     2.95    0.84  10.97    13.08   \n","7401  0.00  0.00   0.00  0.00   0.00   5.00     0.00    0.00  10.00    10.00   \n","7402  0.00  0.00   0.00  0.00   5.26   5.26     0.00    0.00  15.79     5.26   \n","7403  2.63  2.63   0.00  2.63   5.26  13.16     2.63    2.63  15.79     7.89   \n","7404  0.00  4.84   0.00  0.54   8.60  10.75     4.30    2.69  15.05     9.68   \n","\n","      adverb   conj  negate   verb    adj  quantity  Drives  affiliation  \\\n","0       7.08   6.59    0.82  15.82   5.11      5.11    3.46         0.49   \n","1       5.74   6.23    2.00  15.21   5.24      3.24    1.50         0.25   \n","2       6.95   9.42    1.35  18.39   6.73      4.71    2.69         0.22   \n","3       4.19   7.62    1.14  23.24   3.24      3.05    1.90         0.95   \n","4       6.81   6.81    3.10  20.43   6.50      1.86    2.79         0.31   \n","...      ...    ...     ...    ...    ...       ...     ...          ...   \n","7400    7.17   6.33    1.69  25.74   8.02      2.95    2.53         0.84   \n","7401    0.00   5.00    0.00  30.00   5.00      0.00   10.00         0.00   \n","7402    5.26   5.26    0.00  21.05  10.53      0.00   10.53         0.00   \n","7403    5.26  10.53    0.00  15.79   5.26      5.26    2.63         2.63   \n","7404    5.91   8.60    2.69  20.97   5.91      5.38    1.08         0.00   \n","\n","      achieve  power  Cognition  allnone  cogproc  insight  cause  discrep  \\\n","0        1.15   1.81      18.62     0.33    18.29     2.80   3.13     2.47   \n","1        0.75   0.50      11.97     2.74     9.23     1.50   2.00     2.49   \n","2        1.57   0.90      19.51     1.12    18.39     3.36   2.24     2.02   \n","3        0.57   0.38       7.62     1.33     6.29     1.33   0.95     1.71   \n","4        1.24   1.24      14.55     1.24    13.31     3.72   0.62     3.72   \n","...       ...    ...        ...      ...      ...      ...    ...      ...   \n","7400     0.84   0.84      17.30     1.27    16.03     2.95   2.11     3.80   \n","7401    10.00   0.00      25.00     0.00    25.00     5.00   5.00    10.00   \n","7402    10.53   0.00      15.79     0.00    15.79     5.26   0.00     5.26   \n","7403     0.00   0.00      13.16     0.00    13.16     5.26   2.63     2.63   \n","7404     1.08   0.00      13.98     1.61    12.37     5.91   1.08     0.54   \n","\n","      tentat  certitude  differ  memory  Affect  tone_pos  tone_neg  emotion  \\\n","0       4.45       0.99    4.45    0.00   10.71      0.99      9.56     7.74   \n","1       1.00       0.25    2.49    0.00    4.24      1.50      2.74     1.50   \n","2       4.93       0.67    4.04    0.00    9.64      1.35      8.07     6.28   \n","3       0.57       0.00    1.90    0.19    4.00      1.14      2.67     1.71   \n","4       3.10       0.00    4.33    0.00    6.50      1.55      4.95     2.79   \n","...      ...        ...     ...     ...     ...       ...       ...      ...   \n","7400    2.95       0.42    3.38    0.00    6.75      2.95      3.38     3.80   \n","7401    0.00       0.00    5.00    0.00   15.00     15.00      0.00     0.00   \n","7402    0.00       5.26    0.00    0.00   10.53     10.53      0.00     0.00   \n","7403    2.63       0.00    2.63    0.00   15.79     10.53      5.26     7.89   \n","7404    2.15       0.54    2.15    0.00   10.22      3.76      6.45     6.45   \n","\n","      emo_pos  emo_neg  emo_anx  emo_anger  emo_sad  swear  Social  socbehav  \\\n","0        0.16     7.58     6.92       0.16     0.16   0.16    8.73      3.46   \n","1        0.75     0.75     0.00       0.00     0.00   0.00    4.74      2.24   \n","2        0.45     5.61     4.71       0.00     0.45   0.00   11.88      1.57   \n","3        0.57     1.14     0.38       0.00     0.38   0.19    8.19      3.62   \n","4        0.31     2.48     1.86       0.00     0.00   0.00    3.41      0.93   \n","...       ...      ...      ...        ...      ...    ...     ...       ...   \n","7400     0.42     2.95     1.69       0.00     0.00   0.00   11.81      5.91   \n","7401     0.00     0.00     0.00       0.00     0.00   0.00   20.00     15.00   \n","7402     0.00     0.00     0.00       0.00     0.00   0.00   21.05     15.79   \n","7403     2.63     5.26     2.63       0.00     2.63   0.00   15.79      5.26   \n","7404     1.08     5.38     3.76       0.00     0.00   0.00    6.99      1.61   \n","\n","      prosocial  polite  conflict  moral   comm  socrefs  family  friend  \\\n","0          0.33    0.33      1.32   0.00   1.15     5.27    0.33     0.0   \n","1          0.25    0.00      0.25   0.00   1.00     2.49    0.00     0.0   \n","2          0.22    0.00      0.67   0.00   0.45    10.09    0.00     0.0   \n","3          0.19    0.19      0.19   0.00   2.48     4.19    0.95     0.0   \n","4          0.31    0.00      0.00   0.00   0.31     2.48    0.31     0.0   \n","...         ...     ...       ...    ...    ...      ...     ...     ...   \n","7400       1.27    0.84      0.00   0.42   3.38     5.91    0.00     0.0   \n","7401       5.00   10.00      0.00   0.00  15.00     5.00    0.00     0.0   \n","7402       5.26   10.53      0.00   0.00  15.79     5.26    0.00     0.0   \n","7403       0.00    2.63      0.00   0.00   2.63    10.53    0.00     0.0   \n","7404       0.54    0.54      0.00   0.00   0.54     5.38    0.00     0.0   \n","\n","      female  male  Culture  politic  ethnicity  tech  Lifestyle  leisure  \\\n","0       0.00  0.00     0.00      0.0        0.0  0.00       2.31     0.00   \n","1       0.25  0.00     0.00      0.0        0.0  0.00       3.49     1.00   \n","2       0.00  0.00     0.00      0.0        0.0  0.00       1.79     0.45   \n","3       2.67  0.19     0.76      0.0        0.0  0.76       2.67     0.19   \n","4       0.62  0.00     0.00      0.0        0.0  0.00       1.55     0.31   \n","...      ...   ...      ...      ...        ...   ...        ...      ...   \n","7400    0.42  0.00     0.00      0.0        0.0  0.00       0.84     0.00   \n","7401    0.00  0.00     0.00      0.0        0.0  0.00       5.00     0.00   \n","7402    0.00  0.00     0.00      0.0        0.0  0.00       5.26     0.00   \n","7403    0.00  0.00     0.00      0.0        0.0  0.00       2.63     0.00   \n","7404    0.00  0.00     0.00      0.0        0.0  0.00       3.23     1.61   \n","\n","      home  work  money  relig  Physical  health  illness  wellness  mental  \\\n","0     0.00  1.98   0.33   0.00      9.88    8.07     0.82      0.00    5.11   \n","1     0.25  2.24   0.00   0.00      3.99    0.50     0.00      0.50    0.00   \n","2     0.45  0.90   0.22   0.00      7.40    2.47     0.00      0.00    1.35   \n","3     0.76  0.00   0.00   1.71      5.33    1.14     0.38      0.00    0.00   \n","4     0.31  0.31   0.62   0.00      7.43    2.17     0.93      0.00    0.62   \n","...    ...   ...    ...    ...       ...     ...      ...       ...     ...   \n","7400  0.00  0.84   0.00   0.00      4.22    4.22     2.53      0.00    0.00   \n","7401  0.00  5.00   0.00   0.00     15.00   15.00     0.00      0.00   10.00   \n","7402  0.00  5.26   0.00   0.00     10.53   10.53     0.00      0.00    5.26   \n","7403  0.00  2.63   0.00   0.00     13.16   13.16     0.00      0.00    5.26   \n","7404  0.00  1.08   0.00   0.54      3.76    1.61     0.00      1.08    0.00   \n","\n","      substances  sexual  food  death  need  want  acquire  lack  fulfill  \\\n","0            0.0     0.0  0.00   0.16  0.33  0.33     0.16  0.33     0.16   \n","1            0.0     0.0  0.00   0.00  0.00  0.25     0.50  0.00     0.00   \n","2            0.0     0.0  0.67   0.45  0.00  0.00     1.35  0.00     0.22   \n","3            0.0     0.0  0.38   1.33  0.38  0.00     0.76  0.00     0.19   \n","4            0.0     0.0  0.62   0.00  0.00  0.00     1.24  0.00     0.00   \n","...          ...     ...   ...    ...   ...   ...      ...   ...      ...   \n","7400         0.0     0.0  0.00   0.00  0.42  0.42     0.84  0.00     0.42   \n","7401         0.0     0.0  0.00   0.00  5.00  0.00     0.00  0.00     0.00   \n","7402         0.0     0.0  0.00   0.00  0.00  0.00     0.00  0.00     0.00   \n","7403         0.0     0.0  0.00   0.00  0.00  0.00     0.00  0.00     0.00   \n","7404         0.0     0.0  0.00   0.00  1.08  0.00     2.15  0.00     0.00   \n","\n","      fatigue  reward  risk  curiosity  allure  Perception  attention  motion  \\\n","0        0.49    0.00  0.49       0.16    6.26        7.08       0.33    0.66   \n","1        0.00    0.00  0.25       0.00    3.49       14.71       0.25    3.24   \n","2        0.22    0.00  0.90       0.22    6.28       11.43       0.90    1.79   \n","3        0.00    0.00  0.57       0.00    7.81       13.33       0.38    2.86   \n","4        0.00    0.00  0.31       0.00    8.98       17.65       0.62    2.17   \n","...       ...     ...   ...        ...     ...         ...        ...     ...   \n","7400     0.00    0.42  0.42       0.00    5.49        7.17       0.42    0.84   \n","7401     0.00    0.00  0.00       0.00   10.00        0.00       0.00    0.00   \n","7402     0.00    0.00  0.00       0.00   10.53        0.00       0.00    0.00   \n","7403     0.00    0.00  2.63       0.00   13.16        5.26       0.00    2.63   \n","7404     0.54    0.00  0.00       0.00   12.37       10.75       0.00    2.69   \n","\n","      space  visual  auditory  feeling  time  focuspast  focuspresent  \\\n","0      4.78    0.16      0.00     1.65  4.61       1.81          8.07   \n","1      8.98    0.75      0.75     1.50  7.98      10.22          0.75   \n","2      5.16    0.22      0.00     3.81  6.73       1.79          9.87   \n","3      6.86    1.71      1.52     0.95  5.52       0.95         10.29   \n","4     10.84    2.17      0.93     2.17  3.41       0.62         11.76   \n","...     ...     ...       ...      ...   ...        ...           ...   \n","7400   3.80    0.42      0.42     0.84  3.38       8.02          7.17   \n","7401   0.00    0.00      0.00     0.00  0.00       0.00          5.00   \n","7402   0.00    0.00      0.00     0.00  5.26       0.00         10.53   \n","7403   5.26    0.00      0.00     0.00  0.00       0.00          5.26   \n","7404   5.91    0.00      0.54     1.61  4.84       1.61          5.91   \n","\n","      focusfuture  Conversation  netspeak  assent  nonflu  filler  AllPunc  \\\n","0            1.48          0.00      0.00    0.00     0.0     0.0    19.93   \n","1            0.75          0.00      0.00    0.00     0.0     0.0    13.47   \n","2            0.45          0.22      0.00    0.22     0.0     0.0    16.59   \n","3            3.81          2.67      0.19    0.57     1.9     0.0    21.52   \n","4            1.24          0.00      0.00    0.00     0.0     0.0    18.27   \n","...           ...           ...       ...     ...     ...     ...      ...   \n","7400         1.27          0.00      0.00    0.00     0.0     0.0    10.13   \n","7401         5.00          0.00      0.00    0.00     0.0     0.0     0.00   \n","7402         5.26          0.00      0.00    0.00     0.0     0.0     0.00   \n","7403         0.00          2.63      0.00    2.63     0.0     0.0     2.63   \n","7404         2.15          0.00      0.00    0.00     0.0     0.0    12.90   \n","\n","      Period  Comma  QMark  Exclam  Apostro  OtherP  Emoji  admiration  \\\n","0       4.61   9.88   0.16    0.16     2.80    2.31   0.33    0.000000   \n","1       6.98   4.74   0.25    0.00     1.50    0.00   0.00    0.000000   \n","2       8.07   2.69   0.22    0.00     4.71    0.90   0.00    0.000000   \n","3       5.33   4.95   1.14    0.00     6.67    3.43   0.00    0.000000   \n","4       7.12   4.33   0.93    0.00     5.26    0.62   0.00    0.000000   \n","...      ...    ...    ...     ...      ...     ...    ...         ...   \n","7400    5.49   1.69   0.00    0.00     2.53    0.42   0.00    0.000908   \n","7401    0.00   0.00   0.00    0.00     0.00    0.00   0.00    0.000452   \n","7402    0.00   0.00   0.00    0.00     0.00    0.00   0.00    0.000000   \n","7403    2.63   0.00   0.00    0.00     0.00    0.00   2.63    0.000000   \n","7404    3.76   5.38   0.00    0.54     1.08    2.15   0.00    0.000000   \n","\n","      amusement     anger  annoyance  approval    caring  confusion  \\\n","0      0.000000  0.000000   0.000000  0.000000  0.009124   0.048904   \n","1      0.168074  0.000000   0.201718  0.000000  0.000000   0.000000   \n","2      0.000000  0.000000   0.000000  0.000000  0.000000   0.412951   \n","3      0.000000  0.043745   0.000000  0.000000  0.000000   0.000000   \n","4      0.000000  0.000000   0.000000  0.000000  0.000000   0.001675   \n","...         ...       ...        ...       ...       ...        ...   \n","7400   0.000000  0.000000   0.000000  0.000360  0.001515   0.000000   \n","7401   0.000000  0.000000   0.000000  0.000737  0.000000   0.000000   \n","7402   0.000000  0.000000   0.000000  0.000406  0.000365   0.000000   \n","7403   0.000000  0.000000   0.000000  0.012053  0.933693   0.000000   \n","7404   0.000000  0.000000   0.000000  0.039173  0.921558   0.000000   \n","\n","      curiosity.1  desire  disappointment  disapproval  disgust  \\\n","0        0.010023     0.0        0.000000          0.0      0.0   \n","1        0.000000     0.0        0.053545          0.0      0.0   \n","2        0.000000     0.0        0.000000          0.0      0.0   \n","3        0.000000     0.0        0.000000          0.0      0.0   \n","4        0.000000     0.0        0.001537          0.0      0.0   \n","...           ...     ...             ...          ...      ...   \n","7400     0.000000     0.0        0.000000          0.0      0.0   \n","7401     0.000227     0.0        0.000000          0.0      0.0   \n","7402     0.000704     0.0        0.000000          0.0      0.0   \n","7403     0.000000     0.0        0.000000          0.0      0.0   \n","7404     0.000000     0.0        0.000000          0.0      0.0   \n","\n","      embarrassment  excitement      fear  gratitude     grief  joy  love  \\\n","0               0.0         0.0  0.855265   0.000000  0.000000  0.0   0.0   \n","1               0.0         0.0  0.045363   0.000000  0.000000  0.0   0.0   \n","2               0.0         0.0  0.081328   0.000000  0.000000  0.0   0.0   \n","3               0.0         0.0  0.822192   0.000000  0.035818  0.0   0.0   \n","4               0.0         0.0  0.911942   0.000000  0.000000  0.0   0.0   \n","...             ...         ...       ...        ...       ...  ...   ...   \n","7400            0.0         0.0  0.000000   0.996597  0.000000  0.0   0.0   \n","7401            0.0         0.0  0.000000   0.997774  0.000000  0.0   0.0   \n","7402            0.0         0.0  0.000000   0.998274  0.000000  0.0   0.0   \n","7403            0.0         0.0  0.007902   0.000000  0.000000  0.0   0.0   \n","7404            0.0         0.0  0.000000   0.000000  0.000000  0.0   0.0   \n","\n","      nervousness  optimism  pride  realization    relief  remorse   sadness  \\\n","0        0.068940  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","1        0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.416536   \n","2        0.035893  0.000000    0.0     0.336985  0.000000      0.0  0.000000   \n","3        0.010961  0.000000    0.0     0.000000  0.000000      0.0  0.058049   \n","4        0.079510  0.000000    0.0     0.000000  0.000000      0.0  0.002959   \n","...           ...       ...    ...          ...       ...      ...       ...   \n","7400     0.000000  0.000000    0.0     0.000000  0.000448      0.0  0.000000   \n","7401     0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","7402     0.000000  0.000000    0.0     0.000000  0.000000      0.0  0.000000   \n","7403     0.016812  0.016002    0.0     0.000000  0.000000      0.0  0.000000   \n","7404     0.000000  0.007040    0.0     0.000000  0.007071      0.0  0.000000   \n","\n","      surprise   neutral  anger_intensity  anticipation_intensity  \\\n","0          0.0  0.000000         0.415048                0.553423   \n","1          0.0  0.000000         0.530400                0.519750   \n","2          0.0  0.041849         0.428600                0.533500   \n","3          0.0  0.000000         0.567200                0.533462   \n","4          0.0  0.000000         0.487000                0.508000   \n","...        ...       ...              ...                     ...   \n","7400       0.0  0.000000         0.396000                0.609000   \n","7401       0.0  0.001106         0.000000                0.000000   \n","7402       0.0  0.000394         0.000000                0.000000   \n","7403       0.0  0.000000         0.344000                0.528667   \n","7404       0.0  0.015857         0.376750                0.502500   \n","\n","      disgust_intensity  fear_intensity  joy_intensity  sadness_intensity  \\\n","0              0.272333        0.568205       0.409500           0.467625   \n","1              0.541250        0.432167       0.453429           0.315600   \n","2              0.228167        0.526192       0.413444           0.468533   \n","3              0.114667        0.501952       0.505000           0.522095   \n","4              0.482250        0.624833       0.489167           0.505333   \n","...                 ...             ...            ...                ...   \n","7400           0.484000        0.527500       0.434000           0.591000   \n","7401           0.000000        0.156000       0.000000           0.000000   \n","7402           0.000000        0.156000       0.000000           0.000000   \n","7403           0.000000        0.414000       0.515500           0.500000   \n","7404           0.422000        0.515333       0.431900           0.418800   \n","\n","      surprise_intensity  trust_intensity  symptoms_ext_count  label  \n","0               0.434500         0.522773                   8      1  \n","1               0.247333         0.508875                   1      1  \n","2               0.348500         0.504500                   3      1  \n","3               0.320500         0.593615                   1      1  \n","4               0.000000         0.527167                   3      1  \n","...                  ...              ...                 ...    ...  \n","7400            0.793000         0.540800                   1      0  \n","7401            0.000000         0.000000                   0      0  \n","7402            0.000000         0.641000                   0      0  \n","7403            0.363500         0.613000                   0      0  \n","7404            0.316500         0.519286                   0      0  \n","\n","[7405 rows x 184 columns]"],"text/html":["\n","\n","  <div id=\"df-35c12214-679a-462f-8f3e-aebe72f0a86b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentemb1</th>\n","      <th>sentemb2</th>\n","      <th>sentemb3</th>\n","      <th>sentemb4</th>\n","      <th>sentemb5</th>\n","      <th>sentemb6</th>\n","      <th>sentemb7</th>\n","      <th>sentemb8</th>\n","      <th>sentemb9</th>\n","      <th>sentemb10</th>\n","      <th>sentemb11</th>\n","      <th>sentemb12</th>\n","      <th>sentemb13</th>\n","      <th>sentemb14</th>\n","      <th>sentemb15</th>\n","      <th>sentemb16</th>\n","      <th>sentemb17</th>\n","      <th>sentemb18</th>\n","      <th>sentemb19</th>\n","      <th>sentemb20</th>\n","      <th>sentemb21</th>\n","      <th>sentemb22</th>\n","      <th>sentemb23</th>\n","      <th>sentemb24</th>\n","      <th>sentemb25</th>\n","      <th>sentemb26</th>\n","      <th>sentemb27</th>\n","      <th>sentemb28</th>\n","      <th>WC</th>\n","      <th>Analytic</th>\n","      <th>Clout</th>\n","      <th>Authentic</th>\n","      <th>Tone</th>\n","      <th>WPS</th>\n","      <th>BigWords</th>\n","      <th>Dic</th>\n","      <th>Linguistic</th>\n","      <th>function</th>\n","      <th>pronoun</th>\n","      <th>ppron</th>\n","      <th>i</th>\n","      <th>we</th>\n","      <th>you</th>\n","      <th>shehe</th>\n","      <th>they</th>\n","      <th>ipron</th>\n","      <th>det</th>\n","      <th>article</th>\n","      <th>number</th>\n","      <th>prep</th>\n","      <th>auxverb</th>\n","      <th>adverb</th>\n","      <th>conj</th>\n","      <th>negate</th>\n","      <th>verb</th>\n","      <th>adj</th>\n","      <th>quantity</th>\n","      <th>Drives</th>\n","      <th>affiliation</th>\n","      <th>achieve</th>\n","      <th>power</th>\n","      <th>Cognition</th>\n","      <th>allnone</th>\n","      <th>cogproc</th>\n","      <th>insight</th>\n","      <th>cause</th>\n","      <th>discrep</th>\n","      <th>tentat</th>\n","      <th>certitude</th>\n","      <th>differ</th>\n","      <th>memory</th>\n","      <th>Affect</th>\n","      <th>tone_pos</th>\n","      <th>tone_neg</th>\n","      <th>emotion</th>\n","      <th>emo_pos</th>\n","      <th>emo_neg</th>\n","      <th>emo_anx</th>\n","      <th>emo_anger</th>\n","      <th>emo_sad</th>\n","      <th>swear</th>\n","      <th>Social</th>\n","      <th>socbehav</th>\n","      <th>prosocial</th>\n","      <th>polite</th>\n","      <th>conflict</th>\n","      <th>moral</th>\n","      <th>comm</th>\n","      <th>socrefs</th>\n","      <th>family</th>\n","      <th>friend</th>\n","      <th>female</th>\n","      <th>male</th>\n","      <th>Culture</th>\n","      <th>politic</th>\n","      <th>ethnicity</th>\n","      <th>tech</th>\n","      <th>Lifestyle</th>\n","      <th>leisure</th>\n","      <th>home</th>\n","      <th>work</th>\n","      <th>money</th>\n","      <th>relig</th>\n","      <th>Physical</th>\n","      <th>health</th>\n","      <th>illness</th>\n","      <th>wellness</th>\n","      <th>mental</th>\n","      <th>substances</th>\n","      <th>sexual</th>\n","      <th>food</th>\n","      <th>death</th>\n","      <th>need</th>\n","      <th>want</th>\n","      <th>acquire</th>\n","      <th>lack</th>\n","      <th>fulfill</th>\n","      <th>fatigue</th>\n","      <th>reward</th>\n","      <th>risk</th>\n","      <th>curiosity</th>\n","      <th>allure</th>\n","      <th>Perception</th>\n","      <th>attention</th>\n","      <th>motion</th>\n","      <th>space</th>\n","      <th>visual</th>\n","      <th>auditory</th>\n","      <th>feeling</th>\n","      <th>time</th>\n","      <th>focuspast</th>\n","      <th>focuspresent</th>\n","      <th>focusfuture</th>\n","      <th>Conversation</th>\n","      <th>netspeak</th>\n","      <th>assent</th>\n","      <th>nonflu</th>\n","      <th>filler</th>\n","      <th>AllPunc</th>\n","      <th>Period</th>\n","      <th>Comma</th>\n","      <th>QMark</th>\n","      <th>Exclam</th>\n","      <th>Apostro</th>\n","      <th>OtherP</th>\n","      <th>Emoji</th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity.1</th>\n","      <th>desire</th>\n","      <th>disappointment</th>\n","      <th>disapproval</th>\n","      <th>disgust</th>\n","      <th>embarrassment</th>\n","      <th>excitement</th>\n","      <th>fear</th>\n","      <th>gratitude</th>\n","      <th>grief</th>\n","      <th>joy</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","      <th>anger_intensity</th>\n","      <th>anticipation_intensity</th>\n","      <th>disgust_intensity</th>\n","      <th>fear_intensity</th>\n","      <th>joy_intensity</th>\n","      <th>sadness_intensity</th>\n","      <th>surprise_intensity</th>\n","      <th>trust_intensity</th>\n","      <th>symptoms_ext_count</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.641940</td>\n","      <td>5.043077</td>\n","      <td>5.682403</td>\n","      <td>3.366873</td>\n","      <td>3.939331</td>\n","      <td>3.919311</td>\n","      <td>6.919125</td>\n","      <td>1.789982</td>\n","      <td>1.404625</td>\n","      <td>7.134058</td>\n","      <td>4.420479</td>\n","      <td>7.684002</td>\n","      <td>2.286182</td>\n","      <td>2.857458</td>\n","      <td>2.669546</td>\n","      <td>4.379669</td>\n","      <td>3.859759</td>\n","      <td>5.403258</td>\n","      <td>6.754729</td>\n","      <td>4.308768</td>\n","      <td>2.225483</td>\n","      <td>3.196717</td>\n","      <td>5.969540</td>\n","      <td>5.937778</td>\n","      <td>5.176552</td>\n","      <td>4.905828</td>\n","      <td>6.870799</td>\n","      <td>4.538727</td>\n","      <td>607</td>\n","      <td>55.22</td>\n","      <td>35.35</td>\n","      <td>48.82</td>\n","      <td>1.00</td>\n","      <td>26.39</td>\n","      <td>25.86</td>\n","      <td>93.74</td>\n","      <td>68.86</td>\n","      <td>57.50</td>\n","      <td>12.85</td>\n","      <td>6.26</td>\n","      <td>3.13</td>\n","      <td>0.16</td>\n","      <td>2.31</td>\n","      <td>0.00</td>\n","      <td>0.16</td>\n","      <td>6.59</td>\n","      <td>15.16</td>\n","      <td>7.74</td>\n","      <td>1.15</td>\n","      <td>14.66</td>\n","      <td>9.88</td>\n","      <td>7.08</td>\n","      <td>6.59</td>\n","      <td>0.82</td>\n","      <td>15.82</td>\n","      <td>5.11</td>\n","      <td>5.11</td>\n","      <td>3.46</td>\n","      <td>0.49</td>\n","      <td>1.15</td>\n","      <td>1.81</td>\n","      <td>18.62</td>\n","      <td>0.33</td>\n","      <td>18.29</td>\n","      <td>2.80</td>\n","      <td>3.13</td>\n","      <td>2.47</td>\n","      <td>4.45</td>\n","      <td>0.99</td>\n","      <td>4.45</td>\n","      <td>0.00</td>\n","      <td>10.71</td>\n","      <td>0.99</td>\n","      <td>9.56</td>\n","      <td>7.74</td>\n","      <td>0.16</td>\n","      <td>7.58</td>\n","      <td>6.92</td>\n","      <td>0.16</td>\n","      <td>0.16</td>\n","      <td>0.16</td>\n","      <td>8.73</td>\n","      <td>3.46</td>\n","      <td>0.33</td>\n","      <td>0.33</td>\n","      <td>1.32</td>\n","      <td>0.00</td>\n","      <td>1.15</td>\n","      <td>5.27</td>\n","      <td>0.33</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>2.31</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.98</td>\n","      <td>0.33</td>\n","      <td>0.00</td>\n","      <td>9.88</td>\n","      <td>8.07</td>\n","      <td>0.82</td>\n","      <td>0.00</td>\n","      <td>5.11</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.16</td>\n","      <td>0.33</td>\n","      <td>0.33</td>\n","      <td>0.16</td>\n","      <td>0.33</td>\n","      <td>0.16</td>\n","      <td>0.49</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.16</td>\n","      <td>6.26</td>\n","      <td>7.08</td>\n","      <td>0.33</td>\n","      <td>0.66</td>\n","      <td>4.78</td>\n","      <td>0.16</td>\n","      <td>0.00</td>\n","      <td>1.65</td>\n","      <td>4.61</td>\n","      <td>1.81</td>\n","      <td>8.07</td>\n","      <td>1.48</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>19.93</td>\n","      <td>4.61</td>\n","      <td>9.88</td>\n","      <td>0.16</td>\n","      <td>0.16</td>\n","      <td>2.80</td>\n","      <td>2.31</td>\n","      <td>0.33</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.009124</td>\n","      <td>0.048904</td>\n","      <td>0.010023</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.855265</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.068940</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.415048</td>\n","      <td>0.553423</td>\n","      <td>0.272333</td>\n","      <td>0.568205</td>\n","      <td>0.409500</td>\n","      <td>0.467625</td>\n","      <td>0.434500</td>\n","      <td>0.522773</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11.312859</td>\n","      <td>5.364349</td>\n","      <td>4.413650</td>\n","      <td>3.407885</td>\n","      <td>5.023540</td>\n","      <td>4.247440</td>\n","      <td>6.088093</td>\n","      <td>1.780003</td>\n","      <td>1.160577</td>\n","      <td>6.596563</td>\n","      <td>4.065295</td>\n","      <td>7.592488</td>\n","      <td>2.172122</td>\n","      <td>3.172726</td>\n","      <td>2.680326</td>\n","      <td>4.280528</td>\n","      <td>3.796072</td>\n","      <td>5.654799</td>\n","      <td>7.253571</td>\n","      <td>4.070558</td>\n","      <td>2.360492</td>\n","      <td>3.181391</td>\n","      <td>6.144005</td>\n","      <td>5.662973</td>\n","      <td>5.073269</td>\n","      <td>4.922340</td>\n","      <td>6.847518</td>\n","      <td>4.559060</td>\n","      <td>401</td>\n","      <td>54.08</td>\n","      <td>1.31</td>\n","      <td>99.00</td>\n","      <td>8.57</td>\n","      <td>13.83</td>\n","      <td>14.21</td>\n","      <td>90.02</td>\n","      <td>74.81</td>\n","      <td>61.60</td>\n","      <td>17.96</td>\n","      <td>12.72</td>\n","      <td>11.47</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>5.24</td>\n","      <td>15.46</td>\n","      <td>7.98</td>\n","      <td>1.25</td>\n","      <td>15.71</td>\n","      <td>6.98</td>\n","      <td>5.74</td>\n","      <td>6.23</td>\n","      <td>2.00</td>\n","      <td>15.21</td>\n","      <td>5.24</td>\n","      <td>3.24</td>\n","      <td>1.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>11.97</td>\n","      <td>2.74</td>\n","      <td>9.23</td>\n","      <td>1.50</td>\n","      <td>2.00</td>\n","      <td>2.49</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>2.49</td>\n","      <td>0.00</td>\n","      <td>4.24</td>\n","      <td>1.50</td>\n","      <td>2.74</td>\n","      <td>1.50</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>4.74</td>\n","      <td>2.24</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>2.49</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>3.49</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>2.24</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>3.99</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>3.49</td>\n","      <td>14.71</td>\n","      <td>0.25</td>\n","      <td>3.24</td>\n","      <td>8.98</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.50</td>\n","      <td>7.98</td>\n","      <td>10.22</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>13.47</td>\n","      <td>6.98</td>\n","      <td>4.74</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.168074</td>\n","      <td>0.000000</td>\n","      <td>0.201718</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.053545</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.045363</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.416536</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.530400</td>\n","      <td>0.519750</td>\n","      <td>0.541250</td>\n","      <td>0.432167</td>\n","      <td>0.453429</td>\n","      <td>0.315600</td>\n","      <td>0.247333</td>\n","      <td>0.508875</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10.531799</td>\n","      <td>4.894456</td>\n","      <td>5.387705</td>\n","      <td>3.325495</td>\n","      <td>4.051551</td>\n","      <td>4.119681</td>\n","      <td>6.826452</td>\n","      <td>1.776312</td>\n","      <td>1.471099</td>\n","      <td>6.926485</td>\n","      <td>4.441208</td>\n","      <td>7.688570</td>\n","      <td>2.225875</td>\n","      <td>2.871117</td>\n","      <td>2.653050</td>\n","      <td>4.511485</td>\n","      <td>3.862097</td>\n","      <td>5.338437</td>\n","      <td>6.786397</td>\n","      <td>4.259876</td>\n","      <td>2.223957</td>\n","      <td>3.178261</td>\n","      <td>5.996410</td>\n","      <td>5.876356</td>\n","      <td>5.186982</td>\n","      <td>4.896872</td>\n","      <td>6.836080</td>\n","      <td>4.530069</td>\n","      <td>446</td>\n","      <td>25.83</td>\n","      <td>93.36</td>\n","      <td>75.79</td>\n","      <td>1.00</td>\n","      <td>12.74</td>\n","      <td>15.47</td>\n","      <td>95.74</td>\n","      <td>75.34</td>\n","      <td>59.64</td>\n","      <td>17.26</td>\n","      <td>13.00</td>\n","      <td>2.91</td>\n","      <td>0.00</td>\n","      <td>9.87</td>\n","      <td>0.00</td>\n","      <td>0.22</td>\n","      <td>4.26</td>\n","      <td>13.90</td>\n","      <td>6.28</td>\n","      <td>1.35</td>\n","      <td>13.23</td>\n","      <td>9.64</td>\n","      <td>6.95</td>\n","      <td>9.42</td>\n","      <td>1.35</td>\n","      <td>18.39</td>\n","      <td>6.73</td>\n","      <td>4.71</td>\n","      <td>2.69</td>\n","      <td>0.22</td>\n","      <td>1.57</td>\n","      <td>0.90</td>\n","      <td>19.51</td>\n","      <td>1.12</td>\n","      <td>18.39</td>\n","      <td>3.36</td>\n","      <td>2.24</td>\n","      <td>2.02</td>\n","      <td>4.93</td>\n","      <td>0.67</td>\n","      <td>4.04</td>\n","      <td>0.00</td>\n","      <td>9.64</td>\n","      <td>1.35</td>\n","      <td>8.07</td>\n","      <td>6.28</td>\n","      <td>0.45</td>\n","      <td>5.61</td>\n","      <td>4.71</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>11.88</td>\n","      <td>1.57</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>0.67</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>10.09</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>1.79</td>\n","      <td>0.45</td>\n","      <td>0.45</td>\n","      <td>0.90</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>7.40</td>\n","      <td>2.47</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.35</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.67</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.35</td>\n","      <td>0.00</td>\n","      <td>0.22</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>0.90</td>\n","      <td>0.22</td>\n","      <td>6.28</td>\n","      <td>11.43</td>\n","      <td>0.90</td>\n","      <td>1.79</td>\n","      <td>5.16</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>3.81</td>\n","      <td>6.73</td>\n","      <td>1.79</td>\n","      <td>9.87</td>\n","      <td>0.45</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>0.22</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>16.59</td>\n","      <td>8.07</td>\n","      <td>2.69</td>\n","      <td>0.22</td>\n","      <td>0.00</td>\n","      <td>4.71</td>\n","      <td>0.90</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.412951</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.081328</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.035893</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.336985</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.041849</td>\n","      <td>0.428600</td>\n","      <td>0.533500</td>\n","      <td>0.228167</td>\n","      <td>0.526192</td>\n","      <td>0.413444</td>\n","      <td>0.468533</td>\n","      <td>0.348500</td>\n","      <td>0.504500</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.310531</td>\n","      <td>5.330986</td>\n","      <td>4.330414</td>\n","      <td>3.431627</td>\n","      <td>5.095810</td>\n","      <td>4.284573</td>\n","      <td>6.036744</td>\n","      <td>1.780587</td>\n","      <td>1.134592</td>\n","      <td>6.539715</td>\n","      <td>4.022463</td>\n","      <td>7.585912</td>\n","      <td>2.188976</td>\n","      <td>3.194212</td>\n","      <td>2.678760</td>\n","      <td>4.270244</td>\n","      <td>3.784710</td>\n","      <td>5.666665</td>\n","      <td>7.298054</td>\n","      <td>4.057921</td>\n","      <td>2.369854</td>\n","      <td>3.181388</td>\n","      <td>6.149067</td>\n","      <td>5.650796</td>\n","      <td>5.068528</td>\n","      <td>4.935538</td>\n","      <td>6.840829</td>\n","      <td>4.564339</td>\n","      <td>525</td>\n","      <td>30.63</td>\n","      <td>2.97</td>\n","      <td>96.06</td>\n","      <td>6.86</td>\n","      <td>15.44</td>\n","      <td>12.38</td>\n","      <td>90.86</td>\n","      <td>73.33</td>\n","      <td>56.57</td>\n","      <td>20.38</td>\n","      <td>16.19</td>\n","      <td>12.95</td>\n","      <td>0.00</td>\n","      <td>1.14</td>\n","      <td>1.90</td>\n","      <td>0.19</td>\n","      <td>4.19</td>\n","      <td>13.52</td>\n","      <td>5.71</td>\n","      <td>0.76</td>\n","      <td>14.67</td>\n","      <td>10.29</td>\n","      <td>4.19</td>\n","      <td>7.62</td>\n","      <td>1.14</td>\n","      <td>23.24</td>\n","      <td>3.24</td>\n","      <td>3.05</td>\n","      <td>1.90</td>\n","      <td>0.95</td>\n","      <td>0.57</td>\n","      <td>0.38</td>\n","      <td>7.62</td>\n","      <td>1.33</td>\n","      <td>6.29</td>\n","      <td>1.33</td>\n","      <td>0.95</td>\n","      <td>1.71</td>\n","      <td>0.57</td>\n","      <td>0.00</td>\n","      <td>1.90</td>\n","      <td>0.19</td>\n","      <td>4.00</td>\n","      <td>1.14</td>\n","      <td>2.67</td>\n","      <td>1.71</td>\n","      <td>0.57</td>\n","      <td>1.14</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.19</td>\n","      <td>8.19</td>\n","      <td>3.62</td>\n","      <td>0.19</td>\n","      <td>0.19</td>\n","      <td>0.19</td>\n","      <td>0.00</td>\n","      <td>2.48</td>\n","      <td>4.19</td>\n","      <td>0.95</td>\n","      <td>0.0</td>\n","      <td>2.67</td>\n","      <td>0.19</td>\n","      <td>0.76</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.76</td>\n","      <td>2.67</td>\n","      <td>0.19</td>\n","      <td>0.76</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.71</td>\n","      <td>5.33</td>\n","      <td>1.14</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.38</td>\n","      <td>1.33</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.76</td>\n","      <td>0.00</td>\n","      <td>0.19</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.57</td>\n","      <td>0.00</td>\n","      <td>7.81</td>\n","      <td>13.33</td>\n","      <td>0.38</td>\n","      <td>2.86</td>\n","      <td>6.86</td>\n","      <td>1.71</td>\n","      <td>1.52</td>\n","      <td>0.95</td>\n","      <td>5.52</td>\n","      <td>0.95</td>\n","      <td>10.29</td>\n","      <td>3.81</td>\n","      <td>2.67</td>\n","      <td>0.19</td>\n","      <td>0.57</td>\n","      <td>1.9</td>\n","      <td>0.0</td>\n","      <td>21.52</td>\n","      <td>5.33</td>\n","      <td>4.95</td>\n","      <td>1.14</td>\n","      <td>0.00</td>\n","      <td>6.67</td>\n","      <td>3.43</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.043745</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.822192</td>\n","      <td>0.000000</td>\n","      <td>0.035818</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.010961</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.058049</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.567200</td>\n","      <td>0.533462</td>\n","      <td>0.114667</td>\n","      <td>0.501952</td>\n","      <td>0.505000</td>\n","      <td>0.522095</td>\n","      <td>0.320500</td>\n","      <td>0.593615</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.990587</td>\n","      <td>5.196148</td>\n","      <td>4.968183</td>\n","      <td>2.576940</td>\n","      <td>4.233728</td>\n","      <td>4.119200</td>\n","      <td>6.174312</td>\n","      <td>1.775400</td>\n","      <td>1.475707</td>\n","      <td>6.684998</td>\n","      <td>4.217587</td>\n","      <td>7.664200</td>\n","      <td>1.999518</td>\n","      <td>2.998645</td>\n","      <td>2.709311</td>\n","      <td>4.489384</td>\n","      <td>3.847829</td>\n","      <td>5.320369</td>\n","      <td>7.061465</td>\n","      <td>4.159153</td>\n","      <td>2.327722</td>\n","      <td>3.183620</td>\n","      <td>6.181925</td>\n","      <td>5.809379</td>\n","      <td>5.113574</td>\n","      <td>4.873700</td>\n","      <td>6.771983</td>\n","      <td>4.528791</td>\n","      <td>323</td>\n","      <td>21.57</td>\n","      <td>1.00</td>\n","      <td>99.00</td>\n","      <td>1.12</td>\n","      <td>13.46</td>\n","      <td>15.48</td>\n","      <td>93.50</td>\n","      <td>75.85</td>\n","      <td>60.99</td>\n","      <td>20.12</td>\n","      <td>14.86</td>\n","      <td>13.00</td>\n","      <td>0.00</td>\n","      <td>1.55</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>12.69</td>\n","      <td>4.64</td>\n","      <td>0.62</td>\n","      <td>15.79</td>\n","      <td>10.53</td>\n","      <td>6.81</td>\n","      <td>6.81</td>\n","      <td>3.10</td>\n","      <td>20.43</td>\n","      <td>6.50</td>\n","      <td>1.86</td>\n","      <td>2.79</td>\n","      <td>0.31</td>\n","      <td>1.24</td>\n","      <td>1.24</td>\n","      <td>14.55</td>\n","      <td>1.24</td>\n","      <td>13.31</td>\n","      <td>3.72</td>\n","      <td>0.62</td>\n","      <td>3.72</td>\n","      <td>3.10</td>\n","      <td>0.00</td>\n","      <td>4.33</td>\n","      <td>0.00</td>\n","      <td>6.50</td>\n","      <td>1.55</td>\n","      <td>4.95</td>\n","      <td>2.79</td>\n","      <td>0.31</td>\n","      <td>2.48</td>\n","      <td>1.86</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>3.41</td>\n","      <td>0.93</td>\n","      <td>0.31</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.31</td>\n","      <td>2.48</td>\n","      <td>0.31</td>\n","      <td>0.0</td>\n","      <td>0.62</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>1.55</td>\n","      <td>0.31</td>\n","      <td>0.31</td>\n","      <td>0.31</td>\n","      <td>0.62</td>\n","      <td>0.00</td>\n","      <td>7.43</td>\n","      <td>2.17</td>\n","      <td>0.93</td>\n","      <td>0.00</td>\n","      <td>0.62</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.62</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.24</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.31</td>\n","      <td>0.00</td>\n","      <td>8.98</td>\n","      <td>17.65</td>\n","      <td>0.62</td>\n","      <td>2.17</td>\n","      <td>10.84</td>\n","      <td>2.17</td>\n","      <td>0.93</td>\n","      <td>2.17</td>\n","      <td>3.41</td>\n","      <td>0.62</td>\n","      <td>11.76</td>\n","      <td>1.24</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>18.27</td>\n","      <td>7.12</td>\n","      <td>4.33</td>\n","      <td>0.93</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.62</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.001675</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.001537</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.911942</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.079510</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.002959</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.487000</td>\n","      <td>0.508000</td>\n","      <td>0.482250</td>\n","      <td>0.624833</td>\n","      <td>0.489167</td>\n","      <td>0.505333</td>\n","      <td>0.000000</td>\n","      <td>0.527167</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7400</th>\n","      <td>10.507652</td>\n","      <td>4.949894</td>\n","      <td>4.014671</td>\n","      <td>2.117899</td>\n","      <td>4.715438</td>\n","      <td>4.335719</td>\n","      <td>5.412321</td>\n","      <td>1.769552</td>\n","      <td>1.720969</td>\n","      <td>6.056923</td>\n","      <td>4.317115</td>\n","      <td>7.701209</td>\n","      <td>1.577826</td>\n","      <td>3.124527</td>\n","      <td>2.587120</td>\n","      <td>4.895798</td>\n","      <td>3.797136</td>\n","      <td>5.180048</td>\n","      <td>7.375135</td>\n","      <td>3.952120</td>\n","      <td>2.305240</td>\n","      <td>3.210462</td>\n","      <td>6.340523</td>\n","      <td>5.595186</td>\n","      <td>5.102685</td>\n","      <td>4.922517</td>\n","      <td>6.645929</td>\n","      <td>4.465776</td>\n","      <td>237</td>\n","      <td>5.24</td>\n","      <td>20.52</td>\n","      <td>53.23</td>\n","      <td>15.51</td>\n","      <td>16.93</td>\n","      <td>18.57</td>\n","      <td>93.25</td>\n","      <td>81.86</td>\n","      <td>62.87</td>\n","      <td>23.63</td>\n","      <td>14.35</td>\n","      <td>8.44</td>\n","      <td>0.00</td>\n","      <td>3.80</td>\n","      <td>0.42</td>\n","      <td>1.27</td>\n","      <td>9.28</td>\n","      <td>14.35</td>\n","      <td>2.95</td>\n","      <td>0.84</td>\n","      <td>10.97</td>\n","      <td>13.08</td>\n","      <td>7.17</td>\n","      <td>6.33</td>\n","      <td>1.69</td>\n","      <td>25.74</td>\n","      <td>8.02</td>\n","      <td>2.95</td>\n","      <td>2.53</td>\n","      <td>0.84</td>\n","      <td>0.84</td>\n","      <td>0.84</td>\n","      <td>17.30</td>\n","      <td>1.27</td>\n","      <td>16.03</td>\n","      <td>2.95</td>\n","      <td>2.11</td>\n","      <td>3.80</td>\n","      <td>2.95</td>\n","      <td>0.42</td>\n","      <td>3.38</td>\n","      <td>0.00</td>\n","      <td>6.75</td>\n","      <td>2.95</td>\n","      <td>3.38</td>\n","      <td>3.80</td>\n","      <td>0.42</td>\n","      <td>2.95</td>\n","      <td>1.69</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>11.81</td>\n","      <td>5.91</td>\n","      <td>1.27</td>\n","      <td>0.84</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>3.38</td>\n","      <td>5.91</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.84</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.84</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>4.22</td>\n","      <td>4.22</td>\n","      <td>2.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.42</td>\n","      <td>0.84</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>5.49</td>\n","      <td>7.17</td>\n","      <td>0.42</td>\n","      <td>0.84</td>\n","      <td>3.80</td>\n","      <td>0.42</td>\n","      <td>0.42</td>\n","      <td>0.84</td>\n","      <td>3.38</td>\n","      <td>8.02</td>\n","      <td>7.17</td>\n","      <td>1.27</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.13</td>\n","      <td>5.49</td>\n","      <td>1.69</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.53</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.000908</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000360</td>\n","      <td>0.001515</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.996597</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000448</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.396000</td>\n","      <td>0.609000</td>\n","      <td>0.484000</td>\n","      <td>0.527500</td>\n","      <td>0.434000</td>\n","      <td>0.591000</td>\n","      <td>0.793000</td>\n","      <td>0.540800</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7401</th>\n","      <td>10.698858</td>\n","      <td>4.546772</td>\n","      <td>3.540320</td>\n","      <td>3.286137</td>\n","      <td>5.087534</td>\n","      <td>5.156605</td>\n","      <td>5.941041</td>\n","      <td>1.688843</td>\n","      <td>1.656772</td>\n","      <td>5.678233</td>\n","      <td>4.209131</td>\n","      <td>7.742318</td>\n","      <td>2.049652</td>\n","      <td>3.168222</td>\n","      <td>2.397176</td>\n","      <td>5.081987</td>\n","      <td>3.717591</td>\n","      <td>5.339126</td>\n","      <td>7.371179</td>\n","      <td>3.950364</td>\n","      <td>2.242772</td>\n","      <td>3.236705</td>\n","      <td>6.118145</td>\n","      <td>5.502766</td>\n","      <td>5.187657</td>\n","      <td>5.083427</td>\n","      <td>6.556334</td>\n","      <td>4.512558</td>\n","      <td>20</td>\n","      <td>39.70</td>\n","      <td>1.00</td>\n","      <td>28.56</td>\n","      <td>99.00</td>\n","      <td>20.00</td>\n","      <td>25.00</td>\n","      <td>95.00</td>\n","      <td>65.00</td>\n","      <td>40.00</td>\n","      <td>15.00</td>\n","      <td>15.00</td>\n","      <td>15.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>10.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>30.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>25.00</td>\n","      <td>0.00</td>\n","      <td>25.00</td>\n","      <td>5.00</td>\n","      <td>5.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>15.00</td>\n","      <td>15.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>20.00</td>\n","      <td>15.00</td>\n","      <td>5.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>15.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>15.00</td>\n","      <td>15.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>10.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.00</td>\n","      <td>5.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.000452</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000737</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000227</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.997774</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.001106</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7402</th>\n","      <td>10.346373</td>\n","      <td>4.424756</td>\n","      <td>3.581532</td>\n","      <td>3.153925</td>\n","      <td>5.020442</td>\n","      <td>4.948900</td>\n","      <td>5.897359</td>\n","      <td>1.686590</td>\n","      <td>1.781551</td>\n","      <td>5.741636</td>\n","      <td>4.264494</td>\n","      <td>7.766108</td>\n","      <td>1.989159</td>\n","      <td>3.134499</td>\n","      <td>2.402262</td>\n","      <td>5.107181</td>\n","      <td>3.737733</td>\n","      <td>5.321551</td>\n","      <td>7.339849</td>\n","      <td>3.958114</td>\n","      <td>2.226620</td>\n","      <td>3.229278</td>\n","      <td>6.177763</td>\n","      <td>5.498463</td>\n","      <td>5.200507</td>\n","      <td>5.040123</td>\n","      <td>6.552696</td>\n","      <td>4.490976</td>\n","      <td>19</td>\n","      <td>36.67</td>\n","      <td>3.34</td>\n","      <td>70.28</td>\n","      <td>99.00</td>\n","      <td>19.00</td>\n","      <td>21.05</td>\n","      <td>94.74</td>\n","      <td>73.68</td>\n","      <td>47.37</td>\n","      <td>21.05</td>\n","      <td>15.79</td>\n","      <td>15.79</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>5.26</td>\n","      <td>5.26</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>21.05</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>10.53</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>21.05</td>\n","      <td>15.79</td>\n","      <td>5.26</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>10.53</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>10.53</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000406</td>\n","      <td>0.000365</td>\n","      <td>0.000000</td>\n","      <td>0.000704</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.998274</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000394</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.156000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7403</th>\n","      <td>10.454275</td>\n","      <td>4.564041</td>\n","      <td>3.603564</td>\n","      <td>3.078566</td>\n","      <td>4.991286</td>\n","      <td>4.955261</td>\n","      <td>5.872911</td>\n","      <td>1.702558</td>\n","      <td>1.761993</td>\n","      <td>5.793803</td>\n","      <td>4.275763</td>\n","      <td>7.760498</td>\n","      <td>1.955059</td>\n","      <td>3.136624</td>\n","      <td>2.410064</td>\n","      <td>5.094350</td>\n","      <td>3.744368</td>\n","      <td>5.321893</td>\n","      <td>7.350019</td>\n","      <td>3.961870</td>\n","      <td>2.231675</td>\n","      <td>3.226660</td>\n","      <td>6.186529</td>\n","      <td>5.515781</td>\n","      <td>5.197260</td>\n","      <td>5.032067</td>\n","      <td>6.564847</td>\n","      <td>4.491873</td>\n","      <td>38</td>\n","      <td>36.67</td>\n","      <td>90.88</td>\n","      <td>33.61</td>\n","      <td>92.27</td>\n","      <td>19.00</td>\n","      <td>21.05</td>\n","      <td>97.37</td>\n","      <td>63.16</td>\n","      <td>52.63</td>\n","      <td>15.79</td>\n","      <td>10.53</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>5.26</td>\n","      <td>13.16</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>15.79</td>\n","      <td>7.89</td>\n","      <td>5.26</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>5.26</td>\n","      <td>5.26</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>13.16</td>\n","      <td>0.00</td>\n","      <td>13.16</td>\n","      <td>5.26</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>10.53</td>\n","      <td>5.26</td>\n","      <td>7.89</td>\n","      <td>2.63</td>\n","      <td>5.26</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>15.79</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>10.53</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>13.16</td>\n","      <td>13.16</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>13.16</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>5.26</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.63</td>\n","      <td>2.63</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>2.63</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.012053</td>\n","      <td>0.933693</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.007902</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.016812</td>\n","      <td>0.016002</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.344000</td>\n","      <td>0.528667</td>\n","      <td>0.000000</td>\n","      <td>0.414000</td>\n","      <td>0.515500</td>\n","      <td>0.500000</td>\n","      <td>0.363500</td>\n","      <td>0.613000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7404</th>\n","      <td>11.222271</td>\n","      <td>5.146849</td>\n","      <td>4.005402</td>\n","      <td>3.371954</td>\n","      <td>4.682106</td>\n","      <td>5.396935</td>\n","      <td>6.367755</td>\n","      <td>1.753800</td>\n","      <td>1.507380</td>\n","      <td>6.102673</td>\n","      <td>4.176505</td>\n","      <td>7.723337</td>\n","      <td>2.232829</td>\n","      <td>3.068840</td>\n","      <td>2.387916</td>\n","      <td>4.919335</td>\n","      <td>3.758407</td>\n","      <td>5.360199</td>\n","      <td>7.279993</td>\n","      <td>4.084011</td>\n","      <td>2.253345</td>\n","      <td>3.232021</td>\n","      <td>6.046115</td>\n","      <td>5.681019</td>\n","      <td>5.217414</td>\n","      <td>5.079988</td>\n","      <td>6.634119</td>\n","      <td>4.559704</td>\n","      <td>186</td>\n","      <td>26.78</td>\n","      <td>52.89</td>\n","      <td>86.48</td>\n","      <td>2.39</td>\n","      <td>23.25</td>\n","      <td>15.59</td>\n","      <td>98.39</td>\n","      <td>79.03</td>\n","      <td>61.83</td>\n","      <td>17.20</td>\n","      <td>8.60</td>\n","      <td>3.23</td>\n","      <td>0.00</td>\n","      <td>4.84</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>8.60</td>\n","      <td>10.75</td>\n","      <td>4.30</td>\n","      <td>2.69</td>\n","      <td>15.05</td>\n","      <td>9.68</td>\n","      <td>5.91</td>\n","      <td>8.60</td>\n","      <td>2.69</td>\n","      <td>20.97</td>\n","      <td>5.91</td>\n","      <td>5.38</td>\n","      <td>1.08</td>\n","      <td>0.00</td>\n","      <td>1.08</td>\n","      <td>0.00</td>\n","      <td>13.98</td>\n","      <td>1.61</td>\n","      <td>12.37</td>\n","      <td>5.91</td>\n","      <td>1.08</td>\n","      <td>0.54</td>\n","      <td>2.15</td>\n","      <td>0.54</td>\n","      <td>2.15</td>\n","      <td>0.00</td>\n","      <td>10.22</td>\n","      <td>3.76</td>\n","      <td>6.45</td>\n","      <td>6.45</td>\n","      <td>1.08</td>\n","      <td>5.38</td>\n","      <td>3.76</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>6.99</td>\n","      <td>1.61</td>\n","      <td>0.54</td>\n","      <td>0.54</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>5.38</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>3.23</td>\n","      <td>1.61</td>\n","      <td>0.00</td>\n","      <td>1.08</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>3.76</td>\n","      <td>1.61</td>\n","      <td>0.00</td>\n","      <td>1.08</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.08</td>\n","      <td>0.00</td>\n","      <td>2.15</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>12.37</td>\n","      <td>10.75</td>\n","      <td>0.00</td>\n","      <td>2.69</td>\n","      <td>5.91</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>1.61</td>\n","      <td>4.84</td>\n","      <td>1.61</td>\n","      <td>5.91</td>\n","      <td>2.15</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12.90</td>\n","      <td>3.76</td>\n","      <td>5.38</td>\n","      <td>0.00</td>\n","      <td>0.54</td>\n","      <td>1.08</td>\n","      <td>2.15</td>\n","      <td>0.00</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.039173</td>\n","      <td>0.921558</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007040</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.007071</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.015857</td>\n","      <td>0.376750</td>\n","      <td>0.502500</td>\n","      <td>0.422000</td>\n","      <td>0.515333</td>\n","      <td>0.431900</td>\n","      <td>0.418800</td>\n","      <td>0.316500</td>\n","      <td>0.519286</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7405 rows × 184 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35c12214-679a-462f-8f3e-aebe72f0a86b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-e18ad5ec-7128-4577-8e0e-87479928ac23\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e18ad5ec-7128-4577-8e0e-87479928ac23')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-e18ad5ec-7128-4577-8e0e-87479928ac23 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-35c12214-679a-462f-8f3e-aebe72f0a86b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-35c12214-679a-462f-8f3e-aebe72f0a86b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["X_sentemb = final_df.loc[:, 'sentemb1':'sentemb28']\n","# putting panic extended feature with liwc features\n","X_liwc = final_df.loc[:, 'WC':'Emoji']\n","X_liwc['symptoms_ext_count'] = final_df['symptoms_ext_count']\n","X_emotions = final_df.loc[:, 'admiration':'neutral']\n","X_intensity = final_df.loc[:, 'anger_intensity':'trust_intensity']\n","y = final_df['label']"],"metadata":{"id":"QLpsywS2iwMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6kr44uOvnKn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN nothing nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_lstm_emotions, input_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bdnDpcwo2rf","executionInfo":{"status":"ok","timestamp":1690643238248,"user_tz":-330,"elapsed":1051528,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"4e30e345-6e93-468a-a177-553255c08f82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 19s 47ms/step - loss: 0.6269 - accuracy: 0.6450 - f1_score: 0.6284 - val_loss: 0.4763 - val_accuracy: 0.7984 - val_f1_score: 0.8056\n","Epoch 2/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.4834 - accuracy: 0.7725 - f1_score: 0.7641 - val_loss: 0.3750 - val_accuracy: 0.8382 - val_f1_score: 0.8274\n","Epoch 3/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4147 - accuracy: 0.8113 - f1_score: 0.8057 - val_loss: 0.3290 - val_accuracy: 0.8644 - val_f1_score: 0.8596\n","Epoch 4/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3723 - accuracy: 0.8433 - f1_score: 0.8406 - val_loss: 0.3129 - val_accuracy: 0.8671 - val_f1_score: 0.8633\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3508 - accuracy: 0.8541 - f1_score: 0.8507 - val_loss: 0.3079 - val_accuracy: 0.8770 - val_f1_score: 0.8773\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3313 - accuracy: 0.8638 - f1_score: 0.8607 - val_loss: 0.2954 - val_accuracy: 0.8816 - val_f1_score: 0.8827\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3147 - accuracy: 0.8767 - f1_score: 0.8747 - val_loss: 0.3213 - val_accuracy: 0.8698 - val_f1_score: 0.8759\n","Epoch 8/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3182 - accuracy: 0.8710 - f1_score: 0.8689 - val_loss: 0.3019 - val_accuracy: 0.8761 - val_f1_score: 0.8762\n","Epoch 9/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.2958 - accuracy: 0.8743 - f1_score: 0.8717 - val_loss: 0.3012 - val_accuracy: 0.8743 - val_f1_score: 0.8751\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2817 - accuracy: 0.8885 - f1_score: 0.8870 - val_loss: 0.3367 - val_accuracy: 0.8526 - val_f1_score: 0.8375\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2705 - accuracy: 0.8915 - f1_score: 0.8900 - val_loss: 0.3358 - val_accuracy: 0.8635 - val_f1_score: 0.8672\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3346 - accuracy: 0.8521 - f1_score: 0.8748\n","Epoch 1/20\n","104/104 [==============================] - 16s 39ms/step - loss: 0.6392 - accuracy: 0.6383 - f1_score: 0.5951 - val_loss: 0.5840 - val_accuracy: 0.6854 - val_f1_score: 0.7242\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4961 - accuracy: 0.7700 - f1_score: 0.7634 - val_loss: 0.4571 - val_accuracy: 0.7929 - val_f1_score: 0.7775\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4231 - accuracy: 0.8047 - f1_score: 0.8005 - val_loss: 0.4363 - val_accuracy: 0.7939 - val_f1_score: 0.8119\n","Epoch 4/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3656 - accuracy: 0.8448 - f1_score: 0.8443 - val_loss: 0.3878 - val_accuracy: 0.8291 - val_f1_score: 0.8372\n","Epoch 5/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3412 - accuracy: 0.8505 - f1_score: 0.8495 - val_loss: 0.3623 - val_accuracy: 0.8517 - val_f1_score: 0.8435\n","Epoch 6/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3195 - accuracy: 0.8674 - f1_score: 0.8671 - val_loss: 0.3464 - val_accuracy: 0.8599 - val_f1_score: 0.8561\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3245 - accuracy: 0.8641 - f1_score: 0.8634 - val_loss: 0.3892 - val_accuracy: 0.8255 - val_f1_score: 0.8393\n","Epoch 8/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2946 - accuracy: 0.8797 - f1_score: 0.8793 - val_loss: 0.3720 - val_accuracy: 0.8418 - val_f1_score: 0.8477\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2782 - accuracy: 0.8897 - f1_score: 0.8890 - val_loss: 0.3546 - val_accuracy: 0.8635 - val_f1_score: 0.8613\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2918 - accuracy: 0.8755 - f1_score: 0.8754 - val_loss: 0.3476 - val_accuracy: 0.8608 - val_f1_score: 0.8605\n","Epoch 11/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2631 - accuracy: 0.8945 - f1_score: 0.8941 - val_loss: 0.3965 - val_accuracy: 0.8300 - val_f1_score: 0.8431\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3611 - accuracy: 0.8467 - f1_score: 0.8687\n","Epoch 1/20\n","104/104 [==============================] - 19s 40ms/step - loss: 0.6103 - accuracy: 0.6618 - f1_score: 0.6461 - val_loss: 0.5710 - val_accuracy: 0.7152 - val_f1_score: 0.6350\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4744 - accuracy: 0.7752 - f1_score: 0.7683 - val_loss: 0.4316 - val_accuracy: 0.7857 - val_f1_score: 0.7808\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3980 - accuracy: 0.8216 - f1_score: 0.8183 - val_loss: 0.4038 - val_accuracy: 0.8119 - val_f1_score: 0.7961\n","Epoch 4/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.3729 - accuracy: 0.8421 - f1_score: 0.8399 - val_loss: 0.3987 - val_accuracy: 0.8210 - val_f1_score: 0.8296\n","Epoch 5/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3376 - accuracy: 0.8544 - f1_score: 0.8535 - val_loss: 0.3789 - val_accuracy: 0.8264 - val_f1_score: 0.8136\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3167 - accuracy: 0.8638 - f1_score: 0.8627 - val_loss: 0.4248 - val_accuracy: 0.7939 - val_f1_score: 0.8149\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3000 - accuracy: 0.8825 - f1_score: 0.8815 - val_loss: 0.3881 - val_accuracy: 0.8363 - val_f1_score: 0.8224\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2799 - accuracy: 0.8849 - f1_score: 0.8837 - val_loss: 0.3922 - val_accuracy: 0.8273 - val_f1_score: 0.8126\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2650 - accuracy: 0.8987 - f1_score: 0.8975 - val_loss: 0.3948 - val_accuracy: 0.8273 - val_f1_score: 0.8249\n","Epoch 10/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2832 - accuracy: 0.8822 - f1_score: 0.8809 - val_loss: 0.3767 - val_accuracy: 0.8373 - val_f1_score: 0.8289\n","Epoch 11/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.2575 - accuracy: 0.8993 - f1_score: 0.8986 - val_loss: 0.3765 - val_accuracy: 0.8454 - val_f1_score: 0.8435\n","Epoch 12/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2400 - accuracy: 0.9072 - f1_score: 0.9062 - val_loss: 0.3982 - val_accuracy: 0.8409 - val_f1_score: 0.8426\n","Epoch 13/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2301 - accuracy: 0.9063 - f1_score: 0.9057 - val_loss: 0.4128 - val_accuracy: 0.8327 - val_f1_score: 0.8376\n","Epoch 14/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2204 - accuracy: 0.9129 - f1_score: 0.9119 - val_loss: 0.4533 - val_accuracy: 0.8128 - val_f1_score: 0.8276\n","Epoch 15/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2201 - accuracy: 0.9153 - f1_score: 0.9147 - val_loss: 0.4427 - val_accuracy: 0.8481 - val_f1_score: 0.8484\n","Epoch 16/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.1962 - accuracy: 0.9222 - f1_score: 0.9217 - val_loss: 0.4614 - val_accuracy: 0.8282 - val_f1_score: 0.8170\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3373 - accuracy: 0.8670 - f1_score: 0.8883\n","Epoch 1/20\n","104/104 [==============================] - 16s 60ms/step - loss: 0.6425 - accuracy: 0.6200 - f1_score: 0.5894 - val_loss: 0.5258 - val_accuracy: 0.7622 - val_f1_score: 0.7444\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4891 - accuracy: 0.7625 - f1_score: 0.7562 - val_loss: 0.4017 - val_accuracy: 0.8255 - val_f1_score: 0.8332\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3848 - accuracy: 0.8303 - f1_score: 0.8277 - val_loss: 0.3622 - val_accuracy: 0.8508 - val_f1_score: 0.8559\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3524 - accuracy: 0.8535 - f1_score: 0.8517 - val_loss: 0.3570 - val_accuracy: 0.8562 - val_f1_score: 0.8623\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3248 - accuracy: 0.8653 - f1_score: 0.8639 - val_loss: 0.3752 - val_accuracy: 0.8490 - val_f1_score: 0.8586\n","Epoch 6/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3159 - accuracy: 0.8719 - f1_score: 0.8709 - val_loss: 0.3768 - val_accuracy: 0.8291 - val_f1_score: 0.8108\n","Epoch 7/20\n","104/104 [==============================] - 3s 29ms/step - loss: 0.3132 - accuracy: 0.8728 - f1_score: 0.8706 - val_loss: 0.3380 - val_accuracy: 0.8698 - val_f1_score: 0.8717\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2894 - accuracy: 0.8858 - f1_score: 0.8848 - val_loss: 0.3505 - val_accuracy: 0.8580 - val_f1_score: 0.8517\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2814 - accuracy: 0.8858 - f1_score: 0.8839 - val_loss: 0.3689 - val_accuracy: 0.8535 - val_f1_score: 0.8608\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2787 - accuracy: 0.8867 - f1_score: 0.8855 - val_loss: 0.3603 - val_accuracy: 0.8562 - val_f1_score: 0.8604\n","Epoch 11/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2659 - accuracy: 0.8951 - f1_score: 0.8944 - val_loss: 0.3862 - val_accuracy: 0.8553 - val_f1_score: 0.8623\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2662 - accuracy: 0.8894 - f1_score: 0.8881 - val_loss: 0.4722 - val_accuracy: 0.8110 - val_f1_score: 0.8324\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3310 - accuracy: 0.8643 - f1_score: 0.8860\n","Epoch 1/20\n","104/104 [==============================] - 19s 41ms/step - loss: 0.6156 - accuracy: 0.6667 - f1_score: 0.6580 - val_loss: 0.5140 - val_accuracy: 0.7613 - val_f1_score: 0.7452\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4950 - accuracy: 0.7676 - f1_score: 0.7594 - val_loss: 0.4133 - val_accuracy: 0.8101 - val_f1_score: 0.8052\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4224 - accuracy: 0.8053 - f1_score: 0.7998 - val_loss: 0.4060 - val_accuracy: 0.8083 - val_f1_score: 0.8254\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3814 - accuracy: 0.8369 - f1_score: 0.8347 - val_loss: 0.3657 - val_accuracy: 0.8427 - val_f1_score: 0.8482\n","Epoch 5/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.3566 - accuracy: 0.8493 - f1_score: 0.8464 - val_loss: 0.3548 - val_accuracy: 0.8391 - val_f1_score: 0.8463\n","Epoch 6/20\n","104/104 [==============================] - 3s 26ms/step - loss: 0.3353 - accuracy: 0.8571 - f1_score: 0.8555 - val_loss: 0.3417 - val_accuracy: 0.8553 - val_f1_score: 0.8485\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3169 - accuracy: 0.8659 - f1_score: 0.8636 - val_loss: 0.3997 - val_accuracy: 0.8282 - val_f1_score: 0.8440\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3006 - accuracy: 0.8785 - f1_score: 0.8774 - val_loss: 0.4283 - val_accuracy: 0.8074 - val_f1_score: 0.8311\n","Epoch 9/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3032 - accuracy: 0.8716 - f1_score: 0.8706 - val_loss: 0.3325 - val_accuracy: 0.8689 - val_f1_score: 0.8641\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2736 - accuracy: 0.8918 - f1_score: 0.8905 - val_loss: 0.3274 - val_accuracy: 0.8626 - val_f1_score: 0.8621\n","Epoch 11/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2655 - accuracy: 0.8960 - f1_score: 0.8951 - val_loss: 0.3353 - val_accuracy: 0.8608 - val_f1_score: 0.8615\n","Epoch 12/20\n","104/104 [==============================] - 3s 28ms/step - loss: 0.2708 - accuracy: 0.8924 - f1_score: 0.8918 - val_loss: 0.3372 - val_accuracy: 0.8680 - val_f1_score: 0.8673\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2528 - accuracy: 0.9002 - f1_score: 0.8993 - val_loss: 0.3949 - val_accuracy: 0.8454 - val_f1_score: 0.8325\n","Epoch 14/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2464 - accuracy: 0.9027 - f1_score: 0.9013 - val_loss: 0.3574 - val_accuracy: 0.8662 - val_f1_score: 0.8722\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2256 - accuracy: 0.9108 - f1_score: 0.9099 - val_loss: 0.3663 - val_accuracy: 0.8562 - val_f1_score: 0.8621\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8542 - f1_score: 0.8763\n","Epoch 1/20\n","104/104 [==============================] - 17s 60ms/step - loss: 0.6266 - accuracy: 0.6519 - f1_score: 0.6380 - val_loss: 0.5083 - val_accuracy: 0.7622 - val_f1_score: 0.7553\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4808 - accuracy: 0.7709 - f1_score: 0.7612 - val_loss: 0.4235 - val_accuracy: 0.8047 - val_f1_score: 0.8131\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3993 - accuracy: 0.8228 - f1_score: 0.8178 - val_loss: 0.3725 - val_accuracy: 0.8391 - val_f1_score: 0.8198\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3598 - accuracy: 0.8439 - f1_score: 0.8408 - val_loss: 0.3173 - val_accuracy: 0.8743 - val_f1_score: 0.8764\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3226 - accuracy: 0.8635 - f1_score: 0.8625 - val_loss: 0.3002 - val_accuracy: 0.8807 - val_f1_score: 0.8796\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3035 - accuracy: 0.8776 - f1_score: 0.8772 - val_loss: 0.2911 - val_accuracy: 0.8879 - val_f1_score: 0.8845\n","Epoch 7/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2946 - accuracy: 0.8807 - f1_score: 0.8804 - val_loss: 0.2927 - val_accuracy: 0.8816 - val_f1_score: 0.8772\n","Epoch 8/20\n","104/104 [==============================] - 3s 30ms/step - loss: 0.2906 - accuracy: 0.8837 - f1_score: 0.8838 - val_loss: 0.2818 - val_accuracy: 0.8825 - val_f1_score: 0.8814\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2847 - accuracy: 0.8867 - f1_score: 0.8863 - val_loss: 0.3070 - val_accuracy: 0.8671 - val_f1_score: 0.8738\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2678 - accuracy: 0.8912 - f1_score: 0.8904 - val_loss: 0.2837 - val_accuracy: 0.8816 - val_f1_score: 0.8806\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2537 - accuracy: 0.8963 - f1_score: 0.8958 - val_loss: 0.2959 - val_accuracy: 0.8761 - val_f1_score: 0.8711\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2602 - accuracy: 0.8945 - f1_score: 0.8936 - val_loss: 0.2897 - val_accuracy: 0.8797 - val_f1_score: 0.8832\n","Epoch 13/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2265 - accuracy: 0.9129 - f1_score: 0.9128 - val_loss: 0.3014 - val_accuracy: 0.8671 - val_f1_score: 0.8662\n","47/47 [==============================] - 0s 8ms/step - loss: 0.3267 - accuracy: 0.8562 - f1_score: 0.8811\n","Epoch 1/20\n","104/104 [==============================] - 18s 75ms/step - loss: 0.6144 - accuracy: 0.6519 - f1_score: 0.6480 - val_loss: 0.5345 - val_accuracy: 0.7441 - val_f1_score: 0.7604\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4674 - accuracy: 0.7812 - f1_score: 0.7752 - val_loss: 0.4113 - val_accuracy: 0.8273 - val_f1_score: 0.8374\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4055 - accuracy: 0.8219 - f1_score: 0.8175 - val_loss: 0.3862 - val_accuracy: 0.8255 - val_f1_score: 0.8404\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3534 - accuracy: 0.8517 - f1_score: 0.8495 - val_loss: 0.3316 - val_accuracy: 0.8544 - val_f1_score: 0.8574\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3240 - accuracy: 0.8635 - f1_score: 0.8623 - val_loss: 0.3315 - val_accuracy: 0.8580 - val_f1_score: 0.8662\n","Epoch 6/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.3129 - accuracy: 0.8680 - f1_score: 0.8670 - val_loss: 0.3116 - val_accuracy: 0.8671 - val_f1_score: 0.8645\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2920 - accuracy: 0.8794 - f1_score: 0.8785 - val_loss: 0.3176 - val_accuracy: 0.8707 - val_f1_score: 0.8755\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2874 - accuracy: 0.8843 - f1_score: 0.8835 - val_loss: 0.2988 - val_accuracy: 0.8761 - val_f1_score: 0.8762\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2820 - accuracy: 0.8873 - f1_score: 0.8866 - val_loss: 0.3090 - val_accuracy: 0.8716 - val_f1_score: 0.8642\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2774 - accuracy: 0.8816 - f1_score: 0.8811 - val_loss: 0.3529 - val_accuracy: 0.8490 - val_f1_score: 0.8612\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2505 - accuracy: 0.9008 - f1_score: 0.9004 - val_loss: 0.2960 - val_accuracy: 0.8816 - val_f1_score: 0.8775\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2297 - accuracy: 0.9093 - f1_score: 0.9086 - val_loss: 0.3188 - val_accuracy: 0.8716 - val_f1_score: 0.8761\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2275 - accuracy: 0.9096 - f1_score: 0.9092 - val_loss: 0.3234 - val_accuracy: 0.8761 - val_f1_score: 0.8762\n","Epoch 14/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2218 - accuracy: 0.9093 - f1_score: 0.9084 - val_loss: 0.3886 - val_accuracy: 0.8599 - val_f1_score: 0.8701\n","Epoch 15/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2076 - accuracy: 0.9192 - f1_score: 0.9185 - val_loss: 0.4326 - val_accuracy: 0.8345 - val_f1_score: 0.8513\n","Epoch 16/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2153 - accuracy: 0.9126 - f1_score: 0.9121 - val_loss: 0.3192 - val_accuracy: 0.8761 - val_f1_score: 0.8767\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3696 - accuracy: 0.8447 - f1_score: 0.8674\n","Epoch 1/20\n","104/104 [==============================] - 12s 34ms/step - loss: 0.6307 - accuracy: 0.6492 - f1_score: 0.6594 - val_loss: 0.5319 - val_accuracy: 0.7396 - val_f1_score: 0.7278\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4839 - accuracy: 0.7700 - f1_score: 0.7633 - val_loss: 0.4513 - val_accuracy: 0.7939 - val_f1_score: 0.8071\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3979 - accuracy: 0.8291 - f1_score: 0.8252 - val_loss: 0.3999 - val_accuracy: 0.8282 - val_f1_score: 0.8368\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3608 - accuracy: 0.8403 - f1_score: 0.8368 - val_loss: 0.4086 - val_accuracy: 0.8282 - val_f1_score: 0.8430\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3270 - accuracy: 0.8647 - f1_score: 0.8626 - val_loss: 0.3637 - val_accuracy: 0.8526 - val_f1_score: 0.8594\n","Epoch 6/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3102 - accuracy: 0.8689 - f1_score: 0.8682 - val_loss: 0.3302 - val_accuracy: 0.8617 - val_f1_score: 0.8650\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2954 - accuracy: 0.8776 - f1_score: 0.8770 - val_loss: 0.4005 - val_accuracy: 0.8192 - val_f1_score: 0.8387\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2944 - accuracy: 0.8779 - f1_score: 0.8766 - val_loss: 0.3337 - val_accuracy: 0.8698 - val_f1_score: 0.8737\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2755 - accuracy: 0.8906 - f1_score: 0.8894 - val_loss: 0.3402 - val_accuracy: 0.8680 - val_f1_score: 0.8735\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2592 - accuracy: 0.8963 - f1_score: 0.8956 - val_loss: 0.3208 - val_accuracy: 0.8734 - val_f1_score: 0.8759\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2473 - accuracy: 0.9005 - f1_score: 0.8998 - val_loss: 0.3615 - val_accuracy: 0.8472 - val_f1_score: 0.8574\n","Epoch 12/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2442 - accuracy: 0.9011 - f1_score: 0.9005 - val_loss: 0.3238 - val_accuracy: 0.8797 - val_f1_score: 0.8816\n","Epoch 13/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2393 - accuracy: 0.9060 - f1_score: 0.9048 - val_loss: 0.3308 - val_accuracy: 0.8725 - val_f1_score: 0.8738\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2330 - accuracy: 0.9045 - f1_score: 0.9040 - val_loss: 0.3544 - val_accuracy: 0.8562 - val_f1_score: 0.8647\n","Epoch 15/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2042 - accuracy: 0.9244 - f1_score: 0.9241 - val_loss: 0.3445 - val_accuracy: 0.8571 - val_f1_score: 0.8489\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3348 - accuracy: 0.8602 - f1_score: 0.8860\n","Epoch 1/20\n","104/104 [==============================] - 12s 37ms/step - loss: 0.6171 - accuracy: 0.6655 - f1_score: 0.6426 - val_loss: 0.5131 - val_accuracy: 0.7450 - val_f1_score: 0.7309\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4811 - accuracy: 0.7746 - f1_score: 0.7665 - val_loss: 0.4215 - val_accuracy: 0.8110 - val_f1_score: 0.8168\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4007 - accuracy: 0.8125 - f1_score: 0.8099 - val_loss: 0.3716 - val_accuracy: 0.8192 - val_f1_score: 0.8162\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3628 - accuracy: 0.8397 - f1_score: 0.8378 - val_loss: 0.3422 - val_accuracy: 0.8508 - val_f1_score: 0.8504\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3297 - accuracy: 0.8617 - f1_score: 0.8604 - val_loss: 0.3358 - val_accuracy: 0.8544 - val_f1_score: 0.8618\n","Epoch 6/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3100 - accuracy: 0.8653 - f1_score: 0.8642 - val_loss: 0.4156 - val_accuracy: 0.8065 - val_f1_score: 0.8315\n","Epoch 7/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3084 - accuracy: 0.8740 - f1_score: 0.8726 - val_loss: 0.3096 - val_accuracy: 0.8716 - val_f1_score: 0.8748\n","Epoch 8/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2814 - accuracy: 0.8837 - f1_score: 0.8832 - val_loss: 0.3735 - val_accuracy: 0.8264 - val_f1_score: 0.7996\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2752 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3015 - val_accuracy: 0.8843 - val_f1_score: 0.8838\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2547 - accuracy: 0.8960 - f1_score: 0.8957 - val_loss: 0.3145 - val_accuracy: 0.8734 - val_f1_score: 0.8672\n","Epoch 11/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2436 - accuracy: 0.9054 - f1_score: 0.9044 - val_loss: 0.3260 - val_accuracy: 0.8671 - val_f1_score: 0.8609\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2413 - accuracy: 0.9017 - f1_score: 0.9012 - val_loss: 0.3275 - val_accuracy: 0.8626 - val_f1_score: 0.8552\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2153 - accuracy: 0.9183 - f1_score: 0.9181 - val_loss: 0.3473 - val_accuracy: 0.8626 - val_f1_score: 0.8590\n","Epoch 14/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2137 - accuracy: 0.9219 - f1_score: 0.9218 - val_loss: 0.3448 - val_accuracy: 0.8662 - val_f1_score: 0.8622\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3332 - accuracy: 0.8636 - f1_score: 0.8894\n","Epoch 1/20\n","104/104 [==============================] - 13s 52ms/step - loss: 0.6363 - accuracy: 0.6483 - f1_score: 0.6258 - val_loss: 0.5350 - val_accuracy: 0.7450 - val_f1_score: 0.7634\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5024 - accuracy: 0.7646 - f1_score: 0.7551 - val_loss: 0.4479 - val_accuracy: 0.7966 - val_f1_score: 0.7996\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4173 - accuracy: 0.8056 - f1_score: 0.7996 - val_loss: 0.4130 - val_accuracy: 0.8192 - val_f1_score: 0.8252\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4216 - accuracy: 0.8065 - f1_score: 0.8015 - val_loss: 0.4455 - val_accuracy: 0.8137 - val_f1_score: 0.7845\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3623 - accuracy: 0.8424 - f1_score: 0.8403 - val_loss: 0.3653 - val_accuracy: 0.8490 - val_f1_score: 0.8444\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3289 - accuracy: 0.8596 - f1_score: 0.8579 - val_loss: 0.3594 - val_accuracy: 0.8436 - val_f1_score: 0.8486\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3055 - accuracy: 0.8707 - f1_score: 0.8697 - val_loss: 0.3438 - val_accuracy: 0.8580 - val_f1_score: 0.8550\n","Epoch 8/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2911 - accuracy: 0.8870 - f1_score: 0.8861 - val_loss: 0.3784 - val_accuracy: 0.8345 - val_f1_score: 0.8123\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2877 - accuracy: 0.8912 - f1_score: 0.8898 - val_loss: 0.3335 - val_accuracy: 0.8626 - val_f1_score: 0.8574\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2848 - accuracy: 0.8861 - f1_score: 0.8852 - val_loss: 0.3357 - val_accuracy: 0.8580 - val_f1_score: 0.8503\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2535 - accuracy: 0.8972 - f1_score: 0.8968 - val_loss: 0.3369 - val_accuracy: 0.8617 - val_f1_score: 0.8642\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2521 - accuracy: 0.8987 - f1_score: 0.8978 - val_loss: 0.3428 - val_accuracy: 0.8635 - val_f1_score: 0.8595\n","Epoch 13/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2330 - accuracy: 0.9135 - f1_score: 0.9130 - val_loss: 0.3425 - val_accuracy: 0.8662 - val_f1_score: 0.8601\n","Epoch 14/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.2258 - accuracy: 0.9153 - f1_score: 0.9146 - val_loss: 0.3359 - val_accuracy: 0.8725 - val_f1_score: 0.8703\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3388 - accuracy: 0.8521 - f1_score: 0.8783\n","Epoch 1/20\n","104/104 [==============================] - 13s 35ms/step - loss: 0.6246 - accuracy: 0.6537 - f1_score: 0.6381 - val_loss: 0.5426 - val_accuracy: 0.7278 - val_f1_score: 0.7531\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4929 - accuracy: 0.7580 - f1_score: 0.7485 - val_loss: 0.4194 - val_accuracy: 0.8056 - val_f1_score: 0.7935\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4040 - accuracy: 0.8137 - f1_score: 0.8101 - val_loss: 0.4006 - val_accuracy: 0.8255 - val_f1_score: 0.8083\n","Epoch 4/20\n","104/104 [==============================] - 3s 24ms/step - loss: 0.3580 - accuracy: 0.8424 - f1_score: 0.8390 - val_loss: 0.3620 - val_accuracy: 0.8481 - val_f1_score: 0.8406\n","Epoch 5/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.3221 - accuracy: 0.8599 - f1_score: 0.8586 - val_loss: 0.3599 - val_accuracy: 0.8445 - val_f1_score: 0.8510\n","Epoch 6/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3024 - accuracy: 0.8794 - f1_score: 0.8784 - val_loss: 0.3445 - val_accuracy: 0.8599 - val_f1_score: 0.8617\n","Epoch 7/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2994 - accuracy: 0.8770 - f1_score: 0.8762 - val_loss: 0.3518 - val_accuracy: 0.8499 - val_f1_score: 0.8541\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2892 - accuracy: 0.8761 - f1_score: 0.8750 - val_loss: 0.3294 - val_accuracy: 0.8580 - val_f1_score: 0.8529\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2716 - accuracy: 0.8924 - f1_score: 0.8916 - val_loss: 0.3855 - val_accuracy: 0.8318 - val_f1_score: 0.8453\n","Epoch 10/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2689 - accuracy: 0.8963 - f1_score: 0.8960 - val_loss: 0.4109 - val_accuracy: 0.8354 - val_f1_score: 0.8124\n","Epoch 11/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2505 - accuracy: 0.9014 - f1_score: 0.9012 - val_loss: 0.3678 - val_accuracy: 0.8662 - val_f1_score: 0.8606\n","Epoch 12/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.2341 - accuracy: 0.9099 - f1_score: 0.9096 - val_loss: 0.3984 - val_accuracy: 0.8463 - val_f1_score: 0.8559\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2409 - accuracy: 0.9017 - f1_score: 0.9010 - val_loss: 0.3552 - val_accuracy: 0.8553 - val_f1_score: 0.8470\n","47/47 [==============================] - 0s 7ms/step - loss: 0.3166 - accuracy: 0.8690 - f1_score: 0.8932\n","Epoch 1/20\n","104/104 [==============================] - 14s 47ms/step - loss: 0.6250 - accuracy: 0.6612 - f1_score: 0.6546 - val_loss: 0.5176 - val_accuracy: 0.7523 - val_f1_score: 0.7355\n","Epoch 2/20\n","104/104 [==============================] - 2s 24ms/step - loss: 0.5028 - accuracy: 0.7586 - f1_score: 0.7519 - val_loss: 0.4651 - val_accuracy: 0.7703 - val_f1_score: 0.7269\n","Epoch 3/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4343 - accuracy: 0.7996 - f1_score: 0.7917 - val_loss: 0.4048 - val_accuracy: 0.8273 - val_f1_score: 0.8073\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3769 - accuracy: 0.8327 - f1_score: 0.8290 - val_loss: 0.4499 - val_accuracy: 0.8029 - val_f1_score: 0.7671\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3659 - accuracy: 0.8336 - f1_score: 0.8314 - val_loss: 0.4581 - val_accuracy: 0.7984 - val_f1_score: 0.7584\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3276 - accuracy: 0.8638 - f1_score: 0.8615 - val_loss: 0.3301 - val_accuracy: 0.8707 - val_f1_score: 0.8675\n","Epoch 7/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3110 - accuracy: 0.8734 - f1_score: 0.8721 - val_loss: 0.3148 - val_accuracy: 0.8788 - val_f1_score: 0.8755\n","Epoch 8/20\n","104/104 [==============================] - 3s 25ms/step - loss: 0.2959 - accuracy: 0.8788 - f1_score: 0.8772 - val_loss: 0.3334 - val_accuracy: 0.8617 - val_f1_score: 0.8680\n","Epoch 9/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2893 - accuracy: 0.8816 - f1_score: 0.8810 - val_loss: 0.3202 - val_accuracy: 0.8761 - val_f1_score: 0.8704\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2719 - accuracy: 0.8891 - f1_score: 0.8879 - val_loss: 0.3125 - val_accuracy: 0.8743 - val_f1_score: 0.8702\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2661 - accuracy: 0.8921 - f1_score: 0.8909 - val_loss: 0.3145 - val_accuracy: 0.8870 - val_f1_score: 0.8869\n","Epoch 12/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2603 - accuracy: 0.8930 - f1_score: 0.8918 - val_loss: 0.3149 - val_accuracy: 0.8825 - val_f1_score: 0.8825\n","Epoch 13/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2492 - accuracy: 0.9002 - f1_score: 0.8993 - val_loss: 0.3733 - val_accuracy: 0.8508 - val_f1_score: 0.8332\n","Epoch 14/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2532 - accuracy: 0.8978 - f1_score: 0.8969 - val_loss: 0.3866 - val_accuracy: 0.8608 - val_f1_score: 0.8466\n","Epoch 15/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2285 - accuracy: 0.9081 - f1_score: 0.9065 - val_loss: 0.3302 - val_accuracy: 0.8861 - val_f1_score: 0.8857\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.8623 - f1_score: 0.8904\n","Epoch 1/20\n","104/104 [==============================] - 12s 35ms/step - loss: 0.6338 - accuracy: 0.6450 - f1_score: 0.6490 - val_loss: 0.5630 - val_accuracy: 0.7043 - val_f1_score: 0.6305\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4796 - accuracy: 0.7767 - f1_score: 0.7682 - val_loss: 0.4107 - val_accuracy: 0.8183 - val_f1_score: 0.8220\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4034 - accuracy: 0.8186 - f1_score: 0.8151 - val_loss: 0.4125 - val_accuracy: 0.8119 - val_f1_score: 0.7873\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3516 - accuracy: 0.8505 - f1_score: 0.8470 - val_loss: 0.4781 - val_accuracy: 0.7975 - val_f1_score: 0.8225\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3239 - accuracy: 0.8641 - f1_score: 0.8620 - val_loss: 0.4065 - val_accuracy: 0.8264 - val_f1_score: 0.8072\n","Epoch 6/20\n","104/104 [==============================] - 3s 27ms/step - loss: 0.3306 - accuracy: 0.8599 - f1_score: 0.8579 - val_loss: 0.3532 - val_accuracy: 0.8608 - val_f1_score: 0.8582\n","Epoch 7/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2884 - accuracy: 0.8828 - f1_score: 0.8813 - val_loss: 0.3557 - val_accuracy: 0.8644 - val_f1_score: 0.8680\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2712 - accuracy: 0.8891 - f1_score: 0.8879 - val_loss: 0.3536 - val_accuracy: 0.8671 - val_f1_score: 0.8625\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2585 - accuracy: 0.8909 - f1_score: 0.8892 - val_loss: 0.3692 - val_accuracy: 0.8680 - val_f1_score: 0.8710\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2551 - accuracy: 0.8984 - f1_score: 0.8967 - val_loss: 0.4309 - val_accuracy: 0.8454 - val_f1_score: 0.8567\n","Epoch 11/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2465 - accuracy: 0.8978 - f1_score: 0.8967 - val_loss: 0.3878 - val_accuracy: 0.8463 - val_f1_score: 0.8540\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3394 - accuracy: 0.8629 - f1_score: 0.8865\n","Epoch 1/20\n","104/104 [==============================] - 8s 31ms/step - loss: 0.6241 - accuracy: 0.6546 - f1_score: 0.6623 - val_loss: 0.5005 - val_accuracy: 0.7740 - val_f1_score: 0.7646\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4771 - accuracy: 0.7758 - f1_score: 0.7660 - val_loss: 0.4610 - val_accuracy: 0.7875 - val_f1_score: 0.8133\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4023 - accuracy: 0.8098 - f1_score: 0.8037 - val_loss: 0.3766 - val_accuracy: 0.8318 - val_f1_score: 0.8176\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3607 - accuracy: 0.8363 - f1_score: 0.8323 - val_loss: 0.4417 - val_accuracy: 0.7893 - val_f1_score: 0.8190\n","Epoch 5/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3424 - accuracy: 0.8466 - f1_score: 0.8444 - val_loss: 0.6629 - val_accuracy: 0.7170 - val_f1_score: 0.7740\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3169 - accuracy: 0.8653 - f1_score: 0.8629 - val_loss: 0.4135 - val_accuracy: 0.8174 - val_f1_score: 0.8358\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3078 - accuracy: 0.8755 - f1_score: 0.8734 - val_loss: 0.3302 - val_accuracy: 0.8761 - val_f1_score: 0.8749\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2943 - accuracy: 0.8791 - f1_score: 0.8772 - val_loss: 0.3388 - val_accuracy: 0.8571 - val_f1_score: 0.8638\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2785 - accuracy: 0.8864 - f1_score: 0.8848 - val_loss: 0.5810 - val_accuracy: 0.7495 - val_f1_score: 0.7953\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2706 - accuracy: 0.8912 - f1_score: 0.8902 - val_loss: 0.4500 - val_accuracy: 0.8128 - val_f1_score: 0.8345\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2502 - accuracy: 0.8981 - f1_score: 0.8967 - val_loss: 0.3850 - val_accuracy: 0.8553 - val_f1_score: 0.8632\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2386 - accuracy: 0.9051 - f1_score: 0.9040 - val_loss: 0.3632 - val_accuracy: 0.8436 - val_f1_score: 0.8547\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8454 - f1_score: 0.8700\n","Epoch 1/20\n","104/104 [==============================] - 8s 22ms/step - loss: 0.6120 - accuracy: 0.6615 - f1_score: 0.6476 - val_loss: 0.5141 - val_accuracy: 0.7505 - val_f1_score: 0.7166\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4701 - accuracy: 0.7709 - f1_score: 0.7615 - val_loss: 0.4408 - val_accuracy: 0.7984 - val_f1_score: 0.8076\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4075 - accuracy: 0.8107 - f1_score: 0.8059 - val_loss: 0.3951 - val_accuracy: 0.8228 - val_f1_score: 0.8108\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3685 - accuracy: 0.8409 - f1_score: 0.8382 - val_loss: 0.3628 - val_accuracy: 0.8373 - val_f1_score: 0.8390\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3346 - accuracy: 0.8547 - f1_score: 0.8524 - val_loss: 0.4066 - val_accuracy: 0.8101 - val_f1_score: 0.8304\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3158 - accuracy: 0.8647 - f1_score: 0.8629 - val_loss: 0.3501 - val_accuracy: 0.8436 - val_f1_score: 0.8518\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3004 - accuracy: 0.8737 - f1_score: 0.8716 - val_loss: 0.3387 - val_accuracy: 0.8562 - val_f1_score: 0.8626\n","Epoch 8/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3027 - accuracy: 0.8746 - f1_score: 0.8739 - val_loss: 0.3283 - val_accuracy: 0.8608 - val_f1_score: 0.8528\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2864 - accuracy: 0.8870 - f1_score: 0.8857 - val_loss: 0.3166 - val_accuracy: 0.8644 - val_f1_score: 0.8601\n","Epoch 10/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2780 - accuracy: 0.8825 - f1_score: 0.8810 - val_loss: 0.3257 - val_accuracy: 0.8599 - val_f1_score: 0.8646\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2605 - accuracy: 0.8939 - f1_score: 0.8927 - val_loss: 0.3129 - val_accuracy: 0.8644 - val_f1_score: 0.8614\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2547 - accuracy: 0.8984 - f1_score: 0.8978 - val_loss: 0.3137 - val_accuracy: 0.8653 - val_f1_score: 0.8614\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2464 - accuracy: 0.9020 - f1_score: 0.9011 - val_loss: 0.3602 - val_accuracy: 0.8409 - val_f1_score: 0.8526\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2309 - accuracy: 0.9066 - f1_score: 0.9057 - val_loss: 0.3564 - val_accuracy: 0.8517 - val_f1_score: 0.8414\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2189 - accuracy: 0.9084 - f1_score: 0.9074 - val_loss: 0.3570 - val_accuracy: 0.8454 - val_f1_score: 0.8567\n","Epoch 16/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2116 - accuracy: 0.9141 - f1_score: 0.9133 - val_loss: 0.3759 - val_accuracy: 0.8490 - val_f1_score: 0.8358\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8562 - f1_score: 0.8816\n","Epoch 1/20\n","104/104 [==============================] - 7s 20ms/step - loss: 0.6194 - accuracy: 0.6621 - f1_score: 0.6321 - val_loss: 0.5329 - val_accuracy: 0.7468 - val_f1_score: 0.7705\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4775 - accuracy: 0.7731 - f1_score: 0.7656 - val_loss: 0.4450 - val_accuracy: 0.7957 - val_f1_score: 0.8007\n","Epoch 3/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3995 - accuracy: 0.8207 - f1_score: 0.8166 - val_loss: 0.3932 - val_accuracy: 0.8273 - val_f1_score: 0.8338\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3549 - accuracy: 0.8457 - f1_score: 0.8440 - val_loss: 0.3924 - val_accuracy: 0.8255 - val_f1_score: 0.8056\n","Epoch 5/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3468 - accuracy: 0.8514 - f1_score: 0.8492 - val_loss: 0.3435 - val_accuracy: 0.8517 - val_f1_score: 0.8525\n","Epoch 6/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3091 - accuracy: 0.8719 - f1_score: 0.8705 - val_loss: 0.4636 - val_accuracy: 0.7939 - val_f1_score: 0.8233\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2952 - accuracy: 0.8764 - f1_score: 0.8742 - val_loss: 0.3299 - val_accuracy: 0.8653 - val_f1_score: 0.8639\n","Epoch 8/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2786 - accuracy: 0.8840 - f1_score: 0.8844 - val_loss: 0.3745 - val_accuracy: 0.8336 - val_f1_score: 0.8474\n","Epoch 9/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2691 - accuracy: 0.8888 - f1_score: 0.8881 - val_loss: 0.3242 - val_accuracy: 0.8635 - val_f1_score: 0.8631\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2497 - accuracy: 0.8996 - f1_score: 0.8988 - val_loss: 0.3533 - val_accuracy: 0.8544 - val_f1_score: 0.8606\n","Epoch 11/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2456 - accuracy: 0.8963 - f1_score: 0.8954 - val_loss: 0.3493 - val_accuracy: 0.8571 - val_f1_score: 0.8645\n","Epoch 12/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2444 - accuracy: 0.9069 - f1_score: 0.9063 - val_loss: 0.3345 - val_accuracy: 0.8626 - val_f1_score: 0.8530\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2244 - accuracy: 0.9111 - f1_score: 0.9105 - val_loss: 0.3468 - val_accuracy: 0.8698 - val_f1_score: 0.8674\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2056 - accuracy: 0.9156 - f1_score: 0.9152 - val_loss: 0.3486 - val_accuracy: 0.8626 - val_f1_score: 0.8558\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8562 - f1_score: 0.8797\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6248 - accuracy: 0.6483 - f1_score: 0.6475 - val_loss: 0.5292 - val_accuracy: 0.7532 - val_f1_score: 0.7460\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4902 - accuracy: 0.7746 - f1_score: 0.7673 - val_loss: 0.5416 - val_accuracy: 0.7251 - val_f1_score: 0.6415\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4030 - accuracy: 0.8177 - f1_score: 0.8131 - val_loss: 0.3970 - val_accuracy: 0.8165 - val_f1_score: 0.8046\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3621 - accuracy: 0.8409 - f1_score: 0.8368 - val_loss: 0.3928 - val_accuracy: 0.8174 - val_f1_score: 0.8297\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3432 - accuracy: 0.8556 - f1_score: 0.8549 - val_loss: 0.3522 - val_accuracy: 0.8535 - val_f1_score: 0.8436\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3100 - accuracy: 0.8677 - f1_score: 0.8667 - val_loss: 0.3540 - val_accuracy: 0.8436 - val_f1_score: 0.8505\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3028 - accuracy: 0.8683 - f1_score: 0.8670 - val_loss: 0.3543 - val_accuracy: 0.8544 - val_f1_score: 0.8426\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2857 - accuracy: 0.8834 - f1_score: 0.8825 - val_loss: 0.3558 - val_accuracy: 0.8445 - val_f1_score: 0.8512\n","Epoch 9/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2804 - accuracy: 0.8855 - f1_score: 0.8842 - val_loss: 0.3234 - val_accuracy: 0.8653 - val_f1_score: 0.8629\n","Epoch 10/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2622 - accuracy: 0.8942 - f1_score: 0.8933 - val_loss: 0.3354 - val_accuracy: 0.8617 - val_f1_score: 0.8592\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2474 - accuracy: 0.9008 - f1_score: 0.9002 - val_loss: 0.3279 - val_accuracy: 0.8698 - val_f1_score: 0.8669\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2429 - accuracy: 0.9027 - f1_score: 0.9014 - val_loss: 0.3360 - val_accuracy: 0.8644 - val_f1_score: 0.8624\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2260 - accuracy: 0.9096 - f1_score: 0.9087 - val_loss: 0.3735 - val_accuracy: 0.8599 - val_f1_score: 0.8579\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2320 - accuracy: 0.9102 - f1_score: 0.9089 - val_loss: 0.3857 - val_accuracy: 0.8463 - val_f1_score: 0.8310\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.8555 - f1_score: 0.8803\n","Epoch 1/20\n","104/104 [==============================] - 7s 23ms/step - loss: 0.6236 - accuracy: 0.6558 - f1_score: 0.6617 - val_loss: 0.5148 - val_accuracy: 0.7685 - val_f1_score: 0.7440\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.5039 - accuracy: 0.7625 - f1_score: 0.7527 - val_loss: 0.4067 - val_accuracy: 0.8219 - val_f1_score: 0.8133\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4274 - accuracy: 0.7975 - f1_score: 0.7914 - val_loss: 0.3677 - val_accuracy: 0.8481 - val_f1_score: 0.8343\n","Epoch 4/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3611 - accuracy: 0.8433 - f1_score: 0.8418 - val_loss: 0.3571 - val_accuracy: 0.8517 - val_f1_score: 0.8370\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3523 - accuracy: 0.8505 - f1_score: 0.8489 - val_loss: 0.3743 - val_accuracy: 0.8327 - val_f1_score: 0.8087\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3094 - accuracy: 0.8779 - f1_score: 0.8761 - val_loss: 0.3225 - val_accuracy: 0.8626 - val_f1_score: 0.8694\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3031 - accuracy: 0.8716 - f1_score: 0.8708 - val_loss: 0.2982 - val_accuracy: 0.8816 - val_f1_score: 0.8784\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2882 - accuracy: 0.8831 - f1_score: 0.8817 - val_loss: 0.3126 - val_accuracy: 0.8680 - val_f1_score: 0.8612\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2849 - accuracy: 0.8837 - f1_score: 0.8824 - val_loss: 0.2967 - val_accuracy: 0.8843 - val_f1_score: 0.8821\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2695 - accuracy: 0.8897 - f1_score: 0.8879 - val_loss: 0.3361 - val_accuracy: 0.8526 - val_f1_score: 0.8355\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2741 - accuracy: 0.8882 - f1_score: 0.8864 - val_loss: 0.3015 - val_accuracy: 0.8761 - val_f1_score: 0.8793\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2472 - accuracy: 0.9042 - f1_score: 0.9027 - val_loss: 0.3009 - val_accuracy: 0.8807 - val_f1_score: 0.8782\n","Epoch 13/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2412 - accuracy: 0.9036 - f1_score: 0.9029 - val_loss: 0.3327 - val_accuracy: 0.8689 - val_f1_score: 0.8588\n","Epoch 14/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2517 - accuracy: 0.9008 - f1_score: 0.8997 - val_loss: 0.3183 - val_accuracy: 0.8752 - val_f1_score: 0.8779\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8575 - f1_score: 0.8848\n","Epoch 1/20\n","104/104 [==============================] - 9s 24ms/step - loss: 0.6057 - accuracy: 0.6799 - f1_score: 0.6828 - val_loss: 0.5250 - val_accuracy: 0.7423 - val_f1_score: 0.7623\n","Epoch 2/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.4633 - accuracy: 0.7872 - f1_score: 0.7826 - val_loss: 0.4484 - val_accuracy: 0.7866 - val_f1_score: 0.7944\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3959 - accuracy: 0.8258 - f1_score: 0.8209 - val_loss: 0.4437 - val_accuracy: 0.7966 - val_f1_score: 0.8088\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3750 - accuracy: 0.8351 - f1_score: 0.8353 - val_loss: 0.3904 - val_accuracy: 0.8237 - val_f1_score: 0.8169\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3357 - accuracy: 0.8556 - f1_score: 0.8537 - val_loss: 0.3812 - val_accuracy: 0.8427 - val_f1_score: 0.8343\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3376 - accuracy: 0.8535 - f1_score: 0.8514 - val_loss: 0.3759 - val_accuracy: 0.8400 - val_f1_score: 0.8413\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3063 - accuracy: 0.8782 - f1_score: 0.8771 - val_loss: 0.3906 - val_accuracy: 0.8436 - val_f1_score: 0.8391\n","Epoch 8/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2923 - accuracy: 0.8779 - f1_score: 0.8775 - val_loss: 0.3715 - val_accuracy: 0.8508 - val_f1_score: 0.8462\n","Epoch 9/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2725 - accuracy: 0.8897 - f1_score: 0.8887 - val_loss: 0.3651 - val_accuracy: 0.8517 - val_f1_score: 0.8504\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2677 - accuracy: 0.8888 - f1_score: 0.8881 - val_loss: 0.3783 - val_accuracy: 0.8526 - val_f1_score: 0.8596\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2552 - accuracy: 0.9027 - f1_score: 0.9024 - val_loss: 0.3942 - val_accuracy: 0.8490 - val_f1_score: 0.8342\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2465 - accuracy: 0.9024 - f1_score: 0.9019 - val_loss: 0.3677 - val_accuracy: 0.8445 - val_f1_score: 0.8540\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2287 - accuracy: 0.9072 - f1_score: 0.9072 - val_loss: 0.3665 - val_accuracy: 0.8580 - val_f1_score: 0.8584\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2188 - accuracy: 0.9186 - f1_score: 0.9182 - val_loss: 0.3878 - val_accuracy: 0.8535 - val_f1_score: 0.8556\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8508 - f1_score: 0.8797\n","Epoch 1/20\n","104/104 [==============================] - 8s 34ms/step - loss: 0.6231 - accuracy: 0.6709 - f1_score: 0.6709 - val_loss: 0.5118 - val_accuracy: 0.7486 - val_f1_score: 0.6965\n","Epoch 2/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4851 - accuracy: 0.7743 - f1_score: 0.7663 - val_loss: 0.3721 - val_accuracy: 0.8327 - val_f1_score: 0.8226\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4069 - accuracy: 0.8156 - f1_score: 0.8110 - val_loss: 0.3311 - val_accuracy: 0.8626 - val_f1_score: 0.8530\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3635 - accuracy: 0.8391 - f1_score: 0.8370 - val_loss: 0.3035 - val_accuracy: 0.8852 - val_f1_score: 0.8853\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3525 - accuracy: 0.8526 - f1_score: 0.8507 - val_loss: 0.3537 - val_accuracy: 0.8400 - val_f1_score: 0.8562\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3370 - accuracy: 0.8514 - f1_score: 0.8502 - val_loss: 0.2961 - val_accuracy: 0.8797 - val_f1_score: 0.8850\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3132 - accuracy: 0.8710 - f1_score: 0.8704 - val_loss: 0.4088 - val_accuracy: 0.8020 - val_f1_score: 0.8306\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3103 - accuracy: 0.8689 - f1_score: 0.8681 - val_loss: 0.2779 - val_accuracy: 0.8924 - val_f1_score: 0.8889\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2852 - accuracy: 0.8825 - f1_score: 0.8816 - val_loss: 0.2746 - val_accuracy: 0.8924 - val_f1_score: 0.8950\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2756 - accuracy: 0.8891 - f1_score: 0.8882 - val_loss: 0.3635 - val_accuracy: 0.8481 - val_f1_score: 0.8630\n","Epoch 11/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2793 - accuracy: 0.8840 - f1_score: 0.8838 - val_loss: 0.2655 - val_accuracy: 0.9024 - val_f1_score: 0.9007\n","Epoch 12/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2443 - accuracy: 0.9036 - f1_score: 0.9031 - val_loss: 0.2668 - val_accuracy: 0.9105 - val_f1_score: 0.9115\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2501 - accuracy: 0.9033 - f1_score: 0.9023 - val_loss: 0.2594 - val_accuracy: 0.9069 - val_f1_score: 0.9068\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2287 - accuracy: 0.9102 - f1_score: 0.9099 - val_loss: 0.2840 - val_accuracy: 0.8942 - val_f1_score: 0.8958\n","Epoch 15/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2289 - accuracy: 0.9081 - f1_score: 0.9073 - val_loss: 0.2968 - val_accuracy: 0.8888 - val_f1_score: 0.8937\n","Epoch 16/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.2148 - accuracy: 0.9156 - f1_score: 0.9145 - val_loss: 0.2833 - val_accuracy: 0.8915 - val_f1_score: 0.8905\n","Epoch 17/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2078 - accuracy: 0.9168 - f1_score: 0.9162 - val_loss: 0.3019 - val_accuracy: 0.8897 - val_f1_score: 0.8903\n","Epoch 18/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.1898 - accuracy: 0.9268 - f1_score: 0.9262 - val_loss: 0.3492 - val_accuracy: 0.8761 - val_f1_score: 0.8814\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8555 - f1_score: 0.8802\n","Epoch 1/20\n","104/104 [==============================] - 9s 23ms/step - loss: 0.6233 - accuracy: 0.6432 - f1_score: 0.6689 - val_loss: 0.5318 - val_accuracy: 0.7405 - val_f1_score: 0.7305\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.5020 - accuracy: 0.7737 - f1_score: 0.7654 - val_loss: 0.4431 - val_accuracy: 0.7966 - val_f1_score: 0.7805\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4113 - accuracy: 0.8192 - f1_score: 0.8141 - val_loss: 0.4316 - val_accuracy: 0.8074 - val_f1_score: 0.8238\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3659 - accuracy: 0.8436 - f1_score: 0.8421 - val_loss: 0.4370 - val_accuracy: 0.7975 - val_f1_score: 0.8191\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3459 - accuracy: 0.8583 - f1_score: 0.8567 - val_loss: 0.3608 - val_accuracy: 0.8481 - val_f1_score: 0.8508\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3162 - accuracy: 0.8716 - f1_score: 0.8704 - val_loss: 0.4520 - val_accuracy: 0.8165 - val_f1_score: 0.8369\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3014 - accuracy: 0.8782 - f1_score: 0.8768 - val_loss: 0.3417 - val_accuracy: 0.8599 - val_f1_score: 0.8644\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2899 - accuracy: 0.8852 - f1_score: 0.8836 - val_loss: 0.3568 - val_accuracy: 0.8526 - val_f1_score: 0.8608\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2834 - accuracy: 0.8840 - f1_score: 0.8832 - val_loss: 0.3326 - val_accuracy: 0.8653 - val_f1_score: 0.8652\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2724 - accuracy: 0.8978 - f1_score: 0.8963 - val_loss: 0.3458 - val_accuracy: 0.8590 - val_f1_score: 0.8561\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2623 - accuracy: 0.8987 - f1_score: 0.8977 - val_loss: 0.3397 - val_accuracy: 0.8671 - val_f1_score: 0.8627\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2547 - accuracy: 0.9011 - f1_score: 0.8999 - val_loss: 0.3419 - val_accuracy: 0.8662 - val_f1_score: 0.8695\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2410 - accuracy: 0.9084 - f1_score: 0.9075 - val_loss: 0.3395 - val_accuracy: 0.8707 - val_f1_score: 0.8692\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2258 - accuracy: 0.9114 - f1_score: 0.9103 - val_loss: 0.3477 - val_accuracy: 0.8698 - val_f1_score: 0.8654\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8508 - f1_score: 0.8755\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6224 - accuracy: 0.6531 - f1_score: 0.6473 - val_loss: 0.5341 - val_accuracy: 0.7215 - val_f1_score: 0.6630\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4723 - accuracy: 0.7779 - f1_score: 0.7715 - val_loss: 0.4167 - val_accuracy: 0.8156 - val_f1_score: 0.8232\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3799 - accuracy: 0.8327 - f1_score: 0.8301 - val_loss: 0.3835 - val_accuracy: 0.8436 - val_f1_score: 0.8494\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3361 - accuracy: 0.8605 - f1_score: 0.8597 - val_loss: 0.4207 - val_accuracy: 0.8210 - val_f1_score: 0.8347\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3296 - accuracy: 0.8626 - f1_score: 0.8607 - val_loss: 0.4167 - val_accuracy: 0.8228 - val_f1_score: 0.8375\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3047 - accuracy: 0.8755 - f1_score: 0.8750 - val_loss: 0.3597 - val_accuracy: 0.8535 - val_f1_score: 0.8576\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2851 - accuracy: 0.8873 - f1_score: 0.8865 - val_loss: 0.3549 - val_accuracy: 0.8571 - val_f1_score: 0.8584\n","Epoch 8/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2757 - accuracy: 0.8933 - f1_score: 0.8924 - val_loss: 0.3630 - val_accuracy: 0.8544 - val_f1_score: 0.8591\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2607 - accuracy: 0.9014 - f1_score: 0.9005 - val_loss: 0.3476 - val_accuracy: 0.8608 - val_f1_score: 0.8630\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2563 - accuracy: 0.9045 - f1_score: 0.9033 - val_loss: 0.3538 - val_accuracy: 0.8626 - val_f1_score: 0.8655\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2543 - accuracy: 0.9033 - f1_score: 0.9022 - val_loss: 0.3620 - val_accuracy: 0.8590 - val_f1_score: 0.8639\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2386 - accuracy: 0.9108 - f1_score: 0.9100 - val_loss: 0.4572 - val_accuracy: 0.8291 - val_f1_score: 0.8416\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2411 - accuracy: 0.9057 - f1_score: 0.9052 - val_loss: 0.3775 - val_accuracy: 0.8472 - val_f1_score: 0.8511\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2125 - accuracy: 0.9198 - f1_score: 0.9193 - val_loss: 0.4296 - val_accuracy: 0.8382 - val_f1_score: 0.8461\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8778 - f1_score: 0.8995\n","Epoch 1/20\n","104/104 [==============================] - 8s 32ms/step - loss: 0.6245 - accuracy: 0.6561 - f1_score: 0.6661 - val_loss: 0.5287 - val_accuracy: 0.7495 - val_f1_score: 0.7533\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4720 - accuracy: 0.7755 - f1_score: 0.7677 - val_loss: 0.4619 - val_accuracy: 0.7920 - val_f1_score: 0.8148\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3769 - accuracy: 0.8369 - f1_score: 0.8333 - val_loss: 0.3530 - val_accuracy: 0.8608 - val_f1_score: 0.8654\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3376 - accuracy: 0.8574 - f1_score: 0.8561 - val_loss: 0.3358 - val_accuracy: 0.8671 - val_f1_score: 0.8712\n","Epoch 5/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3350 - accuracy: 0.8662 - f1_score: 0.8649 - val_loss: 0.3154 - val_accuracy: 0.8716 - val_f1_score: 0.8700\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3173 - accuracy: 0.8671 - f1_score: 0.8653 - val_loss: 0.3618 - val_accuracy: 0.8544 - val_f1_score: 0.8637\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3081 - accuracy: 0.8728 - f1_score: 0.8716 - val_loss: 0.3944 - val_accuracy: 0.8382 - val_f1_score: 0.8529\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3031 - accuracy: 0.8803 - f1_score: 0.8794 - val_loss: 0.3821 - val_accuracy: 0.8309 - val_f1_score: 0.8471\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2873 - accuracy: 0.8882 - f1_score: 0.8880 - val_loss: 0.3375 - val_accuracy: 0.8608 - val_f1_score: 0.8684\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2680 - accuracy: 0.8939 - f1_score: 0.8930 - val_loss: 0.3195 - val_accuracy: 0.8734 - val_f1_score: 0.8791\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3619 - accuracy: 0.8379 - f1_score: 0.8598\n","Epoch 1/20\n","104/104 [==============================] - 9s 24ms/step - loss: 0.6206 - accuracy: 0.6624 - f1_score: 0.6345 - val_loss: 0.5629 - val_accuracy: 0.7116 - val_f1_score: 0.6529\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4706 - accuracy: 0.7839 - f1_score: 0.7804 - val_loss: 0.4783 - val_accuracy: 0.7794 - val_f1_score: 0.7442\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4115 - accuracy: 0.8201 - f1_score: 0.8188 - val_loss: 0.4549 - val_accuracy: 0.7993 - val_f1_score: 0.7683\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3582 - accuracy: 0.8493 - f1_score: 0.8488 - val_loss: 0.3566 - val_accuracy: 0.8553 - val_f1_score: 0.8510\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3384 - accuracy: 0.8593 - f1_score: 0.8578 - val_loss: 0.3577 - val_accuracy: 0.8553 - val_f1_score: 0.8479\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3170 - accuracy: 0.8674 - f1_score: 0.8663 - val_loss: 0.3505 - val_accuracy: 0.8517 - val_f1_score: 0.8574\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3117 - accuracy: 0.8698 - f1_score: 0.8685 - val_loss: 0.3354 - val_accuracy: 0.8608 - val_f1_score: 0.8649\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2881 - accuracy: 0.8837 - f1_score: 0.8825 - val_loss: 0.3241 - val_accuracy: 0.8779 - val_f1_score: 0.8767\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2783 - accuracy: 0.8849 - f1_score: 0.8834 - val_loss: 0.3486 - val_accuracy: 0.8599 - val_f1_score: 0.8497\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2693 - accuracy: 0.8900 - f1_score: 0.8891 - val_loss: 0.3165 - val_accuracy: 0.8707 - val_f1_score: 0.8696\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2525 - accuracy: 0.9081 - f1_score: 0.9076 - val_loss: 0.3123 - val_accuracy: 0.8879 - val_f1_score: 0.8862\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2519 - accuracy: 0.8972 - f1_score: 0.8967 - val_loss: 0.3365 - val_accuracy: 0.8599 - val_f1_score: 0.8663\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2297 - accuracy: 0.9150 - f1_score: 0.9144 - val_loss: 0.4114 - val_accuracy: 0.8318 - val_f1_score: 0.8114\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2262 - accuracy: 0.9141 - f1_score: 0.9136 - val_loss: 0.3246 - val_accuracy: 0.8807 - val_f1_score: 0.8773\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2159 - accuracy: 0.9174 - f1_score: 0.9169 - val_loss: 0.3605 - val_accuracy: 0.8662 - val_f1_score: 0.8549\n","Epoch 16/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2022 - accuracy: 0.9268 - f1_score: 0.9263 - val_loss: 0.3335 - val_accuracy: 0.8797 - val_f1_score: 0.8776\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8677 - f1_score: 0.8914\n","Epoch 1/20\n","104/104 [==============================] - 7s 22ms/step - loss: 0.6277 - accuracy: 0.6462 - f1_score: 0.6128 - val_loss: 0.4977 - val_accuracy: 0.7731 - val_f1_score: 0.7769\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4760 - accuracy: 0.7770 - f1_score: 0.7680 - val_loss: 0.4079 - val_accuracy: 0.8128 - val_f1_score: 0.7855\n","Epoch 3/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4063 - accuracy: 0.8119 - f1_score: 0.8065 - val_loss: 0.3482 - val_accuracy: 0.8499 - val_f1_score: 0.8488\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3482 - accuracy: 0.8502 - f1_score: 0.8493 - val_loss: 0.3358 - val_accuracy: 0.8617 - val_f1_score: 0.8513\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3403 - accuracy: 0.8547 - f1_score: 0.8529 - val_loss: 0.3612 - val_accuracy: 0.8436 - val_f1_score: 0.8247\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3073 - accuracy: 0.8770 - f1_score: 0.8755 - val_loss: 0.3163 - val_accuracy: 0.8797 - val_f1_score: 0.8765\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2862 - accuracy: 0.8822 - f1_score: 0.8812 - val_loss: 0.3527 - val_accuracy: 0.8481 - val_f1_score: 0.8310\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2757 - accuracy: 0.8873 - f1_score: 0.8860 - val_loss: 0.3298 - val_accuracy: 0.8716 - val_f1_score: 0.8629\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2590 - accuracy: 0.8993 - f1_score: 0.8985 - val_loss: 0.3231 - val_accuracy: 0.8725 - val_f1_score: 0.8664\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2572 - accuracy: 0.8981 - f1_score: 0.8975 - val_loss: 0.3519 - val_accuracy: 0.8526 - val_f1_score: 0.8404\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2514 - accuracy: 0.8990 - f1_score: 0.8980 - val_loss: 0.3591 - val_accuracy: 0.8373 - val_f1_score: 0.8196\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3443 - accuracy: 0.8596 - f1_score: 0.8822\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGZpXpM2o-wi","executionInfo":{"status":"ok","timestamp":1690643238969,"user_tz":-330,"elapsed":737,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"5e5064cc-4a11-4343-f812-264efa284cb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.852126955986023, 0.8467251658439636, 0.8669817447662354, 0.8642808794975281, 0.8541526198387146, 0.8561782836914062, 0.844699501991272, 0.8602295517921448, 0.8636056780815125, 0.852126955986023, 0.869007408618927, 0.8622552156448364, 0.8629304766654968, 0.8453747630119324, 0.8561782836914062, 0.8561782836914062, 0.8555030226707458, 0.8575286865234375, 0.8507764935493469, 0.8555030226707458, 0.8507764935493469, 0.8777852654457092, 0.8379473090171814, 0.8676570057868958, 0.8595543503761292]\n","0.8570425367355347\n","0.008627011230270083\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pmWCKQGo-0v","executionInfo":{"status":"ok","timestamp":1690643238969,"user_tz":-330,"elapsed":10,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"ad49f305-4409-4944-97c0-af6bcc7dc71a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.33460724353790283, 0.36105814576148987, 0.3373117446899414, 0.3310179114341736, 0.34183183312416077, 0.3266707956790924, 0.36960288882255554, 0.3348388671875, 0.33316361904144287, 0.33884212374687195, 0.31664395332336426, 0.3230435848236084, 0.33944207429885864, 0.3656517267227173, 0.3404920995235443, 0.3537580668926239, 0.3361619710922241, 0.34500935673713684, 0.3537887930870056, 0.3477189242839813, 0.3426930606365204, 0.31449761986732483, 0.36191993951797485, 0.3247131407260895, 0.3443228006362915]\n","0.3407520914077759\n","0.014197538237542157\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvyfj8bSo-4a","executionInfo":{"status":"ok","timestamp":1690643238969,"user_tz":-330,"elapsed":6,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"d9f4b8f2-6191-45bc-ee27-44d4267e8a6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8747855424880981, 0.8687101602554321, 0.8882585167884827, 0.8859897255897522, 0.8762885928153992, 0.881071925163269, 0.8673586249351501, 0.8859503269195557, 0.889375627040863, 0.8782656788825989, 0.8931717276573181, 0.8904402852058411, 0.886528730392456, 0.8699601292610168, 0.8816008567810059, 0.8797289133071899, 0.8803130984306335, 0.8847624063491821, 0.8796951174736023, 0.8801791071891785, 0.8754929304122925, 0.8995001912117004, 0.8598130345344543, 0.8913524746894836, 0.882219672203064]\n","0.8812325358390808\n","0.008752472453219553\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nPW2sEXyo-8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM CNN LSTM nothing\n","\n","test_seed = [1,2,3,4,5]\n","train_val_undersample_seed = [6,7,8,9,10]\n","\n","accuracies = []\n","losses = []\n","f1scores = []\n","\n","for i in test_seed:\n","    # Split into remaining data and test\n","    X_sentemb_remaining, X_sentemb_test, y_remaining, y_test = train_test_split(X_sentemb, y, test_size=0.2, random_state=i, stratify=y)\n","    X_emotions_remaining, X_emotions_test, _, _ = train_test_split(X_emotions, X_emotions, test_size=0.2, random_state=i, stratify=y)\n","    X_intensity_remaining, X_intensity_test, _, _ = train_test_split(X_intensity, X_intensity, test_size=0.2, random_state=i, stratify=y)\n","    X_liwc_remaining, X_liwc_test, _, _ = train_test_split(X_liwc, X_liwc, test_size=0.2, random_state=i, stratify=y)\n","\n","    for j in train_val_undersample_seed:\n","\n","        final_rem_df = pd.concat([X_sentemb_remaining, X_emotions_remaining, X_intensity_remaining, X_liwc_remaining, y_remaining], axis=1)\n","\n","        # Undersampling\n","        Xtemp = final_rem_df.drop('label', axis=1)\n","        ytemp = final_rem_df['label']\n","        undersampler = RandomUnderSampler(sampling_strategy=1, random_state=i)  # 1 means the ratio between both classes be 1 - https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n","        X_undersampled, y_undersampled = undersampler.fit_resample(Xtemp, ytemp)\n","        final_rem_df = X_undersampled.copy()\n","        final_rem_df['label'] = y_undersampled\n","\n","        # making different features X and y after undersampling\n","        X_sentemb_remaining = final_rem_df.loc[:, 'sentemb1':'sentemb28']\n","        # putting panic extended feature with liwc features\n","        X_liwc_remaining = final_rem_df.loc[:, 'WC':'Emoji']\n","        X_liwc_remaining['symptoms_ext_count'] = final_rem_df['symptoms_ext_count']\n","        X_emotions_remaining = final_rem_df.loc[:, 'admiration':'neutral']\n","        X_intensity_remaining = final_rem_df.loc[:, 'anger_intensity':'trust_intensity']\n","        y_remaining = final_rem_df['label']\n","\n","        # Split remaining data into train and validation sets\n","        X_sentemb_train, X_sentemb_val, y_train, y_val = train_test_split(X_sentemb_remaining, y_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_emotions_train, X_emotions_val, _, _ = train_test_split(X_emotions_remaining, X_emotions_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_intensity_train, X_intensity_val, _, _ = train_test_split(X_intensity_remaining, X_intensity_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","        X_liwc_train, X_liwc_val, _, _ = train_test_split(X_liwc_remaining, X_liwc_remaining, test_size=0.25, random_state=j, stratify=y_remaining)\n","\n","        # Normalize train and test sentence embeddings so that all values are between 0 and 1 (becasue emotions features are between 0 and 1 too)\n","        train_scaled_sentemb = scaling.fit_transform(X_sentemb_train.values.reshape(-1, 1)).reshape(*X_sentemb_train.shape) # https://stackoverflow.com/questions/75461346/different-result-from-minmaxscaler-with-manual-calculations\n","        val_scaled_sentemb = scaling.fit_transform(X_sentemb_val.values.reshape(-1, 1)).reshape(*X_sentemb_val.shape)\n","        test_scaled_sentemb = scaling.fit_transform(X_sentemb_test.values.reshape(-1, 1)).reshape(*X_sentemb_test.shape)\n","        X_sentemb_train = pd.DataFrame(train_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_val = pd.DataFrame(val_scaled_sentemb, columns=sentemb_column_names)\n","        X_sentemb_test = pd.DataFrame(test_scaled_sentemb, columns=sentemb_column_names)\n","\n","        # Normalization of train and test LIWC features\n","        train_scaled_liwc = scaling.fit_transform(X_liwc_train)\n","        val_scaled_liwc = scaling.fit_transform(X_liwc_val)\n","        test_scaled_liwc = scaling.fit_transform(X_liwc_test)\n","        liwc_column_names = list(df.loc[:, 'WC':'Emoji'].columns) + ['symptoms_ext_count']\n","        X_liwc_train = pd.DataFrame(train_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_val = pd.DataFrame(val_scaled_liwc, columns=liwc_column_names)\n","        X_liwc_test = pd.DataFrame(test_scaled_liwc, columns=liwc_column_names)\n","\n","        # No scaling required in Emotions and Intensity data\n","\n","        # Multi layer DNN\n","\n","        # LSTM for sentemb, CNN for emotions, directly give input for intensity and liwc\n","\n","        # Input for sentemb features\n","        input_sentemb = Input(shape=(28,))\n","        lstm_sentemb = LSTM(32)(Reshape((1, 28))(input_sentemb))\n","\n","        # Input for emotions features\n","        input_emotions = Input(shape=(28,))\n","        reshaped_emotions = Reshape((28, 1))(input_emotions)\n","        cnn_emotions = Conv1D(128, 3, activation='relu')(reshaped_emotions)\n","        cnn_emotions = MaxPooling1D(pool_size=5, strides=2)(cnn_emotions)\n","        cnn_emotions = Flatten()(cnn_emotions)\n","        # + LSTM\n","        cnn_lstm_emotions = LSTM(32)(Reshape((1, cnn_emotions.shape[1]))(cnn_emotions))  # cnn_emotions returns length 3072, ie cnn_emotions.shape[1] = 3072\n","\n","        # Input for intensity features\n","        input_intensity = Input(shape=(8,))\n","        lstm_intensity = LSTM(32)(Reshape((1, 8))(input_intensity))\n","\n","        # Input for LIWC features\n","        input_liwc = Input(shape=(119,))\n","\n","        # Concatenate the outputs of the LSTM and CNN layers\n","        concatenated = Concatenate()([lstm_sentemb, cnn_lstm_emotions, lstm_intensity, input_liwc])\n","\n","        # Additional Dense layers for further processing\n","        #merged_output = Dense(256, activation='relu')(concatenated)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(128, activation='relu')(concatenated)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(64, activation='relu')(merged_output)\n","        #merged_output = Dropout(rate=0.2)(merged_output)\n","        merged_output = Dense(32, activation='relu')(merged_output)\n","        merged_output = Dropout(rate=0.2)(merged_output)\n","\n","        # Output layer\n","        output = Dense(1, activation='sigmoid')(merged_output)\n","\n","        # Create the model\n","        model = Model(inputs=[input_sentemb, input_emotions, input_intensity, input_liwc], outputs=output)  # note that here it should all be input tensors, not lstm_sentemb or cnn_emotions\n","\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","        # Compile the model\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', F1Score()])\n","\n","        model.fit([X_sentemb_train, X_emotions_train, X_intensity_train, X_liwc_train], y_train,\n","                            epochs=20, batch_size=32, validation_data=([X_sentemb_val, X_emotions_val, X_intensity_val, X_liwc_val], y_val), callbacks=[early_stopping])\n","\n","        # Evaluate the model on the test set\n","        loss, accuracy, f1 = model.evaluate([X_sentemb_test, X_emotions_test, X_intensity_test, X_liwc_test], y_test)\n","\n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","        f1scores.append(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdaXR5rpo-_V","executionInfo":{"status":"ok","timestamp":1690644238603,"user_tz":-330,"elapsed":999636,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"364f1abf-d067-4506-c409-8af394adfc6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","104/104 [==============================] - 10s 26ms/step - loss: 0.6303 - accuracy: 0.6392 - f1_score: 0.6184 - val_loss: 0.5077 - val_accuracy: 0.7712 - val_f1_score: 0.7861\n","Epoch 2/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.5187 - accuracy: 0.7498 - f1_score: 0.7403 - val_loss: 0.4224 - val_accuracy: 0.8255 - val_f1_score: 0.8326\n","Epoch 3/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.4349 - accuracy: 0.8029 - f1_score: 0.7965 - val_loss: 0.3434 - val_accuracy: 0.8535 - val_f1_score: 0.8503\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3795 - accuracy: 0.8391 - f1_score: 0.8373 - val_loss: 0.3495 - val_accuracy: 0.8517 - val_f1_score: 0.8610\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3521 - accuracy: 0.8547 - f1_score: 0.8528 - val_loss: 0.3240 - val_accuracy: 0.8626 - val_f1_score: 0.8547\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3365 - accuracy: 0.8617 - f1_score: 0.8590 - val_loss: 0.3147 - val_accuracy: 0.8725 - val_f1_score: 0.8656\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3247 - accuracy: 0.8674 - f1_score: 0.8656 - val_loss: 0.2965 - val_accuracy: 0.8779 - val_f1_score: 0.8776\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3007 - accuracy: 0.8782 - f1_score: 0.8764 - val_loss: 0.3056 - val_accuracy: 0.8698 - val_f1_score: 0.8629\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2993 - accuracy: 0.8755 - f1_score: 0.8736 - val_loss: 0.3040 - val_accuracy: 0.8797 - val_f1_score: 0.8785\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2808 - accuracy: 0.8909 - f1_score: 0.8884 - val_loss: 0.3097 - val_accuracy: 0.8707 - val_f1_score: 0.8753\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2923 - accuracy: 0.8791 - f1_score: 0.8760 - val_loss: 0.3064 - val_accuracy: 0.8662 - val_f1_score: 0.8697\n","Epoch 12/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.2592 - accuracy: 0.8978 - f1_score: 0.8963 - val_loss: 0.3004 - val_accuracy: 0.8743 - val_f1_score: 0.8687\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.8501 - f1_score: 0.8743\n","Epoch 1/20\n","104/104 [==============================] - 11s 38ms/step - loss: 0.6161 - accuracy: 0.6652 - f1_score: 0.6555 - val_loss: 0.5465 - val_accuracy: 0.7378 - val_f1_score: 0.7335\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4908 - accuracy: 0.7670 - f1_score: 0.7536 - val_loss: 0.4499 - val_accuracy: 0.7911 - val_f1_score: 0.7751\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4103 - accuracy: 0.8125 - f1_score: 0.8072 - val_loss: 0.4874 - val_accuracy: 0.7667 - val_f1_score: 0.7068\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3846 - accuracy: 0.8294 - f1_score: 0.8232 - val_loss: 0.3763 - val_accuracy: 0.8336 - val_f1_score: 0.8375\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3407 - accuracy: 0.8544 - f1_score: 0.8527 - val_loss: 0.3715 - val_accuracy: 0.8309 - val_f1_score: 0.8387\n","Epoch 6/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3307 - accuracy: 0.8550 - f1_score: 0.8531 - val_loss: 0.3784 - val_accuracy: 0.8264 - val_f1_score: 0.8376\n","Epoch 7/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3548 - accuracy: 0.8409 - f1_score: 0.8395 - val_loss: 0.3627 - val_accuracy: 0.8562 - val_f1_score: 0.8452\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3048 - accuracy: 0.8692 - f1_score: 0.8682 - val_loss: 0.3520 - val_accuracy: 0.8635 - val_f1_score: 0.8603\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2896 - accuracy: 0.8855 - f1_score: 0.8845 - val_loss: 0.3708 - val_accuracy: 0.8436 - val_f1_score: 0.8494\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2854 - accuracy: 0.8837 - f1_score: 0.8833 - val_loss: 0.3616 - val_accuracy: 0.8562 - val_f1_score: 0.8561\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2741 - accuracy: 0.8876 - f1_score: 0.8873 - val_loss: 0.3724 - val_accuracy: 0.8463 - val_f1_score: 0.8511\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2614 - accuracy: 0.8957 - f1_score: 0.8948 - val_loss: 0.3738 - val_accuracy: 0.8445 - val_f1_score: 0.8491\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2469 - accuracy: 0.8999 - f1_score: 0.8998 - val_loss: 0.4111 - val_accuracy: 0.8382 - val_f1_score: 0.8194\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8440 - f1_score: 0.8678\n","Epoch 1/20\n","104/104 [==============================] - 10s 25ms/step - loss: 0.6260 - accuracy: 0.6567 - f1_score: 0.6455 - val_loss: 0.5596 - val_accuracy: 0.7251 - val_f1_score: 0.6717\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4990 - accuracy: 0.7571 - f1_score: 0.7465 - val_loss: 0.4717 - val_accuracy: 0.7703 - val_f1_score: 0.7768\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4157 - accuracy: 0.8171 - f1_score: 0.8125 - val_loss: 0.4259 - val_accuracy: 0.8056 - val_f1_score: 0.7902\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3609 - accuracy: 0.8418 - f1_score: 0.8390 - val_loss: 0.3992 - val_accuracy: 0.8273 - val_f1_score: 0.8284\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3210 - accuracy: 0.8653 - f1_score: 0.8633 - val_loss: 0.3967 - val_accuracy: 0.8264 - val_f1_score: 0.8242\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3077 - accuracy: 0.8740 - f1_score: 0.8723 - val_loss: 0.3955 - val_accuracy: 0.8327 - val_f1_score: 0.8256\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3077 - accuracy: 0.8686 - f1_score: 0.8675 - val_loss: 0.4519 - val_accuracy: 0.8092 - val_f1_score: 0.7800\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2949 - accuracy: 0.8776 - f1_score: 0.8758 - val_loss: 0.3981 - val_accuracy: 0.8273 - val_f1_score: 0.8200\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2886 - accuracy: 0.8849 - f1_score: 0.8834 - val_loss: 0.4063 - val_accuracy: 0.8318 - val_f1_score: 0.8380\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2654 - accuracy: 0.8915 - f1_score: 0.8906 - val_loss: 0.4166 - val_accuracy: 0.8300 - val_f1_score: 0.8236\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2489 - accuracy: 0.8999 - f1_score: 0.8990 - val_loss: 0.4176 - val_accuracy: 0.8345 - val_f1_score: 0.8288\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8474 - f1_score: 0.8685\n","Epoch 1/20\n","104/104 [==============================] - 11s 28ms/step - loss: 0.6315 - accuracy: 0.6401 - f1_score: 0.6397 - val_loss: 0.5236 - val_accuracy: 0.7586 - val_f1_score: 0.7364\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4944 - accuracy: 0.7628 - f1_score: 0.7483 - val_loss: 0.4205 - val_accuracy: 0.8119 - val_f1_score: 0.8143\n","Epoch 3/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.4192 - accuracy: 0.8086 - f1_score: 0.8015 - val_loss: 0.4075 - val_accuracy: 0.8282 - val_f1_score: 0.8398\n","Epoch 4/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3624 - accuracy: 0.8430 - f1_score: 0.8397 - val_loss: 0.3790 - val_accuracy: 0.8427 - val_f1_score: 0.8503\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3270 - accuracy: 0.8680 - f1_score: 0.8662 - val_loss: 0.3714 - val_accuracy: 0.8454 - val_f1_score: 0.8522\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3254 - accuracy: 0.8656 - f1_score: 0.8636 - val_loss: 0.3467 - val_accuracy: 0.8526 - val_f1_score: 0.8541\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3018 - accuracy: 0.8758 - f1_score: 0.8738 - val_loss: 0.3458 - val_accuracy: 0.8653 - val_f1_score: 0.8604\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3012 - accuracy: 0.8779 - f1_score: 0.8766 - val_loss: 0.3405 - val_accuracy: 0.8635 - val_f1_score: 0.8621\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2909 - accuracy: 0.8852 - f1_score: 0.8839 - val_loss: 0.3996 - val_accuracy: 0.8391 - val_f1_score: 0.8512\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2745 - accuracy: 0.8882 - f1_score: 0.8869 - val_loss: 0.3641 - val_accuracy: 0.8526 - val_f1_score: 0.8591\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2642 - accuracy: 0.8996 - f1_score: 0.8991 - val_loss: 0.3855 - val_accuracy: 0.8354 - val_f1_score: 0.8483\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2458 - accuracy: 0.9033 - f1_score: 0.9023 - val_loss: 0.3941 - val_accuracy: 0.8382 - val_f1_score: 0.8492\n","Epoch 13/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2550 - accuracy: 0.9042 - f1_score: 0.9032 - val_loss: 0.3751 - val_accuracy: 0.8535 - val_f1_score: 0.8581\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3704 - accuracy: 0.8427 - f1_score: 0.8632\n","Epoch 1/20\n","104/104 [==============================] - 9s 25ms/step - loss: 0.6160 - accuracy: 0.6552 - f1_score: 0.6244 - val_loss: 0.5360 - val_accuracy: 0.7315 - val_f1_score: 0.7603\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4922 - accuracy: 0.7694 - f1_score: 0.7625 - val_loss: 0.4511 - val_accuracy: 0.7884 - val_f1_score: 0.8050\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4516 - accuracy: 0.7914 - f1_score: 0.7868 - val_loss: 0.3871 - val_accuracy: 0.8264 - val_f1_score: 0.8267\n","Epoch 4/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3873 - accuracy: 0.8234 - f1_score: 0.8194 - val_loss: 0.3663 - val_accuracy: 0.8391 - val_f1_score: 0.8430\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3631 - accuracy: 0.8418 - f1_score: 0.8387 - val_loss: 0.3894 - val_accuracy: 0.8264 - val_f1_score: 0.8408\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3284 - accuracy: 0.8571 - f1_score: 0.8562 - val_loss: 0.3819 - val_accuracy: 0.8382 - val_f1_score: 0.8502\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3089 - accuracy: 0.8713 - f1_score: 0.8701 - val_loss: 0.3848 - val_accuracy: 0.8382 - val_f1_score: 0.8208\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3075 - accuracy: 0.8722 - f1_score: 0.8707 - val_loss: 0.4573 - val_accuracy: 0.8056 - val_f1_score: 0.8316\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2949 - accuracy: 0.8770 - f1_score: 0.8758 - val_loss: 0.3312 - val_accuracy: 0.8653 - val_f1_score: 0.8680\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2692 - accuracy: 0.8909 - f1_score: 0.8896 - val_loss: 0.3656 - val_accuracy: 0.8662 - val_f1_score: 0.8729\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2744 - accuracy: 0.8879 - f1_score: 0.8862 - val_loss: 0.3567 - val_accuracy: 0.8544 - val_f1_score: 0.8620\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2470 - accuracy: 0.9036 - f1_score: 0.9035 - val_loss: 0.3549 - val_accuracy: 0.8617 - val_f1_score: 0.8668\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2439 - accuracy: 0.9057 - f1_score: 0.9047 - val_loss: 0.3567 - val_accuracy: 0.8635 - val_f1_score: 0.8690\n","Epoch 14/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2421 - accuracy: 0.8990 - f1_score: 0.8981 - val_loss: 0.3728 - val_accuracy: 0.8490 - val_f1_score: 0.8364\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8562 - f1_score: 0.8795\n","Epoch 1/20\n","104/104 [==============================] - 11s 27ms/step - loss: 0.6315 - accuracy: 0.6398 - f1_score: 0.6222 - val_loss: 0.5178 - val_accuracy: 0.7423 - val_f1_score: 0.7549\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4835 - accuracy: 0.7691 - f1_score: 0.7576 - val_loss: 0.4861 - val_accuracy: 0.7649 - val_f1_score: 0.7981\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4043 - accuracy: 0.8149 - f1_score: 0.8106 - val_loss: 0.3538 - val_accuracy: 0.8508 - val_f1_score: 0.8430\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3670 - accuracy: 0.8376 - f1_score: 0.8341 - val_loss: 0.3227 - val_accuracy: 0.8716 - val_f1_score: 0.8707\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3407 - accuracy: 0.8577 - f1_score: 0.8568 - val_loss: 0.3105 - val_accuracy: 0.8788 - val_f1_score: 0.8721\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3286 - accuracy: 0.8623 - f1_score: 0.8617 - val_loss: 0.3486 - val_accuracy: 0.8490 - val_f1_score: 0.8315\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3181 - accuracy: 0.8710 - f1_score: 0.8692 - val_loss: 0.3003 - val_accuracy: 0.8770 - val_f1_score: 0.8817\n","Epoch 8/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2896 - accuracy: 0.8837 - f1_score: 0.8828 - val_loss: 0.2844 - val_accuracy: 0.8906 - val_f1_score: 0.8862\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2994 - accuracy: 0.8758 - f1_score: 0.8745 - val_loss: 0.3366 - val_accuracy: 0.8562 - val_f1_score: 0.8663\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2765 - accuracy: 0.8891 - f1_score: 0.8879 - val_loss: 0.2946 - val_accuracy: 0.8816 - val_f1_score: 0.8751\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2751 - accuracy: 0.8897 - f1_score: 0.8892 - val_loss: 0.3254 - val_accuracy: 0.8626 - val_f1_score: 0.8471\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2659 - accuracy: 0.8951 - f1_score: 0.8942 - val_loss: 0.2848 - val_accuracy: 0.8879 - val_f1_score: 0.8830\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2449 - accuracy: 0.9027 - f1_score: 0.9019 - val_loss: 0.3077 - val_accuracy: 0.8671 - val_f1_score: 0.8555\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8562 - f1_score: 0.8793\n","Epoch 1/20\n","104/104 [==============================] - 10s 24ms/step - loss: 0.6168 - accuracy: 0.6634 - f1_score: 0.6453 - val_loss: 0.6284 - val_accuracy: 0.6492 - val_f1_score: 0.7201\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4883 - accuracy: 0.7725 - f1_score: 0.7683 - val_loss: 0.4663 - val_accuracy: 0.7785 - val_f1_score: 0.7974\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4073 - accuracy: 0.8159 - f1_score: 0.8141 - val_loss: 0.3684 - val_accuracy: 0.8373 - val_f1_score: 0.8429\n","Epoch 4/20\n","104/104 [==============================] - 1s 11ms/step - loss: 0.3623 - accuracy: 0.8424 - f1_score: 0.8404 - val_loss: 0.4103 - val_accuracy: 0.8092 - val_f1_score: 0.7762\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3300 - accuracy: 0.8608 - f1_score: 0.8591 - val_loss: 0.3176 - val_accuracy: 0.8698 - val_f1_score: 0.8669\n","Epoch 6/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.3199 - accuracy: 0.8662 - f1_score: 0.8652 - val_loss: 0.3259 - val_accuracy: 0.8635 - val_f1_score: 0.8674\n","Epoch 7/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3096 - accuracy: 0.8767 - f1_score: 0.8759 - val_loss: 0.4314 - val_accuracy: 0.8156 - val_f1_score: 0.8384\n","Epoch 8/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2906 - accuracy: 0.8764 - f1_score: 0.8758 - val_loss: 0.3103 - val_accuracy: 0.8734 - val_f1_score: 0.8759\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2859 - accuracy: 0.8828 - f1_score: 0.8818 - val_loss: 0.3194 - val_accuracy: 0.8725 - val_f1_score: 0.8666\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2935 - accuracy: 0.8722 - f1_score: 0.8724 - val_loss: 0.4003 - val_accuracy: 0.8391 - val_f1_score: 0.8546\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2607 - accuracy: 0.8900 - f1_score: 0.8895 - val_loss: 0.3150 - val_accuracy: 0.8770 - val_f1_score: 0.8743\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2365 - accuracy: 0.9054 - f1_score: 0.9046 - val_loss: 0.3121 - val_accuracy: 0.8779 - val_f1_score: 0.8765\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2389 - accuracy: 0.9039 - f1_score: 0.9031 - val_loss: 0.5351 - val_accuracy: 0.7902 - val_f1_score: 0.8215\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8521 - f1_score: 0.8777\n","Epoch 1/20\n","104/104 [==============================] - 11s 26ms/step - loss: 0.6060 - accuracy: 0.6709 - f1_score: 0.6663 - val_loss: 0.5548 - val_accuracy: 0.7170 - val_f1_score: 0.7553\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4561 - accuracy: 0.7887 - f1_score: 0.7830 - val_loss: 0.4158 - val_accuracy: 0.8128 - val_f1_score: 0.8064\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3923 - accuracy: 0.8219 - f1_score: 0.8180 - val_loss: 0.3931 - val_accuracy: 0.8228 - val_f1_score: 0.8078\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3456 - accuracy: 0.8484 - f1_score: 0.8458 - val_loss: 0.3554 - val_accuracy: 0.8562 - val_f1_score: 0.8496\n","Epoch 5/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3206 - accuracy: 0.8602 - f1_score: 0.8578 - val_loss: 0.3813 - val_accuracy: 0.8291 - val_f1_score: 0.8085\n","Epoch 6/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3077 - accuracy: 0.8782 - f1_score: 0.8765 - val_loss: 0.3622 - val_accuracy: 0.8490 - val_f1_score: 0.8586\n","Epoch 7/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2840 - accuracy: 0.8837 - f1_score: 0.8834 - val_loss: 0.3395 - val_accuracy: 0.8580 - val_f1_score: 0.8641\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2691 - accuracy: 0.8906 - f1_score: 0.8903 - val_loss: 0.3332 - val_accuracy: 0.8716 - val_f1_score: 0.8721\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2676 - accuracy: 0.8918 - f1_score: 0.8914 - val_loss: 0.3256 - val_accuracy: 0.8788 - val_f1_score: 0.8755\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2565 - accuracy: 0.8963 - f1_score: 0.8957 - val_loss: 0.3656 - val_accuracy: 0.8499 - val_f1_score: 0.8596\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2418 - accuracy: 0.9045 - f1_score: 0.9044 - val_loss: 0.3386 - val_accuracy: 0.8725 - val_f1_score: 0.8717\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2295 - accuracy: 0.9123 - f1_score: 0.9118 - val_loss: 0.3665 - val_accuracy: 0.8707 - val_f1_score: 0.8626\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2387 - accuracy: 0.9027 - f1_score: 0.9018 - val_loss: 0.3608 - val_accuracy: 0.8553 - val_f1_score: 0.8456\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2281 - accuracy: 0.9144 - f1_score: 0.9141 - val_loss: 0.3647 - val_accuracy: 0.8490 - val_f1_score: 0.8569\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8379 - f1_score: 0.8600\n","Epoch 1/20\n","104/104 [==============================] - 11s 43ms/step - loss: 0.6203 - accuracy: 0.6389 - f1_score: 0.6028 - val_loss: 0.5405 - val_accuracy: 0.7360 - val_f1_score: 0.7350\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4968 - accuracy: 0.7580 - f1_score: 0.7507 - val_loss: 0.4426 - val_accuracy: 0.8038 - val_f1_score: 0.7899\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4189 - accuracy: 0.8065 - f1_score: 0.8032 - val_loss: 0.4013 - val_accuracy: 0.8255 - val_f1_score: 0.8335\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3797 - accuracy: 0.8315 - f1_score: 0.8292 - val_loss: 0.3530 - val_accuracy: 0.8463 - val_f1_score: 0.8420\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3512 - accuracy: 0.8499 - f1_score: 0.8483 - val_loss: 0.3521 - val_accuracy: 0.8599 - val_f1_score: 0.8622\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3390 - accuracy: 0.8568 - f1_score: 0.8559 - val_loss: 0.3481 - val_accuracy: 0.8544 - val_f1_score: 0.8462\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3254 - accuracy: 0.8641 - f1_score: 0.8626 - val_loss: 0.3466 - val_accuracy: 0.8617 - val_f1_score: 0.8550\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3091 - accuracy: 0.8791 - f1_score: 0.8790 - val_loss: 0.3540 - val_accuracy: 0.8608 - val_f1_score: 0.8511\n","Epoch 9/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2843 - accuracy: 0.8825 - f1_score: 0.8813 - val_loss: 0.4203 - val_accuracy: 0.8345 - val_f1_score: 0.8157\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2755 - accuracy: 0.8855 - f1_score: 0.8849 - val_loss: 0.3360 - val_accuracy: 0.8725 - val_f1_score: 0.8686\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2694 - accuracy: 0.8888 - f1_score: 0.8879 - val_loss: 0.3503 - val_accuracy: 0.8680 - val_f1_score: 0.8617\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2552 - accuracy: 0.8933 - f1_score: 0.8931 - val_loss: 0.3496 - val_accuracy: 0.8580 - val_f1_score: 0.8626\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2383 - accuracy: 0.9039 - f1_score: 0.9034 - val_loss: 0.3928 - val_accuracy: 0.8445 - val_f1_score: 0.8557\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2304 - accuracy: 0.9042 - f1_score: 0.9035 - val_loss: 0.3721 - val_accuracy: 0.8662 - val_f1_score: 0.8671\n","Epoch 15/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2156 - accuracy: 0.9177 - f1_score: 0.9172 - val_loss: 0.3847 - val_accuracy: 0.8599 - val_f1_score: 0.8511\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8488 - f1_score: 0.8739\n","Epoch 1/20\n","104/104 [==============================] - 10s 26ms/step - loss: 0.6224 - accuracy: 0.6591 - f1_score: 0.6372 - val_loss: 0.5035 - val_accuracy: 0.7703 - val_f1_score: 0.7652\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4738 - accuracy: 0.7800 - f1_score: 0.7716 - val_loss: 0.4575 - val_accuracy: 0.7848 - val_f1_score: 0.8068\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4040 - accuracy: 0.8186 - f1_score: 0.8151 - val_loss: 0.3978 - val_accuracy: 0.8291 - val_f1_score: 0.8123\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3542 - accuracy: 0.8508 - f1_score: 0.8484 - val_loss: 0.3780 - val_accuracy: 0.8373 - val_f1_score: 0.8440\n","Epoch 5/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3273 - accuracy: 0.8602 - f1_score: 0.8575 - val_loss: 0.3660 - val_accuracy: 0.8382 - val_f1_score: 0.8461\n","Epoch 6/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3017 - accuracy: 0.8803 - f1_score: 0.8791 - val_loss: 0.3314 - val_accuracy: 0.8626 - val_f1_score: 0.8569\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3019 - accuracy: 0.8782 - f1_score: 0.8772 - val_loss: 0.3846 - val_accuracy: 0.8454 - val_f1_score: 0.8228\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2922 - accuracy: 0.8831 - f1_score: 0.8817 - val_loss: 0.3459 - val_accuracy: 0.8617 - val_f1_score: 0.8490\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2788 - accuracy: 0.8903 - f1_score: 0.8898 - val_loss: 0.3243 - val_accuracy: 0.8590 - val_f1_score: 0.8597\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2559 - accuracy: 0.9008 - f1_score: 0.9002 - val_loss: 0.3615 - val_accuracy: 0.8526 - val_f1_score: 0.8355\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2540 - accuracy: 0.8996 - f1_score: 0.8991 - val_loss: 0.3086 - val_accuracy: 0.8752 - val_f1_score: 0.8743\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2334 - accuracy: 0.9114 - f1_score: 0.9110 - val_loss: 0.3086 - val_accuracy: 0.8797 - val_f1_score: 0.8796\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2228 - accuracy: 0.9198 - f1_score: 0.9197 - val_loss: 0.5541 - val_accuracy: 0.7984 - val_f1_score: 0.7541\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2249 - accuracy: 0.9165 - f1_score: 0.9163 - val_loss: 0.3729 - val_accuracy: 0.8644 - val_f1_score: 0.8521\n","Epoch 15/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2047 - accuracy: 0.9207 - f1_score: 0.9203 - val_loss: 0.3630 - val_accuracy: 0.8599 - val_f1_score: 0.8476\n","Epoch 16/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2020 - accuracy: 0.9234 - f1_score: 0.9228 - val_loss: 0.3202 - val_accuracy: 0.8779 - val_f1_score: 0.8716\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3447 - accuracy: 0.8569 - f1_score: 0.8830\n","Epoch 1/20\n","104/104 [==============================] - 12s 37ms/step - loss: 0.6252 - accuracy: 0.6567 - f1_score: 0.6399 - val_loss: 0.5226 - val_accuracy: 0.7622 - val_f1_score: 0.7303\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4693 - accuracy: 0.7761 - f1_score: 0.7706 - val_loss: 0.4004 - val_accuracy: 0.8273 - val_f1_score: 0.8323\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3804 - accuracy: 0.8297 - f1_score: 0.8275 - val_loss: 0.3831 - val_accuracy: 0.8264 - val_f1_score: 0.8376\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3374 - accuracy: 0.8550 - f1_score: 0.8542 - val_loss: 0.4276 - val_accuracy: 0.8282 - val_f1_score: 0.8021\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3240 - accuracy: 0.8629 - f1_score: 0.8626 - val_loss: 0.3367 - val_accuracy: 0.8590 - val_f1_score: 0.8542\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3017 - accuracy: 0.8743 - f1_score: 0.8737 - val_loss: 0.3380 - val_accuracy: 0.8526 - val_f1_score: 0.8561\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3055 - accuracy: 0.8740 - f1_score: 0.8729 - val_loss: 0.3269 - val_accuracy: 0.8671 - val_f1_score: 0.8653\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2870 - accuracy: 0.8825 - f1_score: 0.8815 - val_loss: 0.3652 - val_accuracy: 0.8526 - val_f1_score: 0.8425\n","Epoch 9/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2830 - accuracy: 0.8849 - f1_score: 0.8835 - val_loss: 0.3274 - val_accuracy: 0.8707 - val_f1_score: 0.8655\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2586 - accuracy: 0.8933 - f1_score: 0.8918 - val_loss: 0.3370 - val_accuracy: 0.8716 - val_f1_score: 0.8665\n","Epoch 11/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2542 - accuracy: 0.8981 - f1_score: 0.8970 - val_loss: 0.3505 - val_accuracy: 0.8644 - val_f1_score: 0.8663\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2429 - accuracy: 0.9054 - f1_score: 0.9044 - val_loss: 0.3499 - val_accuracy: 0.8671 - val_f1_score: 0.8615\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8677 - f1_score: 0.8945\n","Epoch 1/20\n","104/104 [==============================] - 10s 37ms/step - loss: 0.6143 - accuracy: 0.6703 - f1_score: 0.6695 - val_loss: 0.6300 - val_accuracy: 0.6655 - val_f1_score: 0.5144\n","Epoch 2/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.4893 - accuracy: 0.7697 - f1_score: 0.7582 - val_loss: 0.4153 - val_accuracy: 0.8047 - val_f1_score: 0.7857\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4204 - accuracy: 0.8107 - f1_score: 0.8024 - val_loss: 0.3771 - val_accuracy: 0.8409 - val_f1_score: 0.8423\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3775 - accuracy: 0.8321 - f1_score: 0.8269 - val_loss: 0.3449 - val_accuracy: 0.8590 - val_f1_score: 0.8574\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3372 - accuracy: 0.8541 - f1_score: 0.8509 - val_loss: 0.3510 - val_accuracy: 0.8508 - val_f1_score: 0.8348\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3127 - accuracy: 0.8629 - f1_score: 0.8603 - val_loss: 0.3619 - val_accuracy: 0.8481 - val_f1_score: 0.8306\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3073 - accuracy: 0.8680 - f1_score: 0.8652 - val_loss: 0.3109 - val_accuracy: 0.8834 - val_f1_score: 0.8798\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2830 - accuracy: 0.8834 - f1_score: 0.8818 - val_loss: 0.3962 - val_accuracy: 0.8409 - val_f1_score: 0.8189\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2746 - accuracy: 0.8843 - f1_score: 0.8824 - val_loss: 0.3567 - val_accuracy: 0.8499 - val_f1_score: 0.8316\n","Epoch 10/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.2680 - accuracy: 0.8909 - f1_score: 0.8894 - val_loss: 0.3119 - val_accuracy: 0.8807 - val_f1_score: 0.8826\n","Epoch 11/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2518 - accuracy: 0.8942 - f1_score: 0.8930 - val_loss: 0.3369 - val_accuracy: 0.8689 - val_f1_score: 0.8585\n","Epoch 12/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.2458 - accuracy: 0.9008 - f1_score: 0.8996 - val_loss: 0.3717 - val_accuracy: 0.8517 - val_f1_score: 0.8347\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8616 - f1_score: 0.8889\n","Epoch 1/20\n","104/104 [==============================] - 11s 26ms/step - loss: 0.6166 - accuracy: 0.6634 - f1_score: 0.6545 - val_loss: 0.5418 - val_accuracy: 0.7360 - val_f1_score: 0.7571\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.5003 - accuracy: 0.7652 - f1_score: 0.7494 - val_loss: 0.4656 - val_accuracy: 0.7911 - val_f1_score: 0.7986\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4169 - accuracy: 0.8014 - f1_score: 0.7926 - val_loss: 0.4029 - val_accuracy: 0.8192 - val_f1_score: 0.8162\n","Epoch 4/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3663 - accuracy: 0.8388 - f1_score: 0.8346 - val_loss: 0.3795 - val_accuracy: 0.8382 - val_f1_score: 0.8310\n","Epoch 5/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3305 - accuracy: 0.8596 - f1_score: 0.8566 - val_loss: 0.4880 - val_accuracy: 0.7957 - val_f1_score: 0.8220\n","Epoch 6/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.3101 - accuracy: 0.8671 - f1_score: 0.8651 - val_loss: 0.3561 - val_accuracy: 0.8571 - val_f1_score: 0.8561\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2935 - accuracy: 0.8797 - f1_score: 0.8782 - val_loss: 0.3567 - val_accuracy: 0.8626 - val_f1_score: 0.8640\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2834 - accuracy: 0.8828 - f1_score: 0.8806 - val_loss: 0.3769 - val_accuracy: 0.8562 - val_f1_score: 0.8597\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2774 - accuracy: 0.8888 - f1_score: 0.8869 - val_loss: 0.3672 - val_accuracy: 0.8608 - val_f1_score: 0.8579\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2717 - accuracy: 0.8858 - f1_score: 0.8840 - val_loss: 0.3607 - val_accuracy: 0.8617 - val_f1_score: 0.8587\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2515 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.3946 - val_accuracy: 0.8644 - val_f1_score: 0.8621\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8569 - f1_score: 0.8822\n","Epoch 1/20\n","104/104 [==============================] - 10s 26ms/step - loss: 0.6207 - accuracy: 0.6670 - f1_score: 0.6595 - val_loss: 0.5187 - val_accuracy: 0.7577 - val_f1_score: 0.7362\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4796 - accuracy: 0.7800 - f1_score: 0.7743 - val_loss: 0.3960 - val_accuracy: 0.8336 - val_f1_score: 0.8290\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4078 - accuracy: 0.8195 - f1_score: 0.8143 - val_loss: 0.4736 - val_accuracy: 0.7957 - val_f1_score: 0.8232\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3727 - accuracy: 0.8385 - f1_score: 0.8358 - val_loss: 0.3400 - val_accuracy: 0.8544 - val_f1_score: 0.8513\n","Epoch 5/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3483 - accuracy: 0.8508 - f1_score: 0.8485 - val_loss: 0.3245 - val_accuracy: 0.8689 - val_f1_score: 0.8676\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3210 - accuracy: 0.8626 - f1_score: 0.8601 - val_loss: 0.3977 - val_accuracy: 0.8210 - val_f1_score: 0.8390\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3002 - accuracy: 0.8728 - f1_score: 0.8711 - val_loss: 0.4706 - val_accuracy: 0.7966 - val_f1_score: 0.8254\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2951 - accuracy: 0.8822 - f1_score: 0.8814 - val_loss: 0.3168 - val_accuracy: 0.8743 - val_f1_score: 0.8753\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2834 - accuracy: 0.8891 - f1_score: 0.8879 - val_loss: 0.3152 - val_accuracy: 0.8761 - val_f1_score: 0.8758\n","Epoch 10/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2836 - accuracy: 0.8846 - f1_score: 0.8831 - val_loss: 0.3985 - val_accuracy: 0.8219 - val_f1_score: 0.8415\n","Epoch 11/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2585 - accuracy: 0.8972 - f1_score: 0.8964 - val_loss: 0.4658 - val_accuracy: 0.7857 - val_f1_score: 0.8170\n","Epoch 12/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2429 - accuracy: 0.9039 - f1_score: 0.9031 - val_loss: 0.3168 - val_accuracy: 0.8743 - val_f1_score: 0.8756\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2366 - accuracy: 0.9060 - f1_score: 0.9044 - val_loss: 0.3556 - val_accuracy: 0.8608 - val_f1_score: 0.8679\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2204 - accuracy: 0.9153 - f1_score: 0.9145 - val_loss: 0.3370 - val_accuracy: 0.8788 - val_f1_score: 0.8808\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8569 - f1_score: 0.8816\n","Epoch 1/20\n","104/104 [==============================] - 11s 29ms/step - loss: 0.6197 - accuracy: 0.6576 - f1_score: 0.6218 - val_loss: 0.5741 - val_accuracy: 0.6935 - val_f1_score: 0.7434\n","Epoch 2/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.4775 - accuracy: 0.7824 - f1_score: 0.7765 - val_loss: 0.4303 - val_accuracy: 0.8047 - val_f1_score: 0.8082\n","Epoch 3/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3968 - accuracy: 0.8222 - f1_score: 0.8197 - val_loss: 0.3962 - val_accuracy: 0.8165 - val_f1_score: 0.8293\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3803 - accuracy: 0.8270 - f1_score: 0.8241 - val_loss: 0.3659 - val_accuracy: 0.8382 - val_f1_score: 0.8458\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3368 - accuracy: 0.8520 - f1_score: 0.8510 - val_loss: 0.3365 - val_accuracy: 0.8599 - val_f1_score: 0.8595\n","Epoch 6/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3208 - accuracy: 0.8698 - f1_score: 0.8680 - val_loss: 0.3637 - val_accuracy: 0.8454 - val_f1_score: 0.8555\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2916 - accuracy: 0.8797 - f1_score: 0.8788 - val_loss: 0.3337 - val_accuracy: 0.8635 - val_f1_score: 0.8679\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2985 - accuracy: 0.8800 - f1_score: 0.8787 - val_loss: 0.3236 - val_accuracy: 0.8644 - val_f1_score: 0.8629\n","Epoch 9/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2795 - accuracy: 0.8858 - f1_score: 0.8851 - val_loss: 0.3179 - val_accuracy: 0.8680 - val_f1_score: 0.8665\n","Epoch 10/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2639 - accuracy: 0.8942 - f1_score: 0.8934 - val_loss: 0.3216 - val_accuracy: 0.8752 - val_f1_score: 0.8755\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2556 - accuracy: 0.8960 - f1_score: 0.8955 - val_loss: 0.3217 - val_accuracy: 0.8698 - val_f1_score: 0.8710\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2427 - accuracy: 0.9048 - f1_score: 0.9043 - val_loss: 0.3342 - val_accuracy: 0.8662 - val_f1_score: 0.8635\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2372 - accuracy: 0.9069 - f1_score: 0.9063 - val_loss: 0.3307 - val_accuracy: 0.8671 - val_f1_score: 0.8607\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2287 - accuracy: 0.9117 - f1_score: 0.9110 - val_loss: 0.3413 - val_accuracy: 0.8608 - val_f1_score: 0.8632\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3444 - accuracy: 0.8548 - f1_score: 0.8827\n","Epoch 1/20\n","104/104 [==============================] - 10s 40ms/step - loss: 0.6108 - accuracy: 0.6751 - f1_score: 0.6853 - val_loss: 0.5643 - val_accuracy: 0.7233 - val_f1_score: 0.7579\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4838 - accuracy: 0.7770 - f1_score: 0.7695 - val_loss: 0.5877 - val_accuracy: 0.7297 - val_f1_score: 0.7777\n","Epoch 3/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.4102 - accuracy: 0.8134 - f1_score: 0.8089 - val_loss: 0.3874 - val_accuracy: 0.8273 - val_f1_score: 0.8281\n","Epoch 4/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3464 - accuracy: 0.8571 - f1_score: 0.8556 - val_loss: 0.4021 - val_accuracy: 0.8219 - val_f1_score: 0.8373\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3326 - accuracy: 0.8623 - f1_score: 0.8609 - val_loss: 0.4408 - val_accuracy: 0.7948 - val_f1_score: 0.8194\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3163 - accuracy: 0.8707 - f1_score: 0.8713 - val_loss: 0.3593 - val_accuracy: 0.8499 - val_f1_score: 0.8579\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3035 - accuracy: 0.8737 - f1_score: 0.8728 - val_loss: 0.4261 - val_accuracy: 0.8092 - val_f1_score: 0.8313\n","Epoch 8/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2765 - accuracy: 0.8885 - f1_score: 0.8884 - val_loss: 0.3344 - val_accuracy: 0.8752 - val_f1_score: 0.8691\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2743 - accuracy: 0.8912 - f1_score: 0.8905 - val_loss: 0.4326 - val_accuracy: 0.8020 - val_f1_score: 0.8272\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2630 - accuracy: 0.8885 - f1_score: 0.8881 - val_loss: 0.4575 - val_accuracy: 0.7993 - val_f1_score: 0.8255\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2473 - accuracy: 0.9005 - f1_score: 0.8998 - val_loss: 0.3311 - val_accuracy: 0.8698 - val_f1_score: 0.8700\n","Epoch 12/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2687 - accuracy: 0.8867 - f1_score: 0.8863 - val_loss: 0.3982 - val_accuracy: 0.8336 - val_f1_score: 0.8474\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2303 - accuracy: 0.9084 - f1_score: 0.9077 - val_loss: 0.3421 - val_accuracy: 0.8644 - val_f1_score: 0.8626\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2154 - accuracy: 0.9183 - f1_score: 0.9177 - val_loss: 0.3484 - val_accuracy: 0.8698 - val_f1_score: 0.8703\n","Epoch 15/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2168 - accuracy: 0.9114 - f1_score: 0.9109 - val_loss: 0.3583 - val_accuracy: 0.8535 - val_f1_score: 0.8564\n","Epoch 16/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1987 - accuracy: 0.9219 - f1_score: 0.9214 - val_loss: 0.3858 - val_accuracy: 0.8517 - val_f1_score: 0.8564\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3560 - accuracy: 0.8575 - f1_score: 0.8831\n","Epoch 1/20\n","104/104 [==============================] - 10s 25ms/step - loss: 0.5981 - accuracy: 0.6896 - f1_score: 0.6651 - val_loss: 0.5022 - val_accuracy: 0.7577 - val_f1_score: 0.7767\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4324 - accuracy: 0.8029 - f1_score: 0.8017 - val_loss: 0.3817 - val_accuracy: 0.8273 - val_f1_score: 0.8265\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3621 - accuracy: 0.8385 - f1_score: 0.8378 - val_loss: 0.3367 - val_accuracy: 0.8427 - val_f1_score: 0.8497\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3483 - accuracy: 0.8496 - f1_score: 0.8501 - val_loss: 0.3502 - val_accuracy: 0.8635 - val_f1_score: 0.8521\n","Epoch 5/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.3251 - accuracy: 0.8659 - f1_score: 0.8646 - val_loss: 0.3217 - val_accuracy: 0.8571 - val_f1_score: 0.8550\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2979 - accuracy: 0.8797 - f1_score: 0.8787 - val_loss: 0.3249 - val_accuracy: 0.8644 - val_f1_score: 0.8636\n","Epoch 7/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3086 - accuracy: 0.8701 - f1_score: 0.8690 - val_loss: 0.3307 - val_accuracy: 0.8617 - val_f1_score: 0.8504\n","Epoch 8/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2922 - accuracy: 0.8822 - f1_score: 0.8810 - val_loss: 0.3391 - val_accuracy: 0.8671 - val_f1_score: 0.8577\n","Epoch 9/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2780 - accuracy: 0.8882 - f1_score: 0.8864 - val_loss: 0.3171 - val_accuracy: 0.8662 - val_f1_score: 0.8645\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2573 - accuracy: 0.8966 - f1_score: 0.8958 - val_loss: 0.3184 - val_accuracy: 0.8680 - val_f1_score: 0.8638\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2679 - accuracy: 0.8870 - f1_score: 0.8855 - val_loss: 0.3365 - val_accuracy: 0.8635 - val_f1_score: 0.8524\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2416 - accuracy: 0.9027 - f1_score: 0.9011 - val_loss: 0.3343 - val_accuracy: 0.8671 - val_f1_score: 0.8677\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2322 - accuracy: 0.9126 - f1_score: 0.9120 - val_loss: 0.3769 - val_accuracy: 0.8517 - val_f1_score: 0.8389\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2168 - accuracy: 0.9123 - f1_score: 0.9114 - val_loss: 0.3390 - val_accuracy: 0.8662 - val_f1_score: 0.8664\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8515 - f1_score: 0.8746\n","Epoch 1/20\n","104/104 [==============================] - 11s 27ms/step - loss: 0.6139 - accuracy: 0.6631 - f1_score: 0.6804 - val_loss: 0.4920 - val_accuracy: 0.7703 - val_f1_score: 0.7490\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4829 - accuracy: 0.7734 - f1_score: 0.7626 - val_loss: 0.4479 - val_accuracy: 0.7839 - val_f1_score: 0.7449\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4047 - accuracy: 0.8264 - f1_score: 0.8213 - val_loss: 0.3505 - val_accuracy: 0.8400 - val_f1_score: 0.8424\n","Epoch 4/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.3649 - accuracy: 0.8460 - f1_score: 0.8439 - val_loss: 0.3413 - val_accuracy: 0.8436 - val_f1_score: 0.8497\n","Epoch 5/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3378 - accuracy: 0.8586 - f1_score: 0.8572 - val_loss: 0.3450 - val_accuracy: 0.8580 - val_f1_score: 0.8441\n","Epoch 6/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3281 - accuracy: 0.8653 - f1_score: 0.8633 - val_loss: 0.3235 - val_accuracy: 0.8608 - val_f1_score: 0.8528\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2991 - accuracy: 0.8807 - f1_score: 0.8793 - val_loss: 0.3391 - val_accuracy: 0.8580 - val_f1_score: 0.8453\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2867 - accuracy: 0.8843 - f1_score: 0.8824 - val_loss: 0.3598 - val_accuracy: 0.8427 - val_f1_score: 0.8249\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2791 - accuracy: 0.8876 - f1_score: 0.8862 - val_loss: 0.3147 - val_accuracy: 0.8725 - val_f1_score: 0.8688\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2644 - accuracy: 0.8990 - f1_score: 0.8985 - val_loss: 0.3465 - val_accuracy: 0.8508 - val_f1_score: 0.8348\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2658 - accuracy: 0.8948 - f1_score: 0.8945 - val_loss: 0.3089 - val_accuracy: 0.8734 - val_f1_score: 0.8770\n","Epoch 12/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2507 - accuracy: 0.9060 - f1_score: 0.9048 - val_loss: 0.3118 - val_accuracy: 0.8779 - val_f1_score: 0.8783\n","Epoch 13/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2492 - accuracy: 0.9020 - f1_score: 0.9005 - val_loss: 0.4137 - val_accuracy: 0.8201 - val_f1_score: 0.7916\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2384 - accuracy: 0.9099 - f1_score: 0.9090 - val_loss: 0.3746 - val_accuracy: 0.8382 - val_f1_score: 0.8160\n","Epoch 15/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2102 - accuracy: 0.9225 - f1_score: 0.9220 - val_loss: 0.3245 - val_accuracy: 0.8743 - val_f1_score: 0.8731\n","Epoch 16/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.1944 - accuracy: 0.9304 - f1_score: 0.9300 - val_loss: 0.3656 - val_accuracy: 0.8562 - val_f1_score: 0.8470\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3263 - accuracy: 0.8623 - f1_score: 0.8919\n","Epoch 1/20\n","104/104 [==============================] - 9s 33ms/step - loss: 0.6019 - accuracy: 0.6917 - f1_score: 0.6812 - val_loss: 0.4987 - val_accuracy: 0.7550 - val_f1_score: 0.7325\n","Epoch 2/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.4603 - accuracy: 0.7845 - f1_score: 0.7783 - val_loss: 0.4214 - val_accuracy: 0.8056 - val_f1_score: 0.8065\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3774 - accuracy: 0.8318 - f1_score: 0.8302 - val_loss: 0.3843 - val_accuracy: 0.8282 - val_f1_score: 0.8197\n","Epoch 4/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3360 - accuracy: 0.8565 - f1_score: 0.8550 - val_loss: 0.3768 - val_accuracy: 0.8391 - val_f1_score: 0.8408\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3272 - accuracy: 0.8611 - f1_score: 0.8599 - val_loss: 0.3688 - val_accuracy: 0.8472 - val_f1_score: 0.8500\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2951 - accuracy: 0.8797 - f1_score: 0.8790 - val_loss: 0.3819 - val_accuracy: 0.8454 - val_f1_score: 0.8315\n","Epoch 7/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2868 - accuracy: 0.8861 - f1_score: 0.8852 - val_loss: 0.3586 - val_accuracy: 0.8490 - val_f1_score: 0.8461\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2868 - accuracy: 0.8828 - f1_score: 0.8817 - val_loss: 0.3624 - val_accuracy: 0.8436 - val_f1_score: 0.8454\n","Epoch 9/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2659 - accuracy: 0.8903 - f1_score: 0.8896 - val_loss: 0.3701 - val_accuracy: 0.8499 - val_f1_score: 0.8496\n","Epoch 10/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2559 - accuracy: 0.9008 - f1_score: 0.9002 - val_loss: 0.3854 - val_accuracy: 0.8490 - val_f1_score: 0.8426\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2554 - accuracy: 0.8990 - f1_score: 0.8980 - val_loss: 0.4127 - val_accuracy: 0.8336 - val_f1_score: 0.8467\n","Epoch 12/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2577 - accuracy: 0.8960 - f1_score: 0.8952 - val_loss: 0.3568 - val_accuracy: 0.8635 - val_f1_score: 0.8665\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2506 - accuracy: 0.8963 - f1_score: 0.8950 - val_loss: 0.3761 - val_accuracy: 0.8553 - val_f1_score: 0.8532\n","Epoch 14/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2354 - accuracy: 0.9114 - f1_score: 0.9112 - val_loss: 0.3972 - val_accuracy: 0.8599 - val_f1_score: 0.8597\n","Epoch 15/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2115 - accuracy: 0.9216 - f1_score: 0.9211 - val_loss: 0.3880 - val_accuracy: 0.8490 - val_f1_score: 0.8547\n","Epoch 16/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2175 - accuracy: 0.9153 - f1_score: 0.9147 - val_loss: 0.4924 - val_accuracy: 0.8291 - val_f1_score: 0.8021\n","Epoch 17/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1986 - accuracy: 0.9247 - f1_score: 0.9238 - val_loss: 0.4288 - val_accuracy: 0.8499 - val_f1_score: 0.8586\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8629 - f1_score: 0.8918\n","Epoch 1/20\n","104/104 [==============================] - 11s 28ms/step - loss: 0.6282 - accuracy: 0.6447 - f1_score: 0.6291 - val_loss: 0.4858 - val_accuracy: 0.7839 - val_f1_score: 0.7726\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4998 - accuracy: 0.7634 - f1_score: 0.7543 - val_loss: 0.4197 - val_accuracy: 0.8128 - val_f1_score: 0.8279\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4208 - accuracy: 0.8098 - f1_score: 0.8042 - val_loss: 0.3828 - val_accuracy: 0.8373 - val_f1_score: 0.8529\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3708 - accuracy: 0.8336 - f1_score: 0.8318 - val_loss: 0.3309 - val_accuracy: 0.8599 - val_f1_score: 0.8699\n","Epoch 5/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3533 - accuracy: 0.8502 - f1_score: 0.8475 - val_loss: 0.2992 - val_accuracy: 0.8734 - val_f1_score: 0.8651\n","Epoch 6/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3338 - accuracy: 0.8605 - f1_score: 0.8583 - val_loss: 0.3261 - val_accuracy: 0.8635 - val_f1_score: 0.8745\n","Epoch 7/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3121 - accuracy: 0.8707 - f1_score: 0.8701 - val_loss: 0.2728 - val_accuracy: 0.8942 - val_f1_score: 0.8964\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3012 - accuracy: 0.8761 - f1_score: 0.8753 - val_loss: 0.3274 - val_accuracy: 0.8571 - val_f1_score: 0.8703\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2838 - accuracy: 0.8831 - f1_score: 0.8821 - val_loss: 0.2771 - val_accuracy: 0.8942 - val_f1_score: 0.8985\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2758 - accuracy: 0.8879 - f1_score: 0.8871 - val_loss: 0.2772 - val_accuracy: 0.8978 - val_f1_score: 0.9027\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2651 - accuracy: 0.8936 - f1_score: 0.8932 - val_loss: 0.2704 - val_accuracy: 0.8978 - val_f1_score: 0.9011\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2517 - accuracy: 0.8987 - f1_score: 0.8983 - val_loss: 0.2645 - val_accuracy: 0.9051 - val_f1_score: 0.9018\n","Epoch 13/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2419 - accuracy: 0.9066 - f1_score: 0.9058 - val_loss: 0.3116 - val_accuracy: 0.8861 - val_f1_score: 0.8927\n","Epoch 14/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2310 - accuracy: 0.9075 - f1_score: 0.9073 - val_loss: 0.2738 - val_accuracy: 0.8996 - val_f1_score: 0.9027\n","Epoch 15/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2091 - accuracy: 0.9198 - f1_score: 0.9195 - val_loss: 0.4429 - val_accuracy: 0.8336 - val_f1_score: 0.8526\n","Epoch 16/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2066 - accuracy: 0.9219 - f1_score: 0.9217 - val_loss: 0.3062 - val_accuracy: 0.8888 - val_f1_score: 0.8812\n","Epoch 17/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.1974 - accuracy: 0.9265 - f1_score: 0.9266 - val_loss: 0.3073 - val_accuracy: 0.8942 - val_f1_score: 0.8971\n","47/47 [==============================] - 0s 3ms/step - loss: 0.3715 - accuracy: 0.8454 - f1_score: 0.8676\n","Epoch 1/20\n","104/104 [==============================] - 10s 37ms/step - loss: 0.6186 - accuracy: 0.6591 - f1_score: 0.6431 - val_loss: 0.5490 - val_accuracy: 0.7089 - val_f1_score: 0.7374\n","Epoch 2/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.4685 - accuracy: 0.7839 - f1_score: 0.7782 - val_loss: 0.4201 - val_accuracy: 0.8056 - val_f1_score: 0.7882\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4002 - accuracy: 0.8261 - f1_score: 0.8221 - val_loss: 0.3988 - val_accuracy: 0.8137 - val_f1_score: 0.7932\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3541 - accuracy: 0.8475 - f1_score: 0.8452 - val_loss: 0.4213 - val_accuracy: 0.8119 - val_f1_score: 0.7806\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3283 - accuracy: 0.8680 - f1_score: 0.8664 - val_loss: 0.3737 - val_accuracy: 0.8445 - val_f1_score: 0.8527\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3070 - accuracy: 0.8752 - f1_score: 0.8745 - val_loss: 0.3609 - val_accuracy: 0.8382 - val_f1_score: 0.8233\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3083 - accuracy: 0.8740 - f1_score: 0.8719 - val_loss: 0.3466 - val_accuracy: 0.8571 - val_f1_score: 0.8616\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2865 - accuracy: 0.8840 - f1_score: 0.8825 - val_loss: 0.3433 - val_accuracy: 0.8644 - val_f1_score: 0.8656\n","Epoch 9/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2721 - accuracy: 0.8882 - f1_score: 0.8868 - val_loss: 0.3445 - val_accuracy: 0.8635 - val_f1_score: 0.8618\n","Epoch 10/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2582 - accuracy: 0.8999 - f1_score: 0.8990 - val_loss: 0.3311 - val_accuracy: 0.8707 - val_f1_score: 0.8670\n","Epoch 11/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.2480 - accuracy: 0.9057 - f1_score: 0.9048 - val_loss: 0.3605 - val_accuracy: 0.8644 - val_f1_score: 0.8603\n","Epoch 12/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2541 - accuracy: 0.9017 - f1_score: 0.9010 - val_loss: 0.3569 - val_accuracy: 0.8662 - val_f1_score: 0.8582\n","Epoch 13/20\n","104/104 [==============================] - 1s 12ms/step - loss: 0.2308 - accuracy: 0.9144 - f1_score: 0.9139 - val_loss: 0.3542 - val_accuracy: 0.8644 - val_f1_score: 0.8606\n","Epoch 14/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2220 - accuracy: 0.9177 - f1_score: 0.9168 - val_loss: 0.3817 - val_accuracy: 0.8481 - val_f1_score: 0.8569\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2155 - accuracy: 0.9195 - f1_score: 0.9189 - val_loss: 0.4115 - val_accuracy: 0.8580 - val_f1_score: 0.8643\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3502 - accuracy: 0.8589 - f1_score: 0.8805\n","Epoch 1/20\n","104/104 [==============================] - 11s 30ms/step - loss: 0.6339 - accuracy: 0.6510 - f1_score: 0.6551 - val_loss: 0.5413 - val_accuracy: 0.7297 - val_f1_score: 0.6933\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4767 - accuracy: 0.7694 - f1_score: 0.7616 - val_loss: 0.4127 - val_accuracy: 0.8210 - val_f1_score: 0.8290\n","Epoch 3/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3649 - accuracy: 0.8433 - f1_score: 0.8409 - val_loss: 0.3830 - val_accuracy: 0.8454 - val_f1_score: 0.8530\n","Epoch 4/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3263 - accuracy: 0.8635 - f1_score: 0.8629 - val_loss: 0.3721 - val_accuracy: 0.8463 - val_f1_score: 0.8498\n","Epoch 5/20\n","104/104 [==============================] - 2s 23ms/step - loss: 0.3153 - accuracy: 0.8773 - f1_score: 0.8758 - val_loss: 0.3768 - val_accuracy: 0.8454 - val_f1_score: 0.8507\n","Epoch 6/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2980 - accuracy: 0.8858 - f1_score: 0.8857 - val_loss: 0.4489 - val_accuracy: 0.8282 - val_f1_score: 0.8453\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2798 - accuracy: 0.8897 - f1_score: 0.8890 - val_loss: 0.3556 - val_accuracy: 0.8553 - val_f1_score: 0.8505\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2731 - accuracy: 0.8960 - f1_score: 0.8945 - val_loss: 0.4304 - val_accuracy: 0.8273 - val_f1_score: 0.8418\n","Epoch 9/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2633 - accuracy: 0.8969 - f1_score: 0.8962 - val_loss: 0.3807 - val_accuracy: 0.8517 - val_f1_score: 0.8501\n","Epoch 10/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2522 - accuracy: 0.9020 - f1_score: 0.9015 - val_loss: 0.3656 - val_accuracy: 0.8517 - val_f1_score: 0.8523\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2459 - accuracy: 0.9051 - f1_score: 0.9041 - val_loss: 0.3930 - val_accuracy: 0.8427 - val_f1_score: 0.8460\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2325 - accuracy: 0.9120 - f1_score: 0.9112 - val_loss: 0.3747 - val_accuracy: 0.8463 - val_f1_score: 0.8468\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3734 - accuracy: 0.8569 - f1_score: 0.8767\n","Epoch 1/20\n","104/104 [==============================] - 11s 37ms/step - loss: 0.6173 - accuracy: 0.6721 - f1_score: 0.6591 - val_loss: 0.5086 - val_accuracy: 0.7676 - val_f1_score: 0.7644\n","Epoch 2/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4896 - accuracy: 0.7655 - f1_score: 0.7560 - val_loss: 0.4319 - val_accuracy: 0.8056 - val_f1_score: 0.8015\n","Epoch 3/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.4157 - accuracy: 0.8153 - f1_score: 0.8105 - val_loss: 0.3777 - val_accuracy: 0.8382 - val_f1_score: 0.8344\n","Epoch 4/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.3751 - accuracy: 0.8421 - f1_score: 0.8384 - val_loss: 0.3552 - val_accuracy: 0.8644 - val_f1_score: 0.8675\n","Epoch 5/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.3443 - accuracy: 0.8535 - f1_score: 0.8526 - val_loss: 0.3984 - val_accuracy: 0.8201 - val_f1_score: 0.7951\n","Epoch 6/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3317 - accuracy: 0.8574 - f1_score: 0.8557 - val_loss: 0.3324 - val_accuracy: 0.8752 - val_f1_score: 0.8783\n","Epoch 7/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3022 - accuracy: 0.8785 - f1_score: 0.8775 - val_loss: 0.3408 - val_accuracy: 0.8689 - val_f1_score: 0.8697\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3009 - accuracy: 0.8840 - f1_score: 0.8827 - val_loss: 0.4392 - val_accuracy: 0.7948 - val_f1_score: 0.8225\n","Epoch 9/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2823 - accuracy: 0.8894 - f1_score: 0.8888 - val_loss: 0.3604 - val_accuracy: 0.8526 - val_f1_score: 0.8624\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2755 - accuracy: 0.8927 - f1_score: 0.8920 - val_loss: 0.3329 - val_accuracy: 0.8644 - val_f1_score: 0.8689\n","Epoch 11/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2568 - accuracy: 0.9008 - f1_score: 0.8995 - val_loss: 0.3276 - val_accuracy: 0.8680 - val_f1_score: 0.8687\n","Epoch 12/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2467 - accuracy: 0.9054 - f1_score: 0.9048 - val_loss: 0.3531 - val_accuracy: 0.8653 - val_f1_score: 0.8546\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2444 - accuracy: 0.9017 - f1_score: 0.9009 - val_loss: 0.3287 - val_accuracy: 0.8725 - val_f1_score: 0.8724\n","Epoch 14/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.2301 - accuracy: 0.9174 - f1_score: 0.9170 - val_loss: 0.3259 - val_accuracy: 0.8816 - val_f1_score: 0.8793\n","Epoch 15/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2082 - accuracy: 0.9222 - f1_score: 0.9215 - val_loss: 0.3462 - val_accuracy: 0.8662 - val_f1_score: 0.8645\n","Epoch 16/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1978 - accuracy: 0.9256 - f1_score: 0.9247 - val_loss: 0.3768 - val_accuracy: 0.8662 - val_f1_score: 0.8650\n","Epoch 17/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1938 - accuracy: 0.9271 - f1_score: 0.9263 - val_loss: 0.3378 - val_accuracy: 0.8698 - val_f1_score: 0.8676\n","Epoch 18/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2022 - accuracy: 0.9207 - f1_score: 0.9199 - val_loss: 0.3849 - val_accuracy: 0.8590 - val_f1_score: 0.8662\n","Epoch 19/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.1743 - accuracy: 0.9349 - f1_score: 0.9346 - val_loss: 0.3738 - val_accuracy: 0.8662 - val_f1_score: 0.8674\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.8542 - f1_score: 0.8760\n","Epoch 1/20\n","104/104 [==============================] - 10s 28ms/step - loss: 0.6186 - accuracy: 0.6733 - f1_score: 0.6725 - val_loss: 0.5346 - val_accuracy: 0.7396 - val_f1_score: 0.7120\n","Epoch 2/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.4803 - accuracy: 0.7758 - f1_score: 0.7697 - val_loss: 0.4347 - val_accuracy: 0.8020 - val_f1_score: 0.7904\n","Epoch 3/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.4210 - accuracy: 0.8089 - f1_score: 0.8042 - val_loss: 0.3941 - val_accuracy: 0.8336 - val_f1_score: 0.8366\n","Epoch 4/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.3603 - accuracy: 0.8499 - f1_score: 0.8485 - val_loss: 0.3676 - val_accuracy: 0.8490 - val_f1_score: 0.8432\n","Epoch 5/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3354 - accuracy: 0.8605 - f1_score: 0.8585 - val_loss: 0.3693 - val_accuracy: 0.8427 - val_f1_score: 0.8287\n","Epoch 6/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.3170 - accuracy: 0.8704 - f1_score: 0.8687 - val_loss: 0.3461 - val_accuracy: 0.8590 - val_f1_score: 0.8539\n","Epoch 7/20\n","104/104 [==============================] - 2s 18ms/step - loss: 0.3136 - accuracy: 0.8698 - f1_score: 0.8692 - val_loss: 0.3655 - val_accuracy: 0.8508 - val_f1_score: 0.8589\n","Epoch 8/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2942 - accuracy: 0.8861 - f1_score: 0.8850 - val_loss: 0.3631 - val_accuracy: 0.8499 - val_f1_score: 0.8603\n","Epoch 9/20\n","104/104 [==============================] - 2s 14ms/step - loss: 0.2837 - accuracy: 0.8906 - f1_score: 0.8906 - val_loss: 0.3289 - val_accuracy: 0.8725 - val_f1_score: 0.8698\n","Epoch 10/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2589 - accuracy: 0.8990 - f1_score: 0.8986 - val_loss: 0.3601 - val_accuracy: 0.8590 - val_f1_score: 0.8485\n","Epoch 11/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2563 - accuracy: 0.8966 - f1_score: 0.8965 - val_loss: 0.3493 - val_accuracy: 0.8689 - val_f1_score: 0.8615\n","Epoch 12/20\n","104/104 [==============================] - 1s 14ms/step - loss: 0.2454 - accuracy: 0.8984 - f1_score: 0.8979 - val_loss: 0.3336 - val_accuracy: 0.8725 - val_f1_score: 0.8710\n","Epoch 13/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2356 - accuracy: 0.9042 - f1_score: 0.9035 - val_loss: 0.3425 - val_accuracy: 0.8707 - val_f1_score: 0.8687\n","Epoch 14/20\n","104/104 [==============================] - 2s 19ms/step - loss: 0.2268 - accuracy: 0.9150 - f1_score: 0.9142 - val_loss: 0.3366 - val_accuracy: 0.8689 - val_f1_score: 0.8693\n","47/47 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8609 - f1_score: 0.8854\n","Epoch 1/20\n","104/104 [==============================] - 11s 29ms/step - loss: 0.6113 - accuracy: 0.6763 - f1_score: 0.6618 - val_loss: 0.4922 - val_accuracy: 0.7640 - val_f1_score: 0.7855\n","Epoch 2/20\n","104/104 [==============================] - 2s 22ms/step - loss: 0.4575 - accuracy: 0.7914 - f1_score: 0.7869 - val_loss: 0.3671 - val_accuracy: 0.8454 - val_f1_score: 0.8450\n","Epoch 3/20\n","104/104 [==============================] - 2s 20ms/step - loss: 0.3863 - accuracy: 0.8312 - f1_score: 0.8274 - val_loss: 0.3790 - val_accuracy: 0.8318 - val_f1_score: 0.8455\n","Epoch 4/20\n","104/104 [==============================] - 2s 17ms/step - loss: 0.3531 - accuracy: 0.8478 - f1_score: 0.8459 - val_loss: 0.4144 - val_accuracy: 0.8146 - val_f1_score: 0.8361\n","Epoch 5/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3306 - accuracy: 0.8671 - f1_score: 0.8656 - val_loss: 0.3314 - val_accuracy: 0.8580 - val_f1_score: 0.8641\n","Epoch 6/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.3092 - accuracy: 0.8695 - f1_score: 0.8683 - val_loss: 0.3169 - val_accuracy: 0.8680 - val_f1_score: 0.8638\n","Epoch 7/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2906 - accuracy: 0.8810 - f1_score: 0.8795 - val_loss: 0.3190 - val_accuracy: 0.8689 - val_f1_score: 0.8725\n","Epoch 8/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2821 - accuracy: 0.8909 - f1_score: 0.8896 - val_loss: 0.3202 - val_accuracy: 0.8698 - val_f1_score: 0.8750\n","Epoch 9/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2721 - accuracy: 0.8942 - f1_score: 0.8930 - val_loss: 0.3122 - val_accuracy: 0.8734 - val_f1_score: 0.8687\n","Epoch 10/20\n","104/104 [==============================] - 1s 13ms/step - loss: 0.2563 - accuracy: 0.9057 - f1_score: 0.9045 - val_loss: 0.3582 - val_accuracy: 0.8590 - val_f1_score: 0.8682\n","Epoch 11/20\n","104/104 [==============================] - 2s 16ms/step - loss: 0.2481 - accuracy: 0.8960 - f1_score: 0.8959 - val_loss: 0.3130 - val_accuracy: 0.8725 - val_f1_score: 0.8742\n","Epoch 12/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2410 - accuracy: 0.9096 - f1_score: 0.9091 - val_loss: 0.3600 - val_accuracy: 0.8590 - val_f1_score: 0.8452\n","Epoch 13/20\n","104/104 [==============================] - 2s 21ms/step - loss: 0.2437 - accuracy: 0.9069 - f1_score: 0.9063 - val_loss: 0.3285 - val_accuracy: 0.8635 - val_f1_score: 0.8552\n","Epoch 14/20\n","104/104 [==============================] - 2s 15ms/step - loss: 0.2109 - accuracy: 0.9174 - f1_score: 0.9168 - val_loss: 0.3678 - val_accuracy: 0.8626 - val_f1_score: 0.8541\n","47/47 [==============================] - 0s 4ms/step - loss: 0.3341 - accuracy: 0.8663 - f1_score: 0.8874\n"]}]},{"cell_type":"code","source":["print(accuracies)\n","print(np.mean(accuracies))\n","print(np.std(accuracies))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0rhKwpyo_CM","executionInfo":{"status":"ok","timestamp":1690644238604,"user_tz":-330,"elapsed":33,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"8d6bc003-cf87-49d1-d04b-66fd3a8c90ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8501012921333313, 0.8440243005752563, 0.847400426864624, 0.8426738977432251, 0.8561782836914062, 0.8561782836914062, 0.852126955986023, 0.8379473090171814, 0.8487508296966553, 0.8568534851074219, 0.8676570057868958, 0.8615800142288208, 0.8568534851074219, 0.8568534851074219, 0.8548278212547302, 0.8575286865234375, 0.8514516949653625, 0.8622552156448364, 0.8629304766654968, 0.8453747630119324, 0.8588791489601135, 0.8568534851074219, 0.8541526198387146, 0.8609048128128052, 0.8663065433502197]\n","0.8546657729148864\n","0.007252953015957885\n"]}]},{"cell_type":"code","source":["print(losses)\n","print(np.mean(losses))\n","print(np.std(losses))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ym1WC25fo_Fc","executionInfo":{"status":"ok","timestamp":1690644238604,"user_tz":-330,"elapsed":13,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"fc01befc-12df-499b-c262-9e8e493188bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3361821472644806, 0.3541821539402008, 0.37627702951431274, 0.37037402391433716, 0.33409440517425537, 0.3408753275871277, 0.34336134791374207, 0.38356485962867737, 0.3763497769832611, 0.34474262595176697, 0.3203795552253723, 0.3224034011363983, 0.33484897017478943, 0.3250531852245331, 0.34442684054374695, 0.355959951877594, 0.35824504494667053, 0.3263366222381592, 0.3378203809261322, 0.37151283025741577, 0.35024794936180115, 0.37343335151672363, 0.3673962354660034, 0.33344003558158875, 0.33409351110458374]\n","0.348624062538147\n","0.018623759242316268\n"]}]},{"cell_type":"code","source":["print(f1scores)\n","print(np.mean(f1scores))\n","print(np.std(f1scores))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhqL1KvFo_Ir","executionInfo":{"status":"ok","timestamp":1690644238604,"user_tz":-330,"elapsed":10,"user":{"displayName":"Suryam Gupta","userId":"07887191304479658761"}},"outputId":"a1ab7c5c-7260-4c5f-d150-5ba975535008"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.8742921352386475, 0.8677732348442078, 0.8684515953063965, 0.863182544708252, 0.8794566988945007, 0.8793200850486755, 0.877721905708313, 0.859976589679718, 0.873873770236969, 0.8830021619796753, 0.8945101499557495, 0.8888888359069824, 0.8822221755981445, 0.881564199924469, 0.8827058672904968, 0.8831024765968323, 0.8745723366737366, 0.8919491171836853, 0.8918485641479492, 0.8675534129142761, 0.8805030584335327, 0.8767441511154175, 0.8760045766830444, 0.8854281902313232, 0.8873718976974487]\n","0.8788807892799377\n","0.008746290074630106\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"unB39uxeo_Lw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Just CNN layer to emotions, maxpooling with pool size = 3 strides = 1**\n","\n","LSTM CNN nothing nothing\n","1. Accuracy - 0.859/0.008\n","2. Loss - 0.337/0.018\n","3. F1 score - 0.883/0.009\n","\n","LSTM CNN LSTM nothing\n","1. Accuracy - 0.859/0.009\n","2. Loss - 0.335/0.015\n","3. F1 score - 0.882/0.009\n","\n","\n","**Just CNN layer to emotions, maxpooling with pool size = 5 strides = 2**\n","\n","LSTM CNN nothing nothing\n","1. Accuracy - 0.857/0.007\n","2. Loss - 0.335/0.011\n","3. F1 score - 0.881/0.007\n","\n","LSTM CNN LSTM nothing\n","1. Accuracy - 0.859/0.010\n","2. Loss - 0.336/0.019\n","3. F1 score - 0.883/0.010\n","\n","\n","**Adding LSTM layer to CNN layer to emotions, maxpooling with pool size = 3 strides = 1**\n","\n","LSTM CNN nothing nothing\n","1. Accuracy - 0.854/0.008\n","2. Loss - 0.344/0.017\n","3. F1 score - 0.879/0.009\n","\n","LSTM CNN LSTM nothing\n","1. Accuracy - 0.857/0.008\n","2. Loss - 0.343/0.026\n","3. F1 score - 0.882/0.009\n","\n","\n","**Adding LSTM layer to CNN layer to emotions, maxpooling with pool size = 5 strides = 2**\n","\n","LSTM CNN nothing nothing\n","1. Accuracy - 0.857/0.008\n","2. Loss - 0.340/0.014\n","3. F1 score - 0.881/0.008\n","\n","LSTM CNN LSTM nothing\n","1. Accuracy - 0.854/0.007\n","2. Loss - 0.348/0.018\n","3. F1 score - 0.878/0.008"],"metadata":{"id":"m6G52xk8wXDI"}},{"cell_type":"code","source":[],"metadata":{"id":"A2vYs-XcwkKg"},"execution_count":null,"outputs":[]}]}